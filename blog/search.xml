<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>[架構設計] 可擴展架構設計</title>
      <link href="/blog/Architecture_Design/Architecture-Design-Scalable-Arch/"/>
      <url>/blog/Architecture_Design/Architecture-Design-Scalable-Arch/</url>
      
        <content type="html"><![CDATA[<h1 id="可擴展架構的基本思想和模式"><a href="#可擴展架構的基本思想和模式" class="headerlink" title="可擴展架構的基本思想和模式"></a>可擴展架構的基本思想和模式</h1><ul><li><p>軟體系統與硬體和建築系統最大的差異在於軟體是可擴展的；一個硬體生產出來後就不會再進行改變、一個建築完工後也不會再改變其整體結構</p></li><li><p>軟體系統的魅力 &amp; 困難之處：</p><ul><li>魅力：可以通過修改和擴展，不斷地讓軟體系統具備更多的功能和特性，滿足新的需求或者順應技術發展的趨勢</li><li>困難：<strong>如何以最小的代價去擴展系統</strong></li></ul></li><li><p>如何避免擴展時改動範圍太大，是軟體架構可擴展性設計的主要思考點</p></li></ul><h2 id="可擴展的基本思想"><a href="#可擴展的基本思想" class="headerlink" title="可擴展的基本思想"></a>可擴展的基本思想</h2><ul><li><p>所有的可擴展性架構設計，背後的基本思想都可以總結為一個字：<strong>拆！</strong></p></li><li><p>拆，就是將原本大一統的系統拆分成多個規模小的部分，擴展時只修改其中一部分即可，無須整個系統到處都改，通過這種方式來減少改動範圍，降低改動風險</p></li><li><p><strong>軟體系統中的”拆”是建設性的</strong>，因此難度要高得多</p></li><li><p>常見的拆分思路有三種：<code>面向流程拆分</code>、<code>面向服務拆分</code>、<code>面向功能拆分</code></p></li><li><p>從範圍來看，從大到小為：<strong>流程 &gt; 服務 &gt; 功能</strong></p></li></ul><h2 id="可擴展方式"><a href="#可擴展方式" class="headerlink" title="可擴展方式"></a>可擴展方式</h2><ul><li><p>合理的拆分，能夠強制保證即使 developer 出錯，出錯的範圍也不會太廣，影響也不會太大</p></li><li><p>不同拆分方式應對擴展時的優勢：</p><ul><li>面向流程拆分：擴展時大部分情況只需要修改某一層，少部分情況可能修改關聯的兩層，不會出現所有層都同時要修改</li><li>面向服務拆分：對某個服務擴展，或者要增加新的服務時，只需要擴展相關服務即可，無須修改所有的服務</li><li>面向功能拆分：對某個功能擴展，或者要增加新的功能時，只需要擴展相關功能即可，無須修改所有的服務</li></ul></li><li><p>典型的可擴展系統架構有：</p><ul><li>面向流程拆分：<strong>分層架構</strong></li><li>面向服務拆分：<strong>SOA &amp; Microservice</strong></li><li>面向功能拆分：<strong>微內核架構</strong></li></ul></li></ul><h1 id="傳統的可擴展架構模式：分層架構和-SOA"><a href="#傳統的可擴展架構模式：分層架構和-SOA" class="headerlink" title="傳統的可擴展架構模式：分層架構和 SOA"></a>傳統的可擴展架構模式：分層架構和 SOA</h1><h2 id="分層架構"><a href="#分層架構" class="headerlink" title="分層架構"></a>分層架構</h2><ul><li><p>分層(Tier)架構是很常見的架構模式，它也叫 N 層架構，通常情況下，N 至少是 2 層</p></li><li><p>需要保證每個 Tier 之間的差異足夠清晰，邊界足夠明顯，讓人看到架構圖後就能看懂整個架構</p></li><li><p>分層架構之所以能夠較好地支撐系統擴展，本質在於**隔離關注點(separation of concerns)**，每個層中的組件只會處理本層的邏輯</p></li><li><p>分層時要保證每層之間的依賴是穩定的，才能真正支撐快速擴展</p></li><li><p>對於 OS 這類複雜的系統，interface 本身也可以成為獨立的一層</p></li><li><p>特點是層層傳遞，一旦分層確定，<strong>整個業務流程是按照層進行依次傳遞的，不能在層之間進行跳躍</strong></p></li><li><p>分層結構的這種約束，好處在於強制將分層依賴限定為兩兩依賴，降低了整體系統複雜度</p></li></ul><h3 id="C-S-架構、B-S-架構"><a href="#C-S-架構、B-S-架構" class="headerlink" title="C/S 架構、B/S 架構"></a>C/S 架構、B/S 架構</h3><p><img src="/blog/images/architecture-design/scalable-arch-cs.png" alt="Client/Server architecture"></p><ul><li><p>劃分的對象是<strong>整個業務系統</strong>，劃分的維度是<strong>用戶交互</strong></p></li><li><p>將和用戶交互的部分獨立為一層，支撐用戶交互的後台作為另外一層</p></li></ul><h3 id="MVC-架構、MVP-架構"><a href="#MVC-架構、MVP-架構" class="headerlink" title="MVC 架構、MVP 架構"></a>MVC 架構、MVP 架構</h3><p><img src="/blog/images/architecture-design/scalable-arch-mvc.png" alt="MVC/MVP architecture"></p><ul><li><p>劃分的對象是<strong>單個業務子系統</strong>，劃分的維度是<strong>職責</strong></p></li><li><p>將不同的職責劃分到獨立層，但各層的依賴關係比較靈活</p></li></ul><h3 id="邏輯分層架構"><a href="#邏輯分層架構" class="headerlink" title="邏輯分層架構"></a>邏輯分層架構</h3><p>以 Android OS 為例：<br><img src="/blog/images/architecture-design/scalable-arch-logic.png" alt="Business Logic architecture"></p><ul><li><p>劃分的對象可以是單個業務子系統，也可以是整個業務系統，劃分的維度也是<strong>職責</strong></p></li><li><p>邏輯分層架構中的層是自最上層向下依賴的</p></li><li><p>典型的有 OS kernel 架構、TCP/IP 架構</p></li></ul><h2 id="SOA"><a href="#SOA" class="headerlink" title="SOA"></a>SOA</h2><ul><li><p>SOA 解決了傳統 IT 系統重複建設和擴展效率低的問題，但其本身也引入了更多的複雜性</p></li><li><p>ESB(Enterprise Service Bus) 要完成這麼多協議和資料格式的互相轉換，工作量和複雜度都很大，而且這種轉換是需要耗費大量計算效能的</p></li><li><p>當 ESB 承載的消息太多時，ESB 本身會成為整個系統的效能瓶頸</p></li></ul><p>為了應對傳統 IT 系統存在的問題，SOA 提出了 3 個關鍵概念：</p><h3 id="服務"><a href="#服務" class="headerlink" title="服務"></a>服務</h3><ul><li><p>所有業務功能都是一項<strong>服務</strong></p></li><li><p>服務就意味著要對外提供開放的能力，當其他系統需要使用這項功能時，無須定製化開發</p></li><li><p>服務可大可小，可簡單也可複雜</p></li></ul><h3 id="ESB-Enterprise-Service-Bus"><a href="#ESB-Enterprise-Service-Bus" class="headerlink" title="ESB (Enterprise Service Bus)"></a>ESB (Enterprise Service Bus)</h3><ul><li><p>ESB 用途是將企業中各個不同的服務連接在一起</p></li><li><p>SOA 使用 ESB 來隱藏異質系統對外提供各種不同 interface 的方式，以此來達到服務間高效率的互連互通</p></li></ul><h3 id="鬆耦合-Loose-Coupling"><a href="#鬆耦合-Loose-Coupling" class="headerlink" title="鬆耦合(Loose Coupling)"></a>鬆耦合(Loose Coupling)</h3><ul><li><p>目的是減少各個服務間的依賴和互相影響</p></li><li><p>要做到完全後向兼容，是一項複雜的任務</p></li></ul><h1 id="深入理解-Microservice-架構：銀彈-or-焦油坑？"><a href="#深入理解-Microservice-架構：銀彈-or-焦油坑？" class="headerlink" title="深入理解 Microservice 架構：銀彈 or 焦油坑？"></a>深入理解 Microservice 架構：銀彈 or 焦油坑？</h1><h2 id="SOA-v-s-Microservice"><a href="#SOA-v-s-Microservice" class="headerlink" title="SOA v.s. Microservice"></a>SOA v.s. Microservice</h2><ul><li><p>SOA 和 Microservice 本質上是兩種不同的架構設計理念，只是在**”服務”**這個點上有交集而已</p></li><li><p>SOA 和 Microservice 是兩種不同理念的架構模式，只是應用場景不同而已，並沒有使用哪種架構一定會比較好的問題</p></li><li><p><strong>Small</strong>、<strong>Lightweight</strong>、<strong>Automated</strong>，基本上濃縮了 Microservice 的精華，也是 Microservice 與 SOA 的本質區別所在</p></li></ul><p>具體做法比較：</p><h3 id="服務粒度"><a href="#服務粒度" class="headerlink" title="服務粒度"></a>服務粒度</h3><ul><li>SOA 的服務粒度要粗一些，而 Microservice 的服務粒度要細一些</li></ul><h3 id="服務通信"><a href="#服務通信" class="headerlink" title="服務通信"></a>服務通信</h3><ul><li><p>SOA 採用了 ESB 作為服務間通信的關鍵元件，負責服務定義、Service Routing、消息轉換、消息傳遞，總體上是重量級的實現</p></li><li><p>Microservice 推薦使用統一的協議和格式，例如，RESTful、RPC，無須 ESB 這樣的重量級實現</p></li></ul><h3 id="服務交付"><a href="#服務交付" class="headerlink" title="服務交付"></a>服務交付</h3><ul><li><p>SOA 對服務的交付並沒有特殊要求，因為 SOA 更多考慮的是兼容已有的系統</p></li><li><p>Microservice 的架構理念要求”快速交付”，相應地要求採取自動化測試、持續集成、自動化部署等敏捷開發相關的最佳實踐</p></li></ul><h3 id="應用場景"><a href="#應用場景" class="headerlink" title="應用場景"></a>應用場景</h3><ul><li><p>SOA 更加適合於龐大、複雜、異質的企業級系統</p></li><li><p>Microservice 更加適合於快速、輕量級、基於 Web 的網際網路系統，這類的系統業務變化快，需要快速嘗試、快速交付</p></li><li><p>整體系統的複雜度是隨著 Microservice 數量的增加，同時也呈現指數級的增加</p></li></ul><h2 id="Microservice的陷阱"><a href="#Microservice的陷阱" class="headerlink" title="Microservice的陷阱"></a>Microservice的陷阱</h2><ul><li>實施 Microservice 需要避免踩的陷阱，簡單來說：<ul><li>Microservice 拆分過細，過分強調 <strong>“Small”</strong></li><li>Microservice 基礎設施不健全，忽略了 <strong>“Automated”</strong></li><li>Microservice 並不輕量級，規模大了後，**”Lightweight”** 不再適應</li></ul></li></ul><h3 id="服務劃分過細，服務間關係複雜"><a href="#服務劃分過細，服務間關係複雜" class="headerlink" title="服務劃分過細，服務間關係複雜"></a>服務劃分過細，服務間關係複雜</h3><ul><li><p>服務劃分過細，單一服務的複雜度確實下降了，但整個系統的複雜度卻上升</p></li><li><p>Microservice 將系統內的複雜度轉移為系統間的複雜度</p></li></ul><h3 id="服務數量太多，團隊效率急劇下降"><a href="#服務數量太多，團隊效率急劇下降" class="headerlink" title="服務數量太多，團隊效率急劇下降"></a>服務數量太多，團隊效率急劇下降</h3><ul><li>很多團隊看到 “micro” 後，就覺得必須將服務拆分得很細</li></ul><h3 id="調用鏈路太長，效能下降"><a href="#調用鏈路太長，效能下降" class="headerlink" title="調用鏈路太長，效能下降"></a>調用鏈路太長，效能下降</h3><ul><li><p>由於 Microservice 之間都是通過 HTTP 或者 RPC 調用的，每次調用必須經過網路</p></li><li><p>為了支撐業務請求，可能需要大幅增加硬體，這就導致了硬體成本的大幅上升</p></li></ul><h3 id="調用鏈太長，問題定位困難"><a href="#調用鏈太長，問題定位困難" class="headerlink" title="調用鏈太長，問題定位困難"></a>調用鏈太長，問題定位困難</h3><ul><li>由於 Microservice 數量較多，且故障存在擴散現象，要快速定位到底是哪個 Microservice 故障是一件複雜的事情</li></ul><h3 id="沒有自動化支撐，無法快速交付"><a href="#沒有自動化支撐，無法快速交付" class="headerlink" title="沒有自動化支撐，無法快速交付"></a>沒有自動化支撐，無法快速交付</h3><ul><li>如果沒有相應的自動化系統進行支撐，都是靠人工去操作，那麼 Microservice 不但達不到快速交付的目的，甚至還不如一個大而全的系統效率高</li></ul><h3 id="沒有服務治理，Microservice-數量多了後管理混亂"><a href="#沒有服務治理，Microservice-數量多了後管理混亂" class="headerlink" title="沒有服務治理，Microservice 數量多了後管理混亂"></a>沒有服務治理，Microservice 數量多了後管理混亂</h3><ul><li><p>隨著 Microservice 種類和數量越來越多，如果沒有服務治理系統進行支撐，Microservice 提倡的 lightweight 就會變成問題</p></li><li><p>主要問題類型：</p><ul><li>Service Routing</li><li>服務故障隔離</li><li>服務註冊和發現</li></ul></li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><h3 id="1"><a href="#1" class="headerlink" title="1"></a>1</h3><p>Microservice 架構其實相當複雜，可以分成好幾個階段理解：</p><ol><li><p>第一階段，Microservice 架構就是去掉了 ESB 的 SOA 架構，只不過是通信的方式和結構變了</p></li><li><p>第二階段，沒有了 ESB，原本很多由 ESB 完成的事情，轉到服務的提供者和調用者這裡了；此時就需要考慮服務的拆分粒度</p></li><li><p>第三階段，隨著服務的數量大幅增加，服務的管理越來越困難，此時 DevOps 出現了，為此階段的 Microservice 架構帶入大量的自動化，已經是跟 SOA 架構是完全不同的東西</p></li></ol><h3 id="2"><a href="#2" class="headerlink" title="2"></a>2</h3><p>大型企業從傳統架構，轉向 Microservice 架構的過程：</p><ol><li><p>建設好基礎設施，RPC、服務治理、日誌、監控、持續集成、持續部署、維運自動化是基本的，其它包括 Service Orchestration、Distributed Tracking … 等等</p></li><li><p>要逐步演進和迭代，不要過於激進，更不要拆分過細，拆分的粒度，要與團隊的架構相互匹配(這裡可參考<strong>康威定律</strong>)</p></li><li><p>Microservice 與 DB 方面，是個很大的困難點，可以深入瞭解下領域驅動設計，做好領域建模，特別是 DB 要隨著服務一起拆分</p></li></ol><h3 id="3"><a href="#3" class="headerlink" title="3"></a>3</h3><ul><li><p>做服務化改造首先問以下問題：</p><ul><li>業務真的需要 Microservice 來解決嗎？</li><li>真的所有 module 的問題都要 Microservice 來解決嗎？</li><li>技術人員的配置和水平達到要求了嗎？</li></ul></li><li><p>個人比較認同康威定律，Microservice 的拆分粒度一定要和組織結構匹配起來，組織結構和開發管理模式是 Microservice 粒度的最重要參考指標之一</p></li><li><p>採用 Microservice 架構確實解決了 monolithic 應用的一些問題，如資源問題(但消耗資源是比以前多的)、耦合問題、快速迭代問題，服務與服務的職責更加清晰，不同的服務粒度設計的系統是天差地別的</p></li><li><p>istio 就是想隱藏 Microservice 的基礎設施，讓業務系統本身不需要感知基礎設施(服務之間的關係、自動化測試部署、服務可視化差錯、容錯、熔斷)</p></li></ul><h1 id="Microservice架構最佳實踐-方法篇"><a href="#Microservice架構最佳實踐-方法篇" class="headerlink" title="Microservice架構最佳實踐 - 方法篇"></a>Microservice架構最佳實踐 - 方法篇</h1><h2 id="服務粒度-1"><a href="#服務粒度-1" class="headerlink" title="服務粒度"></a>服務粒度</h2><ul><li><p>針對 Microservice 拆分過細導致的問題，建議基於團隊規模進行拆分(例如：<strong>2 pizza team</strong> or <strong>三個火槍手</strong>)</p></li><li><p>從系統規模來講，3 個人負責開發一個系統，系統的複雜度剛好達到每個人都能全面理解整個系統，又能夠進行分工的粒度</p></li><li><p>從團隊管理來說，3 個人可以形成一個穩定的備份</p></li><li><p>從技術提升的角度來講，3 個人的技術小組既能夠形成有效的討論，又能夠快速達成一致意見</p></li><li><p>“三個火槍手”的原則主要應用於 Microservice 設計和開發階段，如果經過一段時間發展後已經比較穩定，處於維護期了，無須太多的開發，那麼平均 1 個人維護 1 個 Microservice 甚至幾個 Microservice 都是可以的</p></li></ul><h2 id="拆分方法"><a href="#拆分方法" class="headerlink" title="拆分方法"></a>拆分方法</h2><p>以下拆分方式不是多選一，而是可以根據實際情況自由排列組合：</p><h3 id="基於業務邏輯拆分"><a href="#基於業務邏輯拆分" class="headerlink" title="基於業務邏輯拆分"></a>基於業務邏輯拆分</h3><ul><li><p>雖然看起來很直觀，但在實踐過程中最常見的一個問題就是<strong>團隊成員對於”職責範圍”的理解差異很大</strong>，經常會出現爭論，難以達成一致意見</p></li><li><p>要判斷拆分粒度，不能從業務邏輯角度，而要計算一下大概的服務數量範圍，然後再確定合適的”職責範圍”</p></li></ul><h3 id="基於可擴展拆分"><a href="#基於可擴展拆分" class="headerlink" title="基於可擴展拆分"></a>基於可擴展拆分</h3><ul><li><p>將系統中的業務模組按照穩定性排序，將已經成熟和改動不大的服務拆分為<strong>穩定服務</strong>，將經常變化和迭代的服務拆分為<strong>變動服務</strong></p></li><li><p>不穩定的服務粒度可以細一些，但也不要太細，始終記住要控制服務的總數量</p></li><li><p>主要是為了提升項目快速迭代的效率，避免在開發的時候，不小心影響了已有的成熟功能導致線上問題</p></li></ul><h3 id="基於可靠性拆分"><a href="#基於可靠性拆分" class="headerlink" title="基於可靠性拆分"></a>基於可靠性拆分</h3><ul><li><p>將系統中的業務模組按照優先級排序，將可靠性要求高的核心服務和可靠性要求低的非核心服務拆分開來</p></li><li><p>重點<strong>保證核心服務的高可用</strong></p></li><li><p>優點：</p><ul><li>避免非核心服務故障影響核心服務</li><li>核心服務高可用方案可以更簡單</li><li>能夠降低高可用成本：只針對核心服務做高可用方案，機器、頻寬等成本比不拆分要節省較多</li></ul></li></ul><h3 id="基於效能拆分"><a href="#基於效能拆分" class="headerlink" title="基於效能拆分"></a>基於效能拆分</h3><ul><li><p>將效能要求高或者效能壓力大的模組拆分出來，避免效能壓力大的服務影響其他服務</p></li><li><p>常見的拆分方式和具體的效能瓶頸有關，可以拆分 Web 服務、資料庫、緩存等</p></li></ul><h2 id="基礎設施"><a href="#基礎設施" class="headerlink" title="基礎設施"></a>基礎設施</h2><ul><li><p>大部分人主要關注的是 Microservice 的 <code>Small</code> 和 <code>Lightweight</code> 特性，但實際上真正決定 Microservice 成敗的，都是那個被大部分人都忽略的 <code>Automated</code></p></li><li><p>Microservice 並沒有減少複雜度，而只是將複雜度從 ESB 轉移到了基礎設施</p></li><li><p>建議按照下面優先級來搭建基礎設施：</p><ol><li>Service Discovery、Service Routing、Service Failt Tolerance：這是最基本的 Microservice 基礎設施。</li><li>Interface Framework、API Gateway：主要是為了提升開發效率，Interface Framework 是提升內部服務的開發效率，API Gateway 是為了提升與外部服務對接的效率。</li><li>自動化部署、自動化測試、Configuration Center：主要是為了提升測試和維運效率</li><li>Service Monitoring/Tracking/Security：主要是為了進一步提昇維運效率</li></ol></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li>現在很火的 Service Mesh 本質上就是借鑑了通信領域的控制面資料面分離思想做出的去中心化的 ESB</li></ul><h1 id="Microservice架構最佳實踐-基礎設施篇"><a href="#Microservice架構最佳實踐-基礎設施篇" class="headerlink" title="Microservice架構最佳實踐 - 基礎設施篇"></a>Microservice架構最佳實踐 - 基礎設施篇</h1><p>為了讓 Microservice 可以順利的運行，完善的 infra 是不能少的，而為了有效管理數量龐大的 Microservice，會需要有以下幾項基礎服務：</p><h2 id="自動化測試"><a href="#自動化測試" class="headerlink" title="自動化測試"></a>自動化測試</h2><ul><li><p>Microservice 提倡快速交付，版本週期短，版本更新頻繁，必須通過自動化測試系統來完成絕大部分測試回歸的工作</p></li><li><p>自動化測試涵蓋的範圍包括程式級別的 unit test、單個系統級別的 integration test、系統間的 interface test(優先)，理想情況是每類測試都自動化</p></li></ul><h2 id="自動化部署"><a href="#自動化部署" class="headerlink" title="自動化部署"></a>自動化部署</h2><ul><li><p>Microservice 部署的次數是大一統系統部署次數的幾十倍，採用人工手工處理，需要投入大量的人力，且容易出錯</p></li><li><p>自動化部署系統包括 version control、資源管理（例如，機器管理、虛擬機管理）、部署操作、回退操作等功能</p></li><li><p>Configuration Center 包括配置 version control、增刪改查配置、節點管理、配置同步、配置推送等功能</p></li></ul><h2 id="Configuration-Center"><a href="#Configuration-Center" class="headerlink" title="Configuration Center"></a>Configuration Center</h2><ul><li><p>Microservice 的節點數量非常多，通過人工登錄每台機器手工修改，效率低，容易出錯</p></li><li><p>有的運行期配置需要動態修改並且所有節點即時生效，人工操作是無法做到的</p></li><li><p>Configuration Center 包括 configuration version control、增刪改查配置、節點管理、配置同步、配置推送等功能</p></li></ul><h2 id="Interface-Framework"><a href="#Interface-Framework" class="headerlink" title="Interface Framework"></a>Interface Framework</h2><ul><li><p>Microservice 提倡輕量級的通信方式，一般採用 HTTP/REST or RPC 方式統一 interface 協議</p></li><li><p>還需要統一 interface 傳遞的資料格式</p></li><li><p>可以由不同的程式語言實現(但基本上還是建議使用統一的 technique stack)</p></li></ul><h2 id="API-Gateway"><a href="#API-Gateway" class="headerlink" title="API Gateway"></a>API Gateway</h2><ul><li><p>在外部系統看來，它不需要也沒辦法理解這麼多 Microservice 的職責分工和邊界，它只會關注它需要的能力，而不會關注這個能力應該由哪個Microservice 提供</p></li><li><p>如果外部系統直接訪問某個 Microservice，則意味著每個 Microservice 都要自己實現安全和權限的功能，這樣做不但工作量大，而且都是重複工作</p></li><li><p>需要一個統一的 API Gateway，<strong>負責外部系統的訪問操作</strong></p></li><li><p>API Gateway 是外部系統訪問的 interface ，所有的外部系統接⼊系統都需要通過 API Gateway，主要包括接入認證(是否允許接入)、權限控制(可以訪問哪些功能)、傳輸加密、請求路由、流量控制 … 等功能</p></li><li><p>API Gateway 不要有業務邏輯</p></li></ul><h2 id="Service-Discovery"><a href="#Service-Discovery" class="headerlink" title="Service Discovery"></a>Service Discovery</h2><ul><li><p>Microservice 節點經常變化</p></li><li><p>希望節點的變化能夠及時同步到所有其他依賴的 Microservice；如果採用手工配置，是不可能做到即時更改生效的</p></li><li><p>實現方式：</p><ul><li>自理式：每個 Microservice 自己完成 Service Discovery</li><li>代理式：Microservice 之間有一個負載均衡系統(圖中的 Load Balancer 節點)，由負載均衡系統來完成 Microservice 之間的 Service Discovery</li></ul></li><li><p>代理式的方式看起來更加清晰，Microservice 本身的實現也簡單了很多</p></li><li><p>使用代理式的缺點：</p><ul><li>可用性風險：Load Balancwe 系統故障，就會影響所有 Microservice 之間的調用</li><li>效能風險：所有的 Microservice 之間的調用流量都要經過 Load Balancer 系統，效能壓力會隨著 Microservice 數量和流量增加而不斷增加，最後成為效能瓶頸</li></ul></li><li><p>自理式示意圖：<br><img src="/blog/images/architecture-design/service-discovery-self.png" alt="Service Discovery - self"></p></li><li><p>代理式示意圖：<br><img src="/blog/images/architecture-design/service-discovery-proxy.png" alt="Service Discovery - proxy"></p></li></ul><h2 id="Service-Routing"><a href="#Service-Routing" class="headerlink" title="Service Routing"></a>Service Routing</h2><ul><li><p>Service Routing 核心的功能就是<code>路由算法</code></p></li><li><p>常見的路由算法有：隨機路由、輪詢路由、最小壓力路由、最小連接數路由等</p></li></ul><h2 id="Service-Failt-Tolerance"><a href="#Service-Failt-Tolerance" class="headerlink" title="Service Failt Tolerance"></a>Service Failt Tolerance</h2><ul><li><p>系統拆分為 Microservice 後，單個 Microservice 故障的機率變小，故障影響範圍也減少，但是 Microservice 的節點數量大大增加</p></li><li><p>從整體上來看，系統中某個 Microservice 出故障的機率會大大增加</p></li><li><p>需要 Microservice 能夠自動應對這種出錯場景，及時進行處理</p></li><li><p>常見的 Service Failt Tolerance 包括請求重試、流控和服務隔離</p></li></ul><h2 id="Service-Monitoring"><a href="#Service-Monitoring" class="headerlink" title="Service Monitoring"></a>Service Monitoring</h2><ul><li><p>系統拆分為 Microservice 後，節點數量大大增加，導致需要監控的機器、網路、進程、interface 調用數等監控對象的數量大大增加</p></li><li><p>一旦發生故障，我們需要快速根據各類信息來定位故障</p></li><li><p>主要用途：</p><ul><li>實時蒐集信息並進行分析，避免故障後再來分析，減少了處理時間</li><li>Service Monitoring 可以在實時分析的基礎上進行預警，在問題萌芽的階段發覺並預警，降低了問題影響的範圍和時間</li></ul></li><li><p>需要蒐集並分析大量的資料，因此建議做成獨立的系統</p></li></ul><h2 id="Service-Tracking"><a href="#Service-Tracking" class="headerlink" title="Service Tracking"></a>Service Tracking</h2><ul><li><p>如果需要跟蹤某一個請求在 Microservice 中的完整路徑，Service Monitoring 是難以實現的</p></li><li><p>Service Monitoring 會記錄請求次數、響應時間平均值、響應時間最高值、錯誤碼分佈這些信息</p></li><li><p>Service Tracking 會記錄其中某次請求的發起時間、響應時間、響應錯誤碼、請求參數、返回的 JSON 對象等信息</p></li></ul><h2 id="Service-Security"><a href="#Service-Security" class="headerlink" title="Service Security"></a>Service Security</h2><ul><li><p>從業務的角度來說，部分敏感資料或者操作，只能部分 Microservice 可以訪問，而不是所有的 Microservice 都可以訪問</p></li><li><p>需要設計Service Security機制來保證業務和資料的安全性</p></li><li><p>Service Security 主要分為三部分：接入安全、資料安全、傳輸安全</p></li><li><p>Service Security 可以集成到 Configuration Center 系統中進行實現：</p><ul><li>Configuration Center 配置 Microservice 的接入安全策略和資料安全策略</li><li>Microservice 節點從 Configuration Center 獲取這些配置信息</li><li>在處理具體的 Microservice 調用請求時根據安全策略進行處理</li></ul></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></p></li><li><p><a href="https://zh.wikipedia.org/wiki/%E5%BA%B7%E5%A8%81%E5%AE%9A%E5%BE%8B">康威定律 - 維基百科，自由的百科全書</a></p></li><li><p><a href="https://kknews.cc/zh-tw/news/6q2l2rm.html">Microservice架構的理論基礎-康威定律 - 每日頭條</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 如何應對 API 級別故障</title>
      <link href="/blog/Architecture_Design/Architecture-Design-Howto-deal-with-API-failure/"/>
      <url>/blog/Architecture_Design/Architecture-Design-Howto-deal-with-API-failure/</url>
      
        <content type="html"><![CDATA[<h1 id="API-故障"><a href="#API-故障" class="headerlink" title="API 故障"></a>API 故障</h1><ul><li><p>multi-site HA 主要應對系統級的故障，例如，機器當機、機房故障、網路故障等問題，這些系統級的故障雖然影響很大，但發生機率較小</p></li><li><p>而 API 級別的故障發生機率就相對高了</p></li><li><p>API 級別故障的典型表現就是系統並沒有當機，網路也沒有中斷，但業務卻出現問題了(例如：業務響應緩慢、大量 request timeout、大量訪問出現異常)</p></li><li><p>這類問題的主要原因在於系統壓力太大、負載太高，導致無法快速處理業務請求，由此引發更多的後續問題(例如：DB Slow Query 將資料庫的服務器資源耗盡，導致讀寫超時)</p></li><li><p>導致 API 故障的原因：</p><ul><li>內部原因：程式 bug 導致死循環，某個 API 導致 DB Slow Query，程式邏輯不完善導致耗盡 memory 等</li><li>外部原因：Hacker 攻擊、促銷或者搶購引入了超出平常幾倍甚至幾十倍的使用者，第三方系統大量請求，第三方系統響應緩慢等</li></ul></li></ul><h1 id="解決方式"><a href="#解決方式" class="headerlink" title="解決方式"></a>解決方式</h1><h2 id="降級"><a href="#降級" class="headerlink" title="降級"></a>降級</h2><ul><li><p>系統將某些業務或者 API 的功能降低，可以是只提供部分功能，也可以是完全停掉所有功能</p></li><li><p>降級的核心思想就是棄車保帥，<strong>優先保證核心業務</strong>(例如：對於論壇來說，90% 的流量是看討論串，那我們就優先保證看討論串的功能)</p></li></ul><p>常見的實現降級方式：</p><h3 id="系統後門降級"><a href="#系統後門降級" class="headerlink" title="系統後門降級"></a>系統後門降級</h3><ul><li><p>系統預留了後門用於降級操作(通常以 API 的形式呈現)</p></li><li><p>實作成本低</p></li><li><p>缺點是如果服務器數量多，需要一台一台去操作，效率比較低，在故障處理爭分奪秒的場景下是比較浪費時間的</p></li></ul><h3 id="獨立降級系統"><a href="#獨立降級系統" class="headerlink" title="獨立降級系統"></a>獨立降級系統</h3><p><img src="/blog/images/architecture-design/downgrade-arch.png" alt="downgrade architecture"></p><ul><li>將降級操作獨立到一個單獨的系統中，可以實現複雜的權限管理、批次操作等功能</li></ul><h2 id="熔斷"><a href="#熔斷" class="headerlink" title="熔斷"></a>熔斷</h2><ul><li><p>降級的目的是<strong>應對系統自身的故障</strong>，而熔斷的目的是<strong>應對依賴的外部系統故障的情況</strong></p><blockquote><p>例如：A 服務的 X 功能依賴 B 服務的某個 API，當 B 服務的 API 響應很慢的時候，A 服務的 X 功能響應肯定也會被拖慢，進一步導致 A 服務的 thread 都被卡在 X 功能處理上，此時 A 服務的其他功能都會被卡住或者響應非常慢。這時就需要熔斷機制了，即：A 服務不再請求 B 服務的這個 API ，A 服務內部只要發現是請求 B 服務的這個 API 就立即返回錯誤，從而避免 A 服務整個被拖垮</p></blockquote></li><li><p>熔斷機制實現的關鍵：</p><ul><li><strong>需要有一個統一的 API 調用層，由 API 調用層來進行採樣或者統計</strong>，如果 API 調用散落在程式碼各處就沒法進行統一處理了</li><li><strong>threshold 的設計</strong>：一般都是先根據分析確定 threshold，然後上線觀察效果，再進行調整優化</li></ul></li></ul><h2 id="限流"><a href="#限流" class="headerlink" title="限流"></a>限流</h2><ul><li><p>降級是從<strong>系統功能優先級的角度</strong>考慮如何應對故障，而限流則是從<strong>使用者訪問壓力的角度</strong>來考慮如何應對故障</p></li><li><p>限流指只允許系統能夠承受的訪問量進來，超出系統訪問能力的請求將被丟棄</p></li></ul><p>常見的限流方式可以分為兩類：</p><h3 id="以”請求”為基礎限流"><a href="#以”請求”為基礎限流" class="headerlink" title="以”請求”為基礎限流"></a>以”請求”為基礎限流</h3><ul><li><p>從外部訪問的請求角度考慮限流：限制<strong>總量</strong> &amp; <strong>時間量</strong></p></li><li><p>限制總量的方式是<strong>限制某個指標的累積上限</strong>，常見的是限制當前系統服務的用戶總量(例如：某個直播間限制總用戶數上限為 100 萬)</p></li><li><p>限制時間量指<strong>限制一段時間內某個指標的上限</strong>(例如：1 分鐘內只允許 10000 個用戶訪問)</p></li><li><p>實現簡單，但主要問題是比較難以找到合適的 threshold </p></li><li><p>即使找到了合適的 threshold ，基於請求限流還面臨硬體相關的問題(例如：64 core 的機器比 32 core 的機器，業務處理性能並不是 2 倍的關係，可能是 1.5 倍)</p></li><li><p>為了找到合理的 threshold ，通常情況下可以採用性能壓測來確定 threshold ，但性能壓測也存在覆蓋場景有限的問題</p></li><li><p><strong>逐步優化</strong>算是比較準確的方式，但需要持續觀察 &amp; 調整</p></li></ul><h3 id="以”資源”基礎限流"><a href="#以”資源”基礎限流" class="headerlink" title="以”資源”基礎限流"></a>以”資源”基礎限流</h3><ul><li><p>基於請求限流是從系統外部考慮的，而基於資源限流是從系統內部考慮的</p></li><li><p>找到系統內部影響性能的關鍵資源，對其使用上限進行限制(例如：連接數、file handler、 thread count、請求隊列)</p></li><li><p>基於資源限流相比基於請求限流能夠<strong>更加有效地反映當前系統的壓力</strong></p></li><li><p>實踐中設計也面臨兩個主要的難點：</p><ul><li>如何確定關鍵資源</li><li>如何確定關鍵資源的 threshold </li></ul></li><li><p>同樣也需要<strong>逐步優化</strong>的過程</p></li></ul><h2 id="排隊"><a href="#排隊" class="headerlink" title="排隊"></a>排隊</h2><ul><li><p>排隊實際上是限流的一個變種，限流是直接拒絕用戶，排隊是<strong>讓用戶等待一段時間</strong></p></li><li><p>排隊雖然沒有直接拒絕用戶，但用戶等了很長時間後進入系統，體驗並不一定比限流好</p></li><li><p>排隊需要臨時緩存大量的業務請求，單個系統內部無法緩存這麼多數據，需要用獨立的系統(例如：Kafka)去實現</p></li></ul><h1 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h1><p>設計一個整點限量秒殺系統，包括登錄、搶購、支付(依賴支付寶)等功能，你會如何設計 API 級別的故障應對手段呢?</p><h2 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution 1"></a>Solution 1</h2><ol><li><p>對於用戶服務，在搶購期間可以準備降級策略 =&gt; 壓力過大時保證用戶登入的可用，註冊和修改信息可以做降級處理</p></li><li><p>搶購下單涉及到訂單，庫存，和商品查詢。可通過請求排隊來限流，超出庫存的請求直接返回</p></li><li><p>為了應對庫存和商品服務可能發生的故障，可以提前對商品數據和庫存數據做緩存，如果對端服務故障，本地也可以提供服務</p></li><li><p>支付依賴第三方系統，合理設置熔斷策略，如支付平均時長超過限制可提示用戶稍晚做支付</p></li></ol><h2 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution 2"></a>Solution 2</h2><ol><li>訪問的流量在每個環節可能逐步遞減（登入例外）</li><li>引導部分用戶提前登陸</li><li>秒殺價系統獨立部署（感覺和其他系統部署在一起才需要降級）</li><li>搶購使用排隊方式，隊列大小可以預估較大長度，隊列外的拒絕</li><li>如果要求以支付成功為準，通過隊列和熔斷；如果以下單成功為準，使用熔斷。提醒稍後再付</li></ol><h2 id="其他思考"><a href="#其他思考" class="headerlink" title="其他思考"></a>其他思考</h2><ul><li><p>假如降級時優先保證登錄，但是用戶登錄進來後發現搶購不了，其實體驗也不好</p></li><li><p>已經搶購了的用戶可能無法支付，這樣體驗更不好，甚至會引起投訴，因此搶購類降級是優先降登錄會好些</p></li><li><p>保留搶購和支付，保證進來的用戶能夠完成業務流程</p></li><li><p>支付失敗真沒什麼好辦法了，因為這是核心鏈路的核心功能</p></li><li><p>一般不建議對支付做降級，用戶體驗很不好，還不如登錄和搶購階段限流</p><blockquote><p>這是有心理學理論支撐的，用戶沒搶到前，如果搶不到他會認為自己運氣不好，但如果用戶搶到了卻無法支付，他會覺得自己損失了，會觸發”損失厭惡”心理</p></blockquote></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] Multi-Site High Avalibility</title>
      <link href="/blog/Architecture_Design/Architecture-Design-Multiple-Site-HA/"/>
      <url>/blog/Architecture_Design/Architecture-Design-Multiple-Site-HA/</url>
      
        <content type="html"><![CDATA[<h1 id="業務高可用的保障：Multi-Site-HA-異地多活"><a href="#業務高可用的保障：Multi-Site-HA-異地多活" class="headerlink" title="業務高可用的保障：Multi-Site HA(異地多活)"></a>業務高可用的保障：Multi-Site HA(異地多活)</h1><ul><li><p>由於備份系統平時不對外提供服務，可能會存在很多隱藏的問題沒有發現</p></li><li><p>如果業務期望達到即使在災難性故障發生的情況下，業務也不受影響，或是在短時間內很快恢復，那麼就需要設計 Multi-Site HA 架構</p></li></ul><h2 id="應用場景"><a href="#應用場景" class="headerlink" title="應用場景"></a>應用場景</h2><ul><li><p>Multi-Site HA 架構的關鍵點就是<code>Multi-Site</code>(地理位置上不同的地方)、<code>High Availability(HA)</code>(不同地理位置上的系統都能夠提供業務服務)</p></li><li><p>判斷一個系統是否符合 Multi-Site HA ，需要滿足兩個標準：</p><ul><li>正常情況下，用戶無論訪問哪一個地點的業務系統，都能夠得到正確的業務服務</li><li>某個地方業務異常的時候，用戶訪問其他地方正常的業務系統，能夠得到正確的業務服務</li></ul></li><li><p>實現 Multi-Site HA 架構不是沒有代價的，相反其代價很高：</p><ul><li>系統複雜度會發生質的變化</li><li>成本會上升</li></ul></li><li><p>不是每個業務都要上 Multi-Site HA </p><ul><li>如果無法承受 Multi-Site HA 帶來的複雜度和成本，是可以不做 Multi-Site HA 的，只需要做 Multi-Site backup 即可</li><li>某些業務系統即使中斷，對用戶的影響並不會很大(例如：新聞網站、企業內部的 IT 系統)</li></ul></li><li><p>如果業務規模很大，能夠做 Multi-Site HA 的情況下還是儘量完成</p><ul><li>能夠在異常的場景下給用戶提供更好的體驗</li><li>Multi-Site HA 能夠減少異常發生時所帶來的收入損失</li></ul></li></ul><h2 id="架構模式"><a href="#架構模式" class="headerlink" title="架構模式"></a>架構模式</h2><p>根據地理位置上的距離來劃分，可以分成以下三種架構：</p><h3 id="同城異區"><a href="#同城異區" class="headerlink" title="同城異區"></a>同城異區</h3><ul><li><p>同城異區指的是將業務部署在同一個城市不同區的多個機房</p></li><li><p>同城的兩個機房，距離上一般大約就是幾十公里，通過搭建高速的網路，同城異區的兩個機房能夠實現和同一個機房內幾乎一樣的網路傳輸速度(邏輯上可視為同一個機房，類似 AWS AZ)</p></li><li><p>對於極端場景(例如：大地震)無能為力；但對於機房火災、機房停電、機房空調故障這類問題可以很容易解決(兩種狀況都會讓機房出問題造成業務停擺)</p></li><li><p><strong>結合複雜度、成本、故障發生機率來綜合考慮，同城異區是應對機房級別故障的最優架構</strong></p></li></ul><h3 id="跨城異地"><a href="#跨城異地" class="headerlink" title="跨城異地"></a>跨城異地</h3><ul><li><p>跨城異地指的是業務部署在不同城市的多個機房，而且<strong>距離最好要遠一些</strong></p></li><li><p><strong>距離上比較遠，才能有效應對極端災難事件</strong></p></li><li><p>距離增加帶來的最主要問題是<strong>兩個機房的網路傳輸速度會降低</strong></p></li><li><p>除了距離上的限制，中間傳輸各種不可控的因素也非常多(例如：挖掘機把光纖挖斷、中美海底電纜被拖船扯斷、骨幹網故障 … etc)</p></li><li><p>跨城異區距離太遠，搭建或者使用多通道的成本會高不少</p></li><li><p>如果要做到真正意義上的多活，<strong>業務系統需要考慮部署在不同地點的兩個機房，在資料短時間不一致的情況下，還能夠正常提供業務</strong></p></li><li><p>矛盾點：資料不一致業務肯定不會正常，但跨城異地肯定會導致資料不一致</p></li><li><p>支付寶等金融相關的系統，對<strong>餘額</strong>這類資料，一般不會做跨城異地的多活架構，而只能採用同城異區這種架構</p></li><li><p>對資料一致性要求不那麼高，或者資料不怎麼改變，或者即使資料丟失影響也不大的業務，跨城級別的 Multi-Site HA 就能夠很好地應對極端災難的場景</p></li></ul><h3 id="跨國異地"><a href="#跨國異地" class="headerlink" title="跨國異地"></a>跨國異地</h3><ul><li><p>跨國異地指的是業務部署在不同國家的多個機房</p></li><li><p>跨國 Multi-Site HA 的主要應用場景一般有這幾種情況：</p><ul><li>為不同地區用戶提供服務</li><li>為 read-only 類型的業務做 HA</li></ul></li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>備份系統平常沒有流量，如果直接上線可能觸發平常測試不到的故障</p></li><li><p>再即時的系統也會有資料延時，如果涉及到金融這種系統，仍然是不敢直接切換的。</p></li><li><p>系統運行過程中會有很多中間資料，緩存資料等。系統不經過預熱直接把流量倒過來，大流量會直接把系統拖垮</p></li><li><p>餘額資料同城異區對比跨城異地在資料一致性上同樣存在時間差，只是時間窗口更小而己，如果兩地都支持寫的話，存在被雙花攻擊的可能，風險還是很大 =&gt; <strong>餘額和庫存一般不做雙寫</strong></p></li><li><p>同城異區也是有延遲的，但是延遲小，故障切換時快，但總是有可能有用戶資料不一致，這種數量小就可以容忍，人工修復和事後補償的代價都可以接受，<strong>不存在所有用戶都沒任何問題的方案</strong></p></li></ul><h1 id="Multi-Site-HA-設計-4-大技巧"><a href="#Multi-Site-HA-設計-4-大技巧" class="headerlink" title="Multi-Site HA 設計 4 大技巧"></a>Multi-Site HA 設計 4 大技巧</h1><ul><li>核心理念：<strong>採用多種手段，保證絕大部分用戶的核心業務 Multi-Site HA</strong></li></ul><h2 id="技巧-1：保證核心業務的-Multi-Site-HA"><a href="#技巧-1：保證核心業務的-Multi-Site-HA" class="headerlink" title="技巧 1：保證核心業務的 Multi-Site HA"></a>技巧 1：保證核心業務的 Multi-Site HA</h2><ul><li><p>很多架構師在考慮”業務”時，會不自覺地陷入一個思維誤區：<code>我要保證所有業務都能 Multi-Site HA</code> (但這是不可能的)</p></li><li><p>修改核心業務規則的代價非常大，幾乎所有的業務都要重新設計，為了架構設計去改變業務規則是得不償失的</p></li><li><p>優先實現核心業務的 Multi-Site HA 架構(例如：業務中的”登錄”功能)</p></li></ul><h2 id="技巧-2：保證核心資料最終一致性"><a href="#技巧-2：保證核心資料最終一致性" class="headerlink" title="技巧 2：保證核心資料最終一致性"></a>技巧 2：保證核心資料最終一致性</h2><ul><li><p><strong>資料同步</strong>是Multi-Site HA 架構設計的核心；但大部分架構師在考慮資料同步方案時，會不知不覺地陷入完美主義誤區：<code>所有資料都即時同步</code></p></li><li><p>業務上要求資料快速同步，物理上正好做不到資料快速同步，因此<strong>所有資料都即時同步，實際上是一個無法達到的目標</strong></p></li><li><p>如何減少資料無法快速同步的影響?</p><ul><li>儘量減少 Multi-Site HA 機房的距離，搭建高速網路</li><li>儘量減少資料同步，只同步核心業務相關的資料</li><li>保證最終一致性(Eventually Consistency)，不保證即時一致性</li></ul></li><li><p>最終一致性在具體實現時，還需要根據不同的資料特徵，進行差異化的處理，以滿足業務需要</p></li></ul><h2 id="技巧-3：採用多種手段同步資料"><a href="#技巧-3：採用多種手段同步資料" class="headerlink" title="技巧 3：採用多種手段同步資料"></a>技巧 3：採用多種手段同步資料</h2><ul><li><p>思維誤區：只使用存儲系統(例如：MySQL, Redis, Elasticsearch … etc)的同步功能</p></li><li><p>在某些比較極端的情況下，存儲系統本身的同步功能可能難以滿足業務需求</p></li><li><p>異地多機房這種部署，各種各樣的異常情況都可能出現，當我們只考慮存儲系統本身的同步功能時，就會發現無法做到真正的 Multi-Site HA </p></li><li><p>避免只使用存儲系統的同步功能，可以將多種手段配合存儲系統的同步來使用，甚至可以不採用存儲系統的同步方案，改用自己的同步方案：</p><ul><li>消息隊列方式</li><li>二次讀取方式</li><li>存儲系統同步方式</li><li>回源讀取方式</li><li>重新產生資料方式</li></ul></li><li><p>以下是資料同步方案架構示意圖：<br><img src="/blog/images/architecture-design/multiple-sites-sync-data-solutions.png" alt="multiple sites - data synchronization solutions"></p></li></ul><h2 id="技巧-4：只保證絕大部分用戶的Multi-Site-HA"><a href="#技巧-4：只保證絕大部分用戶的Multi-Site-HA" class="headerlink" title="技巧 4：只保證絕大部分用戶的Multi-Site HA"></a>技巧 4：只保證絕大部分用戶的Multi-Site HA</h2><ul><li><p>Multi-Site HA 也無法保證 100% 的業務可用，這是由物理規律決定的，光速和網路的傳播速度、硬盤的讀寫速度、極端異常情況的不可控等，都是無法 100% 解決的</p></li><li><p>雖然我們無法做到 100% 可用性，但可以採取一些措施進行安撫或者補償：</p><ul><li>發佈公告</li><li>事後對用戶進行補償</li><li>補充體驗 (例如：轉帳完成後的主動通知)</li></ul></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>OceanBase 屬於強一致分佈式資料庫，可以使業務不需要考慮持久層的跨地域資料同步問題</p></li><li><p>但付出的代價是 request response time 會變大，可用性也有降低，所以對 response time 要求非常高的業務可能不會選擇，其實還是對業務有影響的</p></li><li><p>如果代價可以承受，業務端還要解決 cache 的一致性問題，流量切到其它可用區的壓力是不是承受的住，可能還是需要部分業務降級</p></li><li><p>所以分佈式資料庫不能完全做到業務無感知的 Multi-Site HA </p></li><li><p>根據經濟能力、所需時間及業務要求有先後地選擇 Multi-Site HA 的業務及業務多活的形式</p></li></ul><h1 id="Multi-Site-HA-設計-4-步走"><a href="#Multi-Site-HA-設計-4-步走" class="headerlink" title="Multi-Site HA 設計 4 步走"></a>Multi-Site HA 設計 4 步走</h1><h2 id="Step-1：業務分級"><a href="#Step-1：業務分級" class="headerlink" title="Step 1：業務分級"></a>Step 1：業務分級</h2><ul><li><p>按照一定的標準將業務進行分級，挑選出核心的業務，只為核心業務設計 Multi-Site HA ，降低方案整體複雜度和實現成本</p></li><li><p>常見的分級標準：</p><ul><li>訪問量大的業務</li><li>核心業務</li><li>產生大量收入的業務</li></ul></li></ul><blockquote><p>以用戶管理系統為例，”登錄”業務符合”訪問量大的業務”和”核心業務”這兩條標準</p></blockquote><h2 id="Step-2：資料分類"><a href="#Step-2：資料分類" class="headerlink" title="Step 2：資料分類"></a>Step 2：資料分類</h2><ul><li><p>核心業務相關的資料進一步分析，目的在於識別所有的資料及資料特徵，這些資料特徵會影響後面的方案設計</p></li><li><p>常見的資料特徵分析維度：</p><ul><li><strong>資料量</strong>：資料量越大，同步延遲的機率越高，同步方案需要考慮相應的解決方案</li><li><strong>唯一性</strong>：是否要求多個異地機房產生的同類資料必須保證唯一(需要設計一個資料唯一生成的算法)</li><li><strong>即時性</strong>：即時性要求越高，對同步的要求越高，方案越複雜</li><li><strong>可丟失性</strong>：資料是否可以丟失</li><li><strong>可恢復性</strong>：資料丟失後，是否可以通過某種手段進行恢復</li></ul></li><li><p>以用戶管理系統的登錄業務為例的分析範例：<br><img src="/blog/images/architecture-design/multiple-sites-data-leveling-example.png" alt="multiple sites - data leveling example"></p></li></ul><h2 id="Step-3：資料同步"><a href="#Step-3：資料同步" class="headerlink" title="Step 3：資料同步"></a>Step 3：資料同步</h2><p>常見的資料同步方案有以下幾種：</p><h3 id="存儲系統同步"><a href="#存儲系統同步" class="headerlink" title="存儲系統同步"></a>存儲系統同步</h3><ul><li><p>優點是使用簡單，因為幾乎主流的存儲系統都會有自己的同步方案</p></li><li><p>缺點是這類同步方案都是通用的，無法針對業務資料特點做定製化的控制</p><blockquote><p>例如：無論需要同步的資料量有多大，MySQL 都只有一個同步通道。因為要保證事務性，一旦資料量比較大，或者網路有延遲，則同步延遲就會比較嚴重</p></blockquote></li></ul><h3 id="消息隊列同步"><a href="#消息隊列同步" class="headerlink" title="消息隊列同步"></a>消息隊列同步</h3><ul><li>適合無事務性或者無時序性要求的資料</li></ul><h3 id="重複生成"><a href="#重複生成" class="headerlink" title="重複生成"></a>重複生成</h3><ul><li><p>資料不同步到異地機房，每個機房都可以生成資料，這個方案適合於可以重複生成的資料</p></li><li><p>登錄產生的 cookie、session 資料、緩存資料適合此方案</p></li><li><p>以用戶管理系統的登錄業務為例的分析範例：<br><img src="/blog/images/architecture-design/multiple-sites-data-leveling-example.png" alt="multiple sites - data leveling example"></p></li></ul><h2 id="Step-4：異常處理"><a href="#Step-4：異常處理" class="headerlink" title="Step 4：異常處理"></a>Step 4：異常處理</h2><ul><li><p>無論資料同步方案如何設計，一旦出現極端異常的情況，總是會有部分資料出現異常的</p></li><li><p>異常處理就是假設在出現這些問題時，系統將採取什麼措施來應對</p></li><li><p>異常處理主要有以下幾個目的：</p><ul><li>問題發生時，避免少量資料異常導致整體業務不可用。</li><li>問題恢復後，將異常的資料進行修正。</li><li>對用戶進行安撫，彌補用戶損失</li></ul></li><li><p>常見的異常處理措施：</p></li></ul><h3 id="多通道同步"><a href="#多通道同步" class="headerlink" title="多通道同步"></a>多通道同步</h3><ul><li><p>採取多種方式來進行資料同步，其中某條通道故障的情況下，系統可以通過其他方式來進行同步</p></li><li><p>設計的方案關鍵點：</p><ul><li>一般情況下，採取兩通道即可，採取更多通道理論上能夠降低風險，但付出的成本也會增加很多</li><li>資料庫同步通道和消息隊列同步通道不能採用相同的網路連接</li><li>需要資料是可以重複覆蓋的，這表示無論那個通道的資料先到，或是那個通道的資料後到，最終結果是一樣的</li></ul></li></ul><h3 id="同步和訪問結合"><a href="#同步和訪問結合" class="headerlink" title="同步和訪問結合"></a>同步和訪問結合</h3><ul><li><p>指異地機房通過系統開放的接口來進行資料訪問</p></li><li><p>設計關鍵點：</p><ul><li>接口訪問通道和資料庫同步通道不能採用相同的網路連接</li><li>資料有路由規則，可以根據資料來推斷應該訪問哪個機房的接口來讀取資料</li><li>由於有同步通道，優先讀取本地資料，本地資料無法讀取到再通過接口去訪問，這樣可以大大降低跨機房的異地接口訪問數量，適合於即時性要求非常高的資料</li></ul></li></ul><h3 id="日誌記錄"><a href="#日誌記錄" class="headerlink" title="日誌記錄"></a>日誌記錄</h3><ul><li><p>用於用戶故障恢復後對資料進行恢復</p></li><li><p>每個關鍵操作前後都記錄相關一條日誌，然後將日誌保存在一個獨立的地方，當故障恢復後，拿出日誌跟資料進行對比，對資料進行修復</p></li><li><p>應對不同級別的故障，日誌保存的要求也不一樣</p></li><li><p>常見的日誌保存方式：</p><ul><li>服務器上保存日誌，資料庫中保存資料</li><li>本地獨立系統保存日誌</li><li>日誌異地保存</li></ul></li></ul><h3 id="用戶補償"><a href="#用戶補償" class="headerlink" title="用戶補償"></a>用戶補償</h3><ul><li><p>無論採用什麼樣的異常處理措施，都只能最大限度地降低受到影響的範圍和程度，無法完全做到沒有任何影響</p></li><li><p>可以採用人工的方式對用戶進行補償，彌補用戶損失，培養用戶的忠誠度</p></li></ul><h2 id="討論整理精華-2"><a href="#討論整理精華-2" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>可通過預期貨幣價值分析，進行業務分級；大致維度如下：風險發生概率、風險損耗成本、技術改造成本、技術改造時長(月)、改造後成本節省(月)</p></li><li><p>通常來說 to C 的應用，影響用戶使用的應該是比較靠前的，尤其是產品還處於搶市場的階段</p></li><li><p>to B 的應用客戶買的是專業性，相對來說用戶體驗可以往後排，用戶能使用和公司收入還是比較重要的；而客戶相對固定，可以通過客服主動聯繫客戶的方式來做部分挽回</p></li><li><p>不同類型的公司，對於業務重要性分類是不一樣的，公司階段的不同也不一樣(例如：新聞的初創公司，第一步就是積累用戶，體驗第二，最後是收入)</p></li><li><p>公司新推出了一個業務，目的是提高用戶的粘性，這類用戶體驗首先解決，然後投訴，最後收入</p></li><li><p>轉賬收費類業務，那優先保證的就是收入了，其次就是投訴，最後是體驗</p></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Helm] Helm v3 使用簡介</title>
      <link href="/blog/DevOps/Helm-v3-Chart-Template-Guide/"/>
      <url>/blog/DevOps/Helm-v3-Chart-Template-Guide/</url>
      
        <content type="html"><![CDATA[<h1 id="安裝-Helm"><a href="#安裝-Helm" class="headerlink" title="安裝 Helm"></a>安裝 Helm</h1><p>因為我自己用 Ubuntu，所以以下就紀錄 Ubuntu 環境中安裝 Helm 的方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -</span><br><span class="line"></span><br><span class="line">$ sudo apt-get install apt-transport-https --yes</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;deb https://baltocdn.com/helm/stable/debian/ all main&quot;</span> | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list</span><br><span class="line"></span><br><span class="line">$ sudo apt-get update</span><br><span class="line"></span><br><span class="line">$ sudo apt-get -y install helm</span><br></pre></td></tr></table></figure><p>其他環境的安裝方式可以參考 <a href="https://helm.sh/docs/intro/install/">Helm 官方安裝文件</a></p><h1 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h1><h2 id="Chart-基本結構"><a href="#Chart-基本結構" class="headerlink" title="Chart 基本結構"></a>Chart 基本結構</h2><p>Helm chart 是在 <a href="https://helm.sh/docs/topics/architecture/">Helm 的整體架構</a>中被用來佈署的單位，用 chart 佈署到 k8s 後的產出，稱為 <code>release</code> 或是 <code>released object</code>。</p><p>因此 Helm chart 其實就是 k8s 那一堆 API object 的定義 + templating 功能，而衍生來可以管理 k8s workload 發佈的工具，因此先來了解一下 Helm chart 的結構如下：(假設名稱為 <code>mychart</code>)</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mychart/</span><br><span class="line">  Chart.yaml</span><br><span class="line">  values.yaml</span><br><span class="line">  charts/</span><br><span class="line">  templates/</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><ul><li><p><code>templates/</code>：此目錄顧名思義就是存放 template 檔案的位置，佈署時整個目錄會送交到 template rendering engine(目前架構中由 <code>Helm Library</code> 負責此項工作)，搭配其他傳入參數後，產生結果後送給 k8s API server</p></li><li><p><code>values.yaml</code>：此檔案包含了 <strong>templates/</strong> 目錄中的檔案中所需要的預設值，可讓使用者可以透過 <code>helm install</code> 或是 <code>helm upgrade</code> 來 override，進而產生出不同的 release 內容</p></li><li><p><code>charts/</code>：此目錄可能包含了其他不同的 chart，也可稱為 <code>subcharts</code></p></li><li><p><code>Chart.yaml</code>：此檔案包含了 chart 的內容描述</p></li></ul><blockquote><p>請注意目錄 &amp; 檔案名稱有大小寫之分</p></blockquote><h2 id="快速產生-chart-template"><a href="#快速產生-chart-template" class="headerlink" title="快速產生 chart template"></a>快速產生 chart template</h2><p>由於 Helm client 可以協助產生 chart skeleton，因此可以透過以下指令很快的產生出一個範本：</p><blockquote><p>helm create mychart</p></blockquote><p>接著就會出現以下的檔案結構：</p><p><img src="/blog/images/devops/helm_first-chart.png" alt="How Rancher Communicate"></p><p>除了上面提到的幾個重要檔案 &amp; 目錄之外，在 <code>templates/</code> 目錄中多了很多檔案，例如：</p><ul><li><p><code>NOTES.txt</code>：chart 本身的 help text，當使用者執行 <code>helm install</code> 時會顯示</p></li><li><p><code>deployment.yaml</code>：建立 k8s deployment 的基本樣板</p></li><li><p><code>service.yaml</code>：建立 k8s service 的基本樣板</p></li><li><p><code>_helpers.tpl</code>：用來產生 help 資訊的基本樣板</p></li></ul><h2 id="建立新的-namespace-進行開發測試"><a href="#建立新的-namespace-進行開發測試" class="headerlink" title="建立新的 namespace 進行開發測試"></a>建立新的 namespace 進行開發測試</h2><p>首先建立一個新的 namespace 來進行後續的開發測試：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示目前有哪些 context</span></span><br><span class="line">$ kubectl config get-contexts</span><br><span class="line">CURRENT   NAME                                      CLUSTER                                   AUTHINFO             NAMESPACE</span><br><span class="line">*         my-vsphere-cluster                        my-vsphere-cluster                        my-vsphere-cluster   </span><br><span class="line">          my-vsphere-cluster-rancher-k8s-master-1   my-vsphere-cluster-rancher-k8s-master-1   my-vsphere-cluster   </span><br><span class="line">          my-vsphere-cluster-rancher-k8s-master-2   my-vsphere-cluster-rancher-k8s-master-2   my-vsphere-cluster   </span><br><span class="line">          my-vsphere-cluster-rancher-k8s-master-3   my-vsphere-cluster-rancher-k8s-master-3   my-vsphere-cluster</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 namespace</span></span><br><span class="line">$ kubectl create ns/helm-test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 切換目前 context 所使用的 default namespace</span></span><br><span class="line">$ kubectl config set-context my-vsphere-cluster --namespace=helm-test</span><br></pre></td></tr></table></figure><h2 id="建立第一個-template"><a href="#建立第一個-template" class="headerlink" title="建立第一個 template"></a>建立第一個 template</h2><p>首先為了讓事情簡單點，先移除 <code>templates/</code> 目錄中的所有內容，並建立以下檔案：(<code>templates/configmap.yaml</code>)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># &#123;&#123; .Release.Name &#125;&#125; 表示要置換為 Release 物件中的 Name 屬性</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>開發 chart 時，檔案的副檔名規則，建議若是 YAML 使用 <code>.yaml</code>，若是 help 資訊相關的則用 <code>.tpl</code></p></blockquote><p>接著執行安裝 Helm chart 的動作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ helm install clunky-serval ./mychart</span><br><span class="line">NAME: clunky-serval</span><br><span class="line">LAST DEPLOYED: Sat Mar 21 20:22:23 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br></pre></td></tr></table></figure><p>接著可以用以下指令檢視 release 的詳細內容 &amp; 實際上佈署到 k8s 的設定：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># configmap 的名稱套用了 release name</span></span><br><span class="line">$ kubectl get configmap</span><br><span class="line">NAME                      DATA   AGE</span><br><span class="line">clunky-serval-configmap   1      10s</span><br><span class="line"></span><br><span class="line">$ helm get manifest clunky-serval</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Source: mychart/templates/configmap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: clunky-serval-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: <span class="string">&quot;Hello World&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>若 chart 中多個 k8s API object 的定義，上面就會一併顯示出來</p></blockquote><p>最後執行移除 chart 的指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm uninstall clunky-serval</span><br><span class="line">release <span class="string">&quot;clunky-serval&quot;</span> uninstalled</span><br></pre></td></tr></table></figure><p>此外，若是希望可以檢視準備要安裝的 chart 內容再決定是否安裝，可執行以下指令，透過 template 產生出預計會安裝到 k8s 的內容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若 --dry-run 之前加上 --debug 可以獲得更多資訊</span></span><br><span class="line">$ helm install --dry-run goodly-guppy ./mychart</span><br><span class="line">NAME: goodly-guppy</span><br><span class="line">LAST DEPLOYED: Sat Mar 21 20:27:45 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: pending-install</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Source: mychart/templates/configmap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: goodly-guppy-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: <span class="string">&quot;Hello World&quot;</span></span><br></pre></td></tr></table></figure><p>透過 <code>--dry-run</code> 可以檢視準備要安裝到 k8s 的內容，但卻不會進行真正的安裝，在開發 chart 時是個非常好用的功能。</p><h1 id="Built-in-Objects"><a href="#Built-in-Objects" class="headerlink" title="Built-in Objects"></a>Built-in Objects</h1><p>Object 是在 template engine 中傳遞給 template，產生出最終結果用的；而 Helm 提供了很多可程式化的方式來產生 object。</p><p>以上面的範例來說，就是使用了 Release object 中的 Name 屬性(官方文件中依然將 Name 視為 object)，因此透過 <code>&#123;&#123; .Release.Name &#125;&#125;</code> 的方式就可以變更 configmap 的名稱。</p><p>以下介紹幾個內建的 object：(<strong>以下內建的 object 都是以大寫字母開頭命名，自訂物件可不用按照此規則</strong>)</p><h2 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h2><p>很直觀的，Release object 其實就是描述 release 本身，包含了像是 <code>Release.Name</code>, <code>Release.Namespace</code>, <code>Release.IsUpgrade</code>, <code>Release.IsInstall</code>, <code>Release.Revision</code>, <code>Release.Service</code>…等屬性(or object)。</p><h2 id="Values"><a href="#Values" class="headerlink" title="Values"></a>Values</h2><p>這個 object 的值則是直接來自於使用者佈署時所傳入的 <code>values.yaml</code> 檔案內容；若是沒有傳入 <code>values.yaml</code>，那 Values 這個 object 就不會有任何內容。</p><h2 id="Chart"><a href="#Chart" class="headerlink" title="Chart"></a>Chart</h2><p>這個 object 的內容來自於檔案 <code>Chart.yaml</code> 中所定義的內容，例如：<code>Chart.Name</code> &amp; <code>Chart.Version</code></p><h2 id="Files"><a href="#Files" class="headerlink" title="Files"></a>Files</h2><p>Files object 是用來協助存取上述特殊檔案以外的檔案，舉例來說：</p><ul><li><p><code>Files.Get</code>：可用來取得指定路徑的檔案</p></li><li><p><code>Files.GetBytes</code>：從檔案中取得 binary 的內容，通常用來處理像是 image 這一類的檔案</p></li><li><p><code>Files.Lines</code>：可以針對檔案內容一行一行的逐行讀取，常用來以 <code>line</code> 為單位進行處理時使用</p></li><li><p><code>Files.AsSecrets</code>：讀取檔案內容後，轉成以 Base 64 編碼後的內容</p></li><li><p><code>Files.AsConfig</code>：將檔案內容轉換成 YAML map 物件</p></li></ul><h2 id="Capabilities"><a href="#Capabilities" class="headerlink" title="Capabilities"></a>Capabilities</h2><p>此物件用來提供目前 k8s cluster 所具備的功能範圍資訊，例如：<code>Capabilities.APIVersions</code>, <code>Capabilities.APIVersions.Has $version</code>, <code>Capabilities.KubeVersion</code>, <code>Capabilities.KubeVersion.Major</code>, <code>Capabilities.KubeVersion.Minor</code> … 等資訊。</p><h2 id="Template"><a href="#Template" class="headerlink" title="Template"></a>Template</h2><p>此物件提供目前執行中的 template 所包含的訊息，例如：</p><ul><li><p><code>Name</code>：template 在 chart 中的相對路徑，例如：<code>mychart/templates/mytemplate.yaml</code></p></li><li><p><code>BasePath</code>：此 template 位於 chart 中的相對路徑，例如：<code>mychart/templates</code></p></li></ul><h1 id="Values-檔案的使用方式"><a href="#Values-檔案的使用方式" class="headerlink" title="Values 檔案的使用方式"></a>Values 檔案的使用方式</h1><h2 id="標準使用方式"><a href="#標準使用方式" class="headerlink" title="標準使用方式"></a>標準使用方式</h2><p>透過傳遞 Values 檔案的內容，使用者可以根據需求改變 Helm chart 產生出的內容，將正確的內容佈署到 k8s 上，而 Values 檔案在套用上的順序，會按照以下的順序來套用：(越下方的優先權越高)</p><ul><li><p>chart 中所包含的 <code>values.yaml</code> 檔案</p></li><li><p>若有 subchart，則會套用來自 parent chart 的 <code>values.yaml</code></p></li><li><p>使用 <code>helm install -f</code> or <code>helm upgrade -f</code> 時指定的檔案，例如：<code>helm install -f myvals.yaml ./mychart</code></p></li><li><p>透過 <code>--set</code> 傳入的單獨參數，例如：<code>helm install --set foo=bar ./mychart</code></p></li></ul><p>以上面的範例來說，建立檔案 <code>mychart/values.yaml</code>，內容如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">favorite:</span></span><br><span class="line">  <span class="attr">drink:</span> <span class="string">coffee</span></span><br><span class="line">  <span class="attr">food:</span> <span class="string">pizza</span></span><br></pre></td></tr></table></figure><p><code>mychart/templates/configmap.yaml</code> 的內容則調整如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  <span class="attr">drink:</span> &#123;&#123; <span class="string">.Values.favorite.drink</span> &#125;&#125;</span><br><span class="line">  <span class="attr">food:</span> &#123;&#123; <span class="string">.Values.favorite.food</span> &#125;&#125;</span><br></pre></td></tr></table></figure><p>以下則是實際用法說明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以看到 drink 的值出現了來自 values.yaml 中的 coffee</span></span><br><span class="line">$ $ helm install --dry-run good-puppy ./mychart</span><br><span class="line">NAME: good-puppy</span><br><span class="line">LAST DEPLOYED: Sun Mar 22 20:28:11 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: pending-install</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Source: mychart/templates/configmap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: good-puppy-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  drink: coffee</span><br><span class="line">  food: pizza</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以透過 --set 直接取代 values.yaml 中的值</span></span><br><span class="line"><span class="comment"># 其中 drink 被 --set 覆蓋了，而 food 則使用 values.yaml 中的值</span></span><br><span class="line">$ $ helm install --dry-run --<span class="built_in">set</span> favorite.drink=slurm good-puppy ./mychart</span><br><span class="line">NAME: good-puppy</span><br><span class="line">LAST DEPLOYED: Sun Mar 22 20:29:16 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: pending-install</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Source: mychart/templates/configmap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: good-puppy-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  drink: slurm</span><br><span class="line">  food: pizza</span><br></pre></td></tr></table></figure><h2 id="若要刪除-default-key"><a href="#若要刪除-default-key" class="headerlink" title="若要刪除 default key"></a>若要刪除 default key</h2><p>待測試…..</p><h1 id="Template-Functions-and-Pipelines"><a href="#Template-Functions-and-Pipelines" class="headerlink" title="Template Functions and Pipelines"></a>Template Functions and Pipelines</h1><p>此部份介紹如何根據實際需求，將傳入到 template 的值再進行額外的加工；而 Helm 提供了超過六十種的 function 可供使用者處理資料，這些 function 來自於以下兩個地方：</p><ul><li><p><a href="https://godoc.org/text/template">Go template language</a></p></li><li><p><a href="https://masterminds.github.io/sprig/">Sprig template library</a></p></li></ul><p>以下舉個簡單例子：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  <span class="comment"># 範例中的 quote function，可以為傳入的值增加雙引號</span></span><br><span class="line">  <span class="attr">drink:</span> &#123;&#123; <span class="string">quote</span> <span class="string">.Values.favorite.drink</span> &#125;&#125;</span><br><span class="line">  <span class="attr">food:</span> &#123;&#123; <span class="string">quote</span> <span class="string">.Values.favorite.food</span> &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="Pipelines"><a href="#Pipelines" class="headerlink" title="Pipelines"></a>Pipelines</h2><p>pipeline 是來自於 Unix 的概念，在 templating language 是時常被用到的功能，可將多個 function 同時混搭使用，例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  <span class="comment"># 透過 pipeline 來套用 quote function</span></span><br><span class="line">  <span class="comment"># 也可以多個 function 混搭使用</span></span><br><span class="line">  <span class="attr">drink:</span> &#123;&#123; <span class="string">.Values.favorite.drink</span> <span class="string">|</span> <span class="string">repeat</span> <span class="number">5</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  <span class="attr">food:</span> &#123;&#123; <span class="string">.Values.favorite.food</span> <span class="string">|</span> <span class="string">upper</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br></pre></td></tr></table></figure><p>以下是上述範例的輸出結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --dry-run good-puppy ./mychart/</span><br><span class="line">NAME: good-puppy</span><br><span class="line">LAST DEPLOYED: Mon Mar 23 04:38:51 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: pending-install</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Source: mychart/templates/configmap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: good-puppy-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  drink: <span class="string">&quot;coffeecoffeecoffeecoffeecoffee&quot;</span></span><br><span class="line">  food: <span class="string">&quot;PIZZA&quot;</span></span><br></pre></td></tr></table></figure><h1 id="Flow-Control"><a href="#Flow-Control" class="headerlink" title="Flow Control"></a>Flow Control</h1><p>flow control 同樣也是 templating language 中的一個重要功能，而 Helm 提供了 <code>if/else</code>, <code>with</code>, <code>range</code> … 等功能，若有對程式開發有經驗的人，看到上述的關鍵字應該可以很容易的理解。</p><p>此外，還有其他特別的功能，像是：</p><ul><li><p><code>define</code>：在 template 中定義 template</p></li><li><p><code>template</code>：匯入指定的 template</p></li><li><p><code>block</code>：宣告一個 template 區域</p></li></ul><h2 id="if-else"><a href="#if-else" class="headerlink" title="if/else"></a>if/else</h2><p>在以下的情況下，會被 template engine 判定為 false：</p><ul><li><p>boolean 值，false</p></li><li><p>數字，0</p></li><li><p>空白字串</p></li><li><p><code>nil</code> (空白字串 or null)</p></li><li><p>空集合(map, slice, tuple, dict, array)</p></li></ul><p>其他情況則都會被判定為 true。</p><p>以下是個簡單範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  <span class="attr">drink:</span> &#123;&#123; <span class="string">.Values.favorite.drink</span> <span class="string">|</span> <span class="string">default</span> <span class="string">&quot;tea&quot;</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  <span class="attr">food:</span> &#123;&#123; <span class="string">.Values.favorite.food</span> <span class="string">|</span> <span class="string">upper</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  &#123;&#123; <span class="string">if</span> <span class="string">eq</span> <span class="string">.Values.favorite.drink</span> <span class="string">&quot;coffee&quot;</span> &#125;&#125;<span class="attr">mug:</span> <span class="literal">true</span>&#123;&#123; <span class="string">end</span> &#125;&#125;</span><br></pre></td></tr></table></figure><p>會得到下方結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --dry-run good-puppy ./mychart/</span><br><span class="line">NAME: good-puppy</span><br><span class="line">LAST DEPLOYED: Mon Mar 23 05:15:03 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: pending-install</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Source: mychart/templates/configmap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: good-puppy-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  drink: <span class="string">&quot;coffee&quot;</span></span><br><span class="line">  food: <span class="string">&quot;PIZZA&quot;</span></span><br><span class="line">  mug: <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="處理空白字元"><a href="#處理空白字元" class="headerlink" title="處理空白字元"></a>處理空白字元</h2><p>仔細看上面的範例，在 if 那一行看起來就很不習慣，如果改成下面這樣寫法肯定更容易的看懂：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  <span class="attr">drink:</span> &#123;&#123; <span class="string">.Values.favorite.drink</span> <span class="string">|</span> <span class="string">default</span> <span class="string">&quot;tea&quot;</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  <span class="attr">food:</span> &#123;&#123; <span class="string">.Values.favorite.food</span> <span class="string">|</span> <span class="string">upper</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  &#123;&#123; <span class="string">if</span> <span class="string">eq</span> <span class="string">.Values.favorite.drink</span> <span class="string">&quot;coffee&quot;</span> &#125;&#125;</span><br><span class="line">  <span class="attr">mug:</span> <span class="literal">true</span></span><br><span class="line">  &#123;&#123; <span class="string">end</span> &#125;&#125;</span><br></pre></td></tr></table></figure><p>但這樣會造成以下結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --dry-run good-puppy ./mychart/</span><br><span class="line">NAME: good-puppy</span><br><span class="line">LAST DEPLOYED: Mon Mar 23 05:16:52 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: pending-install</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Source: mychart/templates/configmap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: good-puppy-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  drink: <span class="string">&quot;coffee&quot;</span></span><br><span class="line">  food: <span class="string">&quot;PIZZA&quot;</span></span><br><span class="line">  </span><br><span class="line">  mug: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>結果中間出現了一個空白行，雖然可以正常使用但就是不太好看；幸好 Helm 提供了解決方法：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  <span class="attr">drink:</span> &#123;&#123; <span class="string">.Values.favorite.drink</span> <span class="string">|</span> <span class="string">default</span> <span class="string">&quot;tea&quot;</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  <span class="attr">food:</span> &#123;&#123; <span class="string">.Values.favorite.food</span> <span class="string">|</span> <span class="string">upper</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">if</span> <span class="string">eq</span> <span class="string">.Values.favorite.drink</span> <span class="string">&quot;coffee&quot;</span> &#125;&#125;</span><br><span class="line">  <span class="attr">mug:</span> <span class="literal">true</span></span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">end</span> &#125;&#125;</span><br></pre></td></tr></table></figure><p>透過在 if &amp; else 前方加上一個 dash <code>-</code>，就會出現一開始的結果囉!</p><h2 id="透過-“with”-關鍵字來限定變數範圍"><a href="#透過-“with”-關鍵字來限定變數範圍" class="headerlink" title="透過 “with” 關鍵字來限定變數範圍"></a>透過 “with” 關鍵字來限定變數範圍</h2><p>有時候當 value 的階層很多時，透過 <code>with</code> 關鍵字就可以讓 template 內容再更簡潔一點，例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  <span class="comment"># 直接將變數範圍限縮在 `.favorite` 中</span></span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">with</span> <span class="string">.Values.favorite</span> &#125;&#125;</span><br><span class="line">  <span class="attr">drink:</span> &#123;&#123; <span class="string">.drink</span> <span class="string">|</span> <span class="string">default</span> <span class="string">&quot;tea&quot;</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  <span class="attr">food:</span> &#123;&#123; <span class="string">.food</span> <span class="string">|</span> <span class="string">upper</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">end</span> &#125;&#125;</span><br><span class="line">  <span class="attr">release:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="使用-“range”-走訪指定物件的所有元素"><a href="#使用-“range”-走訪指定物件的所有元素" class="headerlink" title="使用 “range” 走訪指定物件的所有元素"></a>使用 “range” 走訪指定物件的所有元素</h2><p><code>range</code>的用法就跟程式語言中的 <code>foreach</code> 是相同的，以下是個範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">with</span> <span class="string">.Values.favorite</span> &#125;&#125;</span><br><span class="line">  <span class="attr">drink:</span> &#123;&#123; <span class="string">.drink</span> <span class="string">|</span> <span class="string">default</span> <span class="string">&quot;tea&quot;</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  <span class="attr">food:</span> &#123;&#123; <span class="string">.food</span> <span class="string">|</span> <span class="string">upper</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">end</span> &#125;&#125;</span><br><span class="line">  <span class="comment"># 透過 range 處理的資料可以來自於 Values 物件</span></span><br><span class="line">  <span class="attr">toppings:</span> <span class="string">|-</span></span><br><span class="line">    &#123;&#123;<span class="bullet">-</span> <span class="string">range</span> <span class="string">$index</span>, <span class="string">$topping</span> <span class="string">:=</span> <span class="string">.Values.pizzaToppings</span> &#125;&#125;</span><br><span class="line">    <span class="bullet">-</span> &#123;&#123; <span class="string">.</span> <span class="string">|</span> <span class="string">title</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">    &#123;&#123;<span class="bullet">-</span> <span class="string">end</span> &#125;&#125;</span><br><span class="line">  <span class="comment"># 也可以直接定義在 template 中</span></span><br><span class="line">  <span class="attr">sizes:</span> <span class="string">|-</span></span><br><span class="line">    &#123;&#123;<span class="bullet">-</span> <span class="string">range</span> <span class="string">tuple</span> <span class="string">&quot;small&quot;</span> <span class="string">&quot;medium&quot;</span> <span class="string">&quot;large&quot;</span> &#125;&#125;</span><br><span class="line">    <span class="bullet">-</span> &#123;&#123; <span class="string">.</span> &#125;&#125;</span><br><span class="line">    &#123;&#123;<span class="bullet">-</span> <span class="string">end</span> &#125;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 執行結果</span></span><br><span class="line">$ helm install --dry-run good-puppy ./mychart/</span><br><span class="line">NAME: good-puppy</span><br><span class="line">LAST DEPLOYED: Wed Mar 25 05:18:48 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: pending-install</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line">---</span><br><span class="line"><span class="comment"># Source: mychart/templates/configmap.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: good-puppy-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  drink: <span class="string">&quot;coffee&quot;</span></span><br><span class="line">  food: <span class="string">&quot;PIZZA&quot;</span></span><br><span class="line">  toppings: |-</span><br><span class="line">    - <span class="string">&quot;Mushrooms&quot;</span></span><br><span class="line">    - <span class="string">&quot;Cheese&quot;</span></span><br><span class="line">    - <span class="string">&quot;Peppers&quot;</span></span><br><span class="line">    - <span class="string">&quot;Onions&quot;</span></span><br><span class="line">  sizes: |-</span><br><span class="line">    - small</span><br><span class="line">    - medium</span><br><span class="line">    - large</span><br></pre></td></tr></table></figure><p>而 range 可以處理的資料結構其實有不少，除了像是一般的 <code>tuple</code> &amp; <code>list</code> 之外，其他像是 key/value 結構的 <code>map</code> &amp; <code>dict</code> 也都是支援的。</p><h1 id="Variables"><a href="#Variables" class="headerlink" title="Variables"></a>Variables</h1><h2 id="scoped-variable"><a href="#scoped-variable" class="headerlink" title="scoped variable"></a>scoped variable</h2><p>variable 的概念其實跟一般程式語言中的變數並無不同，其實就是<code>將特定的值塞入變數中，並在後續的程式中使用</code>，以下是從上面改造的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> &#123;&#123; <span class="string">.Release.Name</span> &#125;&#125;<span class="string">-configmap</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">myvalue:</span> <span class="string">&quot;Hello World&quot;</span></span><br><span class="line">  <span class="comment"># 定義 variable &quot;$relname&quot;</span></span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">$relname</span> <span class="string">:=</span> <span class="string">.Release.Name</span> &#125;&#125;</span><br><span class="line">  <span class="comment"># 使用 range 搭配 $key &amp; $val 的組合，可以走訪每個 dictionary 元素</span></span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">range</span> <span class="string">$key</span>, <span class="string">$val</span> <span class="string">:=</span> <span class="string">.Values.favorite</span> &#125;&#125;</span><br><span class="line">  &#123;&#123; <span class="string">$key</span> &#125;&#125;<span class="string">:</span> &#123;&#123; <span class="string">$val</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">  &#123;&#123;<span class="bullet">-</span> <span class="string">end</span> &#125;&#125;</span><br><span class="line">  <span class="attr">rlease:</span> &#123;&#123; <span class="string">$relname</span> &#125;&#125;</span><br><span class="line">  <span class="comment"># 使用預先定義的 variable</span></span><br><span class="line">  <span class="attr">rlease:</span> &#123;&#123; <span class="string">$relname</span> &#125;&#125;</span><br><span class="line">  <span class="comment"># 使用 range 搭配 index &amp; value 的方式</span></span><br><span class="line">  <span class="comment"># 即可使用每個 value 在集合中對應的 index</span></span><br><span class="line">  <span class="attr">toppings:</span> <span class="string">|-</span></span><br><span class="line">    &#123;&#123;<span class="bullet">-</span> <span class="string">range</span> <span class="string">$index</span>, <span class="string">$topping</span> <span class="string">:=</span> <span class="string">.Values.pizzaToppings</span> &#125;&#125;</span><br><span class="line">      &#123;&#123; <span class="string">$index</span> &#125;&#125; &#123;&#123; <span class="string">$topping</span> <span class="string">|</span> <span class="string">title</span> <span class="string">|</span> <span class="string">quote</span> &#125;&#125;</span><br><span class="line">    &#123;&#123;<span class="bullet">-</span> <span class="string">end</span> &#125;&#125;</span><br><span class="line">  <span class="attr">sizes:</span> <span class="string">|-</span></span><br><span class="line">    &#123;&#123;<span class="bullet">-</span> <span class="string">range</span> <span class="string">tuple</span> <span class="string">&quot;small&quot;</span> <span class="string">&quot;medium&quot;</span> <span class="string">&quot;large&quot;</span> &#125;&#125;</span><br><span class="line">    <span class="bullet">-</span> &#123;&#123; <span class="string">.</span> &#125;&#125;</span><br><span class="line">    &#123;&#123;<span class="bullet">-</span> <span class="string">end</span> &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="global-variable-待補"><a href="#global-variable-待補" class="headerlink" title="global variable (待補)"></a>global variable (待補)</h2><h1 id="Named-Templates"><a href="#Named-Templates" class="headerlink" title="Named Templates"></a>Named Templates</h1><p>named template 跟前面看到一般的 template 是不太一樣的，具有以下幾個特性：</p><ul><li><p>類似 include file 的概念，named template 可透過 <code>include</code> 關鍵字帶入另外一個 template 中</p></li><li><p>也被稱為 <strong>partial template</strong> 或是 <strong>subtemplate</strong> </p></li></ul><p>其他待補……</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://helm.sh/docs/chart_template_guide/getting_started/">Helm | Getting Started</a></p></li><li><p><a href="https://helm.sh/docs/chart_template_guide/builtin_objects/">Helm | Built-in Objects</a></p></li><li><p><a href="https://helm.sh/docs/chart_template_guide/values_files/">Helm | Values Files</a></p></li><li><p><a href="https://helm.sh/docs/chart_template_guide/functions_and_pipelines/">Helm | Template Functions and Pipelines</a></p></li><li><p><a href="https://helm.sh/docs/chart_template_guide/control_structures/">Helm | Flow Control</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> Helm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 高可用計算架構</title>
      <link href="/blog/Architecture_Design/Architecture-Design-HA-Compute/"/>
      <url>/blog/Architecture_Design/Architecture-Design-HA-Compute/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><ul><li><p>Compute HA 的本質是通過 <strong>Redundancy(冗餘)</strong> 來規避部分故障的風險</p></li><li><p>Compute HA 架構的設計複雜度主要體現在任務管理方面：<strong>當任務在某台 server 上執行失敗後，如何將任務重新分配到新的 server 執行</strong></p></li><li><p>架構設計的關鍵點：</p><ul><li>那些 server 可以執行任務?<ol><li>每個 server 都可以執行任務</li><li>只有特定 server(通常叫”master”)可以執行任務</li></ol></li><li>任務如何重新執行?<ol><li>對於已經分配的任務即使執行失敗也不做任何處理</li><li>設計一個任務管理器來管理需要執行的計算任務， server 執行完任務後，需要向任務管理器回報任務執行結果</li></ol></li></ul></li><li><p>“ Job Scheduler “是一個邏輯的概念，並不一定要求系統存在一個獨立的 Job Scheduler module</p></li></ul><h1 id="主備架構"><a href="#主備架構" class="headerlink" title="主備架構"></a>主備架構</h1><p><img src="/blog/images/architecture-design/compute-ha-primary-backup.png" alt="Compute HA - Primary/Backup"></p><ul><li><p>Compute HA 的主備架構無須資料複製</p></li><li><p>如果主機不能夠恢復，則需要人工操作，將備機升為主機，然後讓 Job Scheduler 將任務發送給新的主機</p></li><li><p>主備架構的優點就是<strong>簡單</strong>，主備機之間不需要進行交互，狀態判斷和切換操作由人工執行</p></li><li><p><strong>人工操作的時間不可控制，操作效率也比較低，可能需要半個小時才完成切換操作，而且手動操作過程中容易出錯</strong></p></li><li><p>Compute HA 的主備架構也比較適合與內部管理系統、後台管理系統這類使用人數不多、使用頻率不高的業務，不太適合 online 面對眾多使用者的業務</p></li></ul><h1 id="主從架構"><a href="#主從架構" class="headerlink" title="主從架構"></a>主從架構</h1><p><img src="/blog/images/architecture-design/compute-ha-primary-slave.png" alt="Compute HA - Primary/Slave"></p><ul><li><p>Job Scheduler 需要將任務進行分類，確定哪些任務可以發送給主機執行，哪些任務可以發送給備機執行</p></li><li><p>主從架構與主備架構相比，優缺點有：</p><ul><li>優點：主從架構的從機也執行任務，發揮了從機的硬體效能</li><li>缺點：主從架構需要將任務分類， Job Scheduler 會復雜一些</li></ul></li></ul><h1 id="Cluster-架構"><a href="#Cluster-架構" class="headerlink" title="Cluster 架構"></a>Cluster 架構</h1><ul><li><p>主備架構和主從架構通過冗餘一台 server 來提升可用性，且需要人工來切換主備或者主從，會有以下缺點：</p><ul><li>人工操作效率低</li><li>容易出錯</li><li>不能及時處理故障</li></ul></li><li><p>根據 Cluster 中 server 節點角色的不同：</p><ul><li>Symmetric Cluster ： Cluster 中每個 server 的角色都是一樣的，都可以執行所有任務</li><li>Asymmetric Cluster ： Cluster 中的 server 分為多個不同的角色，不同的角色執行不同的任務</li></ul></li><li><p>在Compute HA  Cluster 架構中，2 台 server 的 Cluster 和多台 server 的 Cluster ，在設計上沒有本質區別</p><blockquote><p>storage HA cluster 基本上會是以 3, 5, 7 … 的方式配置</p></blockquote></li></ul><h2 id="Symmetric-Cluster"><a href="#Symmetric-Cluster" class="headerlink" title="Symmetric Cluster"></a>Symmetric Cluster</h2><p><img src="/blog/images/architecture-design/compute-ha-symmetric-cluster.png" alt="Compute HA - symmetric cluster"></p><ul><li><p>也稱為 <code>負載均衡集群</code></p></li><li><p>運作行為：</p><ul><li>Job Scheduler 採取某種策略(隨機、輪詢等)將計算任務分配給 Cluster 中的不同 server </li><li>當 Cluster 中的某台 server 故障後，Job Scheduler 不再將任務分配給它</li><li>當故障的 server 恢復後，Job Scheduler 重新將任務分配給它執行</li></ul></li><li><p>設計關鍵點：</p><ul><li>Job Scheduler 需要選定分配策略</li><li>Job Scheduler 需要有能力檢測 server 狀態</li></ul></li><li><p>狀態檢測稍微複雜一些，既要檢測 server 的狀態(例如：server 是否當機、網路是否正常等)；同時還要檢測任務的執行狀態(例如任務是否卡死、是否執行時間過長…等)</p></li><li><p>不同業務場景的狀態判斷條件差異很大，實際設計時要根據業務需求來進行設計和優化</p><blockquote><p>一個線上系統，正常情況下頁面平均會在 500 ms 內返回，那麼狀態判斷條件可以設計為：1 min 內回應時間超過 1 sec(包括 timeout)的頁面數量佔了 80% 時，就認為 server 有故障</p></blockquote></li></ul><h2 id="Asymmetric-Cluster"><a href="#Asymmetric-Cluster" class="headerlink" title="Asymmetric Cluster"></a>Asymmetric Cluster</h2><p><img src="/blog/images/architecture-design/compute-ha-asymmetric-cluster.png" alt="Compute HA - asymmetric cluster"></p><ul><li><p>Cluster 中不同 server 的角色是不同的，不同角色的 server 承擔不同的職責；部分任務是 Master server 才能執行，部分任務是 Slave server 才能執行</p></li><li><p>運作行為：</p><ul><li>Cluster 會通過某種方式來區分不同 server 的角色，例如：ZAB 算法選舉</li><li>Job Scheduler 將不同任務發送給不同 server </li><li>當指定類型的 server 故障時，需要重新分配角色</li></ul></li><li><p>與對稱 Cluster 相比，設計複雜度在於：</p><ul><li><strong>任務分配策略更加複雜</strong>：需要將任務劃分為不同類型並分配給不同角色的 Cluster 節點</li><li><strong>角色分配策略實現比較複雜</strong>：例如，可能需要使用 ZAB、Raft 這類複雜的算法來實現 Leader 的選舉</li></ul></li></ul><h1 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h1><ul><li><p>要想 HA 就離不開 redundancy，無論是 Compute HA 還是 Storage HA 都會面對<strong>機器狀態檢測、切換</strong>以及<strong>機器選擇</strong>的問題，在這幾個方面二者複雜度差別不大</p></li><li><p>對於 compute 而言， Cluster 中的機器間之間基本上是無交互的，對於需要重試的計算任務，是有任務管理器來維護處理；而 <strong>Storage HA 還會涉及到機器之間資料的同步和一致性問題，在同步時還需要考慮效能、穩定性、同步中斷、個別失敗、重複同步等問題，因此就會複雜許多</strong></p></li><li><p>Compute HA 系統需考慮 safety 和 liveness，而 Storage HA 除了前面兩個考量點之外，還受 CAP 約束</p></li><li><p>Storage HA 比 Compute HA 要複雜的多，存儲高可用是有狀態的，Compute HA 一般解決的都是無狀態問題，<strong>有狀態就會存在著如何保存狀態、同步狀態的問題</strong></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 高可用儲存架構</title>
      <link href="/blog/Architecture_Design/Architecture-Design-HA-Storage/"/>
      <url>/blog/Architecture_Design/Architecture-Design-HA-Storage/</url>
      
        <content type="html"><![CDATA[<h1 id="高可用儲存架構：雙機架構"><a href="#高可用儲存架構：雙機架構" class="headerlink" title="高可用儲存架構：雙機架構"></a>高可用儲存架構：雙機架構</h1><ul><li><p>儲存高可用方案的本質都是通過將數據複製到多個儲存設備，通過<strong>數據冗餘</strong>的方式來實現高可用</p></li><li><p>複雜性主要體現在<strong>如何應對複製延遲和中斷導致的數據不一致問題</strong>，思考的面向有以下幾個：</p><ul><li>數據如何複製？</li><li>各個節點的職責是什麼？</li><li>如何應對複製延遲？</li><li>如何應對複製中斷？</li></ul></li></ul><h2 id="主備複製"><a href="#主備複製" class="headerlink" title="主備複製"></a>主備複製</h2><p><img src="/blog/images/architecture-design/storage-ha-primary-backup-replication.jpg" alt="Storage HA - primary/backup replication"></p><ul><li><p>主備複製是最常見也是最簡單的一種儲存高可用方案，幾乎所有的儲存系統都提供了主備複製的功能，例如 MySQL、Redis、MongoDB 等</p></li><li><p>主備架構中的”備機”主要作為備份用途，並不承擔實際的業務讀寫操作，如果要把備機改為主機，需要人工操作</p></li><li><p>優點：</p><ul><li>對於 client 來說，不需要感知備機的存在</li><li>對於主機和備機來說，雙方只需要進行數據複製即可，無須進行<strong>狀態判斷</strong>和<strong>主備切換</strong>這類複雜的操作</li></ul></li><li><p>缺點：</p><ul><li>備機僅僅只為備份，並沒有提供讀寫操作，硬體成本上有浪費</li><li>故障後需要人工干預，無法自動恢復</li><li>人工在執行恢復操作的過程中也容易出錯，因為這類操作並不常見，實際操作的時候很可能遇到各種意想不到的問題</li></ul></li></ul><h2 id="主從複製"><a href="#主從複製" class="headerlink" title="主從複製"></a>主從複製</h2><p><img src="/blog/images/architecture-design/storage-ha-master-slave-replication.jpg" alt="Storage HA - master/slave replication"></p><ul><li><p>主機負責讀寫操作，從機只負責讀操作，不負責寫操作</p></li><li><p>與主備複製架構比較類似，主要的差別點在於<strong>從機正常情況下也是要提供讀的操作</strong></p></li><li><p><strong>適合寫少讀多的業務</strong>，例如：論壇、BBS、新聞網站 … etc</p></li><li><p>優點：</p><ul><li>主從複製在主機故障時，讀操作相關的業務可以繼續運行</li><li>主從複製架構的從機提供讀操作，發揮了硬體的性能</li></ul></li><li><p>缺點：</p><ul><li>client 需要感知主從關係，並將不同的操作發給不同的機器進行處理，複雜度比主備複製要高</li><li>從機提供讀業務，如果主從複製延遲比較大，業務會因為數據不一致出現問題</li><li>故障時需要人工干預</li></ul></li></ul><h2 id="雙機-主備-主從-切換"><a href="#雙機-主備-主從-切換" class="headerlink" title="雙機(主備/主從)切換"></a>雙機(主備/主從)切換</h2><ul><li><p>前面兩個架構的問題：</p><ul><li>主機故障後，無法進行寫操作</li><li>如果主機無法恢復，需要人工指定新的主機角色</li></ul></li><li><p>雙機切換即是<strong>在原有方案的基礎上增加”切換”功能，即系統自動決定主機角色，並完成角色切換</strong></p></li><li><p>切換方案比複製方案不只是多了一個切換功能那麼簡單，而是複雜度上升了一個量級</p></li></ul><h3 id="設計關鍵"><a href="#設計關鍵" class="headerlink" title="設計關鍵"></a>設計關鍵</h3><ul><li><p><strong>主備間狀態判斷</strong></p><ul><li>狀態檢測的內容：例如機器是否斷電、process 是否存在、響應是否緩慢…等</li><li>狀態傳遞的管道：是相互間互相連接，還是第三方仲裁？</li></ul></li><li><p><strong>切換決策</strong></p><ul><li>切換時機：什麼情況下備機應該升級為主機？</li><li>切換策略：原來的主機故障恢復後，要再次切換，確保原來的主機繼續做主機，還是原來的主機故障恢復後自動成為新的備機？</li><li>自動程度：切換是完全自動的，還是半自動的？</li></ul></li><li><p><strong>數據衝突解決</strong></p><ul><li>當原有故障的主機恢復後，新舊主機之間可能存在數據衝突</li></ul></li></ul><h3 id="常見架構"><a href="#常見架構" class="headerlink" title="常見架構"></a>常見架構</h3><h4 id="互連式"><a href="#互連式" class="headerlink" title="互連式"></a>互連式</h4><p><img src="/blog/images/architecture-design/storage-ha-server-switch-1.png" alt="Storage HA - server switch 1"></p><ul><li><p><strong>主備機直接建立狀態傳遞的渠道</strong></p></li><li><p>在主備複製的架構基礎上，主機和備機多了一個”狀態傳遞”的通道，這個通道就是用來傳遞狀態信息的</p></li><li><p>client 這裡也會有一些相應的改變：</p><ul><li>主機和備機之間共享一個對 client 來說唯一的地址。例如虛擬 IP，主機需要綁定這個虛擬的 IP</li><li>client 同時記錄主備機的地址，哪個能訪問就訪問哪個；備機雖然能收到 client 的操作請求，但是會直接拒絕，拒絕的原因就是”備機不對外提供服務”</li></ul></li><li><p>缺點：</p><ul><li>如果狀態傳遞的通道本身有故障，而此時主機並沒有故障，最終就可能出現兩個主機</li><li>可以通過增加多個通道來增強狀態傳遞的可靠性，但這樣做只是降低了通道故障機率而已，不能從根本上解決這個缺點</li></ul></li></ul><h4 id="中介式"><a href="#中介式" class="headerlink" title="中介式"></a>中介式</h4><p><img src="/blog/images/architecture-design/storage-ha-server-switch-2.png" alt="Storage HA - server switch 2"></p><ul><li><p>在主備兩者之外引入第三方中介，<strong>主備機之間不直接連接，而都去連接中介，並且通過中介來傳遞狀態信息</strong></p></li><li><p>連接管理更簡單：主備機無須再建立和管理多種類型的狀態傳遞連接通道，只要連接到中介即可，實際上是降低了主備機的連接管理複雜度</p></li><li><p>狀態決策更簡單：主備機的狀態決策簡單了，無須考慮多種類型的連接通道獲取的狀態信息如何決策的問題</p></li><li><p>關鍵代價就在於<strong>如何實現中介本身的高可用</strong></p></li><li><p>在工程實踐中推薦基於 ZooKeeper 搭建中介式切換架構，解決中介本身的可靠性問題</p></li></ul><p><img src="/blog/images/architecture-design/storage-ha-server-mongodb.png" alt="Storage HA - MongoDB"></p><ul><li><p>MongoDB(M) 表示主節點，MongoDB(S) 表示備節點，MongoDB(A) 表示仲裁節點</p></li><li><p>主備節點儲存數據，仲裁節點不儲存數據</p></li><li><p>client 同時連接主節點與備節點，不連接仲裁節點</p></li></ul><h4 id="模擬式"><a href="#模擬式" class="headerlink" title="模擬式"></a>模擬式</h4><ul><li><p>模擬式指主備機之間並不傳遞任何狀態數據，而是備機模擬成一個 client ，向主機發起模擬的讀寫操作，根據讀寫操作的響應情況來判斷主機的狀態</p></li><li><p>實現更加簡單，因為<strong>省去了狀態傳遞通道的建立和管理工作</strong></p></li><li><p>模擬式讀寫操作獲取的狀態信息只有響應信息(例如，HTTP 404，超時、響應時間超過 3 秒等)，沒有互連式那樣多樣(例如：CPU 負載、I/O 負載、吞吐量)，<strong>基於有限的狀態來做狀態決策，可能出現偏差</strong></p></li></ul><h2 id="主主複製"><a href="#主主複製" class="headerlink" title="主主複製"></a>主主複製</h2><p><img src="/blog/images/architecture-design/storage-ha-master-master-replication.png" alt="Storage HA - master/master replication"></p><ul><li><p>兩台都是主機，不存在切換的概念，互相將數據複製給對方</p></li><li><p>client 無須區分不同角色的主機，隨便將讀寫操作發送給哪台主機都可以</p></li><li><p>主主複製架構從總體上來看要簡單很多，無須狀態信息傳遞，也無須狀態決策和狀態切換，但有其獨特的複雜性</p></li><li><p>如果採取主主複製架構，<strong>必須保證數據能夠雙向複製，而很多數據是不能雙向複製的(例如：按照數字增長的 ID、庫存、餘額…etc)</strong></p></li><li><p><strong>主主複製架構對數據的設計有嚴格的要求，一般適合於那些臨時性、可丟失、可覆蓋的數據場景</strong>，例如：用戶登錄產生的 session 數據（可以重新登錄生成）、用戶行為的日誌數據（可以丟失）、論壇的草稿數據（可以丟失）…etc</p></li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>政府信息公開網站的特點：</p><ol><li>用戶量和QPS不會很大</li><li>其次讀/寫都非常少，讀相較於寫多</li><li>可忍受一定時間範圍的不可用</li></ol></li><li><p>主主架構因其固有的雙向複雜性，很少在實際中使用；再加上此場景並不適合用主主架，故排除使用它。</p></li><li><p>根據第 3 個特點就可以排除使用雙機互換架構。</p></li><li><p>對於主備架構來說，主機正常時，備機不提供讀寫服務，比較浪費</p></li><li><p>綜合來看，選擇主從的儲存架構</p></li></ul><h1 id="高可用儲存架構：集群和分區"><a href="#高可用儲存架構：集群和分區" class="headerlink" title="高可用儲存架構：集群和分區"></a>高可用儲存架構：集群和分區</h1><h2 id="數據集群"><a href="#數據集群" class="headerlink" title="數據集群"></a>數據集群</h2><ul><li><p>主備、主從、主主架構本質上都有一個隱含的假設：<strong>主機能夠儲存所有數據，但主機本身的儲存和處理能力肯定是有極限的</strong></p></li><li><p>集群就是多台機器組合在一起形成一個統一的系統，這裡的”多台”，數量上至少是 3 台</p></li></ul><h3 id="數據集中集群"><a href="#數據集中集群" class="headerlink" title="數據集中集群"></a>數據集中集群</h3><p><img src="/blog/images/architecture-design/storage-ha-data-centric.png" alt="Storage HA - data centric"></p><ul><li><p>數據集中集群為 1 主多備或者 1 主多從</p></li><li><p><strong>數據都只能往主機中寫</strong>，而讀操作可以參考主備、主從架構進行靈活多變</p></li><li><p>複雜度整體更高一些，具體體現在：</p><ul><li><strong>主機如何將數據複製給備機</strong>：多條複製通道首先會增大主機複製的壓力</li><li><strong>備機如何檢測主機狀態</strong>：多台備機都需要對主機狀態進行判斷，而不同的備機判斷的結果可能是不同的</li><li><strong>主機故障後，如何決定新的主機</strong></li></ul></li></ul><h3 id="數據分散集群"><a href="#數據分散集群" class="headerlink" title="數據分散集群"></a>數據分散集群</h3><ul><li><p>多個 server 組成一個集群，每台 server 都會負責儲存一部分數據</p></li><li><p>為了提升硬體利用率，每台 server 又會備份一部分數據</p></li><li><p>複雜點在於如何將數據分配到不同的 server 上，算法需要考慮這些設計點：</p><ul><li><strong>均衡性</strong></li><li><strong>容錯性</strong>：當出現部分 server 故障時，算法需要將原來分配給故障 server 的數據分區分配給其他 server</li><li><strong>可伸縮性</strong>：當集群容量不夠，擴充新的 server 後，算法能夠自動將部分數據分區遷移到新 server ，並保證擴容後所有 server 的均衡性</li></ul></li><li><p>數據分散集群和數據集中集群的不同點 =&gt; <strong>數據分散集群中的每台 server 都可以處理讀寫請求，因此不存在數據集中集群中負責寫的主機那樣的角色</strong></p></li><li><p>數據集中集群架構中，client 只能將數據寫到主機；數據分散集群架構中，client 可以向任意 server 中讀寫數據</p></li><li><p><strong>必須有一個角色來負責執行數據分配算法</strong></p><ul><li>Hadoop 的實現就是獨立的 server 負責數據分區的分配，這台 server 叫作 Namenode</li><li>Elasticsearch 集群通過選舉一台 server 來做數據分區的分配，叫作 master node</li></ul></li><li><p>數據集中集群適合數據量不大，集群機器數量不多的場景</p></li><li><p><strong>數據分散集群，由於其良好的可伸縮性，適合業務數據量巨大、集群機器數量龐大的業務場景</strong></p></li></ul><h2 id="數據分區"><a href="#數據分區" class="headerlink" title="數據分區"></a>數據分區</h2><ul><li><p>對於一些影響非常大的災難或者事故來說，有可能所有的硬體全部故障，因此需要基於地理級別的故障來設計高可用架構，這就是<strong>數據分區</strong>架構產生的背景</p></li><li><p>將數據按照一定的規則進行分區，不同分區分佈在不同的地理位置上，每個分區儲存一部分數據，通過這種方式來規避地理級別的故障所造成的巨大影響</p></li></ul><h3 id="數據量"><a href="#數據量" class="headerlink" title="數據量"></a>數據量</h3><ul><li><p><strong>數據量的大小直接決定了分區的規則複雜度</strong></p></li><li><p>數據量越大，分區規則會越複雜，考慮的情況也越多</p><ul><li>故障出現時，不容易定位</li><li>增加新機器，分區配置調整可能會發生錯誤</li><li>還要考慮地理區域級別的容災</li></ul></li></ul><h3 id="分區規則"><a href="#分區規則" class="headerlink" title="分區規則"></a>分區規則</h3><ul><li><p>地理位置有近有遠，因此可以得到不同的分區規則，包括洲際分區、國家分區、城市分區</p></li><li><p>具體採取哪種或者哪幾種規則，需要綜合考慮業務範圍、成本等因素</p></li></ul><h3 id="複製規則"><a href="#複製規則" class="headerlink" title="複製規則"></a>複製規則</h3><ul><li><p>每個分區本身的數據量雖然只是整體數據的一部分，但還是很大，這部分數據如果損壞或者丟失，損失同樣難以接受</p></li><li><p><strong>即使是分區架構，同樣需要考慮複製方案</strong></p></li></ul><h4 id="集中式"><a href="#集中式" class="headerlink" title="集中式"></a>集中式</h4><p><img src="/blog/images/architecture-design/storage-ha-replication-strategy-centric.png" alt="Storage HA - replication strategy 1"></p><ul><li><p>集中式備份指存在一個統一的備份中心，所有的分區都將數據備份到備份中心</p></li><li><p>優點：設計簡單、擴展容易</p></li><li><p>缺點：成本較高</p></li></ul><h4 id="互備式"><a href="#互備式" class="headerlink" title="互備式"></a>互備式</h4><p><img src="/blog/images/architecture-design/storage-ha-replication-strategy-viceversa.png" alt="Storage HA - replication strategy 2"></p><ul><li><p>指每個分區備份另外一個分區的數據</p></li><li><p>優點：成本低</p></li><li><p>缺點：設計比較複雜、擴展麻煩</p></li></ul><h4 id="獨立式"><a href="#獨立式" class="headerlink" title="獨立式"></a>獨立式</h4><p><img src="/blog/images/architecture-design/storage-ha-replication-strategy-standalone.png" alt="Storage HA - replication strategy 3"></p><ul><li><p>每個分區自己有獨立的備份中心，並不和原來的分區在一個地方</p></li><li><p>主要目的是規避同城或者相同地理位置同時發生災難性故障的極端情況</p></li><li><p>優點：設計簡單、擴展容易</p></li><li><p>缺點：成本高，每個分區需要獨立的備份中心</p></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>數據集群可以做到在不同節點之間複製數據，確保當一個集群斷網斷電的情況下，仍然有機房可以讀取數據</p></li><li><p>在對外提供服務時不僅僅考慮是否能讀寫數據，還需要考慮讀寫數據所需的耗時</p></li><li><p>距離過遠意味著耗時較長，如果是搭建專線，成本也會非常高，因而從<code>成本</code>和<code>用戶體驗</code>兩個緯度考量遠距離同步集群不適合直接對外提供服務</p></li><li><p>對於城市級數據集群出故障，主要還是通過短距異地(網路耗時在十毫秒級別)集群來解決，遠距離集群主要還是用於做冷備、數據離線比對等功能</p></li><li><p>數據分散集群具有均衡性，容錯性，可伸縮性特點，響應比較快</p></li><li><p>遠距離分佈的集群可有如下特點：</p><ol><li>更容易出現不可用性，具體表現在業務響應慢，數據讀/寫/複製延遲高</li><li>一致性也難保證</li><li>也難保證均衡性，容錯性，可伸縮性特點</li><li>複雜度較近距離的集群呈現指數級增長</li></ol></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - Advanced IAM</title>
      <link href="/blog/AWS/AWS-CSA-associate-Advanced-IAM/"/>
      <url>/blog/AWS/AWS-CSA-associate-Advanced-IAM/</url>
      
        <content type="html"><![CDATA[<h1 id="AWS-Directory-Service"><a href="#AWS-Directory-Service" class="headerlink" title="AWS Directory Service"></a>AWS Directory Service</h1><h2 id="什麼是-AWS-Directory-Service"><a href="#什麼是-AWS-Directory-Service" class="headerlink" title="什麼是 AWS Directory Service ?"></a>什麼是 AWS Directory Service ?</h2><p>其實這就是 AWS 提供的 AD(Active Directory) 的服務(技術的基礎為 <code>LDAP</code> + <code>DNS</code>)，使用 Microsoft 產品的管理人員幾乎都會用到的一個服務，以下是 AWS 對此服務的介紹：</p><blockquote><p>AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft AD, enables your directory-aware workloads and AWS resources to use managed Active Directory in the AWS Cloud. AWS Managed Microsoft AD is built on actual Microsoft Active Directory and does not require you to synchronize or replicate data from your existing Active Directory to the cloud. You can use standard Active Directory administration tools and take advantage of built-in Active Directory features, such as Group Policy and single sign-on (SSO). With AWS Managed Microsoft AD, you can easily join Amazon EC2 and Amazon RDS for SQL Server instances to your domain, and use AWS Enterprise IT applications such as Amazon WorkSpaces with Active Directory users and groups.</p></blockquote><p>可以歸納出以下重點：</p><ul><li><p>其實這就是讓原本地端的 Microsoft Active Directory 功能可以延伸到 AWS cloud 的一個服務 (不需要將地端資料同步到雲端)</p></li><li><p>AWS resource 的存取認證 &amp; 權限可由在地端(on-premises) 的 AD 來設定 (可以使用現存於地端 AD 的 credential)</p></li><li><p>只要加入 AD domain 的 EC2 instance(甚至是 RDS for SQL Server 也行)，就支援 SSO (single sign-on)，強大的 Group Policy 也同時可以使用</p></li><li><p>可使用原本管理 AD 的工具進行管理，也可以使用 AWS 提供的工具(例如：<a href="https://aws.amazon.com/workspaces">AWS WorkSpaces</a>)</p></li></ul><p>其他的重點觀念：</p><ul><li><p>是一個獨立在 cloud 的 directory 服務</p></li><li><p>AD 支援 Kerberos、LDAP、<a href="https://en.wikipedia.org/wiki/NT_LAN_Manager">NTLM(NT LAN Manager)</a> 認證</p></li><li><p>服務本身有 HA 的設計</p></li><li><p>AWS Directory Service 是由好幾個 managed service 所組合而成的一個 service family，包含：</p><ul><li>AWS Managed Microsoft AD</li><li>Simple AD</li><li>AD Connector</li><li><a href="https://aws.amazon.com/cloud-directory">Cloud Directory</a></li><li><a href="https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html">Cognito User Pools</a></li></ul></li><li><p><strong>上面五個服務中，前三個是 AD compatible，後面兩個則並非 AD compatible</strong></p></li></ul><h2 id="AWS-Managed-Microsoft-AD"><a href="#AWS-Managed-Microsoft-AD" class="headerlink" title="AWS Managed Microsoft AD"></a>AWS Managed Microsoft AD</h2><p><img src="/blog/images/aws/Managed-Microsoft-AD.png" alt="AWS Managed Microsoft AD"></p><ul><li><p>提供運行在 windows server 上的 DC(domain controller)</p></li><li><p>為了確保服務的 HA，會提供兩個 DC，運行在不同的 AZ 上；若是要增加 HA 或是效能，可以增加更多的 DC</p></li><li><p>在同一個 VPC 中的 application 可以連到 DC</p></li><li><p>這些 DC 都完全屬於自己的，不會與其他 AWS 使用者共用</p></li><li><p>可以透過 <a href="https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_connect_existing_infrastructure.html">AD Trust</a> 連線到地端已經存在的 AD</p></li></ul><h3 id="AD-管理權責劃分"><a href="#AD-管理權責劃分" class="headerlink" title="AD 管理權責劃分"></a>AD 管理權責劃分</h3><p>AWS Directory Service 服務看似美好，但其實不少事情是使用者自己要完成的，把這些事情搞清楚，就不會覺得服務好像少了些什麼方便性之類的：</p><h4 id="AWS-負責項目"><a href="#AWS-負責項目" class="headerlink" title="AWS 負責項目"></a>AWS 負責項目</h4><ul><li><p>multiple-AZ deployment</p></li><li><p>patch、monitor, recover</p></li><li><p>instance rotation</p></li><li><p>snapshot、restore</p></li></ul><h4 id="Customer-負責項目"><a href="#Customer-負責項目" class="headerlink" title="Customer 負責項目"></a>Customer 負責項目</h4><ul><li><p>users, groups, GPOs 相關設定</p></li><li><p>慣用工具</p></li><li><p>scale out DCs (自行根據需求決定)</p></li><li><p>是否要設定 AD Trust 功能</p></li><li><p>設定更安全的認證授權機制(例如：LDAPS)</p></li><li><p>設定 AD Federation</p></li></ul><h2 id="Simple-AD"><a href="#Simple-AD" class="headerlink" title="Simple AD"></a>Simple AD</h2><ul><li><p>是一個獨立、全託管的 directoy 服務</p></li><li><p>擁有基本的 AD 功能</p></li><li><p>支援兩種佈署規格，<code>Small</code> 適用於 500 人以下，<code>Large</code> 適用於 5000 人以下</p></li><li><p>可用來很方便管理 EC2 instance 的帳號 (for Windows workload)</p></li><li><p>若是需要 LDAP 服務的 Linux workload，也可以使用這個選項</p></li><li><p><strong>不支援 AD Trust (表示無法讓 Simple AD 加入已經存在於地端的 AD，若有這需求需要使用 AD Connector)</strong></p></li></ul><h2 id="AD-Connector"><a href="#AD-Connector" class="headerlink" title="AD Connector"></a>AD Connector</h2><ul><li><p>當要利用已經存在於地端的 AD 資料時，<code>AD Connector</code> 是最佳選擇</p></li><li><p>以 <code>Directory Gateway(Proxy)</code> 的方式運行，銜接地端 AD 資料</p></li><li><p>可讓地端使用者使用原本的登入資訊登入 AWS</p></li><li><p>可將 EC2 instance 加入現有的 AD domain</p></li><li><p>若未來有成長的需求，可以 scale out 成多個 AD connector</p></li></ul><h2 id="Cloud-Directory"><a href="#Cloud-Directory" class="headerlink" title="Cloud Directory"></a>Cloud Directory</h2><ul><li><p>專門提供給開發者，一種以 directory 結構儲存資料的服務</p></li><li><p>可以使用多階層式的方式儲存數以億計的資料 (例如：組織結構圖、課程目錄、設備註冊資訊 …. 等等)</p></li><li><p>全託管服務</p></li><li><p>跟 Microsoft AD 沒任何關係</p></li></ul><h2 id="Cognito-User-Pools"><a href="#Cognito-User-Pools" class="headerlink" title="Cognito User Pools"></a>Cognito User Pools</h2><ul><li><p>是種全託管的 user directory，可用來與 SaaS 應用整合，提供帳號服務</p></li><li><p>可提供 web or mobile 應用註冊、登入 … 等功能</p></li><li><p>可與其他社群服務(例如：FB)的帳號系統進行整合</p></li><li><p>跟 Microsoft AD 沒任何關係</p></li></ul><h1 id="IAM-Policies"><a href="#IAM-Policies" class="headerlink" title="IAM Policies"></a>IAM Policies</h1><p>要了解 IAM policy 之前，首先要了解<strong>在 AWS 中，是如何識別一種資源的</strong>，答案是透過一個稱為 <code>ARN(Amazon Resource Names)</code> 的概念</p><h2 id="ARN-Amazon-Resource-Names"><a href="#ARN-Amazon-Resource-Names" class="headerlink" title="ARN (Amazon Resource Names)"></a>ARN (Amazon Resource Names)</h2><p><img src="/blog/images/aws/IAM_ARN.png" alt="AWS ARN"></p><p>首先要搞清楚 ARN 的命名格式如下：</p><blockquote><p>arn:partition:service:region:account_id</p></blockquote><p>但上面那一段只有到 account level，如果要往下到 resource level，則必須再補上下面這一段：</p><ul><li><code>resource</code></li><li><code>resource_type/resource</code></li><li><code>resource_type/resource/qualifier</code></li><li><code>resource_type/resource:qualifier</code></li><li><code>resource_type:resource</code></li><li><code>resource_type:resource:qualifier</code></li></ul><p>透過上面 ARN 的命名方式，我們可以很精確的定位到特定的資源，以下是幾個例子：</p><ul><li><p><strong>arn:aws:iam::123456789012:user/mark</strong>：由於 IAM 屬於 global 範圍，因此 region 欄位就保留空白</p></li><li><p><strong>arn:aws:s3:::my_awesome_bucket/image.png</strong>：每個 s3 object 都是全球唯一，無關 region &amp; account，因此這兩個欄位保留空白</p></li><li><p><strong>arn:aws:dynamodb:us-east-1:123456789012:table/order</strong>：可以細到特定 table 層級</p></li><li><p><strong>arn:aws:ec2:us-east-1:123456789012:instance/*</strong>：也可以使用 <code>*</code> 來表示特定種類資源的所有項目</p></li></ul><h2 id="什麼是-IAM-Policy"><a href="#什麼是-IAM-Policy" class="headerlink" title="什麼是 IAM Policy?"></a>什麼是 IAM Policy?</h2><p>IAM policy 是個用來定義 AWS 資源使用權限的 JSON 文件，有包含以下兩種類型：</p><ul><li><p><strong>Identity policy</strong>：用來與 IAM user/group/role 綁定的權限設定</p></li><li><p><strong>Resource policy</strong>：用來與 AWS resource(例如：S3 bucket, SQS queue, KMS 加密用金鑰) 綁定的權限設定，會明確的定義出 <strong>誰可以存取資源 &amp; 可以存取的 API 有哪些</strong></p></li></ul><p>需要注意的是，IAM policy 不是設定好就會有效的，必須要與帳號 or 資源進行綁定(attach)才會有效。</p><h3 id="IAM-Policy-設定格式"><a href="#IAM-Policy-設定格式" class="headerlink" title="IAM Policy 設定格式"></a>IAM Policy 設定格式</h3><p><img src="/blog/images/aws/IAM_Policy_Foramt.png" alt="AWS policy format"></p><p>上圖展示了一個 IAM policy 設定的標準結構，大概有幾個重點：</p><ul><li><p>會帶有 <code>Version</code> 的定義，用來識別這個文件的結構，例如：<code>&quot;version&quot;: &quot;2012-10-17&quot;</code></p></li><li><p>由一連串的 <code>statement</code> 定義所組成，每個 statement 會用大括號(<code>&#123;&#125;</code>)包起來</p></li><li><p>每個 statement 都會符合一組 AWS API request 的定義，指定可以對 AWS resource 執行哪些動作</p></li></ul><p>以下是一個實際範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;Version&quot;</span>: <span class="string">&quot;2012-10-17&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Statement&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">&quot;Sid&quot;</span>: <span class="string">&quot;SpecificTable&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Allow&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;Action&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;dynamodb:BatchGet*&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:DescribeStream&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:DescribeTable&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:Get*&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:Query&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:Scan&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:BatchWrite*&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:CreateTable&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:Delete*&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:Update*&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dynamodb:PutItem&quot;</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;arn:aws:dynamodb:*:*:table/MyTable&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有幾項重點需要知道：</p><ul><li><p><code>Effect</code> 可以是 <strong>Allow</strong> or <strong>Deny</strong></p></li><li><p>API 存取的限制範圍就是定義在 <code>Action</code> 中，可以用 wildcard(<code>*</code>) 來表示具有同樣 prefix 的 API</p></li><li><p><code>Resource</code> 欄位則是指定這個 statement 會對哪個 ARN 生效</p></li></ul><h2 id="Permission-Boundry"><a href="#Permission-Boundry" class="headerlink" title="Permission Boundry"></a>Permission Boundry</h2><p>Permission Boundry 是用來限制使用者存取權限的範圍在那，舉例來說，假設希望達成以下目的：</p><ul><li><p>管理者賦予 Leon <code>AdministratorAccess</code> 的權限</p></li><li><p>但卻希望這個權限僅對 DynamoDB 有效</p></li></ul><p>透過設定 Permission Boundry，加入 <code>AmazonDynamoDBFullAccess</code>，就可以限定 Leon 的權限範圍只會在 DynamoDB 中，而不會有管理者的完整權限。</p><blockquote><p>需要注意的是，Permission Boundry 並不是拿來設定特定 role 允許或拒絕存取特定 resource 之用</p></blockquote><h2 id="實作注意事項"><a href="#實作注意事項" class="headerlink" title="實作注意事項"></a>實作注意事項</h2><ul><li><p>policy type 有 <code>AWS managed</code>(前面會有一個橘色盒子圖案，是 AWS 預先設定好一些常用的 policy 方便使用者直接取用) &amp; <code>Customer managed</code> 兩種</p></li><li><p>在 statement 中，不同的 API 會對應到不同的 ARN 設定，例如：<code>s3:ListBucket</code> 就會用在 bucket level ARN；<code>s3:GetObject</code> 就會用在 object level ARN</p></li><li><p>要將 policy 跟某個 entity(例如：Role) 綁定才會有效；還需要將這樣的綁定組合指定給特定的 AWS resource(例如：EC2 instance)</p></li><li><p>透過 <strong>inline policy</strong>，可以讓 policy 只對特定 role 有效；無法用在其他的 role 上</p></li></ul><h2 id="考試重點"><a href="#考試重點" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>IAM policy 中，對任何 resoure 的存取預設都是拒絕，若要開放，就必須明確設定</p></li><li><p><strong>如果明確的設定拒絕存取，那就是完全拒絕存取，優先權大於任何開放存取的相關權限，即使有綁定開放權限也沒用</strong></p></li><li><p>policy 要生效，就需要做好正確的綁定(with user/group/role)</p></li><li><p>有多個 policy 同時與特定的 entity 或是 resource 綁定時，結果會是 join 後的效果</p></li></ul><h1 id="Resource-Access-Manager-RAM"><a href="#Resource-Access-Manager-RAM" class="headerlink" title="Resource Access Manager (RAM)"></a>Resource Access Manager (RAM)</h1><ul><li><p>在 AWS multiple account 的管理中，一般會將管理、帳務、功能性 …. 等帳號根據不同職能拆分開，但若是有多個帳號需要共享 resource 的存取權限時，就正好是 RAM 可以協助的地方</p></li><li><p>不論是獨立的帳號，或是 organization，都可以透過 RAM 的設定，在不同的帳號間設定資源共享</p><blockquote><p>試想看看如果你要將同樣的 resource 權限設定給 100 個不同的帳號，沒有個簡單的方式可能會很辛苦阿…..</p></blockquote></li><li><p><strong>並非所有 resource 的權限都可以透過 RAM 在不同的帳號間共享</strong>，目前支援的有 <code>App Mesh</code>、<code>Aurora</code>、<code>CodeBuild</code>、<code>EC2</code>、<code>EC2 Image Builder</code>、<code>License Manager</code>、<code>Resource Group</code>、<code>Route 53</code></p></li></ul><h2 id="實作重點"><a href="#實作重點" class="headerlink" title="實作重點"></a>實作重點</h2><ul><li>當 account1 分享資源給 account2 之後，account2 需要到 RAM 管理畫面中 approve 來自於 account1 的分享，才可以在 console 畫面中看到被分享出來的資源</li></ul><p><img src="/blog/images/aws/RAM_share-resource-between-accounts.png" alt="AWS RAM share resource between accounts"></p><ul><li><p>範例中 account1 將 private subnet 分享給 account2</p></li><li><p>account2 可以在 private subnet 中建立 EC2 instance，藉此取得與內部其他資源的聯繫</p></li><li><p>account2 無法修改 account1 分享出來的 subnet，只能加 tag</p></li></ul><h1 id="AWS-Single-Sign-On"><a href="#AWS-Single-Sign-On" class="headerlink" title="AWS Single Sign-On"></a>AWS Single Sign-On</h1><p><img src="/blog/images/aws/SSO_SAML-integration.png" alt="AWS SSO SAML integration"></p><ul><li><p>AWS SSO 可以整合很多外部服務的現存帳號，提供使用者 SSO 的功能</p></li><li><p>成功連結到 AWS SSO 的帳號，就可以給定不同資源的存取權限</p></li><li><p>只要是支援 SAML 2.0 的協定，都可以與 AWS SSO 進行整合來提供帳號認證的服務</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/cloud-directory">Amazon Cloud Directory | Amazon Web Services (AWS)</a></p></li><li><p><a href="https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html">Amazon Cognito User Pools - Amazon Cognito</a></p></li><li><p><a href="https://docs.aws.amazon.com/iam/index.html">AWS Identity and Access Management Documentation</a></p></li><li><p><a href="https://aws.amazon.com/ram/">AWS Resource Access Manager</a></p></li><li><p><a href="https://aws.amazon.com/single-sign-on/">AWS Single Sign-On | Cloud SSO Service | AWS</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> IAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 分散式架構設計中的 CAP 理論</title>
      <link href="/blog/Architecture_Design/Architecture-Design-HA-CAP/"/>
      <url>/blog/Architecture_Design/Architecture-Design-HA-CAP/</url>
      
        <content type="html"><![CDATA[<h1 id="CAP-理論"><a href="#CAP-理論" class="headerlink" title="CAP 理論"></a>CAP 理論</h1><h2 id="CAP-理論演進"><a href="#CAP-理論演進" class="headerlink" title="CAP 理論演進"></a>CAP 理論演進</h2><h3 id="version-1"><a href="#version-1" class="headerlink" title="version 1"></a>version 1</h3><p>Robert Greiner 對於 CAP 理論的<a href="http://robertgreiner.com/2014/06/cap-theorem-explained/">第一版解釋</a>：</p><blockquote><p>Any distributed system cannot guaranty C, A, and P simultaneously.</p></blockquote><p>對於一個分佈式計算系統，不可能同時滿足一致性（Consistence）、可用性（Availability）、分區容錯性（Partition Tolerance）三個設計約束。</p><h3 id="version-2"><a href="#version-2" class="headerlink" title="version 2"></a>version 2</h3><p>Robert Greiner 對於 CAP 理論的<a href="http://robertgreiner.com/2014/08/cap-theorem-revisited/">第二版解釋</a>：</p><blockquote><p>In a distributed system (a collection of interconnected nodes that share data.), you can only have two out of the following three guarantees across a write/read pair: Consistency, Availability, and Partition Tolerance - one of them must be sacrificed.</p></blockquote><p>在一個分散式系統（指互相連接並共享資料的節點的集合）中，當涉及讀寫操作時，只能保證一致性（Consistence）、可用性（Availability）、分區容錯性（Partition Tolerance）三者中的兩個，另外一個必須被犧牲。</p><h3 id="差異之處"><a href="#差異之處" class="headerlink" title="差異之處"></a>差異之處</h3><ul><li><p>CAP 理論探討的分散式系統，強調了兩點：<strong>interconnected</strong> 和 <strong>share data</strong></p></li><li><p>分散式系統並不一定會互聯和共享資料</p><blockquote><p>Memcache 的集群，相互之間就沒有連接和共享資料，因此 Memcache 集群這類分散式系統就不符合 CAP 理論探討的對象；而 MySQL 集群就是互聯和進行資料複製的，因此是 CAP 理論探討的對象</p></blockquote></li><li><p>CAP 關注的是<strong>對資料的讀寫操作</strong>，而不是分散式系統的所有功能，例如：ZooKeeper 的選舉機制就不是 CAP 探討的對象</p></li></ul><h2 id="CAP-內容"><a href="#CAP-內容" class="headerlink" title="CAP 內容"></a>CAP 內容</h2><h3 id="Consistency-一致性"><a href="#Consistency-一致性" class="headerlink" title="Consistency (一致性)"></a>Consistency (一致性)</h3><ul><li><p>站在 client 的角度來觀察系統的行為和特徵</p></li><li><p>從 client 的讀寫角度來描述資料一致性</p></li><li><p>在事務執行過程中，系統其實處於一個不一致的狀態，不同的節點的資料並不完全一致，<strong>但 client 讀操作能夠獲取最新的寫結果就沒有問題</strong></p></li></ul><h3 id="Availability-可用性"><a href="#Availability-可用性" class="headerlink" title="Availability (可用性)"></a>Availability (可用性)</h3><ul><li><p>只有非故障節點才能滿足可用性要求，如果節點本身就故障了，發給節點的請求不一定能得到一個響應</p></li><li><p>不能超時、不能出錯，結果是合理的(但結果有可能因為資料尚未同步完整，導致資料是不正確的)</p></li></ul><h3 id="Partition-Tolerance-分區容忍性"><a href="#Partition-Tolerance-分區容忍性" class="headerlink" title="Partition Tolerance (分區容忍性)"></a>Partition Tolerance (分區容忍性)</h3><ul><li><p>只有返回 reasonable response 才是 function(可用功能)，而 function 強調”發揮作用” &amp; “履行職責”</p></li><li><p>即發生了分區現象，不管是什麼原因，可能是網路封包遺失，也可能是連接中斷，還可能是擁塞，只要導致了網路分區，就通通算在裡面</p></li></ul><h2 id="CAP-應用"><a href="#CAP-應用" class="headerlink" title="CAP 應用"></a>CAP 應用</h2><ul><li><p>雖然 CAP 理論定義是三個要素中只能取兩個，但放到分佈式環境下來思考，會發現<strong>必須選擇 P(分區容忍)要素，因為網路本身無法做到 100% 可靠</strong></p></li><li><p>分散式系統理論上不可能選擇 CA 架構，<strong>只能選擇 CP 或者 AP 架構</strong></p></li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>「一致性」和「可用性」都應該站在 client 側去審視；而「分區容忍性」則是 cluster 在遇到網路分區的問題時，選擇如何去影響 client 感知到的「一致性」和「可用性」</p></li><li><p>P 要求分佈式和資料同步，C 要求資料完全一致，A 要求返回及時</p></li><li><p>CAP 理論是忽略延時的，這就是說理論做了一個假設，<strong>只要網路通資料就會一致</strong>，這也是實際應用CAP時容易踩的坑</p></li><li><p>paxos 協議是為了解決資料一致性而設計的算法，主要是通過投票選舉的方式決定出主節點，之後就以主節點的資料為準，因而屬於 PC 模式</p></li><li><p>不是必然選擇 P，而是說分散式系統必然會出現分區的情況，你要選擇的是要不要”容忍”，<strong>無法容忍分區，那就在出現分區的時候系統不對外提供服務</strong></p></li><li><p>P 是說是否允許網路分區，允許的話可以繼續提供服務，不允許的話，一旦出現分區，系統所有節點停止服務</p></li><li><p>一個分散式系統當P發生時，C 和 A 二者只能選其一</p><ul><li>P：分區容錯性，是站在服務器的角度來講的</li><li>C：資料一致性，是站在 client 的角度來講的</li><li>A：系統可用性，是站在 client 的角度來講的</li></ul></li></ul><blockquote><p>在通過網路構建的分散式系統中，P 是不能 100% 避免的，所以，我們只能考慮，當 P 發生時是要 C 還是要 A</p></blockquote><ul><li><p>區塊鏈不要求所有人同一時刻看到的資料一致，而且某個節點掛掉後其它節點還是可以修改資料，因此是 AP 系統</p></li><li><p>CAP 要求節點間針對同一份資料進行複製和備份</p></li><li><p>網路分區 =&gt; 假設本來5台機器網路都是通的，現在由於交換機故障，其中3台聯通形成小團體A，另外兩台聯通形成小團體B，但是 A 和 B 不聯通</p></li></ul><h1 id="CAP-細節"><a href="#CAP-細節" class="headerlink" title="CAP 細節"></a>CAP 細節</h1><ul><li><p>理論的優點在於清晰簡潔、易於理解，但缺點就是高度抽象化，省略了很多細節，導致在將理論應用到實踐時，由於各種複雜情況，可能出現誤解和偏差</p></li><li><p>談到資料一致性時，CAP、ACID、BASE 難免會被拿出來討論，原因在於這三者都是和資料一致性相關的理論</p></li></ul><h2 id="CAP-關鍵細節點"><a href="#CAP-關鍵細節點" class="headerlink" title="CAP 關鍵細節點"></a>CAP 關鍵細節點</h2><h3 id="CAP-關注的粒度是資料，而不是整個系統"><a href="#CAP-關注的粒度是資料，而不是整個系統" class="headerlink" title="CAP 關注的粒度是資料，而不是整個系統"></a>CAP 關注的粒度是資料，而不是整個系統</h3><ul><li><p>在 CAP 理論落地實踐時，我們<strong>需要將系統內的資料按照不同的應用場景和要求進行分類</strong>，每類資料選擇不同的策略(CP 還是 AP)，而不是直接限定整個系統所有資料都是同一策略</p></li><li><p><strong>實際設計過程中，每個系統不可能只處理一種資料，而是包含多種類型的資料，有的資料必須選擇 CP，有的資料必須選擇 AP</strong></p></li><li><p>如果我們做設計時，從整個系統的角度去選擇 CP 還是 AP，就會發現顧此失彼，無論怎麼做都是有問題的</p></li></ul><blockquote><p>用戶管理系統為例，用戶管理系統包含用戶賬號資料（用戶 ID、密碼）、用戶信息資料（暱稱、興趣、愛好、性別、自我介紹等），前者會選擇 CP，後者會選擇 AP；但所有資料都選擇 CP or AP 都是不合理的</p></blockquote><h3 id="CAP-是忽略網路延遲的"><a href="#CAP-是忽略網路延遲的" class="headerlink" title="CAP 是忽略網路延遲的"></a>CAP 是忽略網路延遲的</h3><ul><li><p>CAP 理論中的 C 在實踐中是不可能完美實現的，在資料複製的過程中，節點 A 和節點 B 的資料並不一致</p></li><li><p>對於某些嚴苛的業務場景，例如和金錢相關的用戶餘額，或者和搶購相關的商品庫存，技術上是無法做到分佈式場景下完美的一致性的</p></li><li><p>這並不意味著這類系統無法應用分佈式架構，只是說”單個用戶餘額、單個商品庫存”無法做分佈式，但系統整體還是可以應用分佈式架構的</p></li></ul><h3 id="正常運行情況下，不存在-CP-和-AP-的選擇，可以同時滿足-CA"><a href="#正常運行情況下，不存在-CP-和-AP-的選擇，可以同時滿足-CA" class="headerlink" title="正常運行情況下，不存在 CP 和 AP 的選擇，可以同時滿足 CA"></a>正常運行情況下，不存在 CP 和 AP 的選擇，可以同時滿足 CA</h3><ul><li><p><strong>架構設計的時候既要考慮分區發生時選擇 CP 還是 AP，也要考慮分區沒有發生時如何保證 CA</strong></p></li><li><p>即使是實現 CA，不同的資料實現方式也可能不一樣 (例如：用戶帳號資料可用 Message Queue 來實現，用戶消息資料可用 DB 同步來實現)</p></li></ul><h3 id="放棄並不等於什麼都不做，需要為分區恢復後做準備"><a href="#放棄並不等於什麼都不做，需要為分區恢復後做準備" class="headerlink" title="放棄並不等於什麼都不做，需要為分區恢復後做準備"></a>放棄並不等於什麼都不做，需要為分區恢復後做準備</h3><ul><li><p>分區期間放棄 C 或者 A，並不意味著永遠放棄 C 和 A，我們可以在分區期間進行一些操作，從而<strong>讓分區故障解決後，系統能夠重新達到 CA 的狀態</strong></p></li><li><p>分區恢復後的資料不一致，可以是軟體 or 工具自動處理，或是人工處理</p></li></ul><h2 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>ACID 是資料庫管理系統為了保證事務的正確性而提出來的一個理論，包含四個限制：</p><ul><li><p><code>Atomicity</code>：一個事務中的所有操作，要嘛全部完成，要嘛全部都沒完成</p></li><li><p><code>Consistency</code>：在事務開始之前和事務結束以後，資料庫的完整性沒有被破壞</p></li><li><p><code>Isolation</code>：防止多個事務並發執行時由於交叉執行而導致資料的不一致</p></li><li><p><code>Durability</code>事務處理結束後，對資料的修改就是永久的，即便系統故障也不會丟失</p></li></ul><h3 id="與-CAP-的差異"><a href="#與-CAP-的差異" class="headerlink" title="與 CAP 的差異"></a>與 CAP 的差異</h3><ul><li><p>ACID 中的 A（Atomicity）和 CAP 中的 A（Availability）意義完全不同</p></li><li><p>ACID 中的 C 是指資料庫的資料完整性，而 CAP 中的 C 是指分佈式節點中的資料一致性</p></li><li><p>ACID 的應用場景是資料庫事務，CAP 關注的是分散式系統資料讀寫這個差異點來看</p></li></ul><h2 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h2><ul><li><p>核心思想是即使無法做到強一致性（CAP 的一致性就是強一致性），但應用可以採用適合的方式達到最終一致性</p></li><li><p>BASE 理論本質上是對 CAP 的延伸和補充</p></li></ul><h3 id="基本可用（Basically-Available）"><a href="#基本可用（Basically-Available）" class="headerlink" title="基本可用（Basically Available）"></a>基本可用（Basically Available）</h3><ul><li><p>分散式系統在出現故障時，允許損失部分可用性，即保證核心可用</p></li><li><p>具體選擇哪些作為可以損失的業務，哪些是必須保證的業務，是一項有挑戰的工作</p><blockquote><p>例如，對於一個用戶管理系統來說，”登錄”是核心功能，而”註冊”可以算作非核心功能</p></blockquote></li></ul><h3 id="軟狀態（Soft-State）"><a href="#軟狀態（Soft-State）" class="headerlink" title="軟狀態（Soft State）"></a>軟狀態（Soft State）</h3><ul><li><p>允許系統存在中間狀態，而該中間狀態不會影響系統整體可用性</p></li><li><p>就是 CAP 理論中的資料不一致</p></li></ul><h3 id="最終一致性（Eventual-Consistency）"><a href="#最終一致性（Eventual-Consistency）" class="headerlink" title="最終一致性（Eventual Consistency）"></a>最終一致性（Eventual Consistency）</h3><ul><li><p>系統中的所有資料副本經過一定時間後，最終能夠達到一致的狀態</p></li><li><p>一定時間”和資料的特性是強關聯的，不同的資料能夠容忍的不一致時間是不同的</p></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>ACID 是資料庫事務完整性的理論，CAP 是分散式系統設計理論，BASE 是 CAP 理論中 AP 方案的延伸</p></li><li><p>一份資料在多個節點有但不是所有節點都有，這是非對稱集群；例如 Elasticsearch；所有資料在所有節點都有，這是對稱集群，例如 zookeeper</p></li><li><p>任何一個正常運行的分散式系統，起源於 CA 狀態，中間(發生分區時)可能經過 CP 和 AP 狀態，最後回到 CA 狀態；所以一個分散式系統，需要考慮實現三點：</p><ul><li>正常運行時的 CA 狀態。</li><li>發生分區時轉變為 CP 或 AP 狀態</li><li>分區解決時如何恢復為 CA 狀態</li></ul></li></ul><h3 id="重點整理"><a href="#重點整理" class="headerlink" title="重點整理"></a>重點整理</h3><p>設計分散式系統的兩大初衷：橫向擴展（scalability）和高可用性（availability）</p><p>“橫向擴展”是為了解決單點瓶頸問題，進而保證高並發量下的「可用性」；”高可用性”是為了解決單點故障（SPOF）問題，進而保證部分節點故障時的「可用性」。由此可以看出，分散式系統的核心訴求就是「可用性」。這個「可用性」正是 CAP 中的 A：用戶訪問系統時，可以在合理的時間內得到合理的響應。</p><p>為了保證「可用性」，一個分散式系統通常由多個節點組成。這些節點各自維護一份資料，但是不管用戶訪問到哪個節點，原則上都應該讀取到相同的資料。為了達到這個效果，一個節點收到寫入請求更新自己的資料後，必須將資料同步到其他節點，以保證各個節點的資料「一致性」。這個「一致性」正是 CAP 中的 C：用戶訪問系統時，可以讀取到最近寫入的資料。</p><p>需要注意的是：CAP 並沒有考慮資料同步的耗時，所以現實中的分散式系統，理論上無法保證任何時刻的絕對「一致性」；不同業務系統對上述耗時的敏感度不同。</p><p>分散式系統中，節點之間的資料同步是基於網路的。由於網路本身固有的不可靠屬性，極端情況下會出現網路不可用的情況，進而將網路兩端的節點孤立開來，這就是所謂的”網路分區”現象。”網路分區”理論上是無法避免的，雖然實際發生的概率較低、時長較短。沒有發生”網路分區”時，系統可以做到同時保證「一致性」和「可用性」。</p><h3 id="電商-CAP-分析-1"><a href="#電商-CAP-分析-1" class="headerlink" title="電商 CAP 分析 (1)"></a>電商 CAP 分析 (1)</h3><ul><li><p>一個電商網站核心模組有會員，訂單，商品，支付，促銷管理等</p></li><li><p>對於會員模組，包括登錄，個人設置，個人訂單，購物車，收藏夾等，這些模組保證 AP，資料短時間不一致不影響使用</p></li><li><p>訂單模組的下單付款扣減庫存操作是整個系統的核心，CA 都需要保證，但在極端情況下犧牲 P 是可以的</p></li><li><p>商品模組的商品上下架和庫存管理保證 CP,搜索功能因為本身就不是實時性非常高的模組，所以保證 AP 就可以了</p></li><li><p>促銷是短時間的資料不一致，結果就是優惠信息看不到，但是已有的優惠要保證可用，而且優惠可以提前預計算，所以可以保證 AP</p></li><li><p>現在大部分的電商網站對於支付這一塊是獨立的系統，或者使用第三方的支付寶，微信。其實 CAP 是由第三方來保證的，支付系統是一個對 CAP 要求極高的系統，C 是必須要保證的，AP 中 A 相對更重要，不能因為分區，導致所有人都不能支付</p></li></ul><h3 id="電商-CAP-分析-2"><a href="#電商-CAP-分析-2" class="headerlink" title="電商 CAP 分析 (2)"></a>電商 CAP 分析 (2)</h3><ul><li><p>電商網站核心功能有用戶、產品、訂單、支付、庫存，相應的資料有用戶、產品、訂單、支付、庫存</p></li><li><p>對於用戶資料，選擇 CP；因為用戶註冊後，可能幾分鐘後重新登錄，所以需要滿足一致性；在網路出現分區時，因為需要滿足一致性而暫時不能提供寫服務，所以無法滿足可用性；對於分區容錯性，只要能返回一個合理的響應就能滿足，這一點能很好滿足</p></li><li><p>對於產品資料，無需滿足一致性，所以選擇 AP</p></li><li><p>對於訂單資料，業務需要滿足一致性，所以選擇 CP</p></li><li><p>對於支付資料，業務需要滿足一致性，所以選擇 CP</p></li><li><p>對於庫存資料，業務需要滿足一致性，所以選擇 CP</p></li></ul><h1 id="FMEA-方法，排除架構可用性隱患的利器"><a href="#FMEA-方法，排除架構可用性隱患的利器" class="headerlink" title="FMEA 方法，排除架構可用性隱患的利器"></a>FMEA 方法，排除架構可用性隱患的利器</h1><h2 id="FMEA-介紹"><a href="#FMEA-介紹" class="headerlink" title="FMEA 介紹"></a>FMEA 介紹</h2><ul><li><p>FMEA（Failure mode and effects analysis，故障模式與影響分析）通過對系統範圍內潛在的故障模式加以分析，並按照嚴重程度進行分類，以確定失效對於系統的最終影響</p></li><li><p>FMEA 是一套分析和思考的方法，而不是某個領域的技能或者工具</p></li><li><p>在軟體架構設計領域，FMEA 並不能指導我們如何做架構設計，而是當我們設計出一個架構後，再使用 FMEA 對這個架構進行分析，看看架構是否還存在某些可用性的隱患</p></li></ul><h2 id="FMEA-方法"><a href="#FMEA-方法" class="headerlink" title="FMEA 方法"></a>FMEA 方法</h2><p>在架構設計領域，FMEA 的具體分析方法是：</p><ul><li>給出初始的架構設計圖。</li><li>假設架構中某個部件發生故障</li><li>分析此故障對系統功能造成的影響</li><li>根據分析結果，判斷架構是否需要進行優化</li></ul><h3 id="功能點"><a href="#功能點" class="headerlink" title="功能點"></a>功能點</h3><p>指的是從用戶角度來看的，而不是從系統各個模組功能點劃分來看的</p><h3 id="故障模式"><a href="#故障模式" class="headerlink" title="故障模式"></a>故障模式</h3><ul><li><p>指的是系統會出現什麼樣的故障，包括故障點和故障形式</p></li><li><p>不需要給出真正的故障原因，我們只需要假設出現某種故障現象即可，例如 MySQL 響應時間達到 3 秒</p></li><li><p>故障模式的描述要儘量精確，多使用量化描述，避免使用泛化的描述</p></li></ul><h3 id="故障影響"><a href="#故障影響" class="headerlink" title="故障影響"></a>故障影響</h3><ul><li><p>當發生故障模式中描述的故障時，功能點具體會受到什麼影響</p></li><li><p>常見的影響有：功能點偶爾不可用、功能點完全不可用、部分用戶功能點不可用、功能點響應緩慢、功能點出錯等</p></li><li><p>故障影響也需要儘量準確描述，例如，推薦使用”20% 的用戶無法登錄”，而不是”大部分用戶無法登錄”</p></li></ul><h3 id="嚴重程度"><a href="#嚴重程度" class="headerlink" title="嚴重程度"></a>嚴重程度</h3><ul><li><p>指站在業務的角度故障的影響程度，一般分為”致命 / 高 / 中 / 低 / 無”五個檔次</p></li><li><p>按公式進行評估：<strong>嚴重程度 = 功能點重要程度 × 故障影響範圍 × 功能點受損程度</strong></p></li><li><p>對於某個故障的影響到底屬於哪個檔次，有時會出現一些爭議，沒有絕對標準</p></li></ul><h3 id="故障原因"><a href="#故障原因" class="headerlink" title="故障原因"></a>故障原因</h3><p>為何要列出故障原因?</p><ul><li><p>不同的故障原因發生概率不相同</p><blockquote><p>例如：導致 MySQL 查詢響應慢的原因可能是 MySQL bug，也可能是沒有索引</p></blockquote></li><li><p>不同的故障原因檢測手段不一樣</p><blockquote><p>例如：磁盤壞道 &amp; slow query 都會導致 MySQL 響應慢，但檢測方式就不會相同</p></blockquote></li><li><p>不同的故障原因的處理措施不一樣</p><blockquote><p>例如：如果是 MySQL bug，我們的應對措施只能是升級 MySQL 版本；如果是沒有索引，我們的應對措施就是增加索引</p></blockquote></li></ul><h3 id="故障概率"><a href="#故障概率" class="headerlink" title="故障概率"></a>故障概率</h3><ul><li><p>指某個具體故障原因發生的概率，一般分為”高 / 中 / 低”</p></li><li><p>需要關注的重點：</p><ul><li>硬體：隨著使用時間推移，故障概率會越來越高</li><li>開源系統：成熟的開源系統 bug 率低，剛發佈的開源系統 bug 率相比會高一些</li><li>自研系統：成熟的自研系統故障概率會低，而新開發的系統故障概率會高</li></ul></li><li><p>高中低是相對的，只是為了確定優先級以決定後續的資源投入，沒有必要絕對量化</p></li></ul><h3 id="風險程度"><a href="#風險程度" class="headerlink" title="風險程度"></a>風險程度</h3><ul><li><p>風險程度就是綜合嚴重程度和故障概率來一起判斷某個故障的最終等級</p></li><li><p>風險程度 = 嚴重程度 × 故障概率</p></li><li><p>同樣的故障影響，不同的故障原因有不同的概率，最終得到的風險級別就是不同的</p></li></ul><h3 id="已有措施"><a href="#已有措施" class="headerlink" title="已有措施"></a>已有措施</h3><p>針對具體的故障原因，系統現在是否提供了某些措施來應對，包括：</p><ul><li>檢測告警：檢測故障，然後告警，系統自己不針對故障進行處理，需要人工干預</li><li>容錯：檢測到故障後，系統能夠通過備份手段應對</li><li>自恢復：檢測到故障後，系統能夠自己恢復</li></ul><h3 id="規避措施"><a href="#規避措施" class="headerlink" title="規避措施"></a>規避措施</h3><p>為了降低故障發生概率而做的一些事情，可以是技術手段，也可以是管理手段，例如：</p><ul><li><p>技術手段：為了避免新引入的 MongoDB 丟失資料，在 MySQL 中冗餘一份</p></li><li><p>管理手段：為了降低磁盤壞道的概率，強制統一更換服務時間超過 2 年的磁盤</p></li></ul><h3 id="解決措施"><a href="#解決措施" class="headerlink" title="解決措施"></a>解決措施</h3><ul><li><p>為了能夠解決問題而做的一些事情，一般都是技術手段，例如：</p><ul><li>為了解決密碼暴力破解，增加密碼重試次數限制</li><li>為了解決從資料庫中導出資料導致資料洩露，將資料庫中的敏感資料加密保存</li><li>為了解決非法訪問，增加白名單控制</li></ul></li><li><p>如果某個故障既可以採取規避措施，又可以採取解決措施，那麼我們會優先選擇解決措施，畢竟能解決問題當然是最好的</p></li></ul><h3 id="後續規劃"><a href="#後續規劃" class="headerlink" title="後續規劃"></a>後續規劃</h3><ul><li><p>針對這些不足的地方，再結合風險程度進行排序，給出後續的改進規劃</p></li><li><p>這些規劃既可以是技術手段，也可以是管理手段；可以是規避措施，也可以是解決措施。</p></li><li><p>同時需要考慮資源的投入情況，優先將風險程度高的系統隱患解決</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 高性能負載均衡</title>
      <link href="/blog/Architecture_Design/Architecture-Design-High-Performance-Load-Balance/"/>
      <url>/blog/Architecture_Design/Architecture-Design-High-Performance-Load-Balance/</url>
      
        <content type="html"><![CDATA[<h1 id="高性能負載均衡：分類及架構"><a href="#高性能負載均衡：分類及架構" class="headerlink" title="高性能負載均衡：分類及架構"></a>高性能負載均衡：分類及架構</h1><ul><li><p>single server 無論如何優化，無論採用多好的硬體，總會有一個性能天花板</p></li><li><p>高性能 cluster 的本質很簡單，通過增加更多的 server 來提升系統整體的計算能力</p></li><li><p>計算本身存在一個特點：同樣的輸入資料和邏輯，無論在那台 server 上執行，都應該得到相同的輸出</p></li><li><p>高性能 cluster 設計的複雜度主要體現在<strong>任務分配</strong>這部分</p></li><li><p>高性能 cluster 的複雜性主要體現在<strong>需要增加一個任務分配器</strong>，以及<strong>為任務選擇一個合適的任務分配算法</strong></p></li><li><p>實際上任務分配並不只是考慮計算單元的負載均衡，不同的任務分配算法目標是不一樣的</p><blockquote><p>有的基於負載考慮，有的基於性能（吞吐量、響應時間）考慮，有的基於業務考慮</p></blockquote></li></ul><h2 id="負載均衡分類"><a href="#負載均衡分類" class="headerlink" title="負載均衡分類"></a>負載均衡分類</h2><h3 id="DNS-負載均衡"><a href="#DNS-負載均衡" class="headerlink" title="DNS 負載均衡"></a>DNS 負載均衡</h3><ul><li>DNS 是最簡單也是最常見的負載均衡方式，一般用來實現地理級別的均衡</li></ul><p><img src="/blog/images/architecture-design/load-balance-by-dns.png" alt="Load Balance by DNS"></p><ul><li><p>優點：</p><ul><li>簡單、成本低</li><li>就近訪問，提升訪問速度</li></ul></li><li><p>缺點：</p><ul><li>更新不即時：DNS cache</li><li>擴展性差：DNS 負載均衡的控制權在域名商那裡，無法根據業務特點針對其做更多的定製化功能和擴展特性</li><li>分配策略比較簡單：DNS 負載均衡支持的算法少；不能區分 server 的差異(不能根據系統與服務的狀態來判斷負載)；也無法感知後端 server 的狀態</li></ul></li><li><p>針對 DNS 負載均衡的一些缺點，對於時延和故障敏感的業務，有一些公司自己實現了 HTTP-DNS 的功能</p></li></ul><h3 id="硬體負載均衡"><a href="#硬體負載均衡" class="headerlink" title="硬體負載均衡"></a>硬體負載均衡</h3><ul><li><p>硬體負載均衡是通過單獨的硬體設備來實現負載均衡功能</p></li><li><p>這類設備性能強勁、功能強大，但價格都不便宜</p></li><li><p>優點：</p><ul><li>功能強大：全面支持各層級的負載均衡，支持全面的負載均衡算法，支持全局負載均衡</li><li>性能強大：對比一下，軟體負載均衡支持到 10 萬級並發已經很厲害了，硬體負載均衡可以支持 100 萬以上的並發</li><li>穩定性高：商用硬體負載均衡，經過了良好的嚴格測試，經過大規模使用，穩定性高。</li><li>支持安全防護：硬體均衡設備除具備負載均衡功能外，還具備防火牆、防 DDoS 攻擊等安全功能。</li></ul></li><li><p>缺點：</p><ul><li>價格昂貴</li><li>擴展能力差：硬體設備，可以根據業務進行配置，但無法進行擴展和定製</li></ul></li></ul><h3 id="軟體負載均衡"><a href="#軟體負載均衡" class="headerlink" title="軟體負載均衡"></a>軟體負載均衡</h3><ul><li><p>軟體負載均衡的最大優勢是<strong>便宜</strong></p></li><li><p>常見的有 Nginx 和 LVS，其中 Nginx 是軟體的 7 層負載均衡，LVS 是 Linux 內核的 4 層負載均衡</p></li><li><p>4 層和 7 層的區別就在於協議和靈活性，Nginx 支持 HTTP、E-mail 協議；而 LVS 是 4 層負載均衡，和協議無關，幾乎所有應用都可以做，例如，聊天、資料庫等</p></li><li><p>軟體和硬體的最主要區別就在於<strong>性能</strong>，硬體負載均衡性能遠遠高於軟體負載均衡性能</p></li><li><p>一般的 Linux  server 上裝一個 Nginx 大概能到 50,000/秒；LVS 的性能是十萬級，據說可達到 800,000/秒；而 F5 性能是百萬級，從 2,000,000/秒到 8,000,000/ 秒都有</p></li></ul><p><img src="/blog/images/architecture-design/load-balance-by-software.png" alt="Load Balance by software(Nginx)"></p><ul><li><p>軟體負載均衡的優點：</p><ul><li>簡單：無論是部署還是維護都比較簡單</li><li>便宜：只要買個 Linux  server ，裝上軟體即可</li><li>靈活：4 層和 7 層負載均衡可以根據業務進行選擇；也可以根據業務進行比較方便的擴展，例如，可以通過 Nginx 的 plugin 來實現業務的定製化功能</li></ul></li><li><p>缺點的部份都是和硬體負載均衡相比的，並不是說軟體負載均衡沒法用：</p><ul><li>性能一般：一個 Nginx 大約能支撐 50,000 並發。</li><li>功能沒有硬體負載均衡那麼強大</li><li>一般不具備防火牆和防 DDoS 攻擊等安全功能</li></ul></li></ul><h2 id="負載均衡典型架構"><a href="#負載均衡典型架構" class="headerlink" title="負載均衡典型架構"></a>負載均衡典型架構</h2><ul><li><p>DNS 負載均衡、硬體負載均衡、軟體負載均衡，每種方式都有一些優缺點，但並不意味著在實際應用中只能基於它們的優缺點進行非此即彼的選擇，反而是基於它們的優缺點進行組合使用</p></li><li><p>組合的基本原則為：</p><ul><li>DNS 負載均衡用於實現地理級別的負載均衡</li><li>硬體負載均衡用於實現 cluster 級別的負載均衡</li><li>軟體負載均衡用於實現機器級別的負載均衡</li></ul></li></ul><p><img src="/blog/images/architecture-design/load-balance-composition.png" alt="Load Balance composition"></p><ul><li>整個系統的負載均衡分為三層。<ul><li>地理級別負載均衡：<a href="http://www.xxx.com/">www.xxx.com</a> 部署在所屬地理區域不同的三個機房</li><li>cluster 級別負載均衡：機房 A 的負載均衡用的是 F5 設備，F5 收到用戶請求後，進行 cluster 級別的負載均衡</li><li>機器級別的負載均衡：cluster 2 的負載均衡用的是 Nginx</li></ul></li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>PV（page view）</p></li><li><p>從 DAU(Daily Active User) 出發，結合業務的特點，計算出來總的 QPS(Query Per Second) 和 TPS(Transactions Per Second)，而後再根據通常規律計算出 QPS 和 TPS 的峰值，加上一定的未來發展空間和高可用冗餘，結合單機能夠支撐的 QPS 和 TPS 量，就可以計算出來整個 cluster 的規模，有了這些資料就可以制定出比較合理的負載均衡的策略</p></li><li><p>nginx 做級聯不太合適，因為頂層的 nginx 性能是瓶頸，多級導流一般用在處理能力有差異的系統上，例如一級用 F5，二級用 LVS，三級用 nginx</p></li><li><p>論壇業務其實可以不用 DNS 做地理位置負載均衡，用CDN效果更好</p></li></ul><h1 id="高性能負載均衡：算法"><a href="#高性能負載均衡：算法" class="headerlink" title="高性能負載均衡：算法"></a>高性能負載均衡：算法</h1><p>負載均衡算法數量較多，根據算法期望達到的目的，大體上可以分為下面幾類：</p><ul><li><p><strong>任務平分類</strong>：負載均衡系統將收到的任務平均分配給 server 進行處理，這裡的”平均”可以是絕對數量的平均，也可以是比例或者權重上的平均</p></li><li><p><strong>負載均衡類</strong>：負載均衡系統根據 server 的負載來進行分配，這裡的負載並不一定是通常意義上我們說的”CPU 負載”，而是系統當前的壓力，可以用 CPU 負載來衡量，也可以用連接數、I/O 使用率、網卡吞吐量等來衡量系統的壓力</p></li><li><p><strong>性能最優類</strong>：負載均衡系統根據 server 的響應時間來進行任務分配，優先將新任務分配給響應最快的 server</p></li><li><p><strong>Hash 類</strong>：負載均衡系統根據任務中的某些關鍵訊息進行 Hash 運算，將相同 Hash 值的請求分配到同一台 server 上。常見的有 source IP hash、target IP hash、session ID hash、user ID hash 等</p></li></ul><h2 id="輪詢-任務平分類"><a href="#輪詢-任務平分類" class="headerlink" title="輪詢(任務平分類)"></a>輪詢(任務平分類)</h2><ul><li><p>負載均衡系統收到請求後，按照順序輪流分配到 server 上</p></li><li><p>是最簡單的一個策略，無須關注 server </p></li><li><p>某個 server 當前因為觸發了程序 bug 進入了死循環導致 CPU 負載很高，負載均衡系統是不感知的，還是會繼續將請求源源不斷地發送給它</p></li><li><p>cluster 中有新的機器是 32 核的，舊的機器是 16 核的，負載均衡系統也是不關注的，新舊機器分配的任務數是一樣的</p></li><li><p>只要 server 在運行，運行狀態是不關注的。但如果 server 直接當機了，或者 server 和負載均衡系統斷連了，這時負載均衡系統是能夠感知的，也需要做出相應的處理</p></li><li><p>“簡單”是輪詢算法的優點，也是它的缺點</p></li></ul><h2 id="加權輪詢-任務平分類"><a href="#加權輪詢-任務平分類" class="headerlink" title="加權輪詢(任務平分類)"></a>加權輪詢(任務平分類)</h2><ul><li><p>權重一般是根據硬體配置進行靜態配置的，採用動態的方式計算會更加契合業務，但複雜度也會更高</p></li><li><p>主要目的就是為了解決不同 server 處理能力有差異的問題</p></li><li><p>加權輪詢解決了輪詢算法中無法根據 server 的配置差異進行任務分配的問題，但同樣存在無法根據 server 的狀態差異進行任務分配的問題</p></li></ul><h2 id="負載最低優先-負載均衡類"><a href="#負載最低優先-負載均衡類" class="headerlink" title="負載最低優先(負載均衡類)"></a>負載最低優先(負載均衡類)</h2><ul><li><p>負載均衡系統將任務分配給當前負載最低的 server ，這裡的負載根據不同的任務類型和業務場景，可以用不同的指標來衡量：</p><ul><li>LVS 這種 4 層網路負載均衡設備，可以以”連接數”來判斷 server 的狀態， server 連接數越大，表明 server 壓力越大。</li><li>Nginx 這種 7 層網路負載系統，可以以”HTTP 請求數”來判斷 server 狀態(Nginx 內建的負載均衡算法不支持這種方式，需要進行擴展)</li><li>如果我們自己開發負載均衡系統，可以根據業務特點來選擇指標衡量系統壓力。如果是 CPU 密集型，可以以”CPU 負載”來衡量系統壓力；如果是 I/O 密集型，可以以”I/O 負載”來衡量系統壓力。</li></ul></li><li><p>負載最低優先的算法解決了輪詢算法中無法感知 server 狀態的問題，由此帶來的代價是複雜度要增加很多</p></li><li><p>最少連接數優先的算法要求負載均衡系統統計每個 server 當前建立的連接，其應用場景僅限於負載均衡接收的任何連接請求都會轉發給 server 進行處理，否則如果負載均衡系統和 server 之間是固定的連接池方式，就不適合採取這種算法</p></li><li><p>CPU 負載最低優先的算法要求負載均衡系統以某種方式收集每個 server 的 CPU 負載，不同業務最優的時間間隔(1~15 mins)是不一樣的，時間間隔太短容易造成頻繁波動，時間間隔太長又可能造成峰值來臨時響應緩慢</p></li><li><p>負載最低優先算法雖然效果看起來很美好，但實際上真正應用的場景反而沒有輪詢(包括加權輪詢)那麼多</p></li></ul><h2 id="性能最優類"><a href="#性能最優類" class="headerlink" title="性能最優類"></a>性能最優類</h2><ul><li><p>負載最低優先類算法是站在 server 的角度來進行分配的，而性能最優優先類算法則是站在客戶端的角度來進行分配的</p></li><li><p>優先將任務分配給處理速度最快的 server ，通過這種方式達到最快響應客戶端的目的</p></li><li><p>通過響應時間這個外部標準來衡量 server 狀態</p></li><li><p>複雜度都很高，主要體現在：</p><ul><li>負載均衡系統需要收集和分析每個 server 每個任務的響應時間，在大量任務處理的場景下，這種收集和統計本身也會消耗較多的性能</li><li>為了減少這種統計上的消耗，可以採取採樣的方式來統計，使用抽樣統計部分任務的響應時間來估算整體任務的響應時間</li><li>採樣統計雖然能夠減少性能消耗，但使得複雜度進一步上升，因為要確定合適的採樣率</li><li>無論是全部統計還是採樣統計，都需要選擇合適的週期，需要根據實際業務進行判斷和選擇</li></ul></li></ul><h2 id="Hash-類"><a href="#Hash-類" class="headerlink" title="Hash 類"></a>Hash 類</h2><ul><li><p>根據任務中的某些關鍵訊息進行 Hash 運算，將相同 Hash 值的請求分配到同一台 server 上</p></li><li><p>source IP Hash：將來源於同一個 s ource IP 地址的任務分配給同一個 server 進行處理，適合於存在事務、會話的業務</p></li><li><p>將某個 ID 標識的業務分配到同一個 server 中進行處理，這裡的 ID 一般是臨時性資料的 ID</p></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li>高並發同步資料代價比較大</li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></p></li><li><p>[百亿级微信红包的高并发资金交易系统设计方案 - InfoQ](百亿级微信红包的高并发资金交易系统设计方案 - InfoQ)</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 單一 Server 高效能模式</title>
      <link href="/blog/Architecture_Design/Architecture-Design-High-Performance-single-server/"/>
      <url>/blog/Architecture_Design/Architecture-Design-High-Performance-single-server/</url>
      
        <content type="html"><![CDATA[<h1 id="單一-server-高效能模式：PPC-與-TPC"><a href="#單一-server-高效能模式：PPC-與-TPC" class="headerlink" title="單一 server 高效能模式：PPC 與 TPC"></a>單一 server 高效能模式：PPC 與 TPC</h1><ul><li><p>OS 、CPU、 Memory、Disk 、cache 、network 、程式語言、架構等，每個都有可能影響系統達到高效能</p></li><li><p>站在架構師的角度，當然需要特別關注高效能架構的設計。高效能架構設計主要集中在兩方面：</p><ul><li>儘量提升單一 server 的效能，將單一 server 的效能發揮到極致</li><li>如果單一 server 無法支撐效能，設計 server cluster 方案</li></ul></li><li><p>架構設計是高效能的基礎，架構設計決定了系統效能的上限，實作細節決定了系統效能的下限</p></li><li><p>單一 server 高效能的關鍵之一就是 <strong>server 採取的並發模型</strong>，並發模型有如下兩個關鍵設計點：</p><ul><li>server 如何管理 connection</li><li>server 如何處理 request</li></ul></li><li><p>以上兩個設計點最後都和 OS 的 I/O Model 及 process Model 相關。</p><ul><li>I/O Model：blocking、non-blocking、blocking、synchronous、asynchronous</li><li>process Model：sinle process、multiple process、multiple thread</li></ul></li></ul><h2 id="PPC-Process-Per-Connection"><a href="#PPC-Process-Per-Connection" class="headerlink" title="PPC(Process Per Connection)"></a>PPC(Process Per Connection)</h2><p><img src="/blog/images/architecture-design/ppc-fork.png" alt="PPC - fork"></p><ul><li><p>每次有新的連接就新建一個 process   去專門處理這個連接的請求，這是傳統的 UNIX network server  所採用的模型：</p><ul><li><p>parent process 接受連接 (圖中 accept)</p></li><li><p>parent process “fork” child process (圖中 fork)</p></li><li><p>child process 處理連接的讀寫請求（圖中 child process read、業務處理、write）。</p></li><li><p>child process 關閉連接 (圖中 child process 中的 close)</p></li></ul></li><li><p>PPC 模式實現簡單，<strong>比較適合 server  的連接數沒那麼多的情況</strong>，例如 DB server</p></li><li><p>此模式有以下幾個問題：</p><ul><li><p><strong>fork 代價高</strong>：站在 OS 的角度，創建一個 process 的代價是很高的，需要分配很多 kernel 資源，需要將 memory image 從 parent process 複製到 child process</p><blockquote><p>即使現在的 OS 在複製 memory image 時用到了 Copy on Write 技術，總體來說創建 process 的代價還是很大的。</p></blockquote></li><li><p><strong>parent/child process 通信複雜</strong>：parent process “fork” child process 時，文件描述符可以通過 memory image 複製從 parent process 傳到 child process，但 “fork” 完成後，parent/child process 之間的通信就比較麻煩了，需要採用 IPC(Interprocess Communication) 之類的 process 通信方案。</p><blockquote><p>例如， child process 需要在 close 之前告訴 parent process 自己處理了多少個請求以支撐 parent process 進行全域的統計，那麼 child process 和 parent process 必須採用 IPC 方案來傳遞信息</p></blockquote></li><li><p><strong>支持的並發連接數量有限</strong>：如果每個連接存活時間比較長，而且新的連接又源源不斷的進來，則 process 數量會越來越多，OS process 調度和切換的頻率也越來越高，系統的壓力也會越來越大。因此，一般情況下，PPC 方案能處理的並發連接數量最大也就幾百</p></li></ul></li></ul><h3 id="prefork"><a href="#prefork" class="headerlink" title="prefork"></a>prefork</h3><p><img src="/blog/images/architecture-design/ppc-prefork.png" alt="PPC - prefork"></p><ul><li><p>PPC 模式中，當連接進來時才 fork new process 來處理連接請求，由於 fork process 代價高，用戶訪問時可能感覺比較慢，prefork 模式的出現就是為了解決這個問題</p></li><li><p>prefork 就是提前創建 process (pre-fork)</p></li><li><p>系統在啟動的時候就預先創建好 process，然後才開始接受用戶的請求，當有新的連接進來的時候，就可以省去 fork process 的操作，讓用戶訪問更快、體驗更好</p></li><li><p>prefork 的實現關鍵就是 multiple child process 都 accept 同一個 socket，當有新的連接進入時， OS 保證只有一個 process 能最後 accept 成功</p></li><li><p>prefork 模式和 PPC 一樣，還是存在 parent/child process 通信複雜、支持的並發連接數量有限的問題，因此目前實際應用也不多</p></li></ul><h2 id="TPC-Thread-Per-Connection"><a href="#TPC-Thread-Per-Connection" class="headerlink" title="TPC (Thread Per Connection)"></a>TPC (Thread Per Connection)</h2><ul><li><p>每次有新的連接就新建一個 thread 去專門處理這個連接的請求</p></li><li><p>與 process 相比，thread 更輕量級，創建 thread 的消耗比 process 要少得多</p></li><li><p>multiple thread 是共享 process memory 空間的，thread 通信相比 process 通信更簡單</p></li><li><p>TPC 實際上是解決或者弱化了 PPC fork 代價高的問題和 parent/child process 通信複雜的問題</p></li></ul><p><img src="/blog/images/architecture-design/tpc-basic-flow.png" alt="TPC - basic flow"></p><ul><li>TPC 基本流程：<ul><li>parent process 接受連接 (圖中 accept)</li><li>parent process 創建 child thread (圖中 pthread)</li><li>child thread 處理連接的讀寫請求 (圖中 child thread read、業務處理、write)</li><li>child thread 關閉連接（圖中子 thread 中的 close）。</li></ul></li></ul><blockquote><p>和 PPC 相比，主 process   不用”close”連接了</p></blockquote><ul><li><p>TPC 帶來的新問題：</p><ul><li>創建 thread 雖然比創建 process 代價低，但並不是沒有代價，高並發時(例如每秒上萬連接)還是有效能問題。</li><li>無須 process 間通信，但是 thread 間的互斥和共享又引入了複雜度，可能一不小心就導致了 dead lock 問題</li><li>multiple thread 會出現互相影響的情況，某個 thread 出現異常時，可能導致整個 process 退出 (例如：memory 越界)</li></ul></li><li><p>在並發幾百連接的場景下，反而更多地是採用 PPC 的方案，因為 PPC 方案不會有死鎖的風險，也不會多 process 互相影響，穩定性更高</p></li></ul><h3 id="prethread"><a href="#prethread" class="headerlink" title="prethread"></a>prethread</h3><ul><li>會預先創建 thread，然後才開始接受用戶的請求，當有新的連接進來的時候，就可以省去創建 thread 的操作，讓用戶感覺更快、體驗更好</li></ul><p><img src="/blog/images/architecture-design/tpc-basic-flow.png" alt="TPC - prethread"></p><ul><li>parent process accept，然後將連接交給某個 thread 處理。</li><li>child thread 都嘗試去 accept，最終只有一個 thread accept 成功</li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>TPC 異常時整個 server 就掛了，而 PPC 不會，所以 PPC 適合 DB，middleware 這類的應用</p></li><li><p>PPC 和 TPC 這2種模式，無論 process 還是 thread 都受 CPU 的限制， process 和 thread 的切換都是有代價的，所以合適小型的系統，所以很多傳統行業選擇的是這 2 種</p></li><li><p>不同並發模式的選擇，還要考察三個指標，分別是響應時間(Response Time)，並發數(Concurrency)，吞吐量(TPS)</p></li><li><p><strong>吞吐量 = 並發數/平均響應時間</strong>。不同類型的系統，對這三個指標的要求不一樣：</p><ul><li>三高系統，比如秒殺、即時通信，不能使用</li><li>三低系統，比如ToB系統，運營類、管理類系統，一般可以使用</li><li>高吞吐系統，如果是 memory 計算為主的，一般可以使用；如果是 network I/O 為主的，一般不能使用</li></ul></li><li><p>高並發需要根據兩個條件劃分：連接數量，請求數量：</p><ol><li>海量連接(成千上萬)海量請求：例如：搶購，雙十一等</li><li>常量連接(幾十上百)海量請求：例如：middleware</li><li>海量連接常量請求：例如：入口網站</li><li>常量連接常量請求：例如：內部運營系統，管理系統</li></ol></li></ul><blockquote><p>一個連接就是 TCP 連接，一個連接每秒可以發一個請求，也可以發幾千個請求</p></blockquote><ul><li><p>PPC 和 TPC 能夠支持的最大連接數差不多，都是幾百個，所以我覺得他們適用的場景也是差不多的</p></li><li><p>從連接數和請求數來劃分，這兩種方式明顯不支持高連接數的場景，所以只有以下兩種場景適用：</p><ul><li>常量連接海量請求，例如：資料庫，redis，kafka 等等</li><li>常量連接常量請求，例如：企業內部網址</li></ul></li><li><p><code>BIO(blocking I/O)</code>：一個 thread 處理一個請求。缺點：並發量高時，thread 數較多，浪費資源</p><blockquote><p>Tomcat7 或以下，在 Linux 系統中預設使用這種方式。可以適用於小到中規模的客戶端並發數場景，無法勝任大規模並發業務。如果程式控制不善，可能造成系統資源耗盡。</p></blockquote></li><li><p><code>NIO(non-blocking I/O)</code>：利用多路復用 I/O 技術，可以通過少量的 thread 處理大量的請求</p><blockquote><p>Tomcat8 在 Linux 系統中預設使用這種方式；Tomcat7 必須修改 Connector 配置來啟動</p></blockquote></li><li><p>NIO 最適用於”高並發”的業務場景，所謂高並發一般是指 1ms 內至少同時有成百上千個連接請求準備就緒，<strong>其他情況下 NIO 技術發揮不出它明顯的優勢</strong></p></li><li><p>BIO/NIO/AIO 說明：</p><ul><li>BIO：PPC 和 TPC 屬於這種</li><li>NIO：多路復用 I/O，Reactor 就是基於這種技術</li><li>AIO：Asynchronous I/O，Proactor 就是基於這種技術</li></ul></li><li><p>nginx 反向代理可以達到 3 萬以上併發量，具體需要根據業務和場景測試</p></li><li><p>若要考慮並發連接數來決定是否部署 cluster；FD 不是關鍵，關鍵是 process 或者 thread 數量太多，所佔資源和上下文切換很耗費效能</p></li></ul><h1 id="單一-server-高效能模式：Reactor-與-Proactor"><a href="#單一-server-高效能模式：Reactor-與-Proactor" class="headerlink" title="單一 server 高效能模式：Reactor 與 Proactor"></a>單一 server 高效能模式：Reactor 與 Proactor</h1><ul><li><p>單一 server 高效能的 PPC 和 TPC 模式，它們的優點是實現簡單，缺點是都無法支撐高並發的場景</p></li><li><p>可以應對高並發場景的單一 server 高效能架構模式：<code>Reactor</code> 和 <code>Proactor</code></p></li></ul><h2 id="解決方法"><a href="#解決方法" class="headerlink" title="解決方法"></a>解決方法</h2><ul><li><p>PPC 模式最主要的問題就是每個連接都要創建 process =&gt; 資源復用</p></li><li><p>不再單獨為每個連接創建 process，而是創建一個 process pool，將連接分配給 process，一個 process 可以處理多個連接的業務</p></li><li><p>引入 resource pool 的處理方式後，會引出一個新的問題： process 如何才能高效地處理多個連接的業務</p></li><li><p>只有當連接上有資料的時候 process 才去處理，這就是 I/O 多路復用技術的來源</p></li><li><p>I/O 多路復用技術有兩個關鍵實現點：</p><ul><li>當多條連接共用一個 blocking 對象後， process 只需要在一個 blocking 對象上等待，而無須再輪詢所有連接，常見的實現方式有 select、epoll、kqueue 等</li><li>當某條連接有新的資料可以處理時， OS 會通知 process， process 從阻塞狀態返回，開始進行業務處理</li></ul></li></ul><h2 id="Reactor"><a href="#Reactor" class="headerlink" title="Reactor"></a>Reactor</h2><ul><li><p>I/O 多路復用結合 thread pool，完美地解決了 PPC 和 TPC 的問題</p></li><li><p>Reactor 是 “事件反應” 的意思，可以通俗地理解為”來了一個事件我就有相應的反應”，這裡的”我”就是 Reactor，具體的反應就是我們寫的代碼，Reactor 會根據事件類型來調用相應的代碼進行處理</p></li><li><p>I/O 多路復用統一監聽事件，收到事件後分配(Dispatch)給某個 process   </p></li><li><p>Reactor 模式的核心組成部分包括 Reactor 和處理資源池(process/thread pool)，其中 Reactor 負責監聽和分配事件，處理資源池負責處理事件</p></li><li><p>Reactor 模式的具體實現方案靈活多變，主要體現在：</p><ul><li>Reactor 的數量可以變化：可以是一個 Reactor，也可以是多個 Reactor</li><li>resource pool 的數量可以變化：以 process 為例，可以是單個 process，也可以是多個 process(thread 類似)</li></ul></li><li><p>最終 Reactor 模式有這三種典型的實現方案：</p><ul><li>single Reactor single process/thread</li><li>single Reactor multiple thread</li><li>multiple Reactor multiple process/thread</li></ul></li></ul><h3 id="single-Reactor-single-process-thread"><a href="#single-Reactor-single-process-thread" class="headerlink" title="single Reactor single process/thread"></a>single Reactor single process/thread</h3><ul><li>select、accept、read、send 是標準的網路程式設計 API，dispatch 和”業務處理”是需要完成的操作</li></ul><p><img src="/blog/images/architecture-design/single-reactor-single-process.png" alt="reactor - single reactor single process/thread"></p><ul><li><p>作法說明：</p><ul><li>Reactor 對象通過 select 監控連接事件，收到事件後通過 dispatch 進行分發。</li><li>如果是連接建立的事件，則由 Acceptor 處理，Acceptor 通過 accept 接受連接，並創建一個 Handler 來處理連接後續的各種事件</li><li>如果不是連接建立事件，則 Reactor 會調用連接對應的 Handler (上一個步驟中創建的 Handler)來進行回應</li><li>Handler 會完成 <strong>read -&gt; 業務處理 -&gt; send</strong> 的完整業務流程。</li></ul></li><li><p>優點：很簡單，沒有 process 間通信，沒有 process 競爭，全部都在同一個 process 內完成</p></li><li><p>缺點：</p><ul><li>只有一個 process，無法發揮多核 CPU 的效能；只能採取部署多個系統來利用多核 CPU</li><li>Handler 在處理某個連接上的業務時，整個 process 無法處理其他連接的事件，很容易導致效能瓶頸</li></ul></li><li><p>實踐中應用場景不多，只適用於業務處理非常快速的場景 (例如：Redis)</p></li><li><p>C 語言編寫系統的一般使用 <code>single Reactor single process</code>，因為沒有必要在 process 中再創建 thread ；而 Java 語言編寫的一般使用 <code>single Reactor single thread</code>，因為 Java 虛擬機是一個 process ，虛擬機中有很多 thread ，業務 thread 只是其中的一個 thread 而已</p></li></ul><h3 id="single-Reactor-multiple-thread"><a href="#single-Reactor-multiple-thread" class="headerlink" title="single Reactor multiple thread"></a>single Reactor multiple thread</h3><p><img src="/blog/images/architecture-design/single-reactor-multiple-threads.png" alt="reactor - single reactor multiple threads"></p><ul><li><p>作法說明：</p><ul><li>主 thread 中，Reactor 對象通過 select 監控連接事件，收到事件後通過 dispatch 進行分發。</li><li>如果是連接建立的事件，則由 Acceptor 處理，Acceptor 通過 accept 接受連接，並創建一個 Handler 來處理連接後續的各種事件。</li><li>如果不是連接建立事件，則 Reactor 會調用連接對應的 Handler（第 2 步中創建的 Handler）來進行響應。</li><li>Handler 只負責響應事件，不進行業務處理；Handler 通過 read 讀取到資料後，會發給 Processor 進行業務處理。</li><li>Processor 會在獨立的子 thread 中完成真正的業務處理，然後將響應結果發給主 process 的 Handler 處理；Handler 收到響應後通過 send 將響應結果返回給 client。</li></ul></li><li><p>優點：單 Reator 多 thread 方案能夠充分利用多核多 CPU 的處理能力</p></li><li><p>缺點：</p><ul><li>多 thread 資料共享和訪問比較複雜</li><li>Reactor 承擔所有事件的監聽和響應，只在主 thread 中運行，瞬間高並發時會成為效能瓶頸</li></ul></li></ul><h3 id="multiple-Reactor-multiple-process-thread"><a href="#multiple-Reactor-multiple-process-thread" class="headerlink" title="multiple Reactor multiple process/thread"></a>multiple Reactor multiple process/thread</h3><p><img src="/blog/images/architecture-design/multiple-reactor-multiple-processes.png" alt="reactor - multiple reactor multiple processes/threads"></p><ul><li><p>作法說明：</p><ul><li>parent process 中 mainReactor 對象通過 select 監控連接建立事件，收到事件後通過 Acceptor 接收，將新的連接分配給某個 child process </li><li>child process 的 subReactor 將 mainReactor 分配的連接加入連接隊列進行監聽，並創建一個 Handler 用於處理連接的各種事件</li><li>當有新的事件發生時，subReactor 會調用連接對應的 Handler（即第 2 步中創建的 Handler）來進行響應</li><li>Handler 完成 <strong>read→業務處理→send</strong> 的完整業務流程</li></ul></li><li><p>multiple Reactor multiple process/thread 的方案看起來比單 Reactor 多 thread 要複雜，但實際實現時反而更加簡單：</p><ul><li>parent process 和 child process 的職責非常明確，parent process 只負責接收新連接，child process 負責完成後續的業務處理</li><li>parent process 和 child process 的交互很簡單，parent process 只需要把新連接傳給 child process，child process 無須返回資料</li><li>child process 之間是互相獨立的，無須同步共享之類的處理</li></ul></li><li><p>Nginx 採用的是 <code>multiple Reactor multiple process</code> ，採用 <code>multiple Reactor multiple thread</code> 的實現有 Memcache 和 Netty</p></li></ul><h2 id="Proactor"><a href="#Proactor" class="headerlink" title="Proactor"></a>Proactor</h2><ul><li><p>如果把 I/O 操作改為 asynchronous 就能夠進一步提升效能，這就是 asynchronous 網路模型 Proactor</p></li><li><p>Reactor 可以理解為”來了事件我通知你，你來處理”，而 Proactor 可以理解為”來了事件我來處理，處理完了我通知你”</p></li></ul><p><img src="/blog/images/architecture-design/proactor.png" alt="proactor"></p><ul><li><p>作法說明：</p><ul><li>Proactor Initiator 負責創建 Proactor 和 Handler，並將 Proactor 和 Handler 都通過 Asynchronous Operation Processor 註冊到 kernel</li><li>Asynchronous Operation Processor 負責處理註冊請求，並完成 I/O 操作</li><li>Asynchronous Operation Processor 完成 I/O 操作後通知 Proactor</li><li>Proactor 根據不同的事件類型回調不同的 Handler 進行業務處理</li><li>Handler 完成業務處理，Handler 也可以註冊新的 Handler 到 kernel process </li></ul></li><li><p>理論上 Proactor 比 Reactor 效率要高一些，asynchronous I/O 能夠充分利用 DMA 特性，讓 I/O 操作與計算重疊</p></li><li><p>但要實現真正的 asynchronous I/O，OS 需要做大量的工作</p></li><li><p>目前 Windows 下通過 IOCP 實現了真正的 asynchronous I/O，而在 Linux 系統下的 AIO 並不完善，因此在 Linux 下實現高並發網路編程時都是以 Reactor 模式為主</p></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>message queue 系統屬於 middleware，連接數相對固定，長連接為主，所以把 accept 分離出來的意義是不大的</p><blockquote><p>message queue 要保證資料持久性，所以入庫操作應該是耗時最大的操作；綜合起來 <code>single Reactor multiple thread</code> 的方式比較合適</p></blockquote></li><li><p>IO操作分兩個階段</p><ol><li>等待資料準備好(讀到 kernel  cache)</li><li>將資料從 kernel 讀到 user space(process 空間)</li></ol></li><li><p>承上，一般來說 1 花費的時間遠遠大於 2</p><ul><li>1 上阻塞 2 上也阻塞的是 synchronous blocking I/O</li><li>1 上非阻塞 2 阻塞的是 synchronous non-blocking I/O =&gt; <code>Reactor</code></li><li>1 上非阻塞 2 上非阻塞是 asynchronous non-blocking I/O =&gt; <code>Proactor</code></li></ul></li><li><p>以生活中的範例來描述 Reactor 與 Proactor：</p><ol><li>假如我們去飯店點餐，飯店人很多，如果我們付了錢後站在收銀台等著飯端上來我們才離開，這就成了 synchronous blocking 了</li><li>如果我們付了錢後給你一個號碼後就可以離開，飯好了老闆會叫號，你過來取 =&gt; Reactor</li><li>如果我們付了錢後給我一個號碼後就可以坐到坐位上該幹啥幹啥，飯好了老闆會把飯端上來送給你 =&gt; Proactor</li></ol></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 高性能 DB 架構設計(RDBMS / NoSQL / Cache)</title>
      <link href="/blog/Architecture_Design/Architecture-Design-High-Performance-db-nosql-cache/"/>
      <url>/blog/Architecture_Design/Architecture-Design-High-Performance-db-nosql-cache/</url>
      
        <content type="html"><![CDATA[<h1 id="高性能-DB-cluster：讀寫分離"><a href="#高性能-DB-cluster：讀寫分離" class="headerlink" title="高性能 DB cluster：讀寫分離"></a>高性能 DB cluster：讀寫分離</h1><ul><li><p>大部分情況下做架構設計主要都是基於已有的成熟模式，結合業務和團隊的具體情況，進行一定的優化或者調整</p></li><li><p>即使少部分情況需要進行較大的創新，前提也是需要對已有的各種架構模式和技術非常熟悉</p></li><li><p>不管是為了滿足業務發展的需要，還是為了提升自己的競爭力，RDBMS 廠商(Oracle、DB2、MySQL … 等)在優化和提升單一 DB server 的性能方面也做了非常多的技術優化和改進</p><blockquote><p>但業務發展速度和數據增長速度，遠遠超出 DB 廠商的優化速度</p></blockquote></li><li><p>互聯網業務興起之後，海量用戶加上海量數據的特點，單個 DB server 已經難以滿足業務需要，必須考慮 DB cluster 的方式來提升性能</p></li></ul><h2 id="讀寫分離原理"><a href="#讀寫分離原理" class="headerlink" title="讀寫分離原理"></a>讀寫分離原理</h2><ul><li>讀寫分離的基本原理是將 DB 讀寫操作分散到不同的 node 上</li></ul><p><img src="/blog/images/architecture-design/db-read-write-separation.png" alt="DB 讀寫分離"></p><ul><li><p>讀寫分離的基本實現是：</p><ul><li>DB  server 搭建 master/slave cluster ，one master/one slave、one master/multiple slave 都可以</li><li>DB master 負責 read/write 作業，slave 只負責 read 作業</li><li>DB master 通過複製將數據同步到 slave，每台 DB server 都儲存了所有的業務相關資料</li><li>線上服務將 write 作業發給 DB master，將 read 作業發給 DB slave</li></ul></li><li><p>需要注意的是，這裡用的是 <code>master/slave cluster</code>，而不是 `primary/backup cluster”</p></li><li><p><code>slave</code> 需要提供讀數據的功能的；而 <code>backup</code> 一般被認為僅僅提供備份功能，不提供存取功能</p></li><li><p>讀寫分離的實現邏輯並不複雜，但是會引入兩個複雜度：<code>主從複製延遲</code> &amp; <code>分配機制</code></p></li></ul><h2 id="主從複製延遲"><a href="#主從複製延遲" class="headerlink" title="主從複製延遲"></a>主從複製延遲</h2><ul><li><p>以 MySQL 為例，主從複製延遲可能達到 1 秒，如果有大量數據同步，延遲 1 分鐘也是有可能的</p></li><li><p>解決主從複製延遲有幾種常見的方法：</p><ul><li>write 操作後的 read 操作指定發給 DB master server：和業務強綁定，對業務的侵入和影響較大</li><li>讀 slave 失敗後再讀一次 master：如果有很多二次讀取，將大大增加主機的 read 操作壓力</li><li>關鍵業務 read/write 操作全部指向 master，非關鍵業務採用讀寫分離</li></ul></li></ul><h2 id="分配機制"><a href="#分配機制" class="headerlink" title="分配機制"></a>分配機制</h2><p>將 read/write 操作區分開來，然後訪問不同的 DB server ，一般有兩種方式：程序程式封裝和 middleware 封裝</p><h3 id="程式封裝"><a href="#程式封裝" class="headerlink" title="程式封裝"></a>程式封裝</h3><p>程式封裝指在程式中抽象出一個<strong>數據訪問層</strong>(or <strong>中間層封裝</strong>)，實現讀寫操作分離和 DB server 連接的管理<br><img src="/blog/images/architecture-design/db-read-write-encapsulation-by-code.png" alt="DB 讀寫分離 - 分配機制 - 程式封裝"></p><p>程式封裝的方式具備幾個特點：</p><ul><li>實現簡單，而且可以根據業務做較多定製化的功能。</li><li>每個程式語言都需要自己實現一次，無法通用，如果一個業務包含多個程式語言寫的多個子系統，則重複開發的工作量比較大。</li><li>故障情況下，如果主從發生切換，則可能需要所有系統都修改設定 &amp; 重啟。</li></ul><h3 id="middleware-封裝"><a href="#middleware-封裝" class="headerlink" title="middleware 封裝"></a>middleware 封裝</h3><ul><li><p>middleware 封裝指的是獨立一套系統出來，實現讀寫操作分離和 DB server 連接的管理</p></li><li><p>middleware 對服務提供 SQL 兼容的協議，服務本身無須自己進行讀寫分離</p></li></ul><p><img src="/blog/images/architecture-design/db-read-write-encapsulation-by-middleware.png" alt="DB 讀寫分離 - 分配機制 - middleware 封裝"></p><ul><li><p>DB middleware 的方式具備的特點是：</p><ul><li>能夠支持多種程式語言，因為 DB middleware 對服務提供的是標準 SQL 接口</li><li>DB middleware 要支持完整的 SQL 語法和 DB server 的協議(例如：MySQL client 與 server 的連接通訊協定)，實現比較複雜，細節特別多，很容易出現 bug，需要較長的時間才能穩定</li><li>DB middleware 自己不執行真正的讀寫操作，但所有的 DB 操作請求都要經過 middleware，因此對於 middleware 的性能要求也會需要很高</li><li>DB master/slave 切換對服務來說是無感的，DB middleware 可以偵測 DB server 的主從狀態(例如：向某個測試 table 寫入一筆資料，成功的就是 master，失敗的就是 slave)</li></ul></li><li><p>由於 DB middleware 的複雜度要比程式封裝高出許多，一般情況下建議採用程序語言封裝的方式，或者使用成熟的開源 DB  middleware(例如：<a href="https://www.mysql.com/products/enterprise/router.html">MySQL Router</a>, <a href="http://shardingsphere.apache.org/">Apache ShardingSphere</a>, <a href="http://www.mycat.org.cn/">Mycat</a>)<br><img src="/blog/images/architecture-design/mysql-router.png" alt="DB 讀寫分離 - 分配機制 - middleware 封裝 - MySQL Router"></p></li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>不需要有性能問題就馬上進行讀寫分離，而是應該先優化(例如：優化 slow query，調整不合理的業務邏輯，引入 cache 等，只有確定系統沒有優化空間後，才考慮讀寫分離或者 cluster </p></li><li><p>在單機 DB 情況下，table 加上 index 一般對查詢有優化作用卻影響寫入速度，讀寫分離後可以單獨對 read DB 進行優化，write DB 上減少 index，對讀寫的能力都有提升，且讀的提升更多一些</p></li><li><p>如果並發寫入特別高，單機寫入無法支撐，就不適合加入 index 的方式解決(加入 index 只會加重寫入負擔)</p><blockquote><p>可改用  cache 技術或者程序優化的方式滿足要求</p></blockquote></li><li><p>因為業務查詢的關係，multiple table 之間的關聯，聚合，很難避免，但是查詢的條件太多，很動態，設計 cache 的方式：</p><ul><li>按照 80/20 原則，選出佔訪問量 80% 的前 20% 的請求條件進行 cache</li><li>大部分人的查詢不會每次都非常多條件，以手機為例，查詢蘋果加華為的可能佔很大一部分</li></ul></li><li><p>交易型業務 cache 應用不多， cache 一般都會用在查詢類業務上 (事實上大部分業務場景可能並不需要讀寫分離)</p></li><li><p>讀寫分離適用於單一 server 無法滿足所有請求的場景，從請求類型的角度對 server 進行拆分；在要求硬體資源能夠支撐的同時，對程式效能也會有更高的要求</p></li><li><p>一次刪除大量資料可能造成 master/slave 不同步 =&gt; 線上 delete 每次不能超過 1000 筆資料，超過就定時循環操作</p></li><li><p>middleware 並不是性能上有優勢，而是可以跨語言誇系統重複使用，大公司才有實力做(否則就選擇現有的成熟開源方案)</p></li><li><p>同步速度主要受網路延遲影響，和硬體關係不大</p></li><li><p>如果 cache 能承受業務請求，其實一般不用做讀寫分離，例如論壇的帖子瀏覽場景</p></li></ul><h1 id="高性能-DB-cluster：分庫分表"><a href="#高性能-DB-cluster：分庫分表" class="headerlink" title="高性能 DB cluster：分庫分表"></a>高性能 DB cluster：分庫分表</h1><p>讀寫分離分散了 DB 讀寫操作的壓力，但沒有分散儲存壓力，當數據量達到千萬甚至上億條的時候，單台 DB server 的儲存能力會成為系統的瓶頸；這樣的狀況會造成以下問題：</p><ul><li><p>數據量太大，讀寫的性能會下降，即使有 index，index 也會變得很大，性能同樣會下降。</p></li><li><p>數據文件會變得很大， DB backup &amp; restore 需要耗費很長時間</p></li><li><p>數據文件越大，極端情況下丟失數據的風險越高(例如，機房火災導致 DB master/slave 都發生故障)</p></li></ul><h2 id="分庫"><a href="#分庫" class="headerlink" title="分庫"></a>分庫</h2><ul><li><p>業務分庫指的是按照業務模組將數據分散到不同的 DB server<br><img src="/blog/images/architecture-design/db-separate-by-business.png" alt="分庫"></p></li><li><p>雖然業務分庫能夠分散儲存和訪問壓力，但同時也帶來了新的問題：</p><ul><li><ol><li>join 操作問題：原本在同一個 DB 中的 table 分散到不同 DB 中，導致無法使用 SQL 的 join 查詢</li></ol></li><li><ol start="2"><li>事務問題：業務分庫後，table 分散到不同的 DB 中，無法通過事務統一修改</li></ol></li><li><ol start="3"><li>成本問題：業務分庫同時也帶來了成本的代價，本來 1 台 server 可以搞定的事情，現在要 3 台，如果考慮備份，那就是 2 台變成了 6 台</li></ol></li></ul></li><li><p>對於小公司初創業務，並不建議一開始就這樣拆分，主要有幾個原因：</p><ul><li>初創業務存在很大的不確定性，業務不一定能發展起來，業務開始的時候並沒有真正的儲存和訪問壓力，<strong>業務分庫並不能為業務帶來價值</strong></li><li>業務分庫後，表之間的 join 查詢、 DB 事務無法簡單實現了</li><li>業務分庫後，因為不同的數據要讀寫不同的 DB，程式中需要增加根據數據類型對應到不同 DB 的邏輯，增加了工作量；而業務初創期間最重要的是快速實現、快速驗證，<strong>業務分庫會拖慢業務節奏</strong></li></ul></li><li><p>如果業務真的發展很快，豈不是很快就又要進行業務分庫了? 那為何不一開始就設計好呢? </p><ul><li>這裡的”如果”事實上發生的概率比較低</li><li>如果業務真的發展很快，後面進行業務分庫也不遲</li><li>單台 DB server 的性能其實也沒有想像的那麼弱</li></ul></li><li><p>對於業界成熟的大公司來說，由於已經有了業務分庫的成熟解決方案，並且即使是嘗試性的新業務，用戶規模也是海量的，這與前面提到的初創業務的小公司有本質區別，因此最好在業務開始設計時就考慮業務分庫</p></li></ul><h2 id="分表"><a href="#分表" class="headerlink" title="分表"></a>分表</h2><ul><li><p>如果業務繼續發展，同一業務的單一 table 數據也會達到單台 DB server 的處理瓶頸</p></li><li><p>單一 table 數據拆分有兩種方式：</p><ul><li>垂直分表：對應到表的切分就是表記錄數相同但包含不同的 colume</li><li>水平分表：對應到表的切分就是表的 colume 相同但包含不同的 row data<br><img src="/blog/images/architecture-design/db-separate-table.png" alt="分表"></li></ul></li><li><p>實際架構設計過程中並不侷限切分的次數，可以切兩次，也可以切很多次</p></li><li><p>單一 table 進行切分後，是否要將切分後的多個 table 分散在不同的 DB server 中，可以根據實際的切分效果來確定</p></li><li><p>單一 table 切分為多個 table 後，新的 table 即使在同一個 DB  server 中，也可能帶來可觀的性能提升</p></li><li><p>如果單一 table 拆分為多個 table 後，單台 server 依然無法滿足性能要求，那就不得不再次進行業務分庫的設計了</p></li></ul><h2 id="垂直分表引入的複雜性"><a href="#垂直分表引入的複雜性" class="headerlink" title="垂直分表引入的複雜性"></a>垂直分表引入的複雜性</h2><ul><li><p>垂直分表適合將表中某些不常用且佔了大量空間的 colume 拆分出去</p></li><li><p>垂直分表引入的複雜性主要體現在 table 操作的數量要增加</p></li></ul><h2 id="水平分表引入的複雜性"><a href="#水平分表引入的複雜性" class="headerlink" title="水平分表引入的複雜性"></a>水平分表引入的複雜性</h2><h3 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h3><p>常見的路由算法：</p><ul><li><p><strong>範圍路由</strong>：選取有序的 data colume (例如：integer、timestamp … 等等)作為路由的條件，不同分段分散到不同的 DB table 中</p><ul><li>複雜點：主要體現在分段大小的選取上，分段太小會導致切分後子表數量過多，增加維護複雜度</li><li>優點：可以隨著數據的增加平滑地擴充新的表</li><li>缺點：分佈不均勻</li></ul></li><li><p><strong>Hash 路由</strong>：選取某個 colume(或者某幾個 colume 組合也可以)的值進行 Hash 運算，然後根據 Hash 結果分散到不同的 DB table 中</p><ul><li>複雜點：主要體現在初始 table 數量的選取上，table 數量太多維護比較麻煩，table 數量太少又可能導致單一 table 性能存在問題</li><li>優點：table 分佈比較均勻</li><li>缺點：擴充新的 table 很麻煩，所有數據都要重分佈</li></ul></li><li><p><strong>配置路由</strong>：配置路由就是路由表，用一張獨立的 table 來記錄路由信息</p><ul><li>優點：設計簡單，使用起來非常靈活</li><li>缺點：<strong>必須多查詢一次</strong>，會影響整體性能；而且路由表本身如果太大(例如：幾億條數據)，性能同樣可能成為瓶頸</li></ul></li></ul><h3 id="join-操作"><a href="#join-操作" class="headerlink" title="join 操作"></a>join 操作</h3><p>平分 table 後，數據分散在多個 table 中，如果需要與其他 table 進行 join 查詢，需要在業務程式或者 DB middleware 中進行多次 join 查詢，然後將結果合併</p><h3 id="count-操作"><a href="#count-操作" class="headerlink" title="count() 操作"></a>count() 操作</h3><p>水平分表後，雖然物理上數據分散到多個表中，但某些業務邏輯上還是會將這些 table 當作單一 table 表來處理</p><p>常見的處理方式有下面兩種：</p><ul><li><p>count() 相加：實現簡單，缺點就是性能比較低</p></li><li><p>count 資料表</p><ul><li>性能要大大優於 count() 相加的方式</li><li>也增加了 DB 的寫壓力</li><li>複雜度增加不少，對子表的操作要同步操作”記錄數表”，如果有一個業務邏輯遺漏了，數據就會不一致</li></ul></li></ul><blockquote><p>對於一些不要求 count 數據即時保持精確的業務，也可以通過後台定時更新 count 資料表</p></blockquote><h3 id="order-by-操作"><a href="#order-by-操作" class="headerlink" title="order by 操作"></a>order by 操作</h3><p>水平分表後，數據分散到多個子表中，排序操作無法在 DB 中完成，只能由業務程式或者 DB middleware 分別查詢每個子表中的數據，然後彙總進行排序</p><h2 id="實現方法歸納"><a href="#實現方法歸納" class="headerlink" title="實現方法歸納"></a>實現方法歸納</h2><ul><li><p>和 DB 讀寫分離類似，分庫分表具體的實現方式也是<code>程式封裝</code>和<code>middleware 封裝</code>，但實現會更複雜</p></li><li><p>讀寫分離實現時只要識別 SQL 操作是 read 作業還是 write 作業，通過簡單的判斷 SELECT、UPDATE、INSERT、DELETE 幾個關鍵字就可以做到</p></li><li><p>分庫分表的實現除了要判斷操作類型外，還要判斷 SQL 中具體需要操作的 table、操作函數(例如：count)、order by、group by 操作等，然後再根據不同的操作進行不同的處理</p></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>DB 優化的方向順序建議：</p><ol><li>進行硬體優化，例如：將 HDD 換成 SSD、增加 CPU core</li><li>先做 DB server 的優化(例如：增加 index, 優化 slow query)</li><li>引入 cache 技術(例如：redis)減少 DB 壓力</li><li>服務與 DB table 優化，重構(例如：根據業務邏輯對程序邏輯做優化，減少不必要的查詢)</li><li>在這些操作都不能大幅度優化性能的情況下，不能滿足將來的發展，再考慮分庫分表，但也要有預估性</li></ol></li><li><p>針對 Mysql，發現如果 colume 是 blob 型態，select 時有無包含此欄位，效率差異很大啊，這個是什麼原因？</p><blockquote><p>blob 的欄位是和 row data 分開儲存的，而且 disk 上並不是連續的，因此 select blob 欄位會讓 disk 進入 random I/O 模式</p></blockquote></li><li><p>分庫分表的實施前提：</p><ol><li>DB 存在性能問題，且用加索引、 slow query 優化、 cache 和讀寫分離都無法徹底解決問題的時候</li><li>複雜查詢對應的單一 table 數據量級一般超過千萬以上，或簡單查詢的單一 table 數據量級一般超過 5000 萬以上</li><li>對一致性要求不是特別高，只要求最終一致性</li><li>用 Hadoop 等大數據技術，因為業務需求、技術成本、時間成本無法解決該問題</li></ol></li><li><p>分表評估：</p><ul><li>業務對數據的操作主要集中在某些 colume 上，比較適合垂直分表</li><li>業務對數據的操作在整個表層面較均勻分佈，適合水平分表</li></ul></li><li><p>分庫使用時機：</p><ul><li>業務不複雜，但整體數據量已影響了 DB 的性能</li><li>業務複雜，需要分模組由不同開發團隊負責開發，這個時候使用分庫可以減少團隊間交流</li></ul></li><li><p>分表時機：單一 table 數據量太大，拖慢了 SQL 操作性能</p></li><li><p>並不是 DB 性能不夠的時候就分庫分表，提升 DB 性能方式很多，若有其他能在單一 DB 操作的方式則毫不猶豫使用，因為使用分表有固有的複雜性(join操作，事務，order by … 等)</p></li><li><p>通常 DB 剛表現出壓力的時候，大部分原因不是因為業務真的發展到 DB 撐不住了，而是很多 slow query 導致的</p></li></ul><h1 id="高性能-NoSQL"><a href="#高性能-NoSQL" class="headerlink" title="高性能 NoSQL"></a>高性能 NoSQL</h1><p><img src="/blog/images/architecture-design/nosql-history.png" alt="NoSQL History"></p><ul><li><p>RDBMS 存在如下缺點：</p><ul><li>儲存的是 colume data，無法儲存數據結構</li><li>schema 擴展很不方便(修改時可能會長時間鎖表)</li><li>在大數據場景下 I/O 較高(做統計)</li><li>全文搜尋功能比較弱(全文搜尋只能使用 like 進行整表掃瞄匹配，性能非常低)</li></ul></li><li><p>NoSQL 方案帶來的優勢，本質上是犧牲 ACID 中的某個或者某幾個特性</p></li><li><p>不能盲目地迷信 NoSQL 是銀彈，而應該將 NoSQL 作為 SQL 的一個加強方式</p></li><li><p>常見的 NoSQL 方案分為 4 類：</p><ul><li>Key/Value store：解決 RDBMS 無法儲存數據結構的問題，以 redis 為代表。</li><li>Document DB：解決 RDBMS 強 schema 約束的問題，以 MongoDB 為代表。</li><li>Colume-Based DB：解決 RDBMS 大數據場景下的 I/O 問題，以 HBase 為代表。</li><li>全文搜尋引擎：解決 RDBMS 的全文搜尋性能問題，以 Elasticsearch 為代表。</li></ul></li></ul><h2 id="Key-Value-store"><a href="#Key-Value-store" class="headerlink" title="Key/Value store"></a>Key/Value store</h2><ul><li><p>redis 是 Key/Value store 的典型代表，它是一款 open source 的高性能 Key/value cache 和儲存系統</p></li><li><p>RDBMS 實現 Key/Value 的功能很麻煩，而且需要進行多次 SQL 操作，性能很低</p></li><li><p>redis 的缺點主要體現在並不支持完整的 ACID 事務</p><blockquote><p>redis 的事務只能保證隔離性(isolation)和一致性(consistency)，無法保證原子性(atomic)和持久性(durability)</p></blockquote></li><li><p>雖然 redis 並沒有嚴格遵循 ACID 原則，但實際上大部分業務也不需要嚴格遵循 ACID 原則</p></li><li><p>在設計方案時，需要根據業務特性和要求來確定是否可以用 redis，而不能因為 redis 不遵循 ACID 原則就直接放棄</p></li></ul><h2 id="Document-DB"><a href="#Document-DB" class="headerlink" title="Document DB"></a>Document DB</h2><ul><li><p>Document DB 最大的特點就是 no-schema，可以儲存和讀取任意的數據</p></li><li><p>目前絕大部分 Document DB 儲存的數據格式是 JSON(or BSON)</p></li><li><p>Document DB的 no-schema 特性，給業務開發帶來了幾個明顯的優勢：</p><ul><li>新增欄位簡單</li><li>歷史數據不會出錯</li><li>可以很容易儲存複雜數據</li></ul></li><li><p>Document DB 的這個特點，特別適合電商和遊戲這類的業務場景：</p><ul><li>以電商為例，不同商品的屬性差異很大</li><li>即使是同類商品也有不同的屬性</li></ul></li><li><p>上述的業務場景如果使用 RDBMS 來儲存數據，就會很麻煩，而使用 Document DB，會簡單、方便許多，擴展新的屬性也更加容易</p></li><li><p>缺點：</p><ul><li><strong>不支持事務</strong>，因此某些對事務要求嚴格的業務場景是不能使用 Document DB的</li><li><strong>無法實現 RDBMS 的 join 操作</strong>，須多次查詢</li></ul></li></ul><h2 id="Colume-Based-DB"><a href="#Colume-Based-DB" class="headerlink" title="Colume-Based DB"></a>Colume-Based DB</h2><ul><li><p>傳統 RDBMS 被稱為 Row-Based DB，優勢在於：</p><ul><li>業務同時讀取 multiple row 時效率高</li><li>能夠一次性完成對 row 中的多個 colume 的寫操作，保證了針對 row data 寫操作的原子性和一致性</li></ul></li><li><p>Colume-Based DB 的優勢是在特定的業務場景(<strong>海量數據進行統計</strong>)下才能體現，如果不存在這樣的業務場景，那麼 Colume-Based DB 的優勢也將不復存在，甚至成為劣勢</p></li><li><p>Colume-Based DB</p><ul><li>優點：<ul><li>節省 I/O</li><li>具備更高的儲存壓縮比</li></ul></li><li>缺點：<ul><li>若需要頻繁地更新多個 colume =&gt; 將不同 colume 儲存在 disk 上不連續的空間，導致更新多個 colume 時 disk 是 random write 操作(效率極差)</li><li>高壓縮率在更新場景下也會成為劣勢，因為更新時需要將儲存數據解壓後更新，然後再壓縮，最後寫入 disk </li></ul></li></ul></li><li><p>一般將 Colume-Based DB 應用在離線的大數據分析和統計場景中，因為這種場景主要是針對部份 row 的單一 colume 進行操作，且數據寫入後就無須再更新刪除</p></li></ul><h2 id="全文搜尋引擎"><a href="#全文搜尋引擎" class="headerlink" title="全文搜尋引擎"></a>全文搜尋引擎</h2><p>傳統的 RDBMS 通過 index 來達到快速查詢的目的，但是在全文搜尋的業務場景下，index 也無能為力，主要體現在：</p><ul><li><p>全文搜尋的條件可以隨意排列組合，如果通過 index 來滿足，則 index 的數量會非常多。</p></li><li><p>全文搜尋的模糊匹配方式，索引無法滿足，只能用 like 查詢，而 like 查詢是 full table scan，效率非常低。</p></li></ul><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><ul><li><p>全文搜尋引擎的技術原理被稱為<code>&quot;倒排索引&quot;(Inverted index)</code>，也常被稱為反向索引、置入檔案或反向檔案，是一種索引方法，其基本原理是<strong>建立單詞到文件的索引</strong></p></li><li><p>正排索引適用於<strong>根據文件名稱來查詢文件內容</strong></p></li><li><p>倒排索引適用於<strong>根據 keyword 來查詢文件內容</strong></p></li></ul><h3 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h3><ul><li><p>全文搜尋引擎的索引對象是<code>單詞</code>和<code>文件</code>，而 RDBMS 的索引對象是 <code>key</code> 和 <code>row</code>，兩者的術語差異很大，不能簡單地等同起來</p></li><li><p>為了讓全文搜尋引擎支持關係型數據的全文搜尋，需要做一些轉換操作，即將關係型數據轉換為文件數據</p></li><li><p>目前常用的轉換方式是將關係型數據按照對象的形式轉換為 JSON 文件，然後將 JSON 文件輸入全文搜尋引擎進行索引</p></li><li><p>Elastcisearch 是分散式的文件儲存方式。它能儲存和檢索複雜的數據結構 - 以即時的方式序列化成為 JSON 文件</p></li><li><p>在 Elasticsearch 中，每個 field 的所有數據都是預設被索引的。即每個字段都有為了快速檢索設置的專用倒排索引</p></li><li><p>不像其他多數的 DB，Elasticsearch 能在相同的查詢中使用所有倒排索引，並以驚人的速度返回結果</p></li></ul><h2 id="討論整理精華-2"><a href="#討論整理精華-2" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>RDBMS 和 NoSQL DB 的選型。考慮幾個指標：<code>數據量</code>、<code>並發量</code>、<code>即時性</code>、<code>一致性要求</code>、<code>讀寫分佈和類型</code>、<code>安全性</code>、<code>運維性</code>等；根據這些指標，軟體系統可分成幾類。</p><ol><li>管理型系統(例如：運營類系統)，首選 RDBMS</li><li>大流量系統(例如：電商單品頁的某個服務)，後台選 RDBMS，前台選內存型(Key/Value store)</li><li>日誌型系統(例如：原始數據)選 Colume-Based DB，日誌搜尋選倒排索引</li><li>搜尋型系統(例如：站內搜尋，非通用搜尋，如商品搜尋)，後台選 RDBMS，前台選倒排索引。</li><li>事務型系統(例如：庫存、交易、記賬)，選<code>關係型 + cache + 一致性協議</code>，或新型 RDBMS</li><li>離線計算(例如：大量數據分析)，首選 Colume-Based DB，RDBMS 也可以</li><li>即時計算(例如：即時監控)，可以選 time-series DB ，或 colume-based DB </li></ol></li><li><p>將商品/訂單/庫存等相關基本信息放在 RDBMS 中(如MySQL，業務操作上支持事務，保證邏輯正確性)，cache 可以用 redis(減少 DB 壓力)，搜尋可以用 Elasticsearch(提升搜尋性能，可通過定時任務定期將 DB 中的資料同步到 ES 中)</p></li><li><p>多種方案搭配混用必然會增加應用的複雜性與增加運維成本，但同時也帶來了系統更多的靈活性</p></li><li><p>RDBMS 使用 index 加快查詢速度；Key/Value store 使用 memory 加快查詢速度；Elasticsearch 使用倒排索引加快查詢速度；Colume-Based DB 使用將 colume 單獨儲存來加快查詢速度</p></li></ul><h1 id="高性能-cache-架構"><a href="#高性能-cache-架構" class="headerlink" title="高性能 cache 架構"></a>高性能 cache 架構</h1><ul><li><p>在某些複雜的業務場景下，單純依靠儲存系統的性能提升不夠的，典型的場景有：</p><ul><li>需要經過複雜運算後得出的數據，儲存系統無能為力</li><li>read-many, write less(讀多寫少)的數據，儲存系統有心無力</li></ul></li><li><p>cache 就是為了彌補儲存系統在這些複雜業務場景下的不足，基本原理就是<strong>將可能重複使用的數據放到 memory 中</strong>，一次生成、多次使用，避免每次使用都去存取速度相較 memory 慢很多的儲存系統</p></li><li><p>cache 能夠帶來性能的大幅提升</p><blockquote><p>單台 Memcache server 簡單的 key-value 查詢能夠達到 TPS 50,000 以上</p></blockquote></li><li><p>cache 雖然能夠大大減輕儲存系統的壓力，但同時也給架構引入了更多複雜性</p></li></ul><h2 id="Cache-penetration"><a href="#Cache-penetration" class="headerlink" title="Cache penetration"></a>Cache penetration</h2><p> cache penetration 是指 cache 沒有發揮作用，業務系統雖然去 cache 查詢數據，但 cache 中沒有數據，業務系統需要再次去儲存系統查詢數據。</p><p> 通常情況下有兩種情況：</p><h3 id="儲存數據不存在"><a href="#儲存數據不存在" class="headerlink" title="儲存數據不存在"></a>儲存數據不存在</h3><ul><li><p>在 cache 中找不到對應的數據，每次都要去儲存系統中再查詢一遍，然後返回數據不存在</p><blockquote><p>cache 在這個場景中並沒有起到分擔儲存系統訪問壓力的作用</p></blockquote></li><li><p>出現一些異常情況(例如：被 hacker 攻擊)，故意大量訪問某些讀取不存在數據的業務，有可能會將儲存系統拖垮</p></li></ul><p>可能解決方案：</p><ul><li><p>如果查詢儲存系統的數據沒有找到，則直接設置一個預設值(可以是空值，也可以是具體的值)存到 cache 中，這樣第二次讀取 cache 時就會獲取到預設值，而不會繼續訪問儲存系統</p></li><li><p>使用 <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a></p><blockquote><p>實際應用上可能難以將所有 key 列出(例如：在分散式環境下很難完成這件事情)</p></blockquote></li></ul><h3 id="cache-數據生成耗費大量時間或者資源"><a href="#cache-數據生成耗費大量時間或者資源" class="headerlink" title="cache 數據生成耗費大量時間或者資源"></a>cache 數據生成耗費大量時間或者資源</h3><ul><li><p>儲存系統中存在數據，但生成 cache 數據需要耗費較長時間或者耗費大量資源</p></li><li><p>剛好在業務訪問的時候 cache 失效了，那麼也會出現 cache 沒有發揮作用，訪問壓力全部集中在儲存系統上的情況</p></li><li><p>爬蟲來遍歷的時候，系統性能就可能出現問題</p><blockquote><p>由於很多分頁都沒有 cache 數據，從 DB 中生成 cache 數據又非常耗費性能(order by limit 操作)，因此爬蟲會將整個 DB 全部拖慢</p></blockquote></li><li><p>爬蟲問題處理：(爬蟲並非攻擊)</p><ul><li>識別爬蟲然後禁止訪問，但這可能會影響 SEO 和推廣</li><li>做好監控，發現問題後及時處理</li></ul></li></ul><h2 id="Cache-avalanche"><a href="#Cache-avalanche" class="headerlink" title="Cache avalanche"></a>Cache avalanche</h2><ul><li><p>當 cache 失效(過期)後引起系統性能急劇下降的情況</p></li><li><p>舊的 cache 已經被清除，新的 cache 還未生成，並且處理這些請求的 thread 都不知道另外有一個 thread 正在生成 cache ，因此所有的請求都會去重新生成 cache，都會去訪問儲存系統，從而對儲存系統造成巨大的性能壓力</p></li></ul><p>常見解決方法有兩種：</p><ul><li><p><strong>更新鎖</strong>：</p><ul><li>對 cache 更新操作進行加鎖保護，保證只有一個 thread 能夠進行 cache 更新，未能獲取更新鎖的 thread 要嘛等待鎖釋放後重新讀取 cache ，要嘛就返回空值或者預設值</li><li>分散式 cluster 的業務系統要實現更新鎖機制，則需要用到分散式鎖，如 ZooKeeper</li></ul></li><li><p><strong>後台更新</strong>：由後台 thread 來更新 cache ，而不是由業務 thread 來更新 cache </p><ul><li>業務 thread 發現 cache 失效後，通過 message queue 發送一條消息通知後台 thread 更新 cache </li><li>後台更新既適應單機多 thread 的場景，也適合分散式 cluster 的場景，相比更新鎖機制要簡單一些</li><li>後台更新機制還適合業務剛上線的時候進行 cache 預熱</li></ul></li></ul><h2 id="Hotspot-cache"><a href="#Hotspot-cache" class="headerlink" title="Hotspot cache"></a>Hotspot cache</h2><ul><li><p>雖然 cache 系統本身的性能比較高，但對於一些特別熱點的數據，如果大部分甚至所有的業務請求都命中同一份 cache 數據，則這份數據所在的 cache server 的壓力也很大</p></li><li><p>cache 熱點的解決方案就是複製多份 cache 副本，將請求分散到多個 cache server 上，減輕 hotspot cache 導致的單台 cache server 壓力</p></li><li><p>不同的 cache 副本不要設置統一的過期時間，否則就會出現所有 cache 副本同時生成同時失效的情況，從而引發 cache 雪崩效應</p></li></ul><h2 id="討論整理精華-3"><a href="#討論整理精華-3" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>不要只從技術的角度考慮問題，結合業務考慮技術</p></li><li><p>將查詢條件組合成字符串再計算 md5，作為 cache key，優點是簡單靈活，缺點是浪費一部分 cache </p></li><li><p>DB 壓力大就可以考慮 cache 了</p></li><li><p>需要監控 cache 是否很快就滿了 or cache 命中率降低 … 等狀況，搭配告警進行處理</p></li><li><p>一般分頁 cache ， cache id 列表，不要 cache 所有數據</p></li><li><p>系統不是達到一定的系統查詢性能瓶頸，別一開始就用 cache </p></li><li><p>就算是 cache 修改頻率是一分鐘， cache 在這一分鐘也是有很大作用的，因為一分鐘可能就是幾千上萬次讀操作了，所以不要認為一天都不修改的數據才能用 cache </p></li><li><p>MySQL cache &amp; cache server 的比較：</p><ul><li>MySQL 第一種 cache 叫 sql 語句結果 cache ，但條件比較苛刻，程式設計師無法控制，一般 dba 都會關閉這個功能</li><li>MySQL 第二種 cache 是 innodb buffer pool，cache 的是 disk 上的分頁數據，不是 sql 的查詢結果，<strong>sql 的執行過程省不了</strong></li><li>memcached、redis 這些實際上都是 cache sql 的結果，兩種 cache 方式，性能差異是很大的</li></ul></li></ul><blockquote><p>因此，可控性，性能是 DB  cache 和獨立 cache 的主要區別</p></blockquote><ul><li><p>一個好的 cache 設計方案應該從這幾個方面入手設計：</p><ol><li>什麼數據應該 cache </li><li>什麼時機觸發 cache 和以及觸發方式是什麼</li><li>cache 的層次和粒度(網關 cache 如 nginx，本地 cache 如單機文件，分散式 cache 如 redis cluster，process 內 cache 如 global variables)</li><li>cache 的命名規則和失效規則</li><li>cache 的監控指標和故障應對方案</li><li>可視化 cache 數據(例如：redis 具體 key 內容和大小)</li></ol></li><li><p>監控爬蟲，發現問題後及時處理</p><blockquote><p>監控 DB 的各項指標，發現逐步變慢後看看是不是爬蟲，只要系統還撐得住就讓它爬，撐不住就不讓它爬</p></blockquote></li><li><p>分頁數據有很多排序規則，而且可能在某個時間點要上架新商品，就會有即時性的要求，請問這樣使用分頁 cache 真的合適嗎?</p><blockquote><p>cache 是為瞭解決性能問題，實時性要求很高就不能用 cache 了，或者要做 cache 及時更新機制</p></blockquote></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></p></li><li><p><a href="https://www.twblogs.net/a/5db37939bd9eee310ee692db">當我們聊技術實力的時候，我們到底在聊什麼 - 台部落</a></p></li><li><p><a href="https://www.pixelstech.net/article/1586522853-What-is-cache-penetration-cache-breakdown-and-cache-avalanche">What is cache penetration, cache breakdown and cache avalanche? | Pixelstech.net</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
            <tag> RDBMS </tag>
            
            <tag> Performance Tunning </tag>
            
            <tag> NoSQL </tag>
            
            <tag> Redis </tag>
            
            <tag> Elasticsearch </tag>
            
            <tag> Cache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - Serverless</title>
      <link href="/blog/AWS/AWS-CSA-associate-Serverless/"/>
      <url>/blog/AWS/AWS-CSA-associate-Serverless/</url>
      
        <content type="html"><![CDATA[<h1 id="Lambda"><a href="#Lambda" class="headerlink" title="Lambda"></a>Lambda</h1><h2 id="Serverless-與傳統架構的比較"><a href="#Serverless-與傳統架構的比較" class="headerlink" title="Serverless 與傳統架構的比較"></a>Serverless 與傳統架構的比較</h2><p><img src="/blog/images/aws/Serverless_Traditional-vs-Serverless.png" alt="Serverless v.s. Traditional"></p><ul><li><p>傳統架構中，最前端會是 ELB，接到提供運算能力的 EC2 instance，並存取內部的 RDS</p></li><li><p>在 Serverless 架構中，最前端的可能就會改成 API gateway，接到後面的 Lambda function，並存取內部的 DynamoDB</p></li></ul><h2 id="Lambda-相關概念"><a href="#Lambda-相關概念" class="headerlink" title="Lambda 相關概念"></a>Lambda 相關概念</h2><p>若同時有多個 user 發送 request 到 API gateway，對到相同的 Lambda function，其實在後端會觸發多個 Lambda function 起來運行，因此不需要考慮 scaling 的問題</p><p>支援的語言有 <code>.NET Core</code>, <code>Go</code>, <code>Java</code>, <code>node.js</code>, <code>Python</code>, <code>Ruby</code> … 等。</p><p>計費是以以下兩個基準計費：</p><ul><li><p>Request 數量</p></li><li><p>每個 Request 所花費的時間(但單位還是會分配多少 memory 來決定單位價格)</p></li></ul><h2 id="選擇-Lambda-的理由"><a href="#選擇-Lambda-的理由" class="headerlink" title="選擇 Lambda 的理由"></a>選擇 Lambda 的理由</h2><ul><li><p>不需要 server 了! 省掉很多管理成本</p></li><li><p>會根據需求進行 auto scaling</p></li><li><p>可以省很多錢</p></li></ul><h2 id="應考重點"><a href="#應考重點" class="headerlink" title="應考重點"></a>應考重點</h2><ul><li><p>Lambda 會根據需求自動 scale out</p></li><li><p>Lambda function 都是獨立的，每個 event 都會觸發產生一個 Lambda function 運作</p></li><li><p>Serverless</p></li><li><p>了解哪些服務是 serverless</p><blockquote><p>例如：Aurora serverless, Lambda</p></blockquote></li><li><p>Lambda function 可以觸發另一個 Lambda function (可以一個串一個)</p></li><li><p>使用 Lambda 的架構變得複雜時，AWS X-ray 可以協助 debug</p></li><li><p>Lambda 的運行範圍可以是 global，不會被特定的 region 侷限</p></li><li><p>搞清楚 Lambda function trigger 可以有哪些</p></li></ul><h1 id="實作課程重點節錄"><a href="#實作課程重點節錄" class="headerlink" title="實作課程重點節錄"></a>實作課程重點節錄</h1><h2 id="Serverless-Webpage"><a href="#Serverless-Webpage" class="headerlink" title="Serverless Webpage"></a>Serverless Webpage</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><p>Lambda function 的 trigger 有很多種(例如：CodeCommit, DynamoDB, Kinesis, S3, SNS, SQS, AWS IoT, API Gateway … 等等)</p></li><li><p>要透過 Lambda function 提供動態網頁內容，則是需要搭配 <code>API Gateway</code></p></li><li><p>Environment variables 是其中一種將變數傳到 Lambda function 的方法</p></li><li><p>可為 Lambda function 設定執行時所配置的 memory 大小 &amp; timeout</p></li><li><p>Concurrency 預設為 1000，可以調整</p></li></ul><h3 id="設定-trigger"><a href="#設定-trigger" class="headerlink" title="設定 trigger"></a>設定 trigger</h3><p>API 部份：</p><ul><li><p>因為只提供單純的動態頁面，不需要給 ANY(所以可以刪除)，只需要 GET 即可</p></li><li><p>新增 GET method 需要重新設定 Integration Type(Lambda Function)、勾選 Use Lambda Proxy Integration、選擇 Lambda function … 等等</p></li><li><p>API 設定完後，需要執行 <code>Deploy API</code> 工作才能使用</p><blockquote><p>Deployment stage 選擇 <code>default</code> 即可</p></blockquote></li></ul><h1 id="Serverless-Application-Model-SAM"><a href="#Serverless-Application-Model-SAM" class="headerlink" title="Serverless Application Model (SAM)"></a>Serverless Application Model (SAM)</h1><h2 id="什麼是-SAM"><a href="#什麼是-SAM" class="headerlink" title="什麼是 SAM?"></a>什麼是 SAM?</h2><ul><li><p>Serverless Application Model(SAM) 被開發出來的主要用意是要讓開發者能很容易的開發 serverless application </p></li><li><p>一種 CloudFormation extension，用來建立 serverless application (因此所有 CloudFormation 的功能，SAM 都有支援)</p></li><li><p>提供了新的 type，分別是 <code>function</code>, <code>API</code>, <code>table</code>(DynamoDB)</p></li><li><p>可在 local 使用 Docker 運行 serverless application</p></li><li><p>可搭配 CodeDeploy 來打包並佈署程式</p></li></ul><h2 id="使用-SAM-需要注意的事項"><a href="#使用-SAM-需要注意的事項" class="headerlink" title="使用 SAM 需要注意的事項"></a>使用 SAM 需要注意的事項</h2><ul><li>透過 SAM cli 可以很方便的產生 sample template，並根據需求進行調整修改 (以下是一個 SAM template 的範例)</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 宣告這是 SAM template</span></span><br><span class="line"><span class="attr">AWSTemplateFormatVersion:</span> <span class="string">&#x27;2010-09-09&#x27;</span></span><br><span class="line"><span class="attr">Transform:</span> <span class="string">AWS::Serverless-2016-10-31</span></span><br><span class="line"><span class="attr">Description:</span> <span class="string">Hello</span> <span class="string">Worl</span> <span class="string">SAM</span> <span class="string">Template</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可用來套用到所有 function 的 global configuration</span></span><br><span class="line"><span class="attr">Globals:</span></span><br><span class="line">  <span class="attr">Function:</span></span><br><span class="line">    <span class="attr">Timeout:</span> <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用本地端程式建立 Lambda function 的定義</span></span><br><span class="line"><span class="comment"># 會建立 API Gateway endpoint, mappings &amp; permissions</span></span><br><span class="line"><span class="attr">Resources:</span></span><br><span class="line">  <span class="attr">HelloWorldFunction:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">AWS::Serverless::Function</span></span><br><span class="line">    <span class="attr">Properties:</span></span><br><span class="line">      <span class="attr">CodeUri:</span> <span class="string">hello_world/</span></span><br><span class="line">      <span class="attr">Handler:</span> <span class="string">app.lambda_handler</span></span><br><span class="line">      <span class="attr">Runtime:</span> <span class="string">python3.8</span></span><br><span class="line">      <span class="attr">Events:</span></span><br><span class="line">        <span class="attr">HelloWorld:</span></span><br><span class="line">          <span class="attr">Type:</span> <span class="string">Api</span></span><br><span class="line">          <span class="attr">Properties:</span></span><br><span class="line">            <span class="attr">Path:</span> <span class="string">/hello</span></span><br><span class="line">            <span class="attr">Method:</span> <span class="string">get</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 輸出與上述資源相關的資訊</span></span><br><span class="line"><span class="attr">Outputs:</span></span><br><span class="line">  <span class="attr">HelloWorldFunction:</span></span><br><span class="line">    <span class="attr">Description:</span> <span class="string">&quot;Hello World Lambda Function ARN&quot;</span></span><br><span class="line">    <span class="attr">Value:</span> <span class="type">!GetAtt</span> <span class="string">HelloWorldFunction.Arn</span></span><br></pre></td></tr></table></figure><blockquote><p>其實這就是一個 CloudFormation 的定義，只是多了 <code>Type: AWS::Serverless::Function</code> 在上面</p></blockquote><ul><li>透過 <code>sam build</code> &amp; <code>sam deploy --guide</code> 就會出現一系列的互動操作，協助將程式佈署到 AWS Lambda 上</li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>可作為 Lambda Function event source：</p><p><img src="/blog/images/aws/Serverless_Lambda-Function-Event-Sources.png" alt="Lambda Function event source"></p><h1 id="Elastic-Container-Service-ECS"><a href="#Elastic-Container-Service-ECS" class="headerlink" title="Elastic Container Service (ECS)"></a>Elastic Container Service (ECS)</h1><h2 id="什麼是-ECS"><a href="#什麼是-ECS" class="headerlink" title="什麼是 ECS?"></a>什麼是 ECS?</h2><p><img src="/blog/images/aws/ECS_Introduction.png" alt="ECS introduction"></p><p>ECS 其實就是 AWS 幫你運行 container，幫你省掉很多管理的功夫，也整合了原有的 IAM，大概有以下幾個特點：</p><ul><li><p>全託管服務，是種 container orchestration service</p></li><li><p>佈署 container 時，會建立相對應的 cluster</p></li><li><p>如果不想要管理 VM，還可以選擇 serverless Fargate</p></li><li><p>內含 scheduler，會將 container 自動放到最佳的位置運行</p></li><li><p>可定義每個 container 的 CPU &amp; memory 資源需求規則</p></li><li><p>提供完整的監控功能 (當然也能與 CloudTrail &amp; CloudWatch 進行整合)</p></li><li><p>提供開發者方便佈署、更新、roll back workload 的方式</p></li><li><p>上面的管理功能都是免費的，使用者只要為 workload 付費即可</p></li><li><p>可與現有的 VPC, security groups, EBS volumes, ELB …. 等服務整合</p></li></ul><h2 id="ECS-components"><a href="#ECS-components" class="headerlink" title="ECS components"></a>ECS components</h2><p>要正確的使用 ECS，就必須先知道 ECS 中包含了以下幾個 component &amp; concepts：</p><ul><li><p><code>Cluster</code>：一個邏輯的概念，包含了 ECS 的各種資源(包含 EC2 instance or Fargate instance)</p></li><li><p><code>Task Definition</code>：workload 的定義，類似 Dockerfile，但運行在 ECS 中；重點是 Task Definition 中可以包含多個 container 的定義 (類似 k8s pod)</p></li><li><p><code>Container Definition</code>：單一 container 的定義，CPU, memory, port mapping 相關的設定會放在這</p><blockquote><p>想當然爾，這個會包含在 Task Definition 中</p></blockquote></li><li><p><code>Task</code>：就是真實運行的 workload，透過 Task Definition 產生出來</p></li><li><p><code>Service</code>：用來作為 scaling 的設定，可定義 minimum &amp; maximum 的值</p></li><li><p><code>Registry</code>：用來儲存 container image 的服務，例如：ECR or Docker Hub</p></li></ul><h2 id="AWS-Fargate-的特色"><a href="#AWS-Fargate-的特色" class="headerlink" title="AWS Fargate 的特色"></a>AWS Fargate 的特色</h2><p>AWS Fargate 是 AWS 的新服務，其實目的很簡單，就是要減輕使用者管理的負擔，有以下幾個特點：</p><ul><li><p>是種 Serverless cotnainer engine，因此可用在 ECS &amp; EKS 上</p></li><li><p>使用者再也不用管理 VM 了，專注在服務 &amp; 應用上就可以了</p></li><li><p>僅須根據 application 的資源使用量付費</p></li><li><p>每個 workload 都會擁有獨立的 kernel，因此共享 kernel 所造成的安全疑慮也就沒有了</p></li><li><p>每個 workload 都是獨立 &amp; 安全的</p></li></ul><h3 id="使用-EC2-instance-的時機"><a href="#使用-EC2-instance-的時機" class="headerlink" title="使用 EC2 instance 的時機"></a>使用 EC2 instance 的時機</h3><p>Fargate 看似美好，但若是遇到以下情況，可能就還是需要選用 EC2 instance 來管理 workload：</p><ul><li><p>公司規定</p></li><li><p>需要更高度的客製化</p></li><li><p>需要 GPU</p></li></ul><h2 id="如何接入外部的流量"><a href="#如何接入外部的流量" class="headerlink" title="如何接入外部的流量?"></a>如何接入外部的流量?</h2><p>要接入外部的流量，則需要跟 ELB 搭配，而 ECS + ELB 的搭配有以下幾個特性需要注意：</p><ul><li><p>ELB 的部份，有 ALB、NLB、CLB 可選擇</p></li><li><p>ALB 用來分流 HTTP/HTTPS(layer 7) 的流量，同時支援以下特性：</p><ul><li>dynamic host port mapping</li><li>path-based routing</li><li>priority rules</li></ul></li><li><p>NLB &amp; CLB 用來分流 TCP(layer 4) 的流量</p></li><li><p>ALB 是建議優先選用的 ELB type</p></li><li><p>同時支援 EC2 &amp; Fargate</p></li></ul><h2 id="安全性的考量"><a href="#安全性的考量" class="headerlink" title="安全性的考量"></a>安全性的考量</h2><p>在安全相關的設定上，基本上就是<strong>以最小權限原則來進行設定</strong>，那這件事情應該要如何完成呢? 先看下圖：</p><p><img src="/blog/images/aws/ECS_Security.png" alt="ECS security"></p><ul><li><p>圖中的左半部，安全性設定是套用在 EC2 instance上，因此所有運行在 EC2 instance 上的 task 都有存取同樣資源的權限</p></li><li><p>圖中的右半部，安全性設定則是套用在每個 Task 上，每個 task role 都會有不同的資源存取權限</p></li><li><p>綜合以上兩點，使用 ECS 時若要取得較高的安全性，則是可以將 IAM role 設定在 task level 上</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/lambda/">AWS Lambda – Serverless Compute - Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/serverless/sam/">AWS Serverless Application Model - Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/ecs/">Amazon ECS - Run containerized applications in production</a></p></li><li><p><a href="https://aws.amazon.com/fargate/">AWS Fargate - Run containers without having to manage servers or clusters</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> Serverless </tag>
            
            <tag> Lambda </tag>
            
            <tag> ECS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 原則 &amp; 設計流程</title>
      <link href="/blog/Architecture_Design/Architecture-Design-Principles-and-Processes/"/>
      <url>/blog/Architecture_Design/Architecture-Design-Principles-and-Processes/</url>
      
        <content type="html"><![CDATA[<h1 id="架構設計三原則"><a href="#架構設計三原則" class="headerlink" title="架構設計三原則"></a>架構設計三原則</h1><ul><li><p>程式設計師和架構師之間還有一個明顯的鴻溝需要跨越，這個鴻溝就是”<strong>不確定性</strong>“</p></li><li><p>對於開發程式來說，本質上是不能存在不確定的，對於同樣一段程式，不管是誰寫的，不管什麼時候執行，執行的結果應該都是確定的</p></li><li><p>對於架構設計來說，本質上是不確定的，同樣的一個系統，A 公司和 B 公司做出來的架構可能差異很大，但最後都能正常運轉</p></li><li><p>架構設計領域並沒有一套通用的規範來指導架構師進行架構設計，更多是依賴架構師的經驗和直覺</p></li></ul><p>架構設計時遵循以下幾個原則，有助於你做出最好的選擇：</p><h2 id="合適原則"><a href="#合適原則" class="headerlink" title="合適原則"></a>合適原則</h2><ul><li><p>合適原則宣言：**”合適優於業界領先”**</p></li><li><p>常見失敗原因：</p><ol><li>將軍難打無兵之仗：沒那麼多人，卻想幹那麼多活，是失敗的第一個主要原因</li><li>羅馬不是一天建成的：沒有那麼多累積，卻想一步登天，是失敗的第二個主要原因</li><li>冰山下面才是關鍵：沒有那麼卓越的業務場景，卻幻想靈光一閃成為天才，是失敗的第三個主要原因</li></ol></li></ul><h2 id="簡單原則"><a href="#簡單原則" class="headerlink" title="簡單原則"></a>簡單原則</h2><ul><li><p>簡單原則宣言：**”簡單優於複雜”**</p></li><li><p>“複雜”在製造領域代表先進，在建築領域代表領先，但<strong>在軟體領域，卻恰恰相反，代表的是”問題”</strong></p></li><li><p>軟體領域的複雜性體現在兩個方面：</p><ul><li>結構的複雜性：<ul><li>元件越多，就越有可能其中某個元件出現故障</li><li>某個元件改動，會影響關聯的所有元件</li><li>定位一個複雜系統中的問題總是比簡單系統更加困難</li></ul></li><li>邏輯的複雜性：<ul><li>邏輯複雜的元件，一個典型特徵就是單個元件承擔了太多的功能</li><li>邏輯複雜幾乎會導致軟體工程的每個環節都有問題</li></ul></li></ul></li><li><p>一個軟體系統在投入使用後，後續還有源源不斷的需求要實現，因此要不斷地修改系統，複雜性在整個系統生命週期中都有很大影響</p></li><li><p><strong>架構設計時如果簡單的方案和複雜的方案都可以滿足需求，最好選擇簡單的方案</strong></p></li></ul><h2 id="演化原則"><a href="#演化原則" class="headerlink" title="演化原則"></a>演化原則</h2><ul><li><p>演化原則宣言：**”演化優於一步到位”**</p></li><li><p>建築一旦完成（甚至一旦開建）就不可再變，而<strong>軟體卻需要根據業務的發展不斷地變化</strong></p><blockquote><p>對於建築來說，永恆是主題；而對於軟體來說，<strong>變化</strong>才是主題</p></blockquote></li><li><p>如果沒有把握”<strong>軟體架構需要根據業務發展不斷變化</strong>“這個本質，在做架構設計的時候就很容易陷入一個誤區：<strong>試圖一步到位設計一個軟體架構，期望不管業務如何變化，架構都穩如磐石</strong></p></li><li><p>軟體架構設計其實更加類似於大自然”設計”一個生物，通過演化讓生物適應環境，逐步變得更加強大</p></li><li><p>軟體架構設計正常流程：</p><ol><li>設計出來的架構要滿足當時的業務需要。</li><li>架構要不斷地在實際應用過程中迭代，保留優秀的設計，修復有缺陷的設計，改正錯誤的設計，去掉無用的設計，使得架構逐漸完善。</li><li>當業務發生變化時，架構要擴展、重構，甚至重寫；程式也許會重寫，但有價值的經驗、教訓、邏輯、設計等(類似生物體內的基因)卻可以在新架構中延續</li></ol></li><li><p>架構師應該時刻提醒自己不要貪大求全，或者盲目照搬大公司的做法：應該認真分析當前業務的特點，明確業務面臨的主要問題，設計合理的架構，快速落地以滿足業務需要，然後在運行過程中不斷完善架構，不斷隨著業務演化架構</p></li><li><p>不可能完美預測所有的業務發展和變化路徑</p></li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>架構即決策。架構需要面向業務需求，並在各種資源(人、財、物、時、事)限制條件下去做權衡、取捨</p></li><li><p>決策就會存在不確定性。採用一些高屋建瓴的設計原則有助於去消除不確定，去逼近解決問題的最優解</p></li><li><p><strong>合適原則</strong>：架構無優劣，但存合適性。”汝之蜜糖，吾之砒霜”；架構一定要匹配企業所在的業務階段；不要面向簡歷去設計架構，高大上的架構不等於適用；削足適履與打腫充胖都不符合合適原則；所謂合適，一定要匹配業務所處階段，能夠合理地將資源整合在一起並發揮出最大功效，並能夠快速落地</p></li><li><p><strong>簡單原則</strong>：其實，簡單比複雜更加困難。面對系統結構、業務邏輯和複雜性，我們可以編寫出複雜的系統，但在軟體領域，複雜代表的是”問題”。架構設計時如果簡單的方案和複雜的方案都可以滿足需求，最好選擇簡單的方案。但是，事實上，當軟體系統變得太複雜後，就會有人換一個思路進行重構、升級，將它重新變得簡單，這也是軟體開發的大趨勢。簡單原則是一個樸素且偉大的原則，Google 的 MapReduce 系統就採用了分而治之的思想，而背後就是將複雜問題轉化為簡單問題的典型案例。</p></li><li><p><strong>演化原則</strong>：大到人類社會、自然生物，小到一個細胞，似乎都遵循這一普世原則，軟體架構也不例外。業務在發展、技術在創新、外部環境在變化，這一切都是在告誡架構師不要貪大求全，或者盲目照搬大公司的做法。應該認真分析當前業務的特點，明確業務面臨的主要問題，設計合理的架構，快速落地以滿足業務需要，然後在運行過程中不斷完善架構，不斷隨著業務演化架構</p></li></ul><h1 id="架構設計流程：識別複雜度"><a href="#架構設計流程：識別複雜度" class="headerlink" title="架構設計流程：識別複雜度"></a>架構設計流程：識別複雜度</h1><ul><li><p>只有正確分析出了系統的複雜性，後續的架構設計方案才不會偏離方向</p></li><li><p>如果對系統的複雜性判斷錯誤，即使後續的架構設計方案再完美再先進，都是南轅北轍，做的越好，錯的越多、越離譜</p></li><li><p>架構的複雜度主要來源於”<strong>高性能</strong>“、”<strong>高可用</strong>“、”<strong>可擴展</strong>“等幾個方面，但架構師在具體判斷複雜性的時候，不能生搬硬套，認為任何時候架構都必須同時滿足這三方面的要求</p></li><li><p>將主要的複雜度問題列出來，然後根據業務、技術、團隊等綜合情況進行排序，優先解決當前面臨的最主要的複雜度問題</p></li><li><p>對於架構師來說，關注的不是一天的數據，而是 1 秒的數據，即 <code>TPS</code>(Transactions per second) 和 <code>QPS</code>(Queries per second)</p></li><li><p>雖然根據當前業務規模計算的性能要求並不高，但業務會增長，因此系統設計需要考慮一定的性能餘量</p></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>架構設計由需求所驅動，本質目的是<strong>為了解決軟件系統的複雜性</strong>；為此，我們在進行架構設計時，需要以理解需求為前提，首要進行系統複雜性的分析。具體做法是：</p><ol><li>構建複雜度的來源清單 - 高性能、可用性、擴展性、安全、低成本、規模 … 等等</li><li>結合需求、技術、團隊、資源等對上述複雜度逐一分析是否需要？是否關鍵？<ul><li>“高性能”主要從軟件系統未來的TPS、響應時間、server 資源利用率等客觀指標，也可以從用戶的主觀感受方面去考慮</li><li>“可用性”主要從服務不中斷等質量屬性，符合行業政策、國家法規等方面去考慮</li><li>“擴展性”則主要從功能需求的未來變更幅度等方面去考慮</li></ul></li><li>按照上述的分析結論，得到複雜度按照優先級的排序清單，越是排在前面的複雜度，就越關鍵，就越優先解決；不建議一下子同時出來過多的複雜性來源，例如同時處理高性能、高可用和可擴展是不合理的</li></ol></li><li><p>需要特別注意的是：隨著所處的業務階段不同、外部的技術條件和環境的不同，得到的複雜度問題的優先級排序就會有所不同。一切皆變化</p></li><li><p>如果運氣真的不好，接手了一個每個複雜度都存在問題的系統，那應該怎麼辦呢？答案是一個個來解決問題，不要幻想一次架構重構解決所有問題</p></li><li><p>識別複雜度對架構師來說是一項挑戰，因為原始的需求中並沒有哪個地方會明確地說明複雜度在哪裡，需要架構師在理解需求的基礎上進行分析</p><blockquote><p>有經驗的架構師可能一看需求就知道複雜度大概在哪裡；如果經驗不足，那只能採取”排查法”，從不同的角度逐一進行分析</p></blockquote></li></ul><h1 id="架構設計流程：設計備選方案"><a href="#架構設計流程：設計備選方案" class="headerlink" title="架構設計流程：設計備選方案"></a>架構設計流程：設計備選方案</h1><ul><li><p>成熟的架構師需要對已經存在的技術非常熟悉，對已經經過驗證的架構模式爛熟於心，然後根據自己對業務的理解，挑選合適的架構模式進行組合，再對組合後的方案進行修改和調整</p></li><li><p>架構設計備選方案的工作更多的是從需求、團隊、技術、資源等綜合情況出發，對主流、成熟的架構模式進行選擇、組合、調整、創新</p></li><li><p>雖說基於已有的技術或者架構模式進行組合，然後調整，大部分情況下就能夠得到我們需要的方案，但並不意味著架構設計是一件很簡單的事情</p></li></ul><h2 id="常見錯誤"><a href="#常見錯誤" class="headerlink" title="常見錯誤"></a>常見錯誤</h2><h3 id="設計最優秀的方案"><a href="#設計最優秀的方案" class="headerlink" title="設計最優秀的方案"></a>設計最優秀的方案</h3><p>根據架構設計原則中”合適原則”和”簡單原則”的要求，挑選合適自己業務、團隊、技術能力的方案才是好方案；否則可能會浪費大量資源開發了無用的系統</p><h3 id="只做一個方案"><a href="#只做一個方案" class="headerlink" title="只做一個方案"></a>只做一個方案</h3><ul><li><p>心裡評估過於簡單，可能沒有想得全面</p></li><li><p>架構師再怎麼牛，經驗知識和技能也有侷限，有可能某個評估的標準或者經驗是不正確的</p></li><li><p>單一方案設計會出現過度辯護的情況</p></li></ul><p>合理的作法：</p><ul><li>備選方案的數量以 3 ~ 5 個為最佳</li><li>備選方案的差異要比較明顯</li><li>備選方案的技術不要只侷限於已經熟悉的技術</li></ul><h3 id="備選方案過於詳細"><a href="#備選方案過於詳細" class="headerlink" title="備選方案過於詳細"></a>備選方案過於詳細</h3><p>正確的做法是備選階段關注的是技術選型，而不是技術細節</p><h2 id="討論整理精華-2"><a href="#討論整理精華-2" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>從架構設計三原則出發，也可考慮第四個備選方案：上雲方案，該方案是直接採用商業解決方案，就好比阿里前期採用 IOE 類似</p></li><li><p>如果是創業公司的業務早、中期階段，可直接考慮採用阿里雲/騰訊雲，性能、HA、伸縮性都有保證</p></li><li><p><strong>做事情永遠都要有 plan B</strong></p></li></ul><h1 id="架構設計流程：評估和選擇備選方案"><a href="#架構設計流程：評估和選擇備選方案" class="headerlink" title="架構設計流程：評估和選擇備選方案"></a>架構設計流程：評估和選擇備選方案</h1><ul><li><p>如何挑選出最終的方案也是一個很大的挑戰，主要原因有：</p><ul><li>每個方案都是可行的，如果方案不可行就根本不應該作為備選方案</li><li>沒有哪個方案是完美的</li><li>評價標準主觀性比較強</li></ul></li><li><p>實踐中很多設計師或者架構師就採取了下面幾種指導思想：</p><ul><li>最簡派</li><li>最牛派</li><li>最熟派</li><li>領導派</li></ul></li><li><p>列出我們需要關注的質量屬性點，然後分別從這些質量屬性的維度去評估每個方案，再綜合挑選適合當時情況的最優方案</p></li><li><p>常見的方案質量屬性點有：性能、可用性、硬體成本、項目投入、複雜度、安全性、可擴展性等</p></li><li><p>在評估這些質量屬性時，需要遵循架構設計原則(合適、簡單)，避免貪大求全，基本上某個質量屬性能夠滿足一定時期內業務發展就可以了</p></li><li><p>如果每次做方案都考慮這種發生機率很小的事件，我們的方案會出現過度設計，導致投入浪費</p></li><li><p>量變會引起質變，具體哪些地方質變，是很難提前很長時間預測到的</p></li><li><p>備選方案的選擇上，有幾種看似正確但實際錯誤的做法：</p><ul><li><strong>數量對比法</strong>：簡單地看哪個方案的優點多就選哪個 =&gt; 這種方案主要的問題在於把所有質量屬性的重要性等同，而沒有考慮質量屬性的優先級</li><li><strong>加權法</strong>：每個質量屬性給一個權重 =&gt; 無法客觀地給出每個質量屬性的權重得分</li></ul></li><li><p>架構師綜合當前的<strong>業務發展情況</strong>、<strong>團隊人員規模和技能</strong>、<strong>業務發展預測</strong>等因素，將質量屬性按照優先級排序</p></li><li><p>備選方案的選擇和很多因素相關，並不單單考慮性能高低、技術是否優越這些純技術因素。業務的需求特點、運維團隊的經驗、已有的技術體系、團隊人員的技術水平都會影響備選方案的選擇</p></li></ul><h1 id="架構設計流程：詳細方案設計"><a href="#架構設計流程：詳細方案設計" class="headerlink" title="架構設計流程：詳細方案設計"></a>架構設計流程：詳細方案設計</h1><ul><li><p>Nginx 的負載均衡策略，簡單按照下面的規則選擇就可以了：</p><ul><li>**輪詢(預設)**：每個請求按時間順序逐一分配到不同的後端 server，後端 server 分配的請求數基本一致，如果後端 server 無法正常提供服務，能自動剔除。</li><li><strong>加權輪詢</strong>：根據權重來進行輪詢，權重高的 server 分配的請求更多，主要適應於後端 server 性能不均的情況，如新舊 server 混用</li><li><strong>ip_hash</strong>：每個請求按訪問 IP 的 hash 結果分配，這樣每個訪客固定訪問一個後端 server ，主要用於解決 session 的問題，如購物車類的應用</li><li><strong>fair</strong>：按後端 server 的響應時間來分配請求，響應時間短的優先分配，能夠最大化地平衡各後端 server 的壓力，可以適用於後端server 性能不均衡的情況，也可以防止某台後端 server 性能不足的情況下還繼續接收同樣多的請求從而造成雪崩效應</li><li><strong>url_hash</strong>：按訪問 URL 的 hash 結果來分配請求，每個 URL 定向到同一個後端 server ，適用於後端 server 能夠將 URL 的響應結果緩存的情況。</li></ul></li><li><p>詳細設計方案階段可能遇到的一種極端情況就是<strong>在詳細設計階段發現備選方案不可行</strong>，一般情況下主要的原因是備選方案設計時<strong>遺漏了某個關鍵技術點或者關鍵的質量屬性</strong>(例如：開發週期)</p></li><li><p>如何避免上述情況發生：</p><ul><li>架構師不但要進行備選方案設計和選型，還需要對備選方案的關鍵細節有較深入的理解</li><li>通過分步驟、分階段、分系統等方式，儘量降低方案複雜度</li><li>如果方案本身就很複雜，那就採取設計團隊的方式來進行設計</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[架構設計] 目的 &amp; 複雜度來源</title>
      <link href="/blog/Architecture_Design/Architecture-Design-Complexity-Source/"/>
      <url>/blog/Architecture_Design/Architecture-Design-Complexity-Source/</url>
      
        <content type="html"><![CDATA[<h1 id="架構設計的目的"><a href="#架構設計的目的" class="headerlink" title="架構設計的目的"></a>架構設計的目的</h1><h2 id="對架構設計常見的誤解"><a href="#對架構設計常見的誤解" class="headerlink" title="對架構設計常見的誤解"></a>對架構設計常見的誤解</h2><h3 id="因為架構很重要，所以要做架構設計"><a href="#因為架構很重要，所以要做架構設計" class="headerlink" title="因為架構很重要，所以要做架構設計"></a>因為架構很重要，所以要做架構設計</h3><ul><li><p>其實不一定，一間公司的初期產品可能是沒有架構設計的，而一開始沒有架構設計時，反而開發效率是最高的，進行架構設計畢竟需要投入時間和人力</p></li><li><p>如果系統使用人數沒有很多，或是資料量不大，沒有架構設計也不一定會發生什麼問題</p></li><li><p>架構設計並無法保證可以促進業務發展</p></li></ul><h3 id="不是每個系統都要做架構設計嗎"><a href="#不是每個系統都要做架構設計嗎" class="headerlink" title="不是每個系統都要做架構設計嗎?"></a>不是每個系統都要做架構設計嗎?</h3><ul><li><p>架構師很容易直接套用業界其他公司的架構，但卻沒仔細思考是否符合公司實際需求</p></li><li><p>不適合的系統架構可能會導致不斷重構、甚至砍掉重練</p></li></ul><h3 id="公司流程中要求系統開發過程中必須有架構設計"><a href="#公司流程中要求系統開發過程中必須有架構設計" class="headerlink" title="公司流程中要求系統開發過程中必須有架構設計"></a>公司流程中要求系統開發過程中必須有架構設計</h3><ul><li>不要為了做架構設計而做</li></ul><h3 id="為了高性能、高可用、可擴展，所以要做架構設計"><a href="#為了高性能、高可用、可擴展，所以要做架構設計" class="headerlink" title="為了高性能、高可用、可擴展，所以要做架構設計"></a>為了高性能、高可用、可擴展，所以要做架構設計</h3><p>不經過審慎思考，直接就設計高性能、高可用、可擴展的架構，可能會出現以下問題：</p><ul><li><p>系統初期就過於複雜</p></li><li><p>專案項目落地的時間遙遙無期</p></li><li><p>上線後運行不夠穩定，遇到問題不容易定位，很難解決</p></li></ul><h2 id="架構設計的真正目的"><a href="#架構設計的真正目的" class="headerlink" title="架構設計的真正目的"></a>架構設計的真正目的</h2><ul><li><p>架構設計的主要目的是<strong>為了解決軟體系統複雜度帶來的問題</strong></p><blockquote><p>遵循這條準則能夠讓”新手”架構師心中有數，而不是一頭霧水。</p></blockquote></li><li><p>架構設計的目的不是為了”防範”後期變化，而是要<strong>適當預測然後做好準備</strong></p></li></ul><p>以下舉例說明：</p><ul><li><p>我們的系統一定要做到每秒 TPS 10 萬</p><blockquote><p>如果系統的複雜度不是在性能這部分，TPS 做到 10 萬並沒有什麼用</p></blockquote></li><li><p>淘寶的架構是這麼做的，我們也要這麼做</p><blockquote><p>淘寶的架構是為瞭解決淘寶業務的複雜度而設計的，淘寶的業務複雜度並不就是我們的業務複雜度，絕大多數業務的用戶量都不可能有淘寶那麼大</p></blockquote></li><li><p>Docker 現在很流行，我們的架構應該將 Docker 應用進來</p><blockquote><p>Docker 並非萬能的，只是為瞭解決資源重用和動態分配而設計的，如果我們的系統複雜度根本不是在這方面，引入 Docker 沒有什麼意義</p></blockquote></li></ul><h2 id="討論整理精華"><a href="#討論整理精華" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>架構是為了應對軟體系統複雜度而提出的一個解決方案</p></li><li><p>架構即(重要)決策，是在一個有約束的盒子裡去求解或接近最合適的解</p></li><li><p>這個有約束的盒子是團隊經驗、成本、資源、進度、業務所處階段等所編織、摻雜在一起的綜合體(人，財，物，時間，事情等)</p></li><li><p>架構無優劣，但是存在恰當的架構用在合適的軟體系統中，而這些就是決策的結果</p></li><li><p>需求驅動架構；在分析設計階段，需要考慮一定的人力與時間去”跳出代碼，總攬全局”，為業務和IT技術之間搭建一座”橋樑”</p></li><li><p>軟體架構是為兩件事服務的：<code>業務架構</code>和<code>業務量級</code>，業務架構和業務量級都是從每個具體項目的實際應用場景中提煉出來的</p></li><li><p>業務架構是對業務需求的提煉和抽象，開發軟體必須要滿足業務需求，否則就是空中樓閣。軟體系統業務上的複雜度問題，可以從業務架構的角度切分工作交界面來解決。設計軟體架構，首先是要保證能和業務架構對的上，這也是從業務邏輯轉向代碼邏輯的過程，所以軟體架構的設計為開發指明了方向。另外架構設計也為接下來的開發工作分工奠定了基礎。</p></li><li><p>業務量級表現在存儲能力、吞吐能力和容錯能力等，主要是軟體運維期業務的複雜度。做軟體架構設計，是要保證軟體有能力托起它在業務量級上的要求的，如果軟體到運行使用期廢了，前面所有的工作都付諸東流了。不同的業務量級，對應的軟體的架構複雜度是不同的，所以對於不同的項目，業務量級不同，架構設計也不同。</p></li><li><p>做業務架構必須與其面向的實際應用場景相匹配；所以每個軟體在開發前，都要結合自己的應用場景設計適合自身的軟體架構，現成的架構方案只能借鑑，不能直接套用</p></li><li><p>由於業務架構和業務量級也會不斷調整或長大，軟體架構也不是一勞永逸的，會隨業務不斷調整</p></li></ul><h1 id="複雜度來源：高性能-High-Performance"><a href="#複雜度來源：高性能-High-Performance" class="headerlink" title="複雜度來源：高性能(High Performance)"></a>複雜度來源：高性能(High Performance)</h1><p>軟體系統中高性能帶來的複雜度主要體現在兩方面：</p><ul><li><p>單機內部為了高性能帶來的複雜度</p></li><li><p>多機集群為了高性能帶來的複雜度</p></li></ul><h2 id="單機複雜度"><a href="#單機複雜度" class="headerlink" title="單機複雜度"></a>單機複雜度</h2><ul><li><p>將硬體性能充分發揮出來的關鍵就是 OS(Operating System)，所以 OS 本身其實也是跟隨硬體的發展而發展的， OS 是軟體系統的運行環境，OS的複雜度直接決定了軟體系統的複雜度</p></li><li><p>OS 中和性能最相關的就是 <code>Process</code> 和 <code>Thread</code></p></li><li><p>為了解決手工操作帶來的低效率，批次處理 OS 應運而生</p></li><li><p>為了進一步提升性能，人們發明了 <code>Process</code> 來對應一個任務，每個任務都有自己獨立的內存空間，process 間互不相關，由 OS 來進行調度</p></li><li><p>此時的 CPU 還沒有 multiple core 和 multiple thread 的概念，為了達到 multiple process 並行運行的目的，採取了分時的方式，即把 CPU 的時間分成很多片段，每個片段只能執行某個 process 中的指令</p></li><li><p>從用戶的角度來看，兩個任務之間能夠在運行過程中就進行通信，會讓任務設計變得更加靈活高效；為了解決這個問題，process 間通信的各種方式被設計出來了，包括 pipeline、message queue、signal、shared storage …. 等等</p></li><li><p>單個 process 內部只能串行處理，而實際上很多 process 內部的子任務並不要求是嚴格按照時間順序來執行的，也需要並行處理；為了解決這個問題，人們又發明了 thread，thread 是 process 內部的子任務，但這些子任務都共享同一份 process 數據。為了保證數據的正確性，又發明了 Mutex(互斥鎖)機制。</p></li><li><p>有了 multiple thread 後， <strong>OS 調度的最小單位就變成了 thread</strong>，而 <strong>process 變成了 OS 分配資源的最小單位</strong></p></li><li><p>multiple process, multiple thread 雖然讓多任務並行處理的性能大大提升，但本質上還是分時系統，並不能做到時間上真正的並行</p></li><li><p>如何達到真正並行處理? =&gt; 讓多個 CPU 能夠同時執行計算任務，從而實現真正意義上的多任務並行，以下是常見架構：</p><ul><li>SMP（Symmetric Multi-Processor，對稱多處理器結構）</li><li>NUMA（Non-Uniform Memory Access，非一致存儲訪問結構）</li><li>MPP（Massive Parallel Processing，海量並行處理結構）</li></ul></li></ul><h2 id="Cluster-集群-的複雜度"><a href="#Cluster-集群-的複雜度" class="headerlink" title="Cluster(集群)的複雜度"></a>Cluster(集群)的複雜度</h2><p>透過大量機器來提升性能，並不僅僅是增加機器這麼簡單，讓多台機器配合起來達到高性能的目的，是一個複雜的任務</p><h3 id="任務分配"><a href="#任務分配" class="headerlink" title="任務分配"></a>任務分配</h3><ul><li><p>任務分配的意思是指每台機器都可以處理完整的業務任務，不同的任務分配到不同的機器上執行</p></li><li><p>實際上”<strong>任務</strong>“涵蓋的範圍很廣，可以指完整的業務處理，也可以單指某個具體的任務。例如：”存儲”、”運算”、”cache”..等都可以作為一項任務，因此存儲系統、運算系統、緩存系統都可以按照任務分配的方式來搭建架構。</p></li><li><p>“任務分配器”也並不一定只能是物理上存在的機器或者一個獨立運行的程序，也可以是嵌入在其他程序中的算法，例如 Memcache 的集群架構</p></li><li><p>雖然系統拆分可能在某種程度上能提升業務處理性能，但提升性能也是有限的，因為<strong>最終決定業務處理性能的還是業務邏輯本身，業務邏輯本身沒有發生大的變化下，理論上的性能是有一個上限的，系統拆分能夠讓性能逼近這個極限，但無法突破這個極限</strong></p></li></ul><h2 id="討論整理精華-1"><a href="#討論整理精華-1" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>性能是軟體的一個重要質量屬性。衡量軟體性能包括了響應時間、TPS、 server 資源利用率等客觀指標，也可以是用戶的主觀感受（從程式設計師、業務用戶、終端用戶/客戶不同的視角，可能會得出不同的結論）。</p></li><li><p>在說性能的時候，有一個概念與之緊密相關 =&gt; <code>伸縮性</code>，這是兩個有區別的概念</p><blockquote><p>性能更多的是衡量軟體系統處理一個請求或執行一個任務需要耗費的時間長短；而伸縮性則更加關注軟體系統在不影響用戶體驗的前提下，能夠隨著請求數量或執行任務數量的增加（減少）而相應地擁有相適應的處理能力</p></blockquote></li><li><p>什麼是**”高”性能**？這可能是一個動態概念，與當前的技術發展狀況與業務所處的階段緊密相關。比如，現在在行業/企業內部認為的高性能，站在5年後來看，未必是高性能</p><ul><li>站在架構師、設計師的角度，高性能需要和業務所處的階段來衡量。高到什麼程度才能與當前或可預見的未來業務增長相匹配。一味去追求絕對意義上的高，沒有太大的實際意義。因為，<strong>伴隨性能越來越高，相應的方法和系統複雜度也是越來越高，而這可能會與當前團隊的人力、技術、資源等不相匹配</strong></li><li>但是什麼才是合適的高性能? 這可能需要從國、內外的同行業規模相當、比自己強的競爭者、終端用戶使用反饋中獲取答案並不斷迭代發展</li></ul></li><li><p>為什麼需要高性能？</p><ul><li>追求良好的用戶體驗</li><li>滿足業務增長的需要</li></ul></li><li><p>如何做好高性能？</p><blockquote><p>可以從垂直與水平兩個維度來考慮。垂直維度主要是針對單台計算機，通過升級軟、硬體能力實現性能提升；水平維度則主要針對集群系統，利用合理的任務分配與任務分解實現性能的提升。</p></blockquote></li><li><p>垂直維度可包括以下措施：</p><ul><li>增加 memory，減少 I/O 操作</li><li>更換 SSD 提升 I/O 訪問速度</li><li>使用 RAID 增加 I/O 吞吐能力</li><li>更換 server 獲得更多的 CPU 或分配更多的 virtual core</li><li>升級網路接口或增加網路接口</li></ul></li><li><p>水平維度可包括以下措施：</p><ul><li>功能分解：基於功能將系統分解為更小的子系統</li><li>多實例副本：同一元件重複部署到多台不同的 server </li><li>數據分割：在每台機器上都只部署一部分數據</li></ul></li><li><p>垂直維度方案比較適合業務階段早期和成本可接受的階段，該方案是提升性能最簡單直接的方式，但是<strong>受成本與硬體能力天花板的限制</strong></p></li><li><p>水平維度方案所帶來的好處要在業務發展的後期才能體現出來；此方案會花費更多的硬體成本，另外一方面對技術團隊也提出了更高的要求；但是沒有垂直方案的天花板問題</p><ul><li>一旦達到一定的業務階段，水平維度是技術發展的必由之路</li><li>作為技術部門，需要提前佈局 ，未雨綢繆，不要被業務拋的太遠</li></ul></li></ul><h1 id="複雜度來源：高可用-High-Availability"><a href="#複雜度來源：高可用-High-Availability" class="headerlink" title="複雜度來源：高可用(High Availability)"></a>複雜度來源：高可用(High Availability)</h1><ul><li><p>高可用定義：<strong>系統無中斷地執行其功能的能力，代表系統的可用性程度</strong>，是進行系統設計時的準則之一</p></li><li><p>這個定義的關鍵在於”無中斷”，但恰好困難點也在”無中斷”上面，因為無論是單個硬體還是單個軟體，都不可能做到無中斷，硬體會出故障，軟體會有 bug；硬體會逐漸老化，軟體會越來越複雜和龐大 …. 等等</p></li><li><p>系統的高可用方案五花八門，但萬變不離其宗，本質上都是通過”<strong>冗餘</strong>“來實現高可用</p></li></ul><h2 id="計算高可用"><a href="#計算高可用" class="headerlink" title="計算高可用"></a>計算高可用</h2><p>計算有一個特點就是無論在哪台機器上進行計算，同樣的算法和輸入數據，產出的結果都是一樣的</p><h2 id="存儲高可用"><a href="#存儲高可用" class="headerlink" title="存儲高可用"></a>存儲高可用</h2><ul><li><p>將數據從一台機器搬到到另一台機器，需要經過線路進行傳輸</p></li><li><p>除了物理上的傳輸速度限制，傳輸線路本身也存在可用性問題，傳輸線路可能中斷、可能擁塞、可能異常（錯包、丟包）</p></li><li><p>存儲高可用的困難點不在於如何備份數據，而在於<strong>如何減少或者規避數據不一致對業務造成的影響</strong></p></li><li><p>存儲高可用不可能同時滿足 **<a href="https://zh.wikipedia.org/wiki/CAP%E5%AE%9A%E7%90%86">CAP(“一致性、可用性、分區容錯性”)</a>**，最多滿足其中兩個，這就要求我們在做架構設計時結合業務進行取捨</p><ul><li>Consistency</li><li>Availability</li><li>Partition tolerance</li></ul></li></ul><h2 id="高可用狀態決策"><a href="#高可用狀態決策" class="headerlink" title="高可用狀態決策"></a>高可用狀態決策</h2><ul><li><p><strong>系統需要能夠判斷當前的狀態是正常還是異常</strong>，如果出現了異常就要採取行動來保證高可用</p></li><li><p>三種常見的決策方式：</p><ul><li><strong>獨裁式</strong>：當決策者本身故障時，整個系統就無法實現準確的狀態決策</li><li><strong>協商式</strong>：協商式決策的架構不複雜，規則也不複雜，其難點在於，如果兩者的信息交換出現問題（比如主備連接中斷），此時狀態決策應該怎麼做</li><li><strong>民主式</strong>：民主式決策還有一個固有的缺陷：腦裂(split brain)。</li></ul></li><li><p>無論採取什麼樣的方案，狀態決策都不可能做到任何場景下都沒有問題，但完全不做高可用方案又會產生更大的問題，如何選取適合系統的高可用方案，也是一個複雜的分析、判斷和選擇的過程</p></li></ul><h2 id="討論整理精華-2"><a href="#討論整理精華-2" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>高可用與高性能，是架構設計中兩個非常重要的決策因素；因此在面對不同業務系統的不同需求，對高可用與高性能也會有不同的決策結論，其實現的複雜度也各不相同</p></li><li><p>支付寶業務，對於可用性和性能就會有很高的要求，在可用性方面希望能提供 7x24 不間斷服務，在高性能方面則希望能即時收付款；而對於一個學生管理系統，在可用性與性能方面就不一定要有多高的要求，比如晚上可關機，幾秒內能查詢到信息也可接受</p><blockquote><p>高可用性與高性能的複雜度討論需要結合業務需求</p></blockquote></li><li><p>可以利用百分比來對網站可用性進行度量：</p><ul><li>網站不可用時間 = 完成故障修復的時間點 - 故障發現的時間點</li><li>網站年度可用時間 = 年度總時間 - 網站不可用時間</li><li>網站年度可用性 = (網站年度可用時間/年度總時間) x 100%</li></ul></li></ul><blockquote><p>高可用性就是技術實力的象徵，高可用性就是競爭力</p></blockquote><ul><li><p>為什麼會出現不可用 ?</p><ul><li>硬體故障：網站多運行在普通的商用 server ，而這些 server 本身就不具備高可用性，再加之網站系統背後有數量眾多 server ，那麼一定時間內 server 當機的機率自然會提昇，直接導致部署在該 server 上的服務受影響</li><li>軟體 bug 或網站更新升級發佈：bug 不能消滅，只能減少；上線後的系統在運行過程中，難免會出現故障，而這些故障同樣直接導致某些網站服務不可用；此外，網站更新升級發佈也可能會引起相對較頻繁的 server 當機</li><li>不可抗拒力：如地震、水災、戰爭…等</li></ul></li><li><p>如何做到高可用? <strong>核心思想：網站高可用的主要技術手段是服務與數據的冗餘備份與失效轉移</strong></p><ul><li>同一服務組件部署在多台 server 上</li><li>數據存儲在多台 server 上互相備份</li></ul></li></ul><blockquote><p>通過上述技術手段，當任何一台 server 當機或出現各種不可預期的問題時，就將相應的服務切換到其他可用的 server 上，不影響系統的整體可用性，也不會導致數據丟失</p></blockquote><ul><li>從架構角度看可用性：當前網站系統多採用經典的分層模型，從上到下為：<strong>應用層</strong>、<strong>服務層</strong>與<strong>數據層</strong><ul><li>應用層主要實現業務邏輯處理</li><li>服務層提供可復用的服務</li><li>數據層負責數據讀寫</li></ul></li></ul><blockquote><p>在部署架構上常採用應用和數據分離部署，應用會部署到不同 server 上，這些 server 被稱為應用層的 server；這些可復用的服務也會各自部署在不同 server 上，稱為服務層的 server；而各類數據庫系統、文件櫃等數據則部署在數據層的 server</p></blockquote><ul><li><p>硬體故障方面引起不可用的技術解決措施：</p><ol><li>應用 server：可通過負載均衡設備將多個應用 server 構建為 cluster 對外提供服務(前提是這些服務需要設計為 stateless，即應用 server 不保存業務的上下文信息，而僅根據每次請求提交的數據進行業務邏輯的操作回應)，當 load balancer 通過心跳檢測手段檢測到應用 server 不可用時，則將其從集群中移除，並將請求切換到其他可用的應用服務上</li><li>服務層 server：這些 server 被應用層通過分佈式服務框架(例如：Dubbo)訪問，分佈式服務框架可在應用層客戶端程序中實現軟體負載均衡，並通過服務註冊中心提供服務的 server 進行心跳檢測，當發現有 server 不可用時，立即通知客戶端程序修改服務列表，同時移除回應的 server</li><li>數據 server：需要在數據寫入時進行數據同步複製，將數據寫入多台 server 上，實現數據冗餘備份；當數據 server 當機時，應用程序將訪問切換到有備份數據的 server 上</li></ol></li><li><p>軟體方面引起不可用的技術解決措施：通過軟體開發過程進行質量保證</p><blockquote><p>通過預發佈驗證、嚴格測試、Gray release 等手段，儘量減少上線服務的故障</p></blockquote></li></ul><h1 id="複雜度來源：可擴展性-Scalability"><a href="#複雜度來源：可擴展性-Scalability" class="headerlink" title="複雜度來源：可擴展性(Scalability)"></a>複雜度來源：可擴展性(Scalability)</h1><ul><li><p>可擴展性(Scalability)指系統為了應對將來需求變化而提供的一種擴展能力，當有新的需求出現時，系統不需要或者僅需要少量修改就可以支持，無須整個系統重構或者重建</p></li><li><p>設計具備良好可擴展性的系統，有兩個基本條件：</p><ol><li><strong>正確預測變化</strong></li><li><strong>完美封裝變化</strong></li></ol></li><li><p>軟體系統與硬體或者建築相比，有一個很大的差異：<strong>軟體系統在發佈後還可以不斷地修改和演進，這就表示不斷有新的需求需要實現</strong></p></li><li><p>預測變化的複雜性在於：</p><ul><li>不能每個設計點都考慮可擴展性</li><li>不能完全不考慮可擴展性</li><li>所有的預測都存在出錯的可能性</li></ul></li><li><p>預測變化是一回事，採取什麼方案來應對變化，又是另外一個複雜的事情；即使預測很準確，如果方案不合適，則系統擴展一樣很麻煩</p></li></ul><h2 id="討論整理精華-3"><a href="#討論整理精華-3" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>什麼是架構的可擴展性? (What)</p><blockquote><p>業務需求、運行環境方面的變化都會導致軟體系統發生變化，而這種軟體系統對上述變化的適應能力就是可擴展性</p></blockquote></li><li><p>一個具備良好可擴展性的架構設計應當符合開閉原則：<strong>對擴展開放，對修改關閉</strong></p></li><li><p>衡量一個軟體系統具備良好可擴展性主要表現但不限於：</p><ol><li>軟體自身內部方面。在軟體系統實現新增的業務功能時，對現有系統功能影響較少，即不需要對現有功能作任何改動或者很少改動</li><li>軟體外部方面。軟體系統本身與其他存在協同關係的外部系統之間存在鬆耦合關係，軟體系統的變化對其他軟體系統無影響，其他軟體系統和功能不需要進行改動</li></ol></li><li><p>為什麼要求架構具備良好的可擴展性? (Why)</p><blockquote><p>伴隨業務的發展、創新，運行環境的變化，對技術也就提出了更多、更高的要求。<strong>能夠快速響應上述變化，並最大程度降低對現有系統的影響</strong>，是設計可擴展性好的架構的主要目的</p></blockquote></li><li><p>如何設計可擴展性好的架構? (How)</p><blockquote><p>object oriented、design pattern 都是為了解決可擴展性的而出現的方法與技術</p></blockquote></li><li><p>設計具備良好可擴展性的系統，有兩個思考角度：</p><ol><li>從業務維度。對業務深入理解，對可預計的業務變化進行預測</li><li>從技術維度。利用擴展性好的技術，實現對變化的封裝</li></ol></li><li><p>在業務維度。對業務深入理解，對業務的發展方向進行預判，也就是不能完全不考慮可擴展性；但是，變化無處不在，在業務看得遠一點的同時，需要注意:警惕過度設計；不能每個設計點都考慮可擴展性；所有的預測都存在不正確的可能性。</p></li><li><p>在技術維度。預測變化是一回事，採取什麼方案來應對變化，又是另外一個複雜的事情。即使預測很準確，如果方案不合適，則系統擴展一樣很麻煩</p><ol><li>第一種應對變化的常見方案是將”變化”封裝在一個”變化層”，將不變的部分封裝在一個獨立的”穩定層”</li><li>第二種常見的應對變化的方案是提煉出一個”抽象層”和一個”實現層”</li></ol></li><li><p>在實際軟體系統架構設計中，常通過以下技術手段實現良好的可擴展性：</p><ol><li>使用分佈式服務(框架)構建可復用的業務平台</li><li>使用分佈式消息隊列降低業務模組間的耦合性</li></ol></li><li><p>利用分佈式服務框架(例如：Dubbo)可以將業務邏輯實現和可復用組件服務分離開，通過接口降低子系統或模組間的耦合性</p><ul><li>新增功能時，可以通過調用可復用的組件實現自身的業務邏輯，而對現有系統沒有任何影響</li><li>可復用組件升級變更的時候，可以提供多版本服務對應用實現透明升級，對現有應用不會造成影響</li></ul></li><li><p>基於<strong>生產者-消費者</strong>開發模式，利用分佈式消息隊列(例如：RabbitMQ、Kafka)：</p><ul><li>將用戶請求、業務請求作為消息發佈者將事件構造成消息發佈到消息隊列</li><li>消息的訂閱者作為消費者從消息隊列中獲取消息進行處理</li></ul></li></ul><blockquote><p>通過這種方式將消息生產和消息處理分離開來，可以透明地增加新的消息生產者任務或者新的消息消費者任務。</p></blockquote><h1 id="複雜度來源：低成本、安全、規模"><a href="#複雜度來源：低成本、安全、規模" class="headerlink" title="複雜度來源：低成本、安全、規模"></a>複雜度來源：低成本、安全、規模</h1><h2 id="低成本"><a href="#低成本" class="headerlink" title="低成本"></a>低成本</h2><ul><li><p>當我們設計”高性能” or “高可用”的架構時，通用的手段都是增加更多 server 來滿足”高性能”和”高可用”的要求；而低成本正好與此相反，我們需要減少 server 的數量才能達成低成本的目標</p></li><li><p>低成本很多時候不會是架構設計的首要目標，而是<strong>架構設計的附加約束</strong></p></li><li><p>低成本給架構設計帶來的主要複雜度體現在 =&gt; <strong>往往只有”創新”才能達到低成本目標</strong></p></li><li><p>引入新技術的主要複雜度在於<strong>需要去熟悉新技術，並且將新技術與已有技術結合起來</strong></p></li><li><p>創造新技術的主要複雜度在於需要自己去創造全新的理念和技術，並且新技術跟舊技術相比，需要有質的飛躍</p></li></ul><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><p>從技術的角度來講，安全可以分為兩類：(<code>功能安全</code> &amp; <code>架構安全</code>)</p><h3 id="功能安全"><a href="#功能安全" class="headerlink" title="功能安全"></a>功能安全</h3><ul><li><p>常見的 XSS 攻擊、CSRF 攻擊、SQL 注入、Windows 漏洞、密碼破解等，本質上是因為系統實現有漏洞，hacker 有了可乘之機，功能安全其實就是”防小偷”</p></li><li><p>從實現的角度來看，功能安全更多地是和具體的編碼相關，與架構關係不大</p></li><li><p>功能安全其實也是一個”攻”與”防”的矛盾，只能在這種攻防大戰中逐步完善，不可能在系統架構設計的時候一勞永逸地解決</p></li></ul><h3 id="架構安全"><a href="#架構安全" class="headerlink" title="架構安全"></a>架構安全</h3><ul><li><p>架構安全就是”防強盜”</p></li><li><p>傳統的架構安全主要依靠防火牆，防火牆最基本的功能就是隔離網路，通過將網路劃分成不同的區域，制定出不同區域之間的訪問控制策略來控制不同信任程度區域間傳送的數據流</p></li><li><p>防火牆的功能雖然強大，<strong>但性能一般</strong>，所以在傳統的銀行和企業應用領域應用較多。但在互聯網領域，防火牆的應用場景並不多</p></li><li><p>DDoS 攻擊最大的影響是大量消耗機房的出口總頻寬</p></li><li><p>不管防火牆處理能力有多強，當出口頻寬被耗盡時，整個業務在用戶看來就是不可用的</p></li><li><p>互聯網系統的架構安全目前並沒有太好的設計手段來實現，更多地是依靠運營商或者雲服務商強大的頻寬和流量清洗的能力</p></li></ul><h2 id="規模"><a href="#規模" class="headerlink" title="規模"></a>規模</h2><ul><li><p>規模帶來複雜度的主要原因就是”量變引起質變”，當數量超過一定的閾值後，複雜度會發生質的變化</p></li><li><p>常見的規模帶來的複雜度有：</p><ul><li>功能越來越多，導致系統複雜度指數級上升</li><li>數據越來越多，系統複雜度發生質變</li></ul></li></ul><blockquote><p>數據太多以後，傳統的數據收集、加工、存儲、分析的手段和工具已經無法適應，必須應用新的技術才能解決</p></blockquote><h2 id="討論整理精華-4"><a href="#討論整理精華-4" class="headerlink" title="討論整理精華"></a>討論整理精華</h2><ul><li><p>低成本是架構設計中需要考慮一個約束條件，但不會是首要目標。低成本本質上是與高性能和高可用衝突的，當無法設計出滿足成本要求的方案，就只能協調並調整成本目標</p></li><li><p>一般通過”創新”達到低成本的目標</p><ul><li>引入新技術。主要複雜度在於需要去熟悉新技術，並且將新技術與已有技術結合；一般中小型公司基本採用該方式達到目標</li><li>開創一個全新技術領域。主要複雜度在於需要去創造全新的理念和技術，並且與舊技術相比，需要有質的飛躍，複雜度更高；一般大公司擁有更多的資源、技術實力會採用該方式來達到低成本的目標</li></ul></li><li><p>安全是一個龐大而又複雜的技術領域，一旦出問題，對業務和企業形象影響非常大。從技術的角度來講，包括：</p><ul><li>功能安全(“防小偷”)，減少系統潛在的缺陷，阻止 hacker 破壞行為</li><li>架構安全(“防強盜”)，保護系統不受惡意訪問和攻擊，保護系統的重要數據不被竊取。由於是蓄意破壞系統，因此對影響也大得多。架構設計時需要特別關注架構安全。</li></ul></li><li><p>規模帶來複雜度的主要原因就是”量變引起質變”，當數量超過一定的閾值後，複雜度會發生質的變化</p><blockquote><p>隨著業務的發展，規模帶來的常見複雜度有：(1)業務功能越來越多，調用邏輯越來越複雜；(2)數據容量、類型、關聯關係越來越多</p></blockquote></li><li><p>規模問題需要與高性能、高可用、高擴展、高伸縮性統一考慮。常採用”分而治之，各個擊破”的方法策略</p></li><li><p>是否還有其他複雜度原因？ =&gt; <strong>可伸縮性</strong></p></li><li><p>當前大型互聯網網站需要面對大量用戶高並發訪問、存儲更多數據、處理更高頻次的用戶交互。網站系統一般通過多種分佈式技術將多台 server 組成集群對外提供服務。伸縮性一般是系統可以根據需求和成本調整自身處理能力的一種能力</p></li><li><p>伸縮性常意味著系統可以通過低成本並能夠快速改變自身的處理能力以滿足更多用戶訪問、處理更多數據而不會對用戶體驗造成任何影響</p></li><li><p>伸縮性度量指標包括：</p><ul><li>處理更高並發</li><li>處理更多數據</li><li>處理更高頻次的用戶交互</li></ul></li><li><p>伸縮性複雜度體現在：</p><ul><li>伸：增強系統在上述三個方面的處理能力</li><li>縮：縮減系統處理能力</li><li>上述伸縮過程還必須相對低成本和快速</li></ul></li></ul><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://time.geekbang.org/column/intro/81">從0開始學架構_架構基礎_架構入門-極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Architecture Design </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Architecture Design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - Security</title>
      <link href="/blog/AWS/AWS-CSA-associate-Security/"/>
      <url>/blog/AWS/AWS-CSA-associate-Security/</url>
      
        <content type="html"><![CDATA[<h1 id="如何有效減少安全威脅"><a href="#如何有效減少安全威脅" class="headerlink" title="如何有效減少安全威脅?"></a>如何有效減少安全威脅?</h1><p>要有效的減少安全威脅，其實總結一句話，就是 <strong>拒絕惡意行為的存取</strong>。</p><p>但在在不同的網路 &amp; 系統架構下，作法就會有些不同，以下舉幾個常見範例：</p><h2 id="只有-EC2-Instance"><a href="#只有-EC2-Instance" class="headerlink" title="只有 EC2 Instance"></a>只有 EC2 Instance</h2><p>在這樣的情況下，我們可以從 VPC &amp; EC2 Instance 兩個地方來著手</p><p><img src="/blog/images/aws/Security_NACL-with-host-firewall.png" alt="Network ACL + host-based firewall"></p><ul><li><p>VPC 中提供的 Network ACL &amp; Secirty Group 可以限制存取 EC2 instance 的來源</p></li><li><p>EC2 Instance 本身的防火牆功能也可以限制 (例如：firewalld, iptables, ufw, windows firewall)</p></li><li><p>這樣的設定通常用在 Layer 4 的阻擋，無法有效的處理 Layer 7 的狀況</p></li></ul><h2 id="使用-ALB"><a href="#使用-ALB" class="headerlink" title="使用 ALB"></a>使用 ALB</h2><p>使用了 ALB 之後，有一點必須注意的是，通過 ALB 進入到 EC2 Instance 的流量，EC2 Instance 看到的來源 IP 永遠是 ALB 本身(看不到真實的使用者來源)，因此在安全設定上就需要調整一下：</p><p><img src="/blog/images/aws/Security_ALB-with-SecurityGroup.png" alt="Network ACL + ALB + Security Group"></p><ul><li><p>EC2 Instance 可透過 Security Group 的設定，僅允許來自 ALB 的流量</p></li><li><p>Network ACL 的設定依然要進行，但 target 則是位於 VPC public subnet 中的 ALB，並非 EC2 Instance</p></li><li><p>這樣的模式還是只能用於 Layer 4 的阻擋</p></li></ul><h2 id="使用-NLB"><a href="#使用-NLB" class="headerlink" title="使用 NLB"></a>使用 NLB</h2><p>若改用 NLB，EC2 Instance 就可以看到真實的使用者來源：</p><p><img src="/blog/images/aws/Security_NLB-with-ACL.png" alt="Network ACL + NLB + Security Group"></p><ul><li><p>可以在 NLB 所在的 public subnet 設定 Network ACL</p></li><li><p>也可以在 EC2 Instance 所在的 private subnet 設定 Network ACL</p></li><li><p>EC2 Instance 本身可以設定 Security Group，或是 host-based firewall</p></li><li><p>適用於 Layer 4 的阻擋</p></li></ul><h2 id="使用-WAF-ALB"><a href="#使用-WAF-ALB" class="headerlink" title="使用 WAF + ALB"></a>使用 WAF + ALB</h2><p>若是 EC2 Instance 提供的是 web service，且需要作到 Layer 7 的惡意行為阻擋，那就會需要用到 WAF：</p><p><img src="/blog/images/aws/Security_WAF-with-ALB.png" alt="WAF + ALB"></p><ul><li><p>Layer 4 的部份，可以在 ALB 所在的 public subnet 設定 Network ACL</p></li><li><p>Layer 7 的部份，則是在 WAF 上設定阻擋條件；WAF 可以看到真實的使用者來源</p></li><li><p>EC2 Instance 則是在 Security Group 中設定僅限來自於 ALB 的流量</p></li></ul><h2 id="使用-CDN-CloudFront-WAF-ALB"><a href="#使用-CDN-CloudFront-WAF-ALB" class="headerlink" title="使用 CDN(CloudFront) + WAF + ALB"></a>使用 CDN(CloudFront) + WAF + ALB</h2><p>若是額外還需要做網頁靜態資源的加速，那就需要搭配 CDN(CloudFront)：</p><p><img src="/blog/images/aws/Security_WAF-with-ALB-and-CloudFront.png" alt="WAF + CloudFront + ALB"></p><ul><li><p>CloudFront 是屬於 edge service，因此可以額外提供使用者所在位置的過濾方式</p></li><li><p>其他網路安全設定方式與上面相同</p></li></ul><h1 id="KMS-Key-Management-Service"><a href="#KMS-Key-Management-Service" class="headerlink" title="KMS(Key Management Service)"></a>KMS(Key Management Service)</h1><h2 id="什麼-KMS"><a href="#什麼-KMS" class="headerlink" title="什麼 KMS?"></a>什麼 KMS?</h2><p>首先看一下 AWS 官方對 KMS 的說明：</p><blockquote><p>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. AWS KMS is a secure and resilient service that uses hardware security modules that have been validated under FIPS 140-2, or are in the process of being validated, to protect your keys. AWS KMS is integrated with AWS CloudTrail to provide you with logs of all key usage to help meet your regulatory and compliance needs.</p></blockquote><p>其實簡單來說，就是幫使用者管理金鑰的服務，然後有很高的安全認證….</p><p>而 KMS 有以下特性：</p><ul><li><p>KMS 本身是屬於 Region 範圍的服務，用來協助金鑰的管理 (當然要加解密資料就必須跟這個服務進行整合囉!)</p></li><li><p>協助使用者管理名為 CMK(Customer Master Key) 的金鑰</p></li><li><p>非常適合用來對 S3 object、資料庫密碼、存在 System Manager Parameter Store 的 API key 進行加解密</p></li><li><p>加解密的來源資料，最大限制為 4KB</p></li><li><p>可與 CloudTrail 整合，提供稽核的功能 (相關的 log 資訊會存於 S3 中)</p></li><li><p>通過 FIPS 140-2 Level 2 的認證</p><blockquote><p>CloudHSM 達到 FIPS 140-2 Level 3</p></blockquote></li></ul><h2 id="CMK-的類型"><a href="#CMK-的類型" class="headerlink" title="CMK 的類型"></a>CMK 的類型</h2><p>CMK(Customer Master Key) 有三種，分別是：</p><ul><li><p><code>AWS Managed CMK</code>：免費，由 AWS 負責管理，使用者碰不到，在使用各個 AWS 服務時會預設拿來做加解密用，但也只有 AWS 服務可以直接使用</p></li><li><p><code>Customer Managed CMK</code>：完全可由使用者定義 rotation、key policy(誰可使用? 在什麼情況下可用? … 等等)，也可以開啟 or 關閉</p></li><li><p><code>AWS Owned CMK</code>：這完全由 AWS 負責管理，不會屬於特定使用者帳號下，也看不到，用來在多個帳號之間，確保使用者帳號使用服務時的安全性。</p></li></ul><h2 id="Symmetric-amp-Asymmetric-CMK-的差異"><a href="#Symmetric-amp-Asymmetric-CMK-的差異" class="headerlink" title="Symmetric &amp; Asymmetric CMK 的差異"></a>Symmetric &amp; Asymmetric CMK 的差異</h2><p>在 KMS 中 CMK 有分為 Symmetric &amp; Asymmetric 兩種，使用上有著不小的差異：</p><h3 id="Symmetric-CMK"><a href="#Symmetric-CMK" class="headerlink" title="Symmetric CMK"></a>Symmetric CMK</h3><ul><li><p>使用同樣一把金鑰進行資料加解密</p></li><li><p>使用 AES-256 加密</p></li><li><p>AWS 不會提供沒加密過的金鑰內容</p></li><li><p>必須呼叫 KMS API 才能使用</p></li><li><p>與 KMS 整合的其他 AWS service 都使用 Symmetric CMK</p></li><li><p>可用來產生 data keys, data key pairs, random byte strings</p></li><li><p>可匯入自訂的 key material</p></li></ul><h3 id="Asymmetric-CMK"><a href="#Asymmetric-CMK" class="headerlink" title="Asymmetric CMK"></a>Asymmetric CMK</h3><ul><li><p>public/private key pair</p></li><li><p>使用 RSA &amp; ECC(Elloptic-Curve Crytography)，ECC 比 RSA 優秀</p></li><li><p>AWS 不會提供沒加密過的 private key 內容</p></li><li><p>必須呼叫 KMS API 才能使用 private key</p></li><li><p>public key 可以下載並在 AWS 之外使用</p></li><li><p>通常是提供給外部無法呼叫 KMS API 的使用者使用</p></li><li><p><strong>AWS 其他服務無法與 Asymmetric CMK 整合</strong></p></li><li><p>常用於訊息的簽名 &amp; 驗證簽名之用</p></li></ul><h2 id="Key-Policy"><a href="#Key-Policy" class="headerlink" title="Key Policy"></a>Key Policy</h2><p>當使用程式 or AWS CLI 建立 CMK 時，可以自訂 key policy，但若是沒有指定 policy，AWS 則會預設提供一個 key policy，內容大概像以下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//允許 AWS account 為 root 的使用者有這個 CMK 的完整權限</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;Sid&quot;</span>: <span class="string">&quot;Enable IAM User Permissions&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Allow&quot;</span>,  </span><br><span class="line">  <span class="attr">&quot;Principal&quot;</span>: &#123;<span class="attr">&quot;AWS&quot;</span>: <span class="string">&quot;arn:aws:iam::111122223333:root&quot;</span>&#125;,</span><br><span class="line">  <span class="attr">&quot;Action&quot;</span>: <span class="string">&quot;kms:*&quot;</span>, </span><br><span class="line">  <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>比較需要注意的是，若是不小心刪除了 key policy，使用者就無法存取 CMK 了；此時只能找 AWS 原廠協助重新設定 key policy 才能再度使用 CMK</p></blockquote><p>以下是其他範例，用來指定特定的 User or Role 擁有 CMK 的權限：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//提供給 Role &quot;EncryptionApp&quot; 呼叫資料加解密的權限</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;Sid&quot;</span>: <span class="string">&quot;Allow use of the key&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Allow&quot;</span>,  </span><br><span class="line">  <span class="attr">&quot;Principal&quot;</span>: &#123;<span class="attr">&quot;AWS&quot;</span>: <span class="string">&quot;arn:aws:iam::111122223333:role/EncryptionApp&quot;</span>&#125;,</span><br><span class="line">  <span class="attr">&quot;Action&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;kms:Decrypt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;kms:DescribeKey&quot;</span>,</span><br><span class="line">    <span class="string">&quot;kms:Encrypt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;kms:GenerateDataKey*&quot;</span>,</span><br><span class="line">    <span class="string">&quot;kms:ReEncrypt*&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//提供給 User &quot;CMKUser&quot; 呼叫資料加解密的權限</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;Sid&quot;</span>: <span class="string">&quot;Allow use of the key&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Allow&quot;</span>,  </span><br><span class="line">  <span class="attr">&quot;Principal&quot;</span>: &#123;<span class="attr">&quot;AWS&quot;</span>: <span class="string">&quot;arn:aws:iam::111122223333:user/CMKUser&quot;</span>&#125;,</span><br><span class="line">  <span class="attr">&quot;Action&quot;</span>: [</span><br><span class="line">    <span class="string">&quot;kms:Decrypt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;kms:DescribeKey&quot;</span>,</span><br><span class="line">    <span class="string">&quot;kms:Encrypt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;kms:GenerateDataKey*&quot;</span>,</span><br><span class="line">    <span class="string">&quot;kms:ReEncrypt*&quot;</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="實作注意事項"><a href="#實作注意事項" class="headerlink" title="實作注意事項"></a>實作注意事項</h2><ul><li><p>在 AWS Managed Keys 中，列出了目前已經與 KMS 整合的服務所使用的 key，這些都是由 AWS 管理，只要了解當前 AWS 有管理這些 key 即可</p></li><li><p>在 Customer Managed Keys 畫面中的內容，才是由使用者自行管理的部份</p></li><li><p>若是要將放在 region A S3 已經加密的檔案移動到 region B，那需要進行下列操作：</p><ol><li>先將檔案解密(使用位於 region A 的 key)</li><li>將檔案移動到 region B</li><li>在使用 region B 的 key 加密<blockquote><p>KMS 的 key 有效範圍是 region，這很重要!</p></blockquote></li></ol></li><li><p>每個 CMK(Customer Managed Key) 建立出來後，用來識別只有 <code>KeyId</code>，但這是很難記住的一串亂碼，<strong>可為每個 CMK 設定 alias 藉以方便識別</strong></p><blockquote><p>使用 AWS CLI 的指令關鍵字為 <code>aws kms create-alias</code></p></blockquote></li><li><p>CMK 建立後會預設給一組 key policy，內容就是給 root 這把 key 所有的權限</p></li><li><p>CMK 可設定 rotate policy 為一年(也可以不設定)；AWS Managed Key 的 rotation policy 則是預設為三年</p></li><li><p>加密後的資料還會再額外進行 base64 encode；而將資料解密後，也還是 base64 encode 後的結果，因此還需要額外做 base64 decode 才能看到正確結果</p></li><li><p>加密資料時需要指定 Key ID，但解密時不需要，是因為加密時已經把 Key ID 相關資訊一起包進去了</p></li><li><p>若要加密超過 4KB 以上的資料，可以先產生一個 DEK(Data Encryption Key)，再用 DEK 進行資料加密</p><blockquote><p>使用 AWS CLI 的指令關鍵字為 <code>aws kms generate-data-key</code></p></blockquote></li></ul><h1 id="CloudHSM"><a href="#CloudHSM" class="headerlink" title="CloudHSM"></a>CloudHSM</h1><p>若是希望 key management 可以自己做，或是需要比 KMS 更高的安全性，那 CloudHSM 可能就是唯一選擇了!</p><h2 id="什麼是-CloudHSM"><a href="#什麼是-CloudHSM" class="headerlink" title="什麼是 CloudHSM ?"></a>什麼是 CloudHSM ?</h2><ul><li><p>Dedicated Cloud Security Module (只服務單一使用者)</p><blockquote><p>single tenant, multi-AZ cluster</p></blockquote></li><li><p>安全等級為 FIPS 140-2 Level 3 (KMS 只有 Level 2)</p></li><li><p>使用者透過 CloudHSM 自行管理 key</p></li><li><p>由於所有 key 都是自行管理，因此無法與 AWS managed service 整合</p><blockquote><p>AWS 完全無法存取 CloudHSM 中的 key</p></blockquote></li><li><p>運行於 VPC 中</p></li><li><p>使用 industry-standard APIs (沒有任何 AWS API 可用)</p></li><li><p>若資料需要用以下加密標準進行加密，CloudHSM 是個合適的選擇：</p><ul><li>PKCS#11</li><li>Java Cryptography Extensions (JCE)</li><li>Microsoft CryptoNG (CNG)</li></ul></li><li><p>key 若是遺失了，AWS 是無法協助的</p></li></ul><h2 id="CloudHSM-如何應用"><a href="#CloudHSM-如何應用" class="headerlink" title="CloudHSM 如何應用?"></a>CloudHSM 如何應用?</h2><p>以下是使用 CloudHSM 的一個範例：</p><p><img src="/blog/images/aws/CloudHSM_Example.png" alt="CloudHSM example"></p><ol><li><p>首先，CloudHSM 要位於 VPC 中 (可用現有的，或是新建立)</p></li><li><p>CloudHSM 會建立 ENI interface，而通過該 ENI interface 的流量就等同於將資料傳送到 CloudHSM 處理</p><blockquote><p>使用上很容易，就建立 EC2 instance，把 CloudHSM 產生的 ENI interface 附加上去即可</p></blockquote></li><li><p>CloudHSM 並沒有 native HA，有需要的話就要自己做(例如上圖)</p></li></ol><h1 id="System-Manager-Parameter-Store"><a href="#System-Manager-Parameter-Store" class="headerlink" title="System Manager Parameter Store"></a>System Manager Parameter Store</h1><p>服務在不同的環境運行時，總是會根據環境的不同，有不同的設定檔 &amp; 服務存取密碼 … 等資訊需要保存，而存放 git repository 肯定不是個好選擇，而 <code>Parameter Store</code> 正好可以用來解決這個問題。</p><h2 id="什麼是-Parameter-Store"><a href="#什麼是-Parameter-Store" class="headerlink" title="什麼是 Parameter Store?"></a>什麼是 Parameter Store?</h2><ul><li><p>屬於 AWS System Manager(SSM) 中的一個元件</p></li><li><p>是種安全的 Serverless storage，適合儲存設定檔、密碼、連線字串 … 等服務運行所必要的資訊</p></li><li><p>存在 paramter store 中的資料可以是明碼純文字，也可以是 KMS 加密後的結果</p></li><li><p>資料可以是以階層式(Hierarchy)的方式儲存</p></li><li><p>提供版本管理的功能</p></li><li><p>可設定每筆資料的 TTL</p></li></ul><p>總而言之，透過 Parameter Store，可以很方便的將程式碼與環境設定分離，進行更好的管理。</p><h2 id="階層式-Hierarchy-資料儲存"><a href="#階層式-Hierarchy-資料儲存" class="headerlink" title="階層式(Hierarchy)資料儲存"></a>階層式(Hierarchy)資料儲存</h2><p>階層式(Hierarchy)資料儲存的樣貌會是類似下圖：</p><p><img src="/blog/images/aws/ParameterStore_Example.png" alt="Parameter Store hierarchy example"></p><p>若透過 AWS client SDK 中的 GetParameterByPath 來取得資料，以下是幾個存取路徑範例：</p><ul><li><p>/dev</p></li><li><p>/dev/db</p></li><li><p>/prod/app</p></li></ul><h2 id="使用存放在-Parameter-Store-中的資料"><a href="#使用存放在-Parameter-Store-中的資料" class="headerlink" title="使用存放在 Parameter Store 中的資料"></a>使用存放在 Parameter Store 中的資料</h2><h3 id="與-CloudFormation-整合"><a href="#與-CloudFormation-整合" class="headerlink" title="與 CloudFormation 整合"></a>與 CloudFormation 整合</h3><p>以下為實際範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Parameters:</span></span><br><span class="line">  <span class="attr">InstanceType :</span></span><br><span class="line">    <span class="attr">Type :</span> <span class="string">&#x27;AWS::SSM::Parameter::Value&lt;String&gt;&#x27;</span></span><br><span class="line">    <span class="attr">Default:</span> <span class="string">myEC2TypeDev</span></span><br><span class="line">  <span class="attr">KeyName :</span></span><br><span class="line">    <span class="attr">Type :</span> <span class="string">&#x27;AWS::SSM::Parameter::Value&lt;AWS::EC2::KeyPair::KeyName&gt;&#x27;</span></span><br><span class="line">    <span class="attr">Default:</span> <span class="string">myEC2Key</span></span><br><span class="line">  <span class="attr">AmiId:</span></span><br><span class="line">    <span class="attr">Type:</span> <span class="string">&#x27;AWS::EC2::Image::Id&#x27;</span></span><br><span class="line">    <span class="attr">Default:</span> <span class="string">&#x27;ami-60b6c60a&#x27;</span></span><br><span class="line">    </span><br><span class="line"><span class="attr">Resources :</span></span><br><span class="line">  <span class="attr">Instance :</span></span><br><span class="line">    <span class="attr">Type :</span> <span class="string">&#x27;AWS::EC2::Instance&#x27;</span></span><br><span class="line">    <span class="attr">Properties :</span></span><br><span class="line">       <span class="attr">Type :</span> <span class="type">!Ref</span> <span class="string">InstanceType</span></span><br><span class="line">       <span class="attr">KeyName :</span> <span class="type">!Ref</span> <span class="string">KeyName</span></span><br><span class="line">       <span class="attr">ImageId :</span> <span class="type">!Ref</span> <span class="string">AmiId</span> </span><br></pre></td></tr></table></figure><h3 id="Lambda-Function"><a href="#Lambda-Function" class="headerlink" title="Lambda Function"></a>Lambda Function</h3><p>以下是幾個標準步驟：</p><h4 id="設定-IAM-存取權限"><a href="#設定-IAM-存取權限" class="headerlink" title="設定 IAM 存取權限"></a>設定 IAM 存取權限</h4><p>首先設定 policy，以下是範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;Version&quot;</span>: <span class="string">&quot;2012-10-17&quot;</span>, </span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span>: [&#123;</span><br><span class="line">    <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Allow&quot;</span>, </span><br><span class="line">    <span class="attr">&quot;Action&quot;</span>: [</span><br><span class="line">      <span class="string">&quot;logs:CreateLogGroup&quot;</span>, </span><br><span class="line">      <span class="string">&quot;logs:CreateLogStream&quot;</span>, </span><br><span class="line">      <span class="string">&quot;logs:PutLogEvents&quot;</span>, </span><br><span class="line">      <span class="string">&quot;ssm:GetParameter&quot;</span>,</span><br><span class="line">      <span class="string">&quot;ssm:GetParameterByPath&quot;</span></span><br><span class="line">    ], </span><br><span class="line">    <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">  &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接著要建立一個 IAM execution role：</p><ul><li><p>建立 type 為 <code>AWS service</code> 的 trusted entity，並選擇 Lambda</p></li><li><p>將上面的 policy 加入這個 role 中</p></li></ul><h4 id="設定-KMS"><a href="#設定-KMS" class="headerlink" title="設定 KMS"></a>設定 KMS</h4><p>此步驟是為了確保存在 Parameter Store 中的資料是經過加密的：</p><ol><li><p>建立 CMK(Customer Managed Key)，選擇 Symmetric</p></li><li><p>指定 Key Usage Permission 為上一個步驟新增的 IAM role (讓 IAM role 有權限可以使用這把 Key)</p></li></ol><h4 id="在-Parameter-Store-中新增資料"><a href="#在-Parameter-Store-中新增資料" class="headerlink" title="在 Parameter Store 中新增資料"></a>在 Parameter Store 中新增資料</h4><p>這個部份只有以下重點：</p><ol><li><p>指定資料的儲存路徑，例如：<code>/prod/myapp/db/password</code></p></li><li><p>指定 type 為 <code>SecureString</code> (此型態的資料才會以 KMS key 進行加密)</p></li><li><p>指定上述步驟建立的 KMS Key</p></li></ol><h4 id="設定-Lambda-Function"><a href="#設定-Lambda-Function" class="headerlink" title="設定 Lambda Function"></a>設定 Lambda Function</h4><ol><li><p>新增 Lambda function，並<strong>指定上述步驟設定的 IAM role</strong></p></li><li><p>程式中只要指定要存取 Paramter Store 的 data path 即可成功取得解密後的資料</p><blockquote><p>不用額外再指定 KMS key，因為這個部份已經在新增資料到 Parameter Store 就已經指定好</p></blockquote></li></ol><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/kms">Key Management Service - Amazon Web Services (AWS)</a></p></li><li><p><a href="https://aws.amazon.com/cloudhsm">AWS CloudHSM</a></p></li><li><p><a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html">AWS Systems Manager Parameter Store - AWS Systems Manager</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> Security </tag>
            
            <tag> KMS </tag>
            
            <tag> CloudHSM </tag>
            
            <tag> SSM Parameter Store </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - Application</title>
      <link href="/blog/AWS/AWS-CSA-associate-Applications/"/>
      <url>/blog/AWS/AWS-CSA-associate-Applications/</url>
      
        <content type="html"><![CDATA[<h1 id="SQS-Simple-Queue-Service"><a href="#SQS-Simple-Queue-Service" class="headerlink" title="SQS(Simple Queue Service)"></a>SQS(Simple Queue Service)</h1><h2 id="什麼是-Message-Queue"><a href="#什麼是-Message-Queue" class="headerlink" title="什麼是 Message Queue ?"></a>什麼是 Message Queue ?</h2><p><img src="/blog/images/aws/SQS_Message-Queue.png" alt="Message Queue"></p><p>Message Queue 這個概念被提出來，有兩個很重要的原因：</p><ol><li><p>將不同服務之間的通訊進行封裝</p></li><li><p>將各個服務進行解耦(decouple)</p></li></ol><p>將服務之間的通訊進行封裝，並帶入了 producer &amp; consumer 的概念，讓原本複雜的訊息傳遞，變成了相對容易的概念，大幅減少程式設計師在處理訊息通訊這件事情所花費的精力，可以把時間花在真正重要的事情上。</p><p>而解耦服務這件事情，就是在微服務時代必備的一項功能了，整個概念大概就是將大型服務拆分成多個小型服務，並透過 messsage queue 來互相交換訊息，藉此讓開發人員可以專心在單一功能，降低錯誤的發生，也可以單獨對服務進行升級。</p><p>關於更多 Message Queue 的知識，Google 找一下就一堆了，這邊就不繼續說下去……</p><h2 id="什麼是-SQS"><a href="#什麼是-SQS" class="headerlink" title="什麼是 SQS ?"></a>什麼是 SQS ?</h2><p>SQS 是 AWS 提供的全託管 message queue 服務，這在目前現在化的系統架構中，是個很重要的一個部份，以下先來看看 SQS 的官方介紹：</p><blockquote><p>Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available. Get started with SQS in minutes using the AWS console, Command Line Interface or SDK of your choice, and three simple commands.</p></blockquote><p>從上述的介紹中，可以看出幾個 SQS 服務的重點：</p><ul><li><p>全託管(fully managed)的 message queue 服務</p></li><li><p>message queue 的用途是在微服務、分散式、Serverless 架構中，將多個功能進行解耦(decouple)與擴展(scale)</p></li><li><p>大幅降低了處理訊息傳遞工作的複雜度，讓開發者可以專住在業務相關的開發上</p></li><li><p>可同時承受極大量的訊息，並確保訊息不會遺失</p></li></ul><h2 id="SQS-的特性"><a href="#SQS-的特性" class="headerlink" title="SQS 的特性"></a>SQS 的特性</h2><p>要使用 SQS 之前，要先搞清楚它本身的特性，才不會犯下沒有必要發生的錯誤……</p><p>AWS SQS 提供了兩種 Queue Type，分別是：</p><h3 id="Standard-Queue-default"><a href="#Standard-Queue-default" class="headerlink" title="Standard Queue (default)"></a>Standard Queue (default)</h3><p><img src="/blog/images/aws/SQS_Standard-Queue.png" alt="SQS"></p><ul><li><p>這是預設的 Queue Type</p></li><li><p>每秒可以承受的訊息量極大(基本上可以當作是無限制)</p></li><li><p>可以確保每個訊息都至少被傳送一次 (<strong>有可能會超過一次….</strong>)</p></li><li><p>若是訊息量真的太大，會有偶發性訊息發送順序跟原有傳送順序不一致的情況發生 (但會盡可能的達到順序一致)</p></li></ul><p>因此可以看出 standard queue 偶而會發生失誤，如果 application level 可以 cover 這個問題，或是服務本身性質對這個問題無感，那 standard queue 絕對是最好的選擇。</p><h3 id="FIFO-Queue"><a href="#FIFO-Queue" class="headerlink" title="FIFO Queue"></a>FIFO Queue</h3><p><img src="/blog/images/aws/SQS_FIFO-Queue.png" alt="SQS"></p><ul><li><p>基本上就是改良 standard 版本的缺點，包含了以下的改良：</p><ul><li>順序一定會保證一致一定會確保訊息只送一次，不會重複發送</li><li>保證訊息在 consumer 取走並刪除前都可以取得</li></ul></li><li><p>支援 message group，在同一個 Queue 中，每個 group 可以有自己的排序設定</p></li><li><p>TPS(Transaction Per Second) 最大只有 300</p></li><li><p>包含 standard queue 中所有的功能</p></li></ul><h2 id="考試重點"><a href="#考試重點" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>SQS 是種 message queue 服務，這類服務被設計出來的很重要的其中一個目的，就是要 <strong>decouple</strong> service，讓不同的 service 可以各自專心處理與自己相關的細節</p></li><li><p>SQS 是 pull-based 類型的服務，非 pushed-based；這表示 SQS 不會主動送訊息出來，需要有個服務主動去拉(例如：EC2 instance)</p></li><li><p>Message 的大小是可以從 1 byte ~ 256 KB (可參考<a href="https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-quotas.html">官方文件</a>)</p><blockquote><p>可以超過 256 KB，但過大的 payload 會需要以 reference 的方式放到 S3，不能直接傳到 SQS 中</p></blockquote></li><li><p>訊息可以保留在 Queue 中的時間範圍為 1 分鐘 ~ 14 天，可以自由設定 (預設是 4 天)</p></li><li><p>Visibility Timeout 是指當 consumer 拿走特定 message 之後，該 message 就會進入 invisible 的狀態，其他 consumer 自然就無法取來處理；一旦超過 timeout 設定，同一個 message 就可以被其他 consumer 取走並處理</p><blockquote><p>**若是 visibility timeout 設定太小，可能工作還沒處理結束，其他 consumer 又從 Queue 上面取下來作(等於傳送兩次)**，這樣就會發生預期之外的結果，因此在設定 visibility timeout 必須搞清楚每個 job 會需要的執行時間才能做正確的設定</p></blockquote></li><li><p>Visibility Timeout 的設定最長是 12 小時(超過了 timeout 設定後，訊息就會重新回到 SQS 中，然後就可能造成重複消費的情況發生)</p><blockquote><p>因此若是 job 會執行超過 12 小時，可能 SQS 就不是適合拿來搭配的服務…..</p></blockquote></li><li><p>SQS 會保證訊息至少會送出一次</p></li><li><p>SQS 支援 long polling 的連接方式，因此即使是 pull-based，還是可以藉此減少建立連線的成本</p><blockquote><p>持續的向 SQS 建立連線是需要額外花費的，使用 long polling 可以省下不少費用</p></blockquote></li></ul><h1 id="SWF-Simple-Workflow-Service"><a href="#SWF-Simple-Workflow-Service" class="headerlink" title="SWF(Simple Workflow Service)"></a>SWF(Simple Workflow Service)</h1><blockquote><p>參考課程中的講者有提到，SWF 幾乎沒在考試中出現，所以大概了解一下應該就可以了…</p></blockquote><div class="video-container"><iframe src="https://www.youtube.com/embed/dFD7CEM1vbc" frameborder="0" loading="lazy" allowfullscreen></iframe></div><ul><li><p>主要目的在協助開發人員建立、執行、調整一連串執行的背景工作(可能某些工作是平行處理的)</p></li><li><p>可將特定的服務/程式與人工動作結合</p></li><li><p>可以協助追蹤每個階段的工作執行狀態</p></li></ul><h2 id="SWF-Actor-重要組成元素"><a href="#SWF-Actor-重要組成元素" class="headerlink" title="SWF Actor (重要組成元素)"></a>SWF Actor (重要組成元素)</h2><p>SWF Actor 共有以下三類：</p><ul><li><p><code>Workflow Starter</code>：用來啟動 workflow 的 application (例如：網站上的下單行為)</p></li><li><p><code>Decider</code>：根據前一個 task 的 input(或是遇到失敗了) 來決定下一個 task 是那一個，或是結束</p></li><li><p><code>Activity Worker</code>：就是實際執行每個 task 內容的角色</p></li></ul><h2 id="SWF-與-SQS-比較"><a href="#SWF-與-SQS-比較" class="headerlink" title="SWF 與 SQS 比較"></a>SWF 與 SQS 比較</h2><ul><li><p>SQS 的訊息最多保留 14 天，但在 SWF 中的工作最長可以執行一年</p></li><li><p>SWF 提供了 task-oriented API；SQS 提供了 message-oriented API</p></li><li><p>SWF 會確保每個 task 只會被分派一次，不會重複；但 SQS 有可能會重複(standard queue + 大量訊息)</p></li><li><p>SWF 會追蹤應用程式中所有的 task &amp; event；但 SQS 不支援 application-level tracking，必須得自己實作(如果使用多個 queue 時)</p></li></ul><h1 id="SNS-Simple-Notification-Service"><a href="#SNS-Simple-Notification-Service" class="headerlink" title="SNS(Simple Notification Service)"></a>SNS(Simple Notification Service)</h1><h2 id="什麼是-SNS"><a href="#什麼是-SNS" class="headerlink" title="什麼是 SNS ?"></a>什麼是 SNS ?</h2><p>SNS 是 AWS 提供作為訊息通知的服務，但其實不僅僅是簡單的訊息通知而已，其實 SNS 的服務在設計上，所涵蓋的範圍是很廣的，可以滿足大多數訊息發送的要求；以下是 AWS 對 SNS 的介紹：</p><blockquote><p>Amazon Simple Notification Service (SNS) is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Amazon SNS provides topics for high-throughput, push-based, many-to-many messaging. Using Amazon SNS topics, your publisher systems can fan out messages to a large number of subscriber endpoints for parallel processing, including Amazon SQS queues, AWS Lambda functions, and HTTP/S webhooks. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email.</p></blockquote><p>從上面的描述可以看出以下重點：</p><ul><li><p><strong>push-based</strong>：這跟 SQS 不一樣，SNS 會主動將訊息發送出去</p></li><li><p><strong>高可用</strong>：當要轉發的訊息送到 SNS 後，會自動被複製到多個 AZ 上，確保訊息不會遺失</p></li><li><p><strong>訂閱機制</strong>：SNS 本身就是個 pub/sub 的設計，搭配 <code>topic</code>，可以讓使用者(or 服務)進行訂閱，當有訊息傳到 topic 後，SNS 就會主動將訊息傳送給訂閱者</p></li><li><p><strong>支援各式 endpoint</strong>：訊息不僅可以傳給一般的使用者，也可以傳給其他服務；此外，傳遞的方式也有多種，可以是 SMS, email，甚至是直接呼叫 Lambda function 或是將資料再丟進 SQS 中都可以</p><blockquote><p>藉由 SNS 支援的多種 endpoint type，開發者就可以很輕易的根據需求將訊息傳遞到不同的使用者、device，甚至是 service 了</p></blockquote></li></ul><h2 id="SNS-的優點"><a href="#SNS-的優點" class="headerlink" title="SNS 的優點"></a>SNS 的優點</h2><ul><li><p>push based，因此在費用上就會相對比 pull-based 省不少</p></li><li><p>提供簡單的 API，並容易與其他的應用進行整合</p></li><li><p>彈性的訊息傳遞設定(也支援多種不同的傳送協定)</p></li></ul><h2 id="SNS-與-SQS-比較"><a href="#SNS-與-SQS-比較" class="headerlink" title="SNS 與 SQS 比較"></a>SNS 與 SQS 比較</h2><ul><li><p>同樣都是 messaging service</p></li><li><p>SNS 為 push-based</p></li><li><p>SQS 為 pull-based</p></li></ul><h1 id="Elastic-Transcoder"><a href="#Elastic-Transcoder" class="headerlink" title="Elastic Transcoder"></a>Elastic Transcoder</h1><p>首先看一下 AWS 官網對 Elastic Transcoder 的介紹：</p><blockquote><p>Amazon Elastic Transcoder is media transcoding in the cloud. It is designed to be a highly scalable, easy to use and a cost effective way for developers and businesses to convert (or “transcode”) media files from their source format into versions that will playback on devices like smartphones, tablets and PCs.</p></blockquote><p>從上面的描述可以看出來 Elastic Transcoder 有幾個特點：</p><ul><li><p>highly scalable &amp; cost effective 就不用說了，AWS 每個服務都是這麼介紹自己的….</p></li><li><p>此服務是作為多媒體檔案的轉檔服務(media transcoding)用</p></li><li><p>來源為 file(並非 streaming)，因此很直覺就可以聯想到一定可以跟 S3 串接</p></li><li><p>可以輕易的將來源檔案轉換成不同設備可以看的版本 &amp; 解析度</p></li></ul><p>有了以上的說明之後，以下這個簡單的應用範例看起來就很直覺了：</p><p><img src="/blog/images/aws/ElasticTranscoder_Example.png" alt="Elastic Transcoder Example"></p><p>上面的服務串接做了以下事情：</p><ol><li><p>將錄製好的影像檔案上傳 S3</p></li><li><p>透過 Lambda function 拉取 S3 中的影像檔案，並丟給 Elastic Transcoder，根據設定轉成符合各種設備 &amp; 不同解析度的檔案</p></li><li><p>將轉檔後的結果上傳到 S3</p></li></ol><h1 id="API-Gateway"><a href="#API-Gateway" class="headerlink" title="API Gateway"></a>API Gateway</h1><h2 id="什麼是-API-Gateway"><a href="#什麼是-API-Gateway" class="headerlink" title="什麼是 API Gateway ?"></a>什麼是 API Gateway ?</h2><p>首先，AWS 原廠對 API Gateway 的解釋有兩段，第一段大致說明了 API Gateway 的特色：</p><blockquote><p>Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the “front door” for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.</p></blockquote><ul><li><p>全託管服務，代表你不用自己建立一台 EC2 instance 來提供服務</p></li><li><p>讓開發者可以更容易建立、發佈、維護、監控並安全地提供 API 服務</p></li><li><p>API Gateway 可以提供 RESTful &amp; WebSocket API，作為存取內部服務的大門</p></li><li><p>後端的服務可以以 Container(由 EKS or ECS 提供)、Serverless(由 Lambda 提供) 或是一般的 web application(由 EC2 提供)，也可以是類似 DynamoDB 這樣的服務</p></li></ul><p><img src="/blog/images/aws/APIGateway_Example.png" alt="API Gateway Example"></p><h2 id="API-Gateway-的細部功能"><a href="#API-Gateway-的細部功能" class="headerlink" title="API Gateway 的細部功能"></a>API Gateway 的細部功能</h2><p>第二段官方說明中就比較著重在細部的功能：</p><p>API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. API Gateway has no minimum fees or startup costs. You pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales.</p><ul><li><p>API Gateway 可以根據 request 數量 auto scale，因此同時承受數以十萬計的 API call 是沒有問題的</p></li><li><p>若要防止 DDOS，也可以進行流量的管理</p></li><li><p>支援 CORS，可以讓不同 domain 的 web site 之間可以互相存取 API</p></li><li><p>可搭配其他安全機制進行授權存取的管理</p></li><li><p>可搭配 CloudWatch 進行監控</p></li><li><p>版本管理</p></li><li><p>提供多種不同成本控制 &amp; 管理的方式</p></li></ul><h2 id="如何設定-amp-佈署-API-Gateway"><a href="#如何設定-amp-佈署-API-Gateway" class="headerlink" title="如何設定 &amp; 佈署 API Gateway"></a>如何設定 &amp; 佈署 API Gateway</h2><h3 id="設定-API-Gateway"><a href="#設定-API-Gateway" class="headerlink" title="設定 API Gateway"></a>設定 API Gateway</h3><p>設定 API Gateway 大致上的流程如下：</p><ol><li><p>定義一個 <code>API</code></p></li><li><p>定義 <code>Resource</code> &amp; 其他的 nested Resource(以 <code>URL path</code> 的形式)</p></li><li><p>對每一個 Resource 進行設定：</p><ul><li>選擇使用的 <code>HTTP method</code> (GET, POST …. etc)</li><li>進行安全性相關設定</li><li>選擇 <code>Target</code> (EC2, Lambda, DynamoDB …. etc)</li><li>設定 request &amp; response 的轉換格式</li></ul></li></ol><h3 id="佈署-API-Gateway"><a href="#佈署-API-Gateway" class="headerlink" title="佈署 API Gateway"></a>佈署 API Gateway</h3><p>佈署 API Gateway 的流程則是如下：</p><ul><li>將 API Gateway 佈署到指定的 <code>stage</code><ul><li>預設使用的是 API Gateway 服務提供的 domain</li><li>也可以使用自訂的 domain</li><li>可搭配 AWS Certificate Manager 取得免費的 SSL/TLS 憑證</li></ul></li></ul><h2 id="Cache-快取-功能"><a href="#Cache-快取-功能" class="headerlink" title="Cache(快取)功能"></a>Cache(快取)功能</h2><p>API Gateway 還提供了一個很重要的 cache 功能，說明如下：</p><ul><li><p>透過 cache，實際上到達後端的 request 會減少，有些 API call 可以在 API Gateway 這一層就直接回應了</p></li><li><p>可以降低 Latency</p></li><li><p>若要開啟 cache 功能，則需要到 <code>stage</code> 的設定中開啟</p></li><li><p>可透過設定 cache TTL，決定 cache 要存活的時間長短 (在設定的 TTL 時限內，API Gateway 就會直接使用 cache 回應，不會將 API call 轉到內部服務去)</p></li></ul><h2 id="考試重點-1"><a href="#考試重點-1" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>API Gateway 運作的類似像 load balancer，但是個全託管服務</p></li><li><p>對外提供 HTTPS endpoint，對內可指向多種內部服務</p></li><li><p>提供 cache 機制來提昇服務效能</p></li><li><p>低成本高效能，可自動擴展</p></li><li><p>可透過限流的方式來阻止攻擊行為</p></li><li><p>可搭配 CloudWatch 服務，存放 log 相關資訊</p></li><li><p>若使用類似 AJAX 的功能存取位於不同 domain 的 API gateway，則需要開啟 CORS 的功能</p><blockquote><p>CORS 本身在 browser 中是預設啟用的</p></blockquote></li></ul><h1 id="Kinesis"><a href="#Kinesis" class="headerlink" title="Kinesis"></a>Kinesis</h1><h2 id="什麼是-Kinesis"><a href="#什麼是-Kinesis" class="headerlink" title="什麼是 Kinesis?"></a>什麼是 Kinesis?</h2><p>以下是 AWS 官網對 Kinesis 的介紹：</p><blockquote><p>Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.</p></blockquote><p>從上面的官方介紹可以整理出幾個 Kinesis 的服務重點：</p><ul><li><p>處理 streaming data 用</p></li><li><p>可針對資料進行即時分析處理，不需要等待資料全部到齊</p></li><li><p>資料來源可以從 video, audio, application log, 網站點擊所產生的資料、IoT 遙測資料 … 等等</p></li><li><p>可針對資料進行分析，甚至可以搭配其他服務進行 machine learning 相關的工作</p></li></ul><h2 id="Kinesis-可以提供哪些類型的服務"><a href="#Kinesis-可以提供哪些類型的服務" class="headerlink" title="Kinesis 可以提供哪些類型的服務?"></a>Kinesis 可以提供哪些類型的服務?</h2><p>Kinesis 根據功能共分為以下四類：</p><ul><li><p>Kinesis Video Streams</p></li><li><p>Kinesis Data Streams</p></li><li><p>Kinesis Data Firehose</p></li><li><p>Kinesis Data Analytics</p></li></ul><h3 id="Kinesis-Video-Streams"><a href="#Kinesis-Video-Streams" class="headerlink" title="Kinesis Video Streams"></a>Kinesis Video Streams</h3><p><img src="/blog/images/aws/KinesisVideoStreams_Example.png" alt="Kinesis Video Streams Example"></p><p>可針對視訊的串流資料進行回放、安全監控、臉部辨識、機器學習，或是其他分析….等工作。</p><h3 id="Kinesis-Data-Streams"><a href="#Kinesis-Data-Streams" class="headerlink" title="Kinesis Data Streams"></a>Kinesis Data Streams</h3><p><img src="/blog/images/aws/KinesisDataStreams_Example.png" alt="Kinesis Data Streams Example"></p><ul><li><p>資料在 Kinesis 中會以 shard 的形式存在，保留在 persistent storage 中</p></li><li><p>搭配後方的服務(例如：EC2, Lambda, Kinesis Data Analytics) 進行後續的分析處理工作</p></li><li><p>處理完的資料可以存到 S3, EMR, Redshift, DynamoDB …. 等地方做後續運用</p></li></ul><h3 id="Kinesis-Data-Firehose"><a href="#Kinesis-Data-Firehose" class="headerlink" title="Kinesis Data Firehose"></a>Kinesis Data Firehose</h3><p><img src="/blog/images/aws/KinesisDataFirehose_Example.png" alt="Kinesis Data Firehose Example"></p><ul><li><p>沒有 persistent storage，資料進入 Kinesis 後要立即處理</p></li><li><p>可選擇搭配 Lambda or Kinesis Data Analytics 來進行資料處理</p></li><li><p>處理完後的資料可以轉存入 S3 or Elasticsearch 服務中</p></li></ul><h3 id="Kinesis-Data-Analytics"><a href="#Kinesis-Data-Analytics" class="headerlink" title="Kinesis Data Analytics"></a>Kinesis Data Analytics</h3><p><img src="/blog/images/aws/KinesisDataAnalytics_Example.png" alt="Kinesis Data Analytics Example"></p><ul><li><p>此類型的 Kinesis 則是將查詢和分析直接設定在 Kinesis 服務本身</p></li><li><p>輸出的結果可以存到 S3, Redshift, Elasticsearch … 等服務</p></li></ul><h2 id="考試重點-2"><a href="#考試重點-2" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>了解 <code>Kinesis Data Streams</code> &amp; <code>Kinesis Data Firehose</code> 的差異</p></li><li><p>了解 Kinesis Data Analytics 可以如何被使用</p></li></ul><h1 id="Cognito"><a href="#Cognito" class="headerlink" title="Cognito"></a>Cognito</h1><h2 id="什麼是-Cognito"><a href="#什麼是-Cognito" class="headerlink" title="什麼是 Cognito?"></a>什麼是 Cognito?</h2><p>AWS Cognito 是個用來解決”使用者認證”這件事情的服務。</p><p>在網路上提供服務，使用者認證這件事情總是讓人覺得很麻煩，但不做又不行，到底是要自己建立帳號管理系統，還是直接去接 Google or FB 的認證就好，開發也其實並非很簡單，因此 AWS Cognito 這項服務就是要讓這件事情變得容易。</p><p>先看看 AWS 官網對 Cognito 的說明：</p><blockquote><p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.</p></blockquote><p>以下整理出幾個重點：</p><ul><li><p>Cognito 是用來讓開發者很方便的可以在 web or mobile app 上提供使用者登入、登出、存取控制…等功能</p></li><li><p>可與 FB, Google, Amazon, Microsoft Azure Directory ….等服務的認證進行整合(透過 SAML 2.0)</p></li></ul><h2 id="服務特色"><a href="#服務特色" class="headerlink" title="服務特色"></a>服務特色</h2><ul><li><p>不用擔心可服務的使用者數量，AWS Cognito 可以全部扛起來….</p></li><li><p>可與常見的 social identity provider(FB, Google …. etc) 進行整合</p></li><li><p>支援各種帳號整合標準，例如：Oauth 2.0, SAML 2.0, OpenID Connect … 等等</p></li><li><p>支援 multi-factor authentication 以及多項資安標準</p></li><li><p>可與 AWS IAM 搭配，對 AWS 其他服務 &amp; 資源進行存取控制，例如下圖所示：</p></li></ul><p><img src="/blog/images/aws/Cognito_Example.png" alt="Cognito Example 1"></p><ul><li><p>使用內建 UI，搭配一些客製化，很容易就可以整合到現有的服務中，不需要撰寫任何程式碼</p></li><li><p>可將使用者變更後的資料同步到多個裝置中，並發送通知，例如下圖所示：</p></li></ul><p><img src="/blog/images/aws/Cognito_Example3.png" alt="Cognito Example 3"></p><h2 id="User-Pool-amp-Identity-Pool"><a href="#User-Pool-amp-Identity-Pool" class="headerlink" title="User Pool &amp; Identity Pool"></a>User Pool &amp; Identity Pool</h2><p>在 Cognito 服務中，有兩個部份需要特別注意，分別是 User Pool &amp; Identity Pool</p><h3 id="User-Pool"><a href="#User-Pool" class="headerlink" title="User Pool"></a>User Pool</h3><ul><li><p>User Pool 是 Cognito 服務中實際存放使用者資訊的地方</p></li><li><p>使用者不一定要透過 FB or Google 認證進行登入，也可以使用 User Pool 中的資訊進行登入</p></li><li><p>Cognito 就只是一個 broker，將認認需求傳送到 User Pool or FB or Google</p></li><li><p>成功認證後的使用者可以取得一個 JWT(Json Web Token)</p></li></ul><h3 id="Identity-Pool"><a href="#Identity-Pool" class="headerlink" title="Identity Pool"></a>Identity Pool</h3><p>Identity Pool 則是提供臨時的權限，用來存取 AWS 資源 or 服務</p><h3 id="搭配使用的場景"><a href="#搭配使用的場景" class="headerlink" title="搭配使用的場景"></a>搭配使用的場景</h3><p>以下是一個 User Pool 搭配 Identity Pool 的場景：</p><p><img src="/blog/images/aws/Cognito_Example2.png" alt="Cognito Example 2"></p><ul><li><p>使用者透過 User Pool 進行認證，認證資料可能來自於 User Pool 本身或是 FB or Google</p></li><li><p>認證成功後取得 JWT</p></li><li><p>使用者拿著 JWT 去向 Identity Pool 換取一個臨時用的 AWS credential</p></li><li><p>使用者有了 AWS credential 後就可以存取特定的 AWS 資源或服務</p></li></ul><h2 id="考試重點-3"><a href="#考試重點-3" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>Cognito 可與 Google, FB, Amazon 等 social identity provider 進行認證</p></li><li><p>Cognito 只扮演 broker 的角色，協助向不同 provider 進行認證</p></li><li><p>認證成功後會取得 token，可用來交換成 AWS credential，進而具有特定的 IAM role 的身份</p></li><li><p>User Pool 負責使用者註冊、認證、帳號恢復…等工作</p></li><li><p>Identity Pool 則是用來賦予 AWS 資源與服務的存取權限</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/sqs/">Amazon Simple Queue Service (SQS) | Message Queuing for Messaging Applications | AWS</a></p></li><li><p><a href="https://columns.chicken-house.net/2019/01/01/microservice12-mqrpc/">高可靠度的微服務通訊 - Message Queue — 安德魯的部落格</a></p></li><li><p><a href="https://aws.amazon.com/sqs/">Amazon Simple Queue Service (SQS) | Message Queuing for Messaging Applications | AWS</a></p></li><li><p><a href="https://aws.amazon.com/swf/">Amazon Simple Workflow Service - Cloud Workflow Development - AWS</a></p></li><li><p><a href="https://aws.amazon.com/sns">Amazon Simple Notification Service (SNS) | AWS</a></p></li><li><p><a href="https://aws.amazon.com/elastictranscoder">AWS | Amazon Elastic Transcoder - Media &amp; Video Transcoding in the Cloud</a></p></li><li><p><a href="https://aws.amazon.com/api-gateway">Amazon API Gateway - Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/kinesis">Amazon Kinesis - Process &amp; Analyze Streaming Data - Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/cognito">Amazon Cognito - Simple and Secure User Sign Up &amp; Sign In | Amazon Web Services (AWS)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> SQS </tag>
            
            <tag> SNS </tag>
            
            <tag> SWF </tag>
            
            <tag> Elastic Transcoder </tag>
            
            <tag> API Gateway </tag>
            
            <tag> Kinesis </tag>
            
            <tag> Cognito </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - HA Architecture</title>
      <link href="/blog/AWS/AWS-CSA-associate-HA/"/>
      <url>/blog/AWS/AWS-CSA-associate-HA/</url>
      
        <content type="html"><![CDATA[<h1 id="Load-Balancer"><a href="#Load-Balancer" class="headerlink" title="Load Balancer"></a>Load Balancer</h1><p>AWS 提供三種 Load Balancer 類型，分別是：</p><ul><li><p>Application Load Balancer</p></li><li><p>Network Load Balancer</p></li><li><p>Classic Load Balancer</p></li></ul><h2 id="Application-Load-Balancer"><a href="#Application-Load-Balancer" class="headerlink" title="Application Load Balancer"></a>Application Load Balancer</h2><ul><li><p>分流 HTTP &amp; HTTPS 流量的最佳選擇</p></li><li><p>Application aware，可以讓源端的服務獲得額外的資訊(例如：使用者所在位置、使用的語系…等等)</p></li><li><p>可以根據使用者資訊 &amp; 條件設定 request routing policy，將流量導向特定的服務</p></li><li><p>可以透過設定 <code>target group</code>，讓 traffic 根據不同的規則進到不同 target group 中的 instance</p></li><li><p>支援很多進階功能，例如：ECS、HTTPS、HTTP2、WebSockets、Access Logs、Sticky Sessions、AWS WAF(Web Application Firewall) … 等功能</p></li></ul><h2 id="Network-Load-Balancer"><a href="#Network-Load-Balancer" class="headerlink" title="Network Load Balancer"></a>Network Load Balancer</h2><ul><li><p>分流 TCP traffic(Layer 4) 的最佳選擇</p></li><li><p>效能非常好，可以輕鬆處理每秒百萬級別的 request，且保持低延遲的狀態</p></li></ul><h2 id="Classic-Load-Balancer"><a href="#Classic-Load-Balancer" class="headerlink" title="Classic Load Balancer"></a>Classic Load Balancer</h2><ul><li><p>其實前身就是 ELB(Elastic Load Balancer)</p></li><li><p>可以同時處理 Layer 7 &amp; Layer 4 的流量</p></li><li><p>支援 Layer 7 特性，例如：X-Forwarded-For &amp; sticky session</p></li><li><p>但其他更進階的設定則是沒有支援，必須改用 ALB or NLB</p></li><li><p>源端服務若是有問題，會回應 504 Error(Gateway Timeout)</p></li><li><p>只能將 traffic 平均分散在後方所有的 EC2 instance 上</p></li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li><p><strong>ELB 應該與 Auto Scaling Group 搭配使用</strong>，藉此達到 HA、fault tolerance、automatic scale out/in … 等目的</p></li><li><p>ELB 也可以作為內部流量(private subnet)的 load balancer 之用</p></li><li><p>若有設定好 health check，ELB 就不會將 traffic 導向當下狀態是故障的 instance</p></li><li><p>若希望服務可以掛上 TLS(HTTPS)，ELB 可以協助做 SSL offloading 的工作，可以節省 EC2 instance 的計算資源</p></li><li><p>若是 Load Balancer 回應 504 Error(Gateway Timeout)，就是源端的服務出問題，超過 idle timeout 的設定</p><blockquote><p>此時就需要去檢查 web server or DB service … 等等</p></blockquote></li><li><p>當網路流量經過 load balancer 進入終端 server，且需要在終端 server 取得來源使用者的 IP，就需要查看 <code>X-Forwarded-For</code> header </p></li><li><p>Load Balancer 都會提供一個 DNS name，不會給 IP address</p></li><li><p><strong>考試前請認真讀一下 <a href="https://aws.amazon.com/elasticloadbalancing/faqs">Elastic Load Balancing FAQs</a>，會出不少關於三種不同 LB 差異的考題</strong></p></li><li><p>當要考慮到 <code>HA</code> &amp; <code>fault tolerance</code>，就必須至少要有 <strong>ELB + ASG(cross AZ + 最少兩個 instance)</strong> 這樣的組合 </p></li></ul><h3 id="Health-Check"><a href="#Health-Check" class="headerlink" title="Health Check"></a>Health Check</h3><ul><li><p>從 Load Balancer 角度來監控 target(Instance, IP or Labmda)，會有 <code>InService</code> &amp; <code>OutofService</code> 兩種狀態</p></li><li><p>Health check 的機制是透過不斷的詢問 target 實現的</p></li></ul><h1 id="Advanced-Load-Balancer-須知"><a href="#Advanced-Load-Balancer-須知" class="headerlink" title="Advanced Load Balancer 須知"></a>Advanced Load Balancer 須知</h1><h2 id="Sticky-Session"><a href="#Sticky-Session" class="headerlink" title="Sticky Session"></a>Sticky Session</h2><p><img src="/blog/images/aws/ELB_Sitcky-Session.png" alt="ELB sticky session"></p><ul><li><p>sticky session 可以讓使用者使用固定的 EC2 instance(透過將 user session 與固定的 instance 綁定)</p><blockquote><p>若是在某些應用會將使用者相關資訊存在本機上時特別有用</p></blockquote></li><li><p>若開啟 sticky session，當某位使用者的流量被導向到特定的 instance 後，他就會一直被導向同樣的 instance</p></li><li><p>ALB 也支援 sticky session，但流量僅能導向 target group level</p></li></ul><h2 id="Cross-Zone-Load-Balancing"><a href="#Cross-Zone-Load-Balancing" class="headerlink" title="Cross Zone Load Balancing"></a>Cross Zone Load Balancing</h2><p>簡單來說，Cross Zone Load Balancing 可以讓 Load Balancer 將流量分散到不同的 AZ 上，細節的部份可用以下三張圖說明，在那之前有幾個假設狀況：</p><ol><li><p>使用者存取某個在 Route 53 設定好的 domain name</p></li><li><p>domain name 指定兩筆記錄，分別指到不同 AZ 的兩個 Load Balancer</p></li><li><p>預設情況下，Route 53 會引導各 50% 的流量到不同 AZ 上的 Load Balancer</p></li></ol><h3 id="未開啟-Cross-Zone-Load-Balancing"><a href="#未開啟-Cross-Zone-Load-Balancing" class="headerlink" title="未開啟 Cross Zone Load Balancing"></a>未開啟 Cross Zone Load Balancing</h3><p>首先是一般情況下沒有開啟 Cross Zone Load Balancing 功能的情況下：</p><p><img src="/blog/images/aws/ELB_without-Cross-Zone-Load-Balancing.png" alt="without Cross Zone Load Balancing"></p><ul><li><p>上面的 AZ 有四個 instance，下方的 AZ 僅有一個 instance</p></li><li><p>每個 AZ 都得到 50% 的流量</p></li><li><p>上方的四個 instance 共同分攤了 50% 的流量，但下方單獨的 instance 一台就扛了該 AZ 所有的流量</p></li></ul><h3 id="開啟-Cross-Zone-Load-Balancing"><a href="#開啟-Cross-Zone-Load-Balancing" class="headerlink" title="開啟 Cross Zone Load Balancing"></a>開啟 Cross Zone Load Balancing</h3><p>很明顯上面的流量分配是有問題，還是有優化空間的，這時候就可以透過開啟 Cross Zone Load Balancing 功能來處理，當開啟了 Cross Zone Load Balancing，流量的分配就會變成如下：</p><p><img src="/blog/images/aws/ELB_with-Cross-Zone-Load-Balancing.png" alt="with Cross Zone Load Balancing"></p><ul><li><p>負載平均是以整體來計算，而非單一 AZ</p></li><li><p>下方 AZ 可以知道上方 AZ 中的 instance 所得到的流量相對少，因此就會有一部份流量會轉回上方 AZ 中</p></li><li><p>透過 ELB 將流量導向另外一個 AZ，藉此平均分散流量到 instance 上</p></li></ul><h3 id="單一-AZ-ELB-開啟-Cross-Zone-Load-Balancing"><a href="#單一-AZ-ELB-開啟-Cross-Zone-Load-Balancing" class="headerlink" title="單一 AZ ELB + 開啟 Cross Zone Load Balancing"></a>單一 AZ ELB + 開啟 Cross Zone Load Balancing</h3><p>如果只有一個 AZ 有 ELB 的設定，加上開啟了 開啟 Cross Zone Load Balancing，會是怎麼樣呢? 結果會變成下圖：</p><p><img src="/blog/images/aws/ELB_Cross-Zone-Load-Balancing-common-scenario.png" alt="single AZ ELB with Cross Zone Load Balancing"></p><ul><li><p>流量 100% 發到上方 AZ 的 ELB 上</p></li><li><p>但有 20% 的流量會跨 AZ 轉發到下方 AZ 的 instance 中</p></li></ul><h2 id="Path-Patterns"><a href="#Path-Patterns" class="headerlink" title="Path Patterns"></a>Path Patterns</h2><p>簡單來說，<code>path pattern</code> 的功能是可以讓管理者根據 request url 來決定將流量導向不同的 EC2 instance 上，那實際上這是如何實現的呢?</p><ol><li><p>建立 listener，並根據 url path 指定 rule</p></li><li><p>設定 target group，並與 listener 進行繫結，讓符合 listener rule 的流量導向指定的 target group</p></li></ol><p>以下用一張圖進行範例說明：</p><p><img src="/blog/images/aws/ELB_Path-Patterns.png" alt="ELB path patterns"></p><ul><li><p>管理者設定兩個 target group，分別是上方 AZ 的四個 instance，與下方 AZ 的一個 instance</p></li><li><p>設定兩個 listener rules，分別是處理送往 <code>www.myurl.com</code> &amp; <code>www.myurl.com/images</code> 的流量</p></li><li><p>將 listener rules 與 target group 繫結，符合第一個存取路徑的流量會導向上方 AZ 的 instance，而 image 相關的流量則會轉發到下方 AZ 中的 instance</p></li></ul><h1 id="Auto-Scaling"><a href="#Auto-Scaling" class="headerlink" title="Auto Scaling"></a>Auto Scaling</h1><h2 id="Auto-Scaling-是什麼"><a href="#Auto-Scaling-是什麼" class="headerlink" title="Auto Scaling 是什麼?"></a>Auto Scaling 是什麼?</h2><p>Auto Scaling 是在公有雲上必用的功能之一，可以根據系統負載來自動調整資源佈署，以下是 AWS 官方對 Auto Scaling 這項功能的介紹：</p><blockquote><p>AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes. The service provides a simple, powerful user interface that lets you build scaling plans for resources including Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB tables and indexes, and Amazon Aurora Replicas. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. If you’re already using Amazon EC2 Auto Scaling to dynamically scale your Amazon EC2 instances, you can now combine it with AWS Auto Scaling to scale additional resources for other AWS services. With AWS Auto Scaling, your applications always have the right resources at the right time.</p></blockquote><p>看完上述介紹，重點就大概可以理解的差不多了，以下整理歸納一下：</p><ul><li><p>Auto Scaling 的運作基礎在監控，有監控數據，才會有實現 auto scaling 的依據 (至於哪些數據可以被監控，就是比較細節的部份了)</p></li><li><p>Auto Scaling 並非 EC2 獨有的選項，也可以與 ECS、DynamoDB、Aurora … 等服務搭配</p></li><li><p>Auto Scaling 提供了一些特性讓整體設定變得簡單(例如：Launch Template、Scaling Option … 等等)</p></li><li><p>透過 Auto Scaling 可以協助優化資源使用率、降低成本</p></li><li><p><strong>若要達成最小程度的 HA &amp; Fault Tolerance，就必須使用 ELB + Auto Scaling Group(使用兩個 instance，設定為 cross AZ)</strong></p></li></ul><p>這功能看起來相當好用，誰不希望系統可以自動幫忙些什麼事情呢?</p><h2 id="Auto-Scaling-的組成"><a href="#Auto-Scaling-的組成" class="headerlink" title="Auto Scaling 的組成"></a>Auto Scaling 的組成</h2><p>Auto Scaling 共分為三個部份，分別是：</p><ul><li><p>Groups</p><blockquote><p>這屬於邏輯上服務的群組，例如：一群 web server，一群 DB server</p></blockquote></li><li><p>Configuration Templates</p><blockquote><p>用來指示 Auto Scaling 機制，要自動生成的服務的樣貌，以 EC2 為例，就會有 AMI ID、instance type、key pair、security group、storage … 等資訊</p></blockquote></li><li><p>Scaling Options</p><blockquote><p>這是進行 Auto Scaling 的依據(or 規則)，可能根據負載、或是特定時間；以及可以進行 Auto Scaling 的資源上下限；甚至包含當 auto scaling 發生時所要進行通知操作的 SNS 設定</p></blockquote></li></ul><h2 id="有哪些-Scaling-Option-可用"><a href="#有哪些-Scaling-Option-可用" class="headerlink" title="有哪些 Scaling Option 可用?"></a>有哪些 Scaling Option 可用?</h2><p>Group &amp; Configuration Template 都是相對容易理解的部份，這裡需要注意的是有哪些 scaling option 是可以設定的? 目前共支援五種，以下一一介紹。</p><h3 id="Maintain-current-instance-levels-at-all-times"><a href="#Maintain-current-instance-levels-at-all-times" class="headerlink" title="Maintain current instance levels at all times"></a>Maintain current instance levels at all times</h3><ul><li><p>明確告知 ASG，無論何時都一定要有特定數量的 instance 存在</p></li><li><p>AWS 會定期對 instance 進行健康檢查</p></li><li><p>若是發現有不健康的 instance，就會把它移除，並重啟一個新的</p></li></ul><h3 id="Scale-manually"><a href="#Scale-manually" class="headerlink" title="Scale manually"></a>Scale manually</h3><ul><li><p>這是最基礎的 ASG 設定方式</p></li><li><p>其實就只有指定 maximum, minimum, desired capacity 這三個參數作為 ASG 執行的依據(不一定全部都要設定)</p></li></ul><h3 id="Scale-based-on-a-schedule"><a href="#Scale-based-on-a-schedule" class="headerlink" title="Scale based on a schedule"></a>Scale based on a schedule</h3><ul><li><p>根據指定的時間 or 日期進行伸縮</p></li><li><p>適合用在清楚知道 scale out/in 的時機</p></li></ul><h3 id="Scale-based-on-demand"><a href="#Scale-based-on-demand" class="headerlink" title="Scale based on demand"></a>Scale based on demand</h3><ul><li><p>這是進階版的伸縮方式，透過提供 scaling policy 的方式達成</p></li><li><p>scaling policy 的依據可以來自監控的數據，例如：CPU 使用率</p></li><li><p>應該可以搭配自訂的監控數據，但要搞清楚如何串接 (這部份待確認)</p></li></ul><h3 id="Use-predictive-scaling"><a href="#Use-predictive-scaling" class="headerlink" title="Use predictive scaling"></a>Use predictive scaling</h3><ul><li><p>將 EC2 Auto Scaling 搭配 AWS Auto Scaling，進行多個服務的彈性伸縮</p></li><li><p>AWS Auto Scaling 可以透過預測的方式，對資源進行動態的伸縮，協助使用者維持最佳的可用性與系統效能</p></li></ul><h2 id="關於-Maximum-Minimum-Desired-Capacity"><a href="#關於-Maximum-Minimum-Desired-Capacity" class="headerlink" title="關於 Maximum, Minimum, Desired Capacity"></a>關於 Maximum, Minimum, Desired Capacity</h2><p>有人可能會有疑問，若是 Desired Capacity 跟 Maximum or Minimum 設定衝突時怎辦? 原則大概如下：</p><ul><li><p>Desired Capacity 是實際上使用者告訴 ASG 希望可以達到的目標</p></li><li><p>其他可能會造成這數字自動改變的行為，可能來自於 CloudWatch alarm，自動觸發的 instace scale out/in 的行為</p></li><li><p>當自動 scale out/in 行為被觸發，要一直維持 Desired Capacity 就會沒有辦法，因此就會需要 Maximum &amp; Minimum 來規範上下限範圍</p></li></ul><h1 id="Auto-Scaling-實作須知"><a href="#Auto-Scaling-實作須知" class="headerlink" title="Auto Scaling 實作須知"></a>Auto Scaling 實作須知</h1><ul><li><p>設定 Auto Scaling Group 之前，必須先設定 <code>Launch Configurations</code></p></li><li><p>Group size 若是與 subnet 數量一致，佈署的時候就會被平均分散放置</p></li><li><p>作為自動 scale group size 的 metric type 有四種，分別是：</p><ul><li><p>Application Load Balancer Request Per Target</p></li><li><p>Average CPU Utilization</p></li><li><p>Avergae Network In (Bytes)</p></li><li><p>Average Network Out (Bytes)</p></li></ul></li><li><p>可設定新增的 instance 開始提供服務時間之前的 warm up 週期</p><blockquote><p>若是設定 300 秒，那該 instance 有五分鐘可以 warm up 的時間(根據需求自行處理此部份)，然後前端的 ELB 才會將流量送過來</p></blockquote></li><li><p>當 ASG 設定被移除，與其相關的 instance 也會一併被移除</p></li></ul><h1 id="HA-Architechture"><a href="#HA-Architechture" class="headerlink" title="HA Architechture"></a>HA Architechture</h1><p>此部份是記錄設計 HA(高可用) 架構時需要注意的重點</p><h2 id="重點觀念"><a href="#重點觀念" class="headerlink" title="重點觀念"></a>重點觀念</h2><ul><li><p>硬體一定會故障，基礎設施一定會有問題，因此必須 always design for failure</p></li><li><p>multiple AZ 的設計是必須的；如果是非常關鍵的應用，multiple region 的設計也可以考慮進來</p></li><li><p>搞清楚 RDS multi-AZ(避免故障導致無法服務) &amp; read-replica(提昇效能) 的差異</p></li><li><p>設計高可用架構，成本支出必然提昇，因此了解成本結構，如何有效節省支出是一件很重要的事情</p></li><li><p>S3 storage tier 的差異需要了解(standard, standard-IA, Glacier …. etc)</p></li></ul><h2 id="實作課程中的重點整理"><a href="#實作課程中的重點整理" class="headerlink" title="實作課程中的重點整理"></a>實作課程中的重點整理</h2><p>在 <a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a> 課程中，有針對 HA 架構實作了一個 WordPress 的網站，過程中有一些值得記錄的重點：</p><ul><li><p>EC2 instance 中有設定 policy 可以存取 S3，因此可以將靜態資源上傳到 S3 上，但缺乏持續同步靜態資源的部份，因此持續同步的部份還是要另外處理</p></li><li><p>將靜態圖片從 EC2 instance 指向 CloudFront，是透過 url rewrite 功能達成的；這功能一般在 web server 都會有，可以善用</p></li><li><p>範例中 S3 是設定可以 public read，基於安全性需求，可能改成設定只有 CloudFront 可以 read …?</p></li><li><p>設定 ALB 時要指定到後方的 Target Group；而 Target Group 則是指到後方已經設定好 WP 的 EC2 instance</p></li><li><p>若要在二級域名(例如範例中的 <code>acloudguru2019.com</code>)直接指到 ALB，則是在 Route 53 中設定 <code>Alias</code> record</p><blockquote><p>二級域名並非使用 CName，而是要用 Alias</p></blockquote></li></ul><h1 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h1><h2 id="CloudFormation"><a href="#CloudFormation" class="headerlink" title="CloudFormation"></a>CloudFormation</h2><p>CloudFormation 跟之前研究 OpenStack 時學到的 HEAT 看起來幾乎是一模一樣概念的東西；基本上概念是這樣：</p><ul><li><p>是種 Infrastructure as Code 的工具</p></li><li><p>透過預先定義好的 template 快速產生環境(稱為 <code>stack</code>)的一種工具</p></li><li><p>AWS 已經預先定義好非常多 template 了，甚至還有很多廠商也提供了不少現成的 template 可以直接套用</p><blockquote><p>如果你的環境很固定是要跑特定的應用，那倒是可以試試看</p></blockquote></li></ul><p>基本上這套工具並不吸引我，我個人是會選擇泛用性更高的工具，例如：<a href="https://www.terraform.io/">Terraform</a> or <a href="https://www.pulumi.com/">pulumi</a> 來做一樣的事情，避免被特定平台綁死</p><h2 id="Elastic-Beanstalk"><a href="#Elastic-Beanstalk" class="headerlink" title="Elastic Beanstalk"></a>Elastic Beanstalk</h2><p>這服務就是很簡單很快的給使用者一個特定的環境(例如：PHP)，然後後續的調整在自己慢慢來，基本上概念是這樣：</p><ul><li><p>AWS 快速產生一個可運行的特定環境給使用者</p></li><li><p>AWS 幫忙處理掉像是 provisioning、Load Balancing, scaling, application health monitoring 這些複雜瑣碎的事情了</p></li><li><p>使用者再根據需求自己慢慢調整</p></li><li><p>可以後續設定比較進階的功能來搭配，例如：Auto Scaling Group</p></li></ul><p>但其實很多東西在本地環境就可以測試了，尤其現在 Docker 這麼方便…..相信這服務會慢慢式微的….XD</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://aws.amazon.com/blogs/devops/introducing-application-load-balancer-unlocking-and-optimizing-architectures/">Introducing Application Load Balancer – Unlocking and Optimizing Architectures | AWS DevOps Blog</a></p></li><li><p><a href="https://aws.amazon.com/elasticloadbalancing/faqs">Elastic Load Balancing FAQs - Amazon Web Services</a></p></li><li><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html">Configure Sticky Sessions for Your Classic Load Balancer - Elastic Load Balancing</a></p></li><li><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html">Configure Cross-Zone Load Balancing for Your Classic Load Balancer - Elastic Load Balancing</a></p></li><li><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html">Listeners for Your Application Load Balancers - Elastic Load Balancing</a></p></li><li><p><a href="https://aws.amazon.com/autoscaling/">AWS Auto Scaling</a></p></li><li><p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html">What Is Amazon EC2 Auto Scaling? - Amazon EC2 Auto Scaling</a></p></li><li><p><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html#scaling_typesof">Scaling the Size of Your Auto Scaling Group - Amazon EC2 Auto Scaling</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> ELB </tag>
            
            <tag> Auto Scaling </tag>
            
            <tag> High Availability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - Route 53</title>
      <link href="/blog/AWS/AWS-CSA-associate-DNS/"/>
      <url>/blog/AWS/AWS-CSA-associate-DNS/</url>
      
        <content type="html"><![CDATA[<h1 id="DNS-101"><a href="#DNS-101" class="headerlink" title="DNS 101"></a>DNS 101</h1><ul><li><strong>Route53 預設有 50 個 domain 的限制，但可以與 AWS support 聯繫取消此限制</strong></li></ul><h2 id="常見的-DNS-Resource-Record"><a href="#常見的-DNS-Resource-Record" class="headerlink" title="常見的 DNS Resource Record"></a>常見的 DNS Resource Record</h2><p>常見的 DNS Resource Record 有 SOA/NS/A/CNAME/MX/PTR … 等等，可參考以下文章的說明：</p><ul><li><p><a href="http://dns-learning.twnic.net.tw/bind/intro6.html">DNS資源紀錄(Resource Record)介紹</a></p></li><li><p><a href="https://docs.aws.amazon.com/zh_tw/Route53/latest/DeveloperGuide/ResourceRecordTypes.html">支援的 DNS 記錄類型 - Amazon Route 53</a></p></li></ul><h2 id="cname-amp-alias-的差異"><a href="#cname-amp-alias-的差異" class="headerlink" title="cname &amp; alias 的差異"></a>cname &amp; alias 的差異</h2><p>這個部份可以參考以下連結：</p><ul><li><p><a href="https://help.ns1.com/hc/en-us/articles/360017511293-What-is-the-difference-between-CNAME-and-ALIAS-records-">What is the difference between CNAME and ALIAS records? – NS1 Help Center</a></p></li><li><p><a href="https://support.dnsimple.com/articles/differences-between-a-cname-alias-url/">Differences Among A, CNAME, ALIAS, and URL records - DNSimple Help</a></p></li></ul><h1 id="Route53-Routing-Policy"><a href="#Route53-Routing-Policy" class="headerlink" title="Route53 Routing Policy"></a>Route53 Routing Policy</h1><p>目前 Route53 支援的 routing policy 有以下幾種：</p><ul><li><p>Simple Routing</p></li><li><p>Weighted Routing</p></li><li><p>Latency-based Routing</p></li><li><p>Failover Routing</p></li><li><p>Geolocation Routing</p></li><li><p>Geoproximity Routing (Traffic Flow only)</p></li><li><p>Multivalue Answer Routing</p></li></ul><h2 id="Simple-Routing"><a href="#Simple-Routing" class="headerlink" title="Simple Routing"></a>Simple Routing</h2><p><img src="/blog/images/aws/DNS_Simple-Routing.png" alt="Route53 - Simeple Routing"></p><ul><li><p>Simple Routing 可以讓使用者對單一筆 record 設定多個 IP，並以 random 的方式回應結果(還要另外考慮 TTL 的影響)</p></li><li><p>無法搭配 health check 機制</p></li></ul><h2 id="Weighted-Routing"><a href="#Weighted-Routing" class="headerlink" title="Weighted Routing"></a>Weighted Routing</h2><p><img src="/blog/images/aws/DNS_Weighted-Routing.png" alt="Route53 - Weighted Routing"></p><ul><li><p>可以自訂比例，設定某百分比的 DNS 查詢流量回應某個 record</p></li><li><p>百分比跟 record 並非一對一的，在同一個百分比設定下可以有多筆對應的 record (random 回應)</p></li><li><p>可以設定為每一筆 record 設定 health check 並搭配 EC2 instance 的 health check 功能</p></li><li><p>若 health check 失敗，Route53 就不會回應該筆 record</p></li><li><p>health check 失敗可以搭配 SNS 通知讓負責人員知道</p></li></ul><h2 id="Latency-based-Routing"><a href="#Latency-based-Routing" class="headerlink" title="Latency-based Routing"></a>Latency-based Routing</h2><p><img src="/blog/images/aws/DNS_Latency-Based-Routing.png" alt="Route53 - Latency-based Routing"></p><ul><li><p>設定時，每一筆 record 加入時，AWS 會自動偵測合適的 region 回應(也可以自己選)；完成後，同樣的 domain name 在不同的 region 會回應不同的 record</p></li><li><p>使用者查詢時，AWS 會在設定的 region 中對使用者延遲最低的 region 進行回應</p></li><li><p>以上圖為例，來自南非的使用者就會由 eu-west-2 來回應，因為延遲比 ap-southeast-2 小多了</p></li></ul><h2 id="Failover-Routing"><a href="#Failover-Routing" class="headerlink" title="Failover Routing"></a>Failover Routing</h2><p><img src="/blog/images/aws/DNS_Failover-Routing.png" alt="Route53 - Failover Routing"></p><ul><li><p>Failover Routing 用來支援 active(primary) / passive 的設計；當 active site 出問題，Route53 可以回應 passive site record</p></li><li><p>需要搭配 health check 的設定才可以使用</p></li><li><p>當 active(primary) site 的 health check 失敗時，Route53 就會改成回應 passive site record</p></li></ul><h2 id="Geolocation-Routing"><a href="#Geolocation-Routing" class="headerlink" title="Geolocation Routing"></a>Geolocation Routing</h2><p><img src="/blog/images/aws/DNS_Geolocation-Routing.png" alt="Route53 - Geolocation Routing"></p><ul><li><p>可以根據使用者所在的位置回應指定的 record</p></li><li><p>設定時可以指定使用者所在的五大洲 or 國家</p></li></ul><h2 id="Geoproximity-Routing-Traffic-Flow-only"><a href="#Geoproximity-Routing-Traffic-Flow-only" class="headerlink" title="Geoproximity Routing (Traffic Flow only)"></a>Geoproximity Routing (Traffic Flow only)</h2><ul><li><p>這是進階版的 DNS traffic policy 設定，因為實在太複雜，認證考試基本上不會出現</p></li><li><p>policy 設定時，除了可以使用使用者所在位置作為依據，連 resource 所在的位置也可以拿來作為判斷條件</p></li><li><p>透過 Route53 Traffic Flow 的設定(也必須搭配 traffic flow)，可以將各種條件結合起來成為複合型的判斷，因此可以作到很細膩的控制</p></li></ul><h2 id="Multivalue-Answer-Routing"><a href="#Multivalue-Answer-Routing" class="headerlink" title="Multivalue Answer Routing"></a>Multivalue Answer Routing</h2><p>其實就是 Simple Routing，只是可以加上 health check 的功能</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/route53/">Amazon Route 53 - Amazon Web Services</a></p></li><li><p><a href="https://kkc.github.io/2017/07/23/aws-route-53-multivalue/">AWS route53 multivalue 筆記 - Kakashi’s Blog</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> Route 53 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - EC2(Elastic Compute Cloud) Part 3</title>
      <link href="/blog/AWS/AWS-CSA-associate-EC2-part3/"/>
      <url>/blog/AWS/AWS-CSA-associate-EC2-part3/</url>
      
        <content type="html"><![CDATA[<h1 id="Bootstrap-Scripts"><a href="#Bootstrap-Scripts" class="headerlink" title="Bootstrap Scripts"></a>Bootstrap Scripts</h1><ul><li><p>EC2 instance 在設定時可以透過 <code>user-data</code> 欄位指定 bootstrap script，目的是讓 EC2 instance 在佈署完成後，進行指定的工作</p></li><li><p>這功能可以與其他服務搭配，讓一些特殊需求的自動化容易些</p><blockquote><p>例如：EC2 instance 佈署完成後，呼叫 meta data server 取得 public ip，放到 S3；S3 event 觸發 Lambda function 將資料讀出後，寫入 RDS</p></blockquote></li></ul><h1 id="Instance-Meta-Data"><a href="#Instance-Meta-Data" class="headerlink" title="Instance Meta Data"></a>Instance Meta Data</h1><ul><li><p>每個 EC2 instance 可以透過一個特殊的 IP 位址 <code>169.254.169.254</code> 取得 instance meta data (其實就是關於此 instance 的資訊)</p></li><li><p>透過 <code>curl http://169.254.169.254/latest/meta-data/</code> 可以列出可取得哪些 meta data，大概包含 iam/instance-action/instance-id/local-hostname/local-ipv4/mac/metrics/network/public-hostname/public-ipv4 …. 等等，可取得不少資訊</p></li><li><p>透過 <code>curl http://169.254.169.254/latest/user-data</code> 就可以取得在 instance 設定時候填入的 user-data 資訊</p></li></ul><h1 id="EFS-Elastic-File-System"><a href="#EFS-Elastic-File-System" class="headerlink" title="EFS(Elastic File System)"></a>EFS(Elastic File System)</h1><p>首先看看 AWS 對 EFS 的定義：</p><blockquote><p>Amazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.</p></blockquote><blockquote><p>Amazon EFS offers two storage classes: the Standard storage class, and the Infrequent Access storage class (EFS IA). EFS IA provides price/performance that’s cost-optimized for files not accessed every day. By simply enabling EFS Lifecycle Management on your file system, files not accessed according to the lifecycle policy you choose will be automatically and transparently moved into EFS IA.</p></blockquote><blockquote><p>Amazon EFS is designed to provide massively parallel shared access to thousands of Amazon EC2 instances, enabling your applications to achieve high levels of aggregate throughput and IOPS with consistent low latencies.</p></blockquote><blockquote><p>Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability. Amazon EC2 instances can access your file system across AZs, regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN.</p></blockquote><p>大概可以整理出以下重點：</p><ul><li><p>沒有預付費用， 只須根據儲存容量 &amp; 傳輸費用付費</p></li><li><p>可以擴充到 PB 等級大小</p></li><li><p>跟 S3 相同，可協助將不常存取的檔案移到 IA tier 中</p></li><li><p>資料會在指定的 region 中分散不同 AZ 進行存放</p></li></ul><p>其他重點資訊：</p><ul><li><p>EFS 目前支援 NFSv4</p></li><li><p>可以支援同時數千個 NFS 連線</p></li><li><p>提供 read-after-write consistent，表示寫入後馬上可以讀取到</p></li></ul><h1 id="FSX-amp-FSx-for-Lustre"><a href="#FSX-amp-FSx-for-Lustre" class="headerlink" title="FSX &amp; FSx for Lustre"></a>FSX &amp; FSx for Lustre</h1><p>因為對 Windows 不是很有興趣，這邊就重點整理一下吧!</p><h2 id="FSx"><a href="#FSx" class="headerlink" title="FSx"></a>FSx</h2><ul><li><p>FSx 可以想像一下就是 Windows 中的 SMB 協定提供出來的檔案系統，所以其實跟 EFS 之於 Linux 是差不多相同意思</p></li><li><p>FSx 同樣也是全託管(fully managed)服務，資料的 HA 也會自動保證(同 region 跨 AZ，也可以選擇 single AZ)</p></li></ul><h2 id="FSx-for-Lustre"><a href="#FSx-for-Lustre" class="headerlink" title="FSx for Lustre"></a>FSx for Lustre</h2><ul><li><p>這是強化版的共享檔案系統，可同時被 Windows &amp; Linux 使用，總之就是<strong>高效能 + 可擴展</strong></p></li><li><p>可以提供 low latency、high throughtput/IOPS 等特性，用在像是機器學習、HPC …. 這類資料存取繁重的應用是很合適的</p></li><li><p>可與 S3 進行整合，資料處理完就可以直接存放到 S3</p></li></ul><h1 id="Placement-Group"><a href="#Placement-Group" class="headerlink" title="Placement Group"></a>Placement Group</h1><p>試想一下，如果有些應用需要 VM 之間網路延遲很低，在地端的 data center 會怎麼做?</p><blockquote><p>很簡單囉，只要把 VM 起在同一台機器，或是接在同一台網路設備的不同機器，不要讓他們之間距離太遠，問題就搞定了!</p></blockquote><p>那如果希望某台實體機器掛掉了，不會影響到太多的 VM 運作，要怎麼做呢?</p><blockquote><p>這其實也不難，只要將 VM 起在不同的實體機器就可以了!</p></blockquote><p>簡單來說，<code>Placement Group</code> 這個功能可以讓使用者根據需求決定”你想要把 EC2 instance 擺在哪裡?”這件事情</p><p>而 AWS 提供了三種 placement group，分別是 <code>cluster</code>、<code>spread</code>、<code>partition</code>，使用者可以根據需求自行選擇：</p><h2 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h2><ul><li><p>instance 一定會在同一個 AZ 內</p></li><li><p>會盡可能的將 instance 放在一起，甚至同一個機櫃上，以取得更低的網路延遲</p></li><li><p>可以提供 instance 之間的網路有更低的延遲 &amp; 更高的吞吐量</p></li><li><p>承上三點，所以選擇 instance type 時，至少要選擇有 10Gb 以上網路的 instance type 才能享受到 placement group 的所帶來的優勢</p></li></ul><h2 id="Spread"><a href="#Spread" class="headerlink" title="Spread"></a>Spread</h2><ul><li><p>instance 不一定會在同一個 AZ 內</p></li><li><p>不同 placement group 的 instance 絕對不會在同一個 rack 上，因此某個 rack 發生問題不會導致所有 instance 無法使用</p></li><li><p>若是需要在同一個 AZ 中的 HA，可以透過設定 spread placement group 達成</p></li><li><p>隔離的單位為單一 instance</p></li><li><p>每個 placement group 在每個 AZ 最多只能有 7 個 running instance</p></li></ul><h2 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h2><ul><li><p>instance 不一定會在同一個 AZ 內</p></li><li><p>運作方式 &amp; 效果 spread type 類似，只是單位是 instance group (多個 instance 形成一個名稱為 <code>partition</code> 的單位)</p></li><li><p>不同的 partition 不會被安排在同一個 rack 上</p></li><li><p>隔離的單位為 partition(也就是一群 instance)</p></li><li><p>通常像是 HDFS、HBase、Cassandra 可以利用 partition placement group 來額外設定 hardware-level HA</p></li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li><p>同一個 AWS 帳號下的 placement group 名稱不能重複</p></li><li><p>只有某些類型的 instance 才可以放到 placement group 中(例如：compute optimized / GPU / memory optimized / storage optimizted … 等等)</p></li><li><p>若要將 instance 放到 cluster placement group 中，建議是同質性的 instance (若是同樣的 instance type 更好)</p></li><li><p>不同的 placement group 無法進行合併</p></li><li><p>若是將 instance 移到 placement group 中，必須先切到 <code>stopped</code> 狀態；而且這件事情目前僅能透過 AWS CLI or SDK 完成，無法在 console 完成這件事情</p></li><li><p>若是遇到 <code>insufficient capacity error</code>，解決方法是停止所有在 placement group 中的 instance，再重新啟動</p></li></ul><h1 id="WAF-Web-Application-Firewall"><a href="#WAF-Web-Application-Firewall" class="headerlink" title="WAF(Web Application Firewall)"></a>WAF(Web Application Firewall)</h1><p>首先以下是 AWS 原廠對 WAF 服務的說明：</p><blockquote><p>AWS WAF is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define. You can get started quickly using Managed Rules for AWS WAF, a pre-configured set of rules managed by AWS or AWS Marketplace Sellers. The Managed Rules for WAF address issues like the OWASP Top 10 security risks. These rules are regularly updated as new issues emerge. AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of security rules.</p></blockquote><p>WAF 若要是在地端實現，要購買的設備可是相當昂貴的；此外，不論是地端 or 雲端，要善用 WAF 的功能，也要清楚 WAF 的原理 &amp; 相關知識，使用起來才可以符合自己所需，以下是 AWS WAF 的特點：</p><ul><li><p>WAF 主要就是要監控被轉發到 CloudFront、ALB、API Gateway … 等服務的 HTTP &amp; HTTPS 的請求</p><blockquote><p>這表示 AWS AWF 可以掛在 CloudFront、ALB、API Gateway 這幾個地方前面</p></blockquote></li><li><p>可以根據以下條件設定讓某些 request 通過 or 被阻擋：</p><ul><li><p>來源 IP address</p></li><li><p>query string parameters (可搭配正規表示式)</p></li><li><p>來源國家</p></li><li><p>在 HTTP header 中的值</p></li><li><p>request 長度</p></li><li><p>SQL Injection (惡意的 SQL 內容)</p></li><li><p>Cross-Site Scripting (惡意的 script 內容)</p></li></ul></li><li><p>上面三個在 WAF 後方的服務，可以自行決定要接受 or 拒絕 quest，跟 WAF 的設定是分開的</p></li></ul><p>AWS WAS 在過濾流量上提供三種模式供使用者選擇：</p><ul><li><p>除了指定的條件外，允許所有 request 通過 (黑名單設計)</p></li><li><p>除了指定的條件外，拒絕所有 request 通過 (白名單設計)</p></li><li><p>不阻止 request，僅針對指定的條件進行計數 (通常用於觀察之用)</p></li></ul><h2 id="考試重點"><a href="#考試重點" class="headerlink" title="考試重點"></a>考試重點</h2><p>若要需要阻止惡意的來源 IP 位址，可以從哪些服務著手：</p><ul><li><p>Network ACLs</p></li><li><p>AWS WAF</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html">Retrieving instance metadata - Amazon Elastic Compute Cloud</a></p></li><li><p><a href="https://aws.amazon.com/efs/">Amazon Elastic File System (EFS) | Cloud File Storage</a></p></li><li><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html">Placement groups - Amazon Elastic Compute Cloud</a></p></li><li><p><a href="https://aws.amazon.com/waf/">AWS WAF - Web Application Firewall - Amazon Web Services (AWS)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> EC2 </tag>
            
            <tag> EFS </tag>
            
            <tag> FSx </tag>
            
            <tag> FSx for Lustre </tag>
            
            <tag> WAF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - EC2(Elastic Compute Cloud) Part 2</title>
      <link href="/blog/AWS/AWS-CSA-associate-EC2-part2/"/>
      <url>/blog/AWS/AWS-CSA-associate-EC2-part2/</url>
      
        <content type="html"><![CDATA[<h1 id="EBS-Elastic-Block-Store"><a href="#EBS-Elastic-Block-Store" class="headerlink" title="EBS(Elastic Block Store)"></a>EBS(Elastic Block Store)</h1><h2 id="EBS-是什麼"><a href="#EBS-是什麼" class="headerlink" title="EBS 是什麼?"></a>EBS 是什麼?</h2><p>EBS 是 AWS 提供的 block storage 服務，可提供以下服務：</p><ul><li><p>與 EC2 instance 搭配使用的 block storage service</p></li><li><p>EBS 會自動在 volume 所在的 region 產生複本備份，確保資料安全 &amp; 可用性</p></li><li><p>可根據效能需求 or 預算考量，選擇合適的 volume type</p></li><li><p>可使用 EBS snapshot 將資料備份到 S3，並搭配 S3 lifecycle 管理機制來提高資料的安全性</p></li></ul><h2 id="EBS-Volume-Type"><a href="#EBS-Volume-Type" class="headerlink" title="EBS Volume Type"></a>EBS Volume Type</h2><p>EBS volume type 有四種(官方宣稱)，直接使用下表來進行比較：</p><blockquote><p>第五種為前一代的 <code>EBS Magnetic</code>，但應該之後會消失</p></blockquote><p><img src="/blog/images/aws/EBS_Storage-Type-Comparison.png" alt="EBS storage type comparison"></p><ul><li><p>不同的應用其實都有合適的 volume type 可以對應</p></li><li><p>每個 volume type 對應的 API Name 需要稍微記一下</p></li></ul><h2 id="Volumes-amp-Snapshots"><a href="#Volumes-amp-Snapshots" class="headerlink" title="Volumes &amp; Snapshots"></a>Volumes &amp; Snapshots</h2><ul><li><p>EBS volume 與 EC2 instance 會在同一個 AZ 中</p></li><li><p>volume 存在於 EBS 中；snapshot 則是存在於 S3 中</p></li><li><p>snapshot 是 volume 在特定時間點的複本</p></li><li><p>snapshot 的特性是 incremental，因此對 volume 進行 snapshot 只會針對有變動的 block 進行處理(也只有增加的部份容量會被收費)</p><blockquote><p>假設同一個 EBS volume 的 snapshot 有兩個，若是第一個完整的 snapshot 被刪除，還是可以從第二個 snapshot 還原所有資料</p></blockquote></li><li><p>第一次的 snapshot 需要花費較長的時間完成</p></li><li><p>不建議在 EC2 instance 運行中建立 snapshot，會影響效能；但還是可以這麼做</p></li><li><p>EBS volume 可以直接線上擴展容量 or 更改 volume type，不會造成資料毀損 (但需要花幾分鐘的時間完成)</p></li><li><p>EC2 instance 中的 root disk 是其實來自於 AWS 準備的 EBS volume snapshot</p></li><li><p>當 EC2 instance 被移除時，root disk 會跟著被移除，只有額外新增的 volume 保留下來</p></li></ul><h3 id="從-snapshot-產生的-EBS-volume-的效能問題"><a href="#從-snapshot-產生的-EBS-volume-的效能問題" class="headerlink" title="從 snapshot 產生的 EBS volume 的效能問題"></a>從 snapshot 產生的 EBS volume 的效能問題</h3><ul><li><p>透過 snapshot 產生的 EBS volume 建議先做 initialization</p></li><li><p>initialization 這個操作會在 storage block 第一次被讀取時發生，而這個效能可能只有原本的 50%</p></li><li><p>承上兩點，不建議將透過 snapshot 產生出來的 EBS volume，尚未進行完整 initialization 之前就放到生產環境</p></li></ul><h2 id="如何將-EC2-instance-或-EBS-volume-移到不同的-AZ-中"><a href="#如何將-EC2-instance-或-EBS-volume-移到不同的-AZ-中" class="headerlink" title="如何將 EC2 instance 或 EBS volume 移到不同的 AZ 中?"></a>如何將 EC2 instance 或 EBS volume 移到不同的 AZ 中?</h2><ol><li><p>對 EBS volume 進行 snapshot</p></li><li><p>透過上一個步驟產生的 snapshot 建立一個 AMI(Amazon Machine Image)</p><blockquote><p>從 EBS snapshot 建立 Image 時，<strong>Virtualization Type</strong> 建議選用 <code>Hardware-assisted viirtualization</code>，之後使用 image 時可以支援較多種 EC2 instance type；若選擇 <code>paravirtual</code> 會導致可以選擇的 EC2 instance type 變得很少</p></blockquote></li><li><p>使用 AMI 在不同的 AZ 中建立 EC2 instance</p></li></ol><p>上面的範例是說明如何將 EBS volume 移到不同的 AZ 上；但如果要移到另外一個 region 呢?</p><ol><li><p>執行上面的前兩個步驟</p></li><li><p>執行 <strong>Copy AMI</strong> 操作，將 AMI 複製到不同的 region 中</p><blockquote><p>若是要實現跨 region 移動 EBS volume，就要使用 <strong>Copy AMI</strong> 的方式</p></blockquote></li></ol><h2 id="磁碟加密-Encrypt-功能"><a href="#磁碟加密-Encrypt-功能" class="headerlink" title="磁碟加密(Encrypt)功能"></a>磁碟加密(Encrypt)功能</h2><ul><li><p>若是 EBS volume 有加密，則對其進行的 snapshot 也都會被自動加密</p></li><li><p>從加密後的 snapshot 還原成 EBS volume，也會自動被加密</p></li><li><p>snapshot 可以在同一個帳號或是跨帳號，甚至公開進行分享，但必須是沒有加密的狀態下</p></li><li><p>目前在建立 EC2 instance 時，支援將 root device 進行加密</p></li></ul><h3 id="將-EC2-Instance-未加密的-root-device-進行加密的流程"><a href="#將-EC2-Instance-未加密的-root-device-進行加密的流程" class="headerlink" title="將 EC2 Instance 未加密的 root device 進行加密的流程"></a>將 EC2 Instance 未加密的 root device 進行加密的流程</h3><p>若是有某台帶有未 root device 的 EC2 instance，因為某些原因需要將其加密，可透過以下流程進行：</p><ol><li><p>對未加密的 root device volume 進行 snapshot</p></li><li><p><strong>複製 snapshot，並選擇進行加密</strong></p></li><li><p>使用上一個步驟已經加密後的 snapshot 建立一個 AMI</p></li><li><p>透過新建立的 AMI 產生一個新的 EC2 instance (此時 root device 已經加密)</p></li></ol><h1 id="AMI-Type"><a href="#AMI-Type" class="headerlink" title="AMI Type"></a>AMI Type</h1><p>AMI 是產生 EC2 instance 的基礎，使用者在選擇 AMI 時，可基於以下幾個項目進行選擇：</p><ul><li><p>Region</p></li><li><p>作業系統</p></li><li><p>系統架構(32-bit 或是 64-bit)</p></li><li><p>啟動權限</p></li><li><p>root device 所使用的 storage 類型，可能會是 <code>Instance Store</code> 或是 <code>EBS Backed Volume</code></p></li></ul><h2 id="AMI-類型"><a href="#AMI-類型" class="headerlink" title="AMI 類型"></a>AMI 類型</h2><ul><li><p>所有的 AMI 都是來自於兩個來源：<strong>Amazon EBS</strong> or <strong>Instance Store</strong>(Ephemeral Storage)</p></li><li><p>EBS volume 的來源是透過 Amazon 預先準備好的 EBS snapshot 所產生的</p></li><li><p>Instance Store volume 則是從預先儲存在 S3 的 template 中建立出來的 (Console 中的 <strong>Community AMI</strong> 頁面可看到)</p></li></ul><h2 id="考試重點"><a href="#考試重點" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>當啟動 EC2 instance 之前，可以新增額外的 instance store volume &amp; EBS volume</p></li><li><p>但是當 EC2 instance 被啟動後，就無法新增額外的 instance store volume，只能新增 EBS volume</p></li><li><p>Instance Store Volume 所建立的 instance 無法停止，因此一旦 instance 出問題，資料就會遺失了；但透過 EBS backed instance 是可以停止的</p></li><li><p>兩種類型的 instance 重新啟動都不會遺失資料</p><blockquote><p>但若是對 instance 進行 <code>stop</code> 或是 <code>shutdown</code>，會造成 instance store 中的資料遺失</p></blockquote></li><li><p>預設情況下，兩種類型的 instance 被移除後，root device 都會一併移除</p></li><li><p>可以設定當 instance 被移除後不移除 EBS backed volume，不過 instance store volume 無法做這樣的設定</p></li></ul><h1 id="ENI-ENA-EFA-的比較"><a href="#ENI-ENA-EFA-的比較" class="headerlink" title="ENI, ENA, EFA 的比較"></a>ENI, ENA, EFA 的比較</h1><p>這個部份可能會需要曾經有硬體相關的背景可能會看比較懂….</p><p>總之有些時候可能會有一些特殊應用，會需要 VM 可以有接近 Bare Metal(實體機器) 的網路效能(包含延遲)，但又不想管理不甚彈性的實體機器，那 AWS 提供一些額外的加強型網路選項可以協助這個部份；若是真的無法達到需求，再考慮實體機器也可以….</p><h2 id="ENI-Elastic-Network-Interface"><a href="#ENI-Elastic-Network-Interface" class="headerlink" title="ENI (Elastic Network Interface)"></a>ENI (Elastic Network Interface)</h2><ul><li>就是一般啟動 EC2 instance 預設使用的虛擬網路卡</li></ul><blockquote><p>若是需要獨立網路(例如：logging network)，但不需要特別高效能，希望便宜些，就用此選項</p></blockquote><h2 id="ENA-Elastic-Network-Adapter"><a href="#ENA-Elastic-Network-Adapter" class="headerlink" title="ENA (Elastic Network Adapter)"></a>ENA (Elastic Network Adapter)</h2><ul><li><p>加強型的虛擬網路卡，使用 SR-IOV 功能，讓 VM 對硬體設備的存取可以繞過 Hypervisor 直接對硬體存取，大幅提昇網路效能、降低延遲 &amp; CPU 使用率</p><blockquote><p>SR-IOV 技術不僅可用在網路設備上，儲存設備也可以….簡單來說就是支援 SR-IOV 的 PCIe 設備都行</p></blockquote></li><li><p>ENA 在網路部份提供了更高的頻寬、穩定且更低的延遲</p></li><li><p>啟用這個功能不會用任何無外費用產生，但僅有部份 Instance type 支援此功能</p></li><li><p>ENA 可以支援到 100Gbps 的網路頻寬；而在以前的 instance type 有支援 10Gbps 頻寬的 VF(Intel 82599 Virtual Function) 選項，現在都以 ENA 為主</p></li><li><p>若是要建立一個獨立網段做特定的工作(例如：storage data plan)並兼顧低成本且高可用的需求，就可以利用這樣的功能</p></li></ul><blockquote><p>若需要 10Gbps ~ 100Gbps 且穩定的網路品質，可以選用 ENA</p></blockquote><h2 id="EFA-Elastic-Fabric-Adapter"><a href="#EFA-Elastic-Fabric-Adapter" class="headerlink" title="EFA (Elastic Fabric Adapter)"></a>EFA (Elastic Fabric Adapter)</h2><ul><li><p>一種特殊的網路設備，可用來提昇 HPC 或是機器學習相關應用的效能</p></li><li><p>比起傳統 cloud-based HCP 系統，可以提高更低的網路延遲 &amp; 更高的網路輸出入</p></li><li><p>讓 instance 中的應用可以 bypass Linux kernel(OS-bypass)，直接與硬體通訊，藉此大幅提高速度並降低延遲</p></li><li><p>目前此功能僅支援 Linux 平台</p></li></ul><blockquote><p>特殊與 HPC 相關的應用(例如：機器學習)，就可以考慮此選項</p></blockquote><h1 id="CloudWatch-amp-CloudTrail"><a href="#CloudWatch-amp-CloudTrail" class="headerlink" title="CloudWatch &amp; CloudTrail"></a>CloudWatch &amp; CloudTrail</h1><p>CloudWatch &amp; CloudTrail 都是很大的主題，這邊暫時以 CSA 的考試需求為主，因此了解一下兩個服務的功能特性 &amp; 比較即可。</p><h2 id="CloudWatch"><a href="#CloudWatch" class="headerlink" title="CloudWatch"></a>CloudWatch</h2><ul><li><p><strong>主要作為監控效能用</strong>，是 AWS 重要服務之一</p></li><li><p>同時與其他 AWS 服務都有高度整合(例如：EC2, ASG, ELB, Route53, EBS, CloudFront … 等等)，方便進行監控的設定</p></li><li><p>除了 AWS 服務之外，也可以同時用來監控自行開發的應用程式</p></li><li><p>CloudWatch Alarm 可以用來觸發某些特定事件，例如：auto scaling</p></li><li><p>CloudWatch 預設每五分鐘會對 EC2 進行監控資訊的蒐集；但若是希望可以監控到更細節的資訊，可以設定成每一分鐘(在啟用 instance 時勾選 <code>Enable CloudWatch detailed monitoring</code>)</p></li><li><p>CloudWatch 有幾個重要的組成部分：</p><ul><li><p><code>Dashboard</code>：以圖形展現的方式呈現監控數據，可以是 region level or global level</p></li><li><p><code>Alarm</code>：設定某個 thrshold 到達時，觸發告警(可與 SNS 服務進行整合)</p></li><li><p><code>Event</code>：可指定當某個 AWS resource 變更時，執行預先指定的工作</p></li><li><p><code>Logs</code>：CloudWatch 也可以協助蒐集 &amp; 監控、儲存文字型態的 log 資訊</p></li></ul></li></ul><h2 id="CloudTrail"><a href="#CloudTrail" class="headerlink" title="CloudTrail"></a>CloudTrail</h2><ul><li><p>用來檢視使用者對於 AWS 各項資源的使用記錄 &amp; 狀況，<strong>主要用來提供稽核用</strong></p></li><li><p>記錄內容包含 console 的操作 &amp; API call，以及來源 IP … 等資訊</p></li><li><p>不論操作的來源是 CLI, SDK 或是 console 都會被紀錄</p></li></ul><h1 id="與-IAM-Role-的搭配使用"><a href="#與-IAM-Role-的搭配使用" class="headerlink" title="與 IAM Role 的搭配使用"></a>與 IAM Role 的搭配使用</h1><p>不論要存取任何 AWS resource，都必須要有合法且足夠的權限才有辦法進行；在一般情況下，要取得權限，可以到 IAM 中新增 user，指定需要的權限，並取得 Access Key ID &amp; Secret Access Key 來使用。</p><p>以 EC2 instance 為例，若要設定 AWS credential，就必須在檔案 <code>~/.aws/credentials</code> 中，設定以下內容：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[default]</span></span><br><span class="line"><span class="attr">aws_access_key_id</span>=[YOUR_ACCESS_KEY_ID]</span><br><span class="line"><span class="attr">aws_secret_access_key</span>=[YOUR_SECRET_ACCESS_KEY]</span><br></pre></td></tr></table></figure><p>但若是需要串接很多不同服務，就可能需要在很多不同的 EC2 instace 都對 AWS resource 進行存取，在每一台 instance 中設定 Access Key ID &amp; Secret Access Key 並不切實際且難以管理，因此這問題就可以用 <code>IAM Role</code> 來解決。</p><p>流程如下：</p><ol><li><p>在 <code>IAM</code> 中新增 Role，並設定所需要的權限</p></li><li><p>將上一個步驟新增的 role 與 EC2 instance 進行繫結</p></li></ol><p>如此一來，即使在 EC2 instance 沒有 AWS credential 也可以直接對有權限的 AWS resource 進行存取</p><h2 id="考試重點-1"><a href="#考試重點-1" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>若要賦予 EC2 instance 存取其他 AWS resource 的權限，使用 IAM role 會比 Access Key ID &amp; Secret Access Key 更安全</p></li><li><p>Role 的概念比較容易管理 &amp; 直覺</p></li><li><p>Role 可以在 EC2 instance 被建立後，透過 console 或是 command line 進行繫結</p></li><li><p>Role 的有效範圍是 global，因此設定之後可以在所有 region 中使用</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/ebs/?ebs-whats-new.sort-by=item.additionalFields.postDateTime&ebs-whats-new.sort-order=desc">Amazon Elastic Block Store (EBS) - Amazon Web Services</a></p></li><li><p><a href="https://rickhw.github.io/2017/07/16/AWS/Deep-Dive_EBS/">Deep Dive on EBS | Complete Think</a></p></li><li><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html">Enhanced networking on Linux - Amazon Elastic Compute Cloud</a></p></li><li><p><a href="https://b8807053.pixnet.net/blog/post/345974548">SR-IOV 簡介 @ 立你斯學習記錄 :: 痞客邦 ::</a></p></li><li><p><a href="http://www.ipshop.xyz/12470.html">詳解“硬核”虛擬化技術SR-IOV原理-知識星球</a></p></li><li><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html">Elastic Fabric Adapter - Amazon Elastic Compute Cloud</a></p></li><li><p><a href="https://aws.amazon.com/cloudwatch/">Amazon CloudWatch - Application and Infrastructure Monitoring</a></p></li><li><p><a href="https://aws.amazon.com/cloudtrail/">AWS CloudTrail – Amazon Web Services</a></p></li><li><p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">IAM Roles - AWS Identity and Access Management</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> IAM </tag>
            
            <tag> EC2 </tag>
            
            <tag> EBS </tag>
            
            <tag> AMI </tag>
            
            <tag> CloudWatch </tag>
            
            <tag> CloudTrail </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - EC2(Elastic Compute Cloud) Part 1</title>
      <link href="/blog/AWS/AWS-CSA-associate-EC2-part1/"/>
      <url>/blog/AWS/AWS-CSA-associate-EC2-part1/</url>
      
        <content type="html"><![CDATA[<h1 id="EC2-簡介"><a href="#EC2-簡介" class="headerlink" title="EC2 簡介"></a>EC2 簡介</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>首先是原廠介紹：</p><blockquote><p>Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment.</p></blockquote><p>可以看出 EC2 具備以下特性：</p><ul><li><p>提供 compute capacity，也就是俗稱的 virtual machine(VM)</p></li><li><p>包含了許多安全性功能</p></li><li><p>可以因應使用者需求，提供可大可小運算能力的 VM，並且可隨時 scale up &amp; down</p></li><li><p>大大降低了產生 VM 所花費的時間，讓使用者可以很低門檻的開始使用 VM 相關服務</p></li><li><p>VM 是運行在 AWS 長久以來被驗證很穩定的基礎建設上</p></li></ul><h2 id="Pricing-Type"><a href="#Pricing-Type" class="headerlink" title="Pricing Type"></a>Pricing Type</h2><ul><li><p><strong>Om Demand</strong>：用多少算多少，不用保證用量，但每個時間單位費用是最高的</p></li><li><p><strong>Reserved</strong>：預先跟 AWS 保留使用量，需要簽約 &amp; 預付一大筆金額，但整體來說可以得到很大的折扣(相較於 on demand 模式)，</p><blockquote><p>使用這類型的 instance 無法做 region 之間的移動(但可以更換 AZ、網路設定、instance size …. 等等)</p></blockquote></li><li><p><strong>Spot</strong>：透過競價模式取得的 EC2 instance，最便宜，但由於價格是浮動的，一旦價格高過原本設定的出現，instance 就會被砍掉；適合用在使用時間彈性的應用上(例如：大數據資料處理)</p></li><li><p><strong>Dedicated Host</strong>：整台實體機器都給你用了，如果軟體必須跟實體機器綁定時 or 想要先把地端的運用完全不動的先移植到雲端時，才會用到的選項。</p></li><li><p><strong>Saving Plan</strong>：這是新一代的省錢方式，對使用者來說，運用彈性變大的(例如：可改變 instance type)，詳情可參考<a href="https://aws.amazon.com/tw/savingsplans/">官網說明</a>，但同樣會需要使用者預付一筆費用</p></li></ul><h2 id="Spot-Instance-計費"><a href="#Spot-Instance-計費" class="headerlink" title="Spot Instance 計費"></a>Spot Instance 計費</h2><ul><li><p>spot instance 是以小時(hour)為單位計費</p></li><li><p>如果因為出價太低，導致 AWS 主動砍了 spot instance，未滿一小時的部份則不收費</p></li><li><p>如果使用者自行砍掉 VM，未滿一小時的部份則會被收一小時整的費用</p></li></ul><h2 id="Instance-Type"><a href="#Instance-Type" class="headerlink" title="Instance Type"></a>Instance Type</h2><p>每個 instance type 的代碼其實都有其意義，可以對應到特別的場景 &amp; 應用，以下列出說明：</p><ul><li><p><code>F</code>：FPGA</p></li><li><p><code>I</code>：IOPS</p></li><li><p><code>G</code>：Graphics</p></li><li><p><code>H</code>：High Disk Throughput</p></li><li><p><code>T</code>：Cheap general purpose (例如：<strong>t2.micro</strong>)</p></li><li><p><code>D</code>：Density</p></li><li><p><code>R</code>：RAM</p></li><li><p><code>M</code>：Main choice for general purpose apps</p></li><li><p><code>C</code>：Compute</p></li><li><p><code>P</code>：Graphics (例如：Pics)</p></li><li><p><code>X</code>：Extreme Memory</p></li><li><p><code>Z</code>：Extreme Memory &amp; CPU</p></li><li><p><code>A</code>：ARM-based workloads</p></li><li><p><code>U</code>：Bare Metal</p></li></ul><h2 id="AMI-Virtualization-Type"><a href="#AMI-Virtualization-Type" class="headerlink" title="AMI Virtualization Type"></a>AMI Virtualization Type</h2><p>目前有以下兩種：</p><ul><li><p>HVM(Hardware Virtual Machine)：可利用到特殊的硬體所提供的效能</p></li><li><p>PV(Paravirtual, 全虛擬化)：可以在不確定可以支援虛擬化的硬體上運作，若要模擬很特殊的作業系統，這個選項可能會用到</p></li></ul><p>基本上，如果不是很特殊的應用，為了要得到相對較好的效能，直接選擇 <code>HVM</code> 就對了。</p><h1 id="關於-EC2-設定的選項"><a href="#關於-EC2-設定的選項" class="headerlink" title="關於 EC2 設定的選項"></a>關於 EC2 設定的選項</h1><h2 id="設定-Instance"><a href="#設定-Instance" class="headerlink" title="設定 Instance"></a>設定 Instance</h2><h3 id="選項中的-AZ-跟實際中的-AZ-並不同"><a href="#選項中的-AZ-跟實際中的-AZ-並不同" class="headerlink" title="選項中的 AZ 跟實際中的 AZ 並不同"></a>選項中的 AZ 跟實際中的 AZ 並不同</h3><p>在 subnet 的部份，可以選擇對應到不同 AZ 的 subnet，但上面的選項不一定會對應到實際的 AZ，例如在 region <strong>us-east-1</strong> 中，有以下 AZ：</p><ul><li><p><code>us-east-1a</code></p></li><li><p><code>us-east-1b</code></p></li><li><p><code>us-east-1c</code></p></li></ul><p>假設使用者 Andy 選了 <code>us-east-1a</code>，另外一位使用者 Bill 也選了 <code>us-east-1a</code>，但實際上兩個使用者的 VM 並不一定放在同一個 AZ 中。</p><p>但對同一個使用者來說，<code>us-east-1a</code>、<code>us-east-1b</code>、<code>us-east-1c</code> 就是三個不同實體的 AZ，只是你不需要去關心到底對應到那個實體 AZ；這表示使用者其實不需要知道或關心 AZ 代碼對應到的實際哪個 AZ，只要知道若是當 cloud resource 指定放置在不同的 AZ，AWS 就會幫你在實體上放在不同的 AZ 中。</p><h3 id="監控"><a href="#監控" class="headerlink" title="監控"></a>監控</h3><ul><li><p>預設情況下，CloudWatch 每五分鐘會收集 EC2 instance metrics</p></li><li><p>若希望可以取得更細節的資訊(例如：每一分鐘)，那可以勾選 <strong>Monitoring</strong> 選項中的 <code>Enable CloudWatch detailed monitoring</code></p></li></ul><h3 id="一般選項"><a href="#一般選項" class="headerlink" title="一般選項"></a>一般選項</h3><ul><li><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html"><strong>Capacity Reservation</strong></a>：跟 saving plan 有關係，看起來像是要在省錢 &amp; 確保有特定容量的資源可用所設計的機制，詳細的內容可以參考<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html">官方文件說明</a>。</p></li><li><p><strong>IAM Role</strong>：可以指定 EC2 instance 所使用的 IAM 身份為何，若是需要串接其他服務，這個部份需要有正確的設定才行</p></li><li><p><strong>Shutdown Behavior</strong>：可指定關閉 instance 時，只是單純的停止(stop)，或是直接將其終止(terminate)</p></li><li><p><strong>Enable Termination Protection</strong>：可以用來保護此 VM 不會被誤刪</p></li><li><p><strong>Tenancy</strong>：可用來決定是否可以跟別的使用者共享同一台實體機的資源，有 <code>Shared</code>、<code>Dedicated</code>、<code>Dedicated host</code> 三個選項</p></li></ul><h3 id="進階選項"><a href="#進階選項" class="headerlink" title="進階選項"></a>進階選項</h3><ul><li><strong>User data</strong>：這裡可以用來指定 bootstrap script</li></ul><h2 id="增加-Storage"><a href="#增加-Storage" class="headerlink" title="增加 Storage"></a>增加 Storage</h2><ul><li><p>預設會有一個 Volume Type 為 <code>Root</code> 的磁碟，這個就是 EC2 instance 的開機磁碟，容量通常很小，預設只有 8GB</p></li><li><p>Root volume 只能選擇 <code>SSD</code> &amp; <code>Magnetic(standard)</code> 兩種 volume type</p></li><li><p>Root volume 中的 <code>Delete on Termination</code> 是固定會勾選的，表示 instance 砍了，Root volume 也會跟著不見</p></li><li><p>可以指定增加 EBS volume，指定容量 &amp; Volume Type(HDD, SSD … 等等)</p></li><li><p>EBS volume 則可以選擇更多種 Volume Type，例如：Cold HDD、Throughput Optimized HDD … 等等</p></li><li><p>若是選擇 <strong>Throughput Optimized HDD</strong>，畫面就會出現 Throughtput 能力(MB/s)的資訊，基本上容量越大 throughtput 就會越大</p></li><li><p>指定不同的 Volume Type 會顯示對應的 IOPS 資訊</p></li><li><p>General Purpose SSD 能提供的 IOPS 是根據磁碟容量而定的，另外有一個 burst 的上限</p></li><li><p>Provisioned IOPS SSD 則是可以提供固定 IOPS 能力的磁碟 (<strong>可能效能無法達到預期</strong>)</p><blockquote><p>若使用 Provisioned IOPS SSD 也無法達到預期的效能，那 instance type 可以改選 <code>EBS optimized instance type</code>，這類型的 instance 在網路傳輸上，EBS traffic 會優先處理，因此可以確保達到 provisioned IOPS 的效能</p></blockquote></li><li><p>原本 Root volume 是無法加密的，但現在已經可以進行加密了</p></li></ul><h2 id="設定-Security-Group"><a href="#設定-Security-Group" class="headerlink" title="設定 Security Group"></a>設定 Security Group</h2><ul><li><p>Security Group 就是個虛擬的防火牆</p></li><li><p>若要連到 EC2 instance，設定正確的 security group rule 是需要的</p></li><li><p>security group rule 在設定上不同的 protocol, port, source, destination …. 等規則</p></li></ul><h2 id="設定-Key-Pair"><a href="#設定-Key-Pair" class="headerlink" title="設定 Key Pair"></a>設定 Key Pair</h2><ul><li><p>要用來設定連線到 VM 的 ssk keypair，可以預先上傳自己的 key，也可以直接在 console 中產生</p></li><li><p>透過 SSH 連線，使用者名稱為 <code>ec2-user</code></p></li></ul><h1 id="EC2-Instance-管理功能"><a href="#EC2-Instance-管理功能" class="headerlink" title="EC2 Instance 管理功能"></a>EC2 Instance 管理功能</h1><h2 id="Status-Check"><a href="#Status-Check" class="headerlink" title="Status Check"></a>Status Check</h2><ul><li><p><strong>System Status Checks</strong>：這是用來檢查底層實體機器 &amp; Hypervisor 的狀態</p></li><li><p><strong>Instance Status Check</strong>：這是用來檢查 VM 本身的狀態</p></li></ul><h2 id="Actions"><a href="#Actions" class="headerlink" title="Actions"></a>Actions</h2><ul><li>若有開啟 <strong>Termination Protection</strong> 的功能，要先在 <strong>Instance Settings -&gt; Change Termination Protection</strong> 中關閉此功能，才可以砍掉此 VM</li></ul><h1 id="Security-Group"><a href="#Security-Group" class="headerlink" title="Security Group"></a>Security Group</h1><h2 id="EC2-amp-Security-Group"><a href="#EC2-amp-Security-Group" class="headerlink" title="EC2 &amp; Security Group"></a>EC2 &amp; Security Group</h2><ul><li><p>EC2 instance 可以同時與多個 security group 綁定</p></li><li><p>EC2 instance 一旦綁定 security group，規則馬上就會生效</p></li><li><p>一個 security group 可以對應到多個 EC2 instance</p></li></ul><blockquote><p>所以 EC2 instance &amp; security group 是多對多的關係</p></blockquote><h2 id="Security-Group-v-s-VPC-Network-ACL-Access-Control-List"><a href="#Security-Group-v-s-VPC-Network-ACL-Access-Control-List" class="headerlink" title="Security Group v.s. VPC Network ACL(Access Control List)"></a>Security Group v.s. VPC Network ACL(Access Control List)</h2><h3 id="Security-Group-1"><a href="#Security-Group-1" class="headerlink" title="Security Group"></a>Security Group</h3><ul><li><p>Security Group rule 是 stateful 的</p></li><li><p>承上，若是 allow HTTP in，就會有一個隱性的 allow HTTP out 存在</p></li><li><p>承上，即使將 outbound rule 中允許向外的流量規則刪除，EC2 還是可以回應 HTTP request</p></li><li><p>預設所有 inbound traffic 都會被擋住，需要根據需求一條一條規則打開</p></li><li><p>預設所有 outbound traffic 都是開放的</p></li><li><p>無法設定黑名單(blacklist)，因此無法進行類似 “<strong>阻擋存取特定服務的 traffic</strong>“ 這樣的設定</p></li></ul><h3 id="VPC-Network-ACL-Access-Control-List"><a href="#VPC-Network-ACL-Access-Control-List" class="headerlink" title="VPC Network ACL(Access Control List)"></a>VPC Network ACL(Access Control List)</h3><ul><li><p>VPC network ACL 是 stateless 的</p></li><li><p>承上，若是建立了 inbound rule，就要建立相對應的 outbound rule，否則對應的服務不會通</p></li><li><p>可以設定黑名單，因此 “<strong>阻擋存取特定服務的 traffic</strong>“ 這類的規則是可以設定的</p></li></ul><h1 id="應考重點整理"><a href="#應考重點整理" class="headerlink" title="應考重點整理"></a>應考重點整理</h1><h2 id="EC2-概觀"><a href="#EC2-概觀" class="headerlink" title="EC2 概觀"></a>EC2 概觀</h2><ul><li><p><strong>Termination Protection 預設是關閉的</strong>，有需要就必須自己打開</p></li><li><p>VM 被刪除(Terminated)，Root volume 會一起被刪除</p></li><li><p>Root volume 可以加密，可以搭配第三方工具(例如：bot locker)使用，或是透過 AWS 提供的 API</p></li><li><p>額外加入的 volume 也可以加密</p></li></ul><h2 id="Security-Group-2"><a href="#Security-Group-2" class="headerlink" title="Security Group"></a>Security Group</h2><ul><li><p>所有 inbound traffic 預設都會被擋住</p></li><li><p>所有 outbound traffic 預設是開放的</p></li><li><p>security group 規則修改後馬上會生效</p></li><li><p>EC2 instance &amp; security group 是多對多的關係</p></li><li><p>security group 是 stateful，在 inbound 設定的規則會同時在 outbound 中自動開放</p></li><li><p>security group 無法設定黑名單，若需要黑名單功能，則需要使用 Network ACL 來做 (只可以設定 allow rules，無法設定 deny rules)</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/tw/ec2/">Amazon EC2</a></p></li><li><p><a href="https://aws.amazon.com/tw/ec2/pricing/">Amazon EC2 定價 – Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/tw/savingsplans/">Savings Plans – Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/tw/ec2/faqs/">Amazon EC2 常見問答集 – Amazon Web Services</a> <a href="https://aws.amazon.com/ec2/faqs/?nc1=h_ls">(英文版)</a></p></li><li><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html">Security groups for your VPC - Amazon Virtual Private Cloud</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> EC2 </tag>
            
            <tag> VPC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - VPC(Virtual Private Cloud) Part 2</title>
      <link href="/blog/AWS/AWS-CSA-associate-VPC-part2/"/>
      <url>/blog/AWS/AWS-CSA-associate-VPC-part2/</url>
      
        <content type="html"><![CDATA[<h1 id="VPC-Flow-Logs"><a href="#VPC-Flow-Logs" class="headerlink" title="VPC Flow Logs"></a>VPC Flow Logs</h1><h2 id="What-is-VPC-Flow-Logs"><a href="#What-is-VPC-Flow-Logs" class="headerlink" title="What is VPC Flow Logs?"></a>What is VPC Flow Logs?</h2><p>首先看看 AWS 官網對於 VPC Flow Logs 的說明：</p><blockquote><p>VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you’ve created a flow log, you can retrieve and view its data in the chosen destination.</p></blockquote><p>可以歸納出幾個重點：</p><ul><li><p>可協助擷取指定 VPC 網路界面的 inbound &amp; outbound 流量的相關資訊</p></li><li><p>資料可以存放在 CloudWatch 或是 S3</p></li></ul><h2 id="功能用途"><a href="#功能用途" class="headerlink" title="功能用途"></a>功能用途</h2><ul><li><p>可用來診斷 security group 規則是否設定正確</p></li><li><p>監控網路流量是否有到達預期的 instance</p></li></ul><h2 id="實作重點"><a href="#實作重點" class="headerlink" title="實作重點"></a>實作重點</h2><ul><li><p>Flow Logs 可建立在三個不同層級上，分別是 <code>VPC</code> / <code>Subnet</code> / <code>Network Interface</code></p></li><li><p>若是選擇要將 log 存入 CloudWatch，必須先在 CloudWatch 設定好對應的 log group，並且在 IAM 中設定好對應的 role</p></li><li><p>可以選擇記錄 Accept/Reject/All 三種不同的條件</p></li></ul><h2 id="考試重點"><a href="#考試重點" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>若是 VPC 與其他帳號的 VPC 設定 peering，那就無法在上面啟用 flow log</p><blockquote><p>若是兩個 VPC 都屬於同一個帳號就可以</p></blockquote></li><li><p>可以針對 flow log 設定 tag</p></li><li><p>一旦 flow log 建立了，就無法改變其設定 (例如：修改與 flow log 關聯的 IAM role)</p></li><li><p>並非所有流量都可以監控的，包含以下幾項：</p><ul><li><p>查詢 Amazon DNS 服務所產生的流量 (除非自己架設 DNS service)</p></li><li><p>啟用 Amazon Windows license 所產生的網路流量</p></li><li><p>與 IP <code>169.254.169.254</code> 往來的流量</p></li><li><p>DHCP 相關流量</p></li><li><p>到 default VPC router 所在的 IP 的流量</p></li></ul></li></ul><h1 id="Bastion-Host"><a href="#Bastion-Host" class="headerlink" title="Bastion Host"></a>Bastion Host</h1><h2 id="服務說明"><a href="#服務說明" class="headerlink" title="服務說明"></a>服務說明</h2><p>當服務規劃階段，為了安全性考量，把服務 or EC2 instance 都往 private subnet 放以後，就會遇到一個問題：</p><blockquote><p>要如何存取放在 private subnet 的資源或是進行佈署?</p></blockquote><p>這時候就需要 Bastion Host 的協助，以下是一個架構示意圖：</p><p><img src="/blog/images/aws/VPC_Bastion-Host.png" alt="Bastion Host"></p><p>從上面的架構示意圖可以看出幾個重點：</p><ul><li><p>Bastion Host 必須放在 public subnet 上，因為它是可以從外面連入的跳板</p></li><li><p>要進入 Bastion Host，要搞定 <code>Internet Gateway</code>、<code>Route Table</code>、<code>Network ACLs</code>、<code>Security Group</code>，必須層層關卡都打通，才可以從外面透過 SSH or RDP 連到 Bastion Host</p></li><li><p>Bastion Host 會將管理流量導入在 private subnet 的服務 or instance</p></li><li><p>NAT Gateway(or NAT instance) 是用來提供在 private subnet 中的 EC2 instance 之用，跟 Bastion Host 沒有關係；同時也無法作為 Bastion Host 的用途</p></li></ul><h2 id="優點"><a href="#優點" class="headerlink" title="優點"></a>優點</h2><ul><li><p>只要盡可能的強化 Bastion Host 的安全性管理即可</p></li><li><p>在 private subnet 中的服務 or instance 就可以暫時先不用花心力處理安全性的部份</p></li></ul><h1 id="Direct-Connect"><a href="#Direct-Connect" class="headerlink" title="Direct Connect"></a>Direct Connect</h1><h2 id="服務說明-1"><a href="#服務說明-1" class="headerlink" title="服務說明"></a>服務說明</h2><p>Direct Connect 服務是讓使用者可以從地端資料中心建立一條私有的專線到 AWS 環境中存取資源用，此服務的架構圖 &amp; 說明如下：</p><p><img src="/blog/images/aws/VPC_Direct-Connect.png" alt="Direct Connect"></p><ul><li><p>AWS 在各地提供許多 <strong>Direct Connect Location</strong>，讓使用者可以就近選擇合適的點接入</p></li><li><p>在 Direct Connect Location 中有許多 AWS Cage，裡面有 Direct Connect Router；使用者也會放置自己的 Router，然後透過 cross line 進行實體對接</p></li><li><p>使用者還是需要將地端設備網路接到 Router 中，但這一段通常都會落在同一個資料中心</p></li><li><p>Direct Connect Router 則會有 AWS 自己佈建的專線網路回到 AWS 中</p></li><li><p>藉由此專線，可以使用專線，同時存取 AWS 公用資源(例如：S3)，也可以直接存取位於 VPC private subnet 中的服務</p></li></ul><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><ul><li><p>連接地端資料中心 &amp; AWS</p></li><li><p>若是地端到 AWS 的流量很大，很適合使用 Direct Connect 來處理</p></li><li><p>Direct Connect 可提供一條安全的私有專線</p></li></ul><h2 id="建立-Direct-Connect-的步驟"><a href="#建立-Direct-Connect-的步驟" class="headerlink" title="建立 Direct Connect 的步驟"></a>建立 Direct Connect 的步驟</h2><blockquote><p>建立 Direct Connect 的步驟常會考試中出現，務必要記住</p></blockquote><ol><li><p>在 Direct Connect console 中建立一個 virtual interface (這是 public virtual interface)</p></li><li><p>到 <code>VPC -&gt; VPN</code>，建立 Customer Gateway</p></li><li><p>同樣在 <code>VPC -&gt; VPN</code>，建立 Virtual Private Gateway</p></li><li><p>將上一個步驟建立的 Virtual Private Gateway 與想要串連的 VPC 進行連接</p></li><li><p>選擇並建立新的 VPN 連線，並選擇上面步驟中產生的 Customer Gateway &amp; Virtual Private Gateway</p></li><li><p>當 VPN 連線建立完成，就可以設定在地端機房中的 Gateway 或是防火牆</p></li></ol><h1 id="Global-Accelerator"><a href="#Global-Accelerator" class="headerlink" title="Global Accelerator"></a>Global Accelerator</h1><p>首先看看 AWS 對於 Global Accelerator 這項服務的定義：</p><blockquote><p>AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in a single or multiple AWS Regions, such as your Application Load Balancers, Network Load Balancers or Amazon EC2 instances.</p></blockquote><blockquote><p>AWS Global Accelerator uses the AWS global network to optimize the path from your users to your applications, improving the performance of your traffic by as much as 60%. You can test the performance benefits from your location with a speed comparison tool. AWS Global Accelerator continually monitors the health of your application endpoints and redirects traffic to healthy endpoints in less than 30 seconds.</p></blockquote><p><img src="/blog/images/aws/VPC_Global-Accelerator.png" alt="Global Accelerator"></p><p>從服務說明中可以看出幾個特性：</p><ul><li><p>Global Accelerator 的用途是要讓全球不同地區的使用者，提昇服務的體驗 &amp; 可用性</p></li><li><p>為了作到這件事情，可以直接利用的就是 AWS 佈建在全世界各地的 Edge Location</p></li><li><p>根據 Edge server 的狀態、權重的設定 … 等因素，AWS 會自動給出一個對使用者最佳的 edge location</p></li></ul><h2 id="Global-Accelerator-改變了什麼"><a href="#Global-Accelerator-改變了什麼" class="headerlink" title="Global Accelerator 改變了什麼?"></a>Global Accelerator 改變了什麼?</h2><p>沒有設定 Global Accelerator 之前，使用者要連線到放置在 AWS 的服務，完全使用各個 ISP 所提供的 routing，整體的模樣會是下圖的樣子：</p><p><img src="/blog/images/aws/VPC_without-Global-Accelerator.png" alt="without Global Accelerator"></p><p>但使用了 Global Accelerator 服務，連線的路徑則是透過 Edge location，經由 AWS 骨幹網路連線到 AWS 服務，連線狀況就會變成如下圖：</p><p><img src="/blog/images/aws/VPC_with-Global-Accelerator.png" alt="with Global Accelerator"></p><h2 id="設定-Global-Accelerator-需要了解的概念"><a href="#設定-Global-Accelerator-需要了解的概念" class="headerlink" title="設定 Global Accelerator 需要了解的概念"></a>設定 Global Accelerator 需要了解的概念</h2><p>Global Accelerator 是由很多部份設定所組成，因此了解 Global Accelerator 中的各元件是一件很重要的事情。</p><h3 id="固定-IP"><a href="#固定-IP" class="headerlink" title="固定 IP"></a>固定 IP</h3><ul><li><p>Global Accelerator 會提供兩組固定 IP，當一個 IP 因為某原因無法存取時，就可以改用另外一個 IP。</p></li><li><p>也可以使用自己的 IP</p></li></ul><h3 id="Accelerator"><a href="#Accelerator" class="headerlink" title="Accelerator"></a>Accelerator</h3><ul><li><p>Accelerator 可以利用 AWS 全球骨幹網路，將流量導入最佳的節點上，藉此提昇效率 &amp; 可用性。</p></li><li><p>每個 Accelerator 包含一個 or 多個 listener</p></li></ul><h3 id="DNS-Name"><a href="#DNS-Name" class="headerlink" title="DNS Name"></a>DNS Name</h3><ul><li><p>Global Accelerator 會分配給每個 Accelerator 一個 DNS name(例如：<code>abcdexxxxx.awsglobalaccerlerator.com</code>)，並指向預先分配好的固定 IP</p></li><li><p>IP or DNS name 都可以使用，也可以使用自己的 domain name 搭配 CNAME 設定的方式指向 Global Accelerator 派發的 DNS name</p></li></ul><h3 id="Network-Zone"><a href="#Network-Zone" class="headerlink" title="Network Zone"></a>Network Zone</h3><ul><li><p>Network Zone 是實際提供 Accelerator 固定 IP 的地方，類似 AZ，會有一個獨立的 infra</p></li><li><p>當因為特殊原因導致 IP 無法存取，就可以使用另外一個 IP</p></li><li><p>由此可知，Global Accelerator 所配發的兩個固定 IP，將會有兩個不同的 Network zone 來提供服務</p></li></ul><h3 id="Listener"><a href="#Listener" class="headerlink" title="Listener"></a>Listener</h3><ul><li><p>在 Global Accelerator 中必須預先定義要加速的流量(例如：tcp 80 &amp; 443)，Listner 就是用來設定這一類的規則</p></li><li><p>每個 Listener 可以與多個 Endpoint Group 關聯，並會將流量根據規則導向 endpoint group</p></li><li><p>AWS 會自動根據導向的目標來決定最佳路徑</p></li></ul><h3 id="Endpoint-Group"><a href="#Endpoint-Group" class="headerlink" title="Endpoint Group"></a>Endpoint Group</h3><ul><li><p>每個 Endpoint Group 只會在一個 region 內，包含了同一個 region 內的多個 endpoint</p></li><li><p>可以透過 <code>traffic dial</code> 的功能，調整流量導向不同的 endpoint group 的百分比</p></li><li><p>透過 <code>traffic dial</code> 的協助，就可以容易的進行藍綠佈署，甚至要跨 region 也都沒問題 </p></li></ul><h3 id="Endpoint"><a href="#Endpoint" class="headerlink" title="Endpoint"></a>Endpoint</h3><ul><li><p>Endpoint 可以是 NLB(Network Load Balancer)、ALB(Application Load Balancer)、EC2 instance 或是 Elastic IP address</p></li><li><p>簡單來說，大多數的應用下，Endpoint 會是直接提供服務給使用者的端點</p></li><li><p>可以針對每個 endpoint 設定權重，進而將網路流量按照比例導向不同的 endpoint(例如：讓流量全部進到某個 region，讓另外一個 region 保持淨空以方便進行性能測試)</p></li></ul><h1 id="VPC-Endpoint"><a href="#VPC-Endpoint" class="headerlink" title="VPC Endpoint"></a>VPC Endpoint</h1><h2 id="簡介"><a href="#簡介" class="headerlink" title="簡介"></a>簡介</h2><p>為了資訊安全，在網路架構的規劃中，我們可能會進可能的將服務放在 private subnet 中，並禁止其對外的流量。</p><p>但如果服務有對外與其他 managed service(例如：S3) 連接的需求呢?</p><p>原本的作法可以透過 NAT gateway 讓服務取得連網的能力；但另外一種較好的方式，則是透過 <code>VPC enpoint</code> 將 managed service 黏到 VPC private subnet 中，變成 private subnet 中的一個 endpoint。</p><p><strong>如此一來，即使沒有 NAT gateway，也可以讓服務直接存取 AWS managed service。</strong></p><p>舉例來說，原來要存取 S3 必須要有連網能力，因此需要透過 NAT Gateway，所以會是以下的架構：</p><p><img src="/blog/images/aws/VPC_Endpoint-1.png" alt="without VPC Endpoint"></p><p>若是透過 VPC endpoint，則可以在 VPC private subnet 中建立出一個虛擬裝置，透過該裝置就可以直接與 S3 通訊：</p><p><img src="/blog/images/aws/VPC_Endpoint-2.png" alt="with VPC Endpoint"></p><p>但並非所有服務都支援 VPC endpoint，詳細列表可以參考 <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html">AWS 官網資訊</a>。</p><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul><li><p>VPC endpoint 是透過 PrivateLink 的方式提供的</p></li><li><p>使用 VPC endpoint，就不需要 Internet Gateway、NAT device、VPN connection … 等方式來取得對外的連網能力才能存取 AWS managed services</p></li><li><p>VPC enpoint 是種黏在 VPC subnet 上的虛擬裝置，可以輕易的作到水平擴展並達到 redundant &amp; HA 的目的</p></li><li><p>VPC endpoint 類型有兩種，分別是 <code>Interface Endpoint</code> &amp; <code>Gateway Endpoint</code></p></li><li><p>目前支援 Gateway Endpoint 的服務有 <code>S3</code> &amp; <code>DynamoDB</code> </p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://docs.aws.amazon.com/zh_tw/vpc/latest/userguide/flow-logs.html">VPC 流程日誌 - Amazon Virtual Private Cloud</a></p></li><li><p><a href="https://aws.amazon.com/tw/quickstart/architecture/linux-bastion/">AWS 上的 Linux 堡壘主機 - 快速入門</a> (<a href="https://aws.amazon.com/quickstart/architecture/linux-bastion/">英文</a>)</p></li><li><p><a href="https://aws.amazon.com/tw/directconnect/">AWS Direct Connect</a> (<a href="https://aws.amazon.com/directconnect/">英文</a>)</p></li><li><p><a href="https://aws.amazon.com/tw/global-accelerator/">AWS Global Accelerator – Amazon Web Services</a> (<a href="https://aws.amazon.com/global-accelerator/">英文</a>)</p></li><li><p><a href="https://docs.aws.amazon.com/zh_tw/vpc/latest/userguide/vpc-endpoints.html">VPC 端點 - Amazon Virtual Private Cloud</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> VPC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - DB</title>
      <link href="/blog/AWS/AWS-CSA-associate-DB/"/>
      <url>/blog/AWS/AWS-CSA-associate-DB/</url>
      
        <content type="html"><![CDATA[<h1 id="AWS-提供的-DB-相關服務"><a href="#AWS-提供的-DB-相關服務" class="headerlink" title="AWS 提供的 DB 相關服務"></a>AWS 提供的 DB 相關服務</h1><h2 id="RDBMS-關聯式資料庫基本概念"><a href="#RDBMS-關聯式資料庫基本概念" class="headerlink" title="RDBMS 關聯式資料庫基本概念"></a>RDBMS 關聯式資料庫基本概念</h2><p>RDBMS 的基本概念(例如：Database、Table、Row、Field )這邊就略過了….</p><h3 id="Data-Warehousing"><a href="#Data-Warehousing" class="headerlink" title="Data Warehousing"></a>Data Warehousing</h3><ul><li><p>包含大量資料</p></li><li><p>搭配 business intelligence 的工具(例如：Cognos, Jspersoft, SQL Server Reporing Service, Oracle Hyperion … 等等)，從大量的資料中取得有效益的數據</p></li><li><p>這類的運用通常都與管理決策有關</p></li></ul><h3 id="OLTP-與-OLAP"><a href="#OLTP-與-OLAP" class="headerlink" title="OLTP 與 OLAP"></a>OLTP 與 OLAP</h3><ul><li><p>兩者的不同在於查詢類型的不同</p></li><li><p>OLTP 查詢的只是一般事務類型的資料，例如：訂單編號 123 對應到的日期、訂購者姓名、住址 … 等資料，頂多是幾筆資料的查詢</p></li><li><p>OLAP 則會涵蓋大量資料的存取 &amp; 分析，例如：數位產品在 EMEA 地區的淨利資訊</p></li><li><p>因此與 OLAP 相對應的 data warehousing 的技術，在本質上(資料庫 &amp; infra 層面)，就與 OLTP 對應的一般 DB 是不一樣的</p></li><li><p>AWS 提供了 <code>Redshift</code> 作為提供 data warehousing 的工具，可用來進行 OLAP 的工作</p></li></ul><h2 id="Elasticache"><a href="#Elasticache" class="headerlink" title="Elasticache"></a>Elasticache</h2><ul><li><p>Elasticacahe 提供 memory cache 的全託管服務</p></li><li><p>由於資料存取只會從記憶體，所以速度比起磁碟機快上很多</p></li><li><p>目前支援 <code>Redis</code> &amp; <code>Memcached</code> 兩種</p></li></ul><h2 id="各類-DB-服務與合適的應用"><a href="#各類-DB-服務與合適的應用" class="headerlink" title="各類 DB 服務與合適的應用"></a>各類 DB 服務與合適的應用</h2><ul><li><p>RDS (for OLTP)</p><ul><li>SQL Server</li><li>MySQL</li><li>PostgreSQL</li><li>Oracle</li><li>Aurora</li><li>MariaDB</li></ul></li><li><p>Redshift (for OLAP, 與 Business Intelligence &amp; Data Warehousing 應用搭配使用)</p></li><li><p>DynamoDB(NoSQL)</p></li><li><p>Elasticacahe (memory cache，用來加速存取)</p><ul><li>Memcached</li><li>Redis</li></ul></li></ul><h2 id="考試重點"><a href="#考試重點" class="headerlink" title="考試重點"></a>考試重點</h2><ul><li><p>RDS 是運行在 VM 上的</p></li><li><p>承上，但無法登入系統中執行任何動作</p></li><li><p>更新 RDS 是 AWS 的工作，跟使用者無關</p></li><li><p>RDS 並非 serverless (Aurora serverless 則是 serverless)</p></li><li><p><strong>若是需要更高的 storage 性能，RDS 還提供了 provisioned IOPS storage 選項可用；最大可以達到 10,000 IOPS &amp; 16TB 空間</strong></p></li></ul><h1 id="RDS-服務提供的備份-Multi-AZ-Read-Replicas-功能"><a href="#RDS-服務提供的備份-Multi-AZ-Read-Replicas-功能" class="headerlink" title="RDS 服務提供的備份/Multi-AZ/Read Replicas 功能"></a>RDS 服務提供的備份/Multi-AZ/Read Replicas 功能</h1><h2 id="備份"><a href="#備份" class="headerlink" title="備份"></a>備份</h2><p>RDS 提供兩種備份類型：</p><ul><li><p>Automatic Backup</p></li><li><p>DB snapshot</p></li></ul><h3 id="Automatic-Backup"><a href="#Automatic-Backup" class="headerlink" title="Automatic Backup"></a>Automatic Backup</h3><ul><li><p>Automatic Backup 每天會執行一個完整的 full snapshot，資料保存週期可以設定 1~35 天，這個週期稱為 <code>retention period</code></p></li><li><p>進行復原程序時，AWS 會自動選擇最近時間點的 backup 來還原；使用者可以決定恢復到 retention period 中的任何一個時間點</p></li><li><p>Automatic Backup 是預設啟用的，資料會存到 S3</p></li><li><p>S3 會提供與 RDS storage 容量相同的免費額度，假設啟動 5GB 容量的 RDS，S3 就會相對應提供免費的 5GB 備份空間，超過才需要收費</p></li><li><p>備份工作會在使用者預先定義好的 backup window 進行</p></li><li><p>備份工作進行時，會需要耗費一定量的 storage I/O，因此資料當下在存取時會稍微有延遲</p></li><li><p>當 RDS instance 被移除後，相對應的備份資料也會一併被移除</p></li></ul><h3 id="DB-snapshot"><a href="#DB-snapshot" class="headerlink" title="DB snapshot"></a>DB snapshot</h3><ul><li><p>需要使用者手動執行</p></li><li><p>即使 RDS instance 被移除，snapshot 依然會繼續存在</p></li></ul><h3 id="還原"><a href="#還原" class="headerlink" title="還原"></a>還原</h3><p>不論是透過 Automatic Backup 或是 snapshot 還原，原本的 RDS instance 都會消失，會產生一個全新的 RDS instance &amp; DNS endpoint</p><p><img src="/blog/images/aws/RDS_restore.png" alt="RDS restore from backup"></p><h2 id="加密"><a href="#加密" class="headerlink" title="加密"></a>加密</h2><ul><li><p>加密功能目前支援 MySQL, Oracle, SQL Server, PostgreSQL, MariaDB, Aurora</p></li><li><p>需要搭配 AWS KMS(Key MAnagement Service) 使用</p></li><li><p>DB 資料進資料庫後會以加密後的形式存在</p></li><li><p>連同 automatic backup, read replicas, snapshot … 這幾個部份都是以加密的方式存在</p></li></ul><h2 id="Multi-AZ"><a href="#Multi-AZ" class="headerlink" title="Multi-AZ"></a>Multi-AZ</h2><ul><li><p>架構中會有兩台 RDS instance 分別位於不同的 AZ 中，預設是第一個 AZ 中的 instance 被存取</p></li><li><p>資料一旦寫入 RDS instance，就會立刻被同步到位於另外一個 AZ 作為備援用的 RDS instance 中</p></li></ul><p><img src="/blog/images/aws/RDS_Multi-AZ-1.png" alt="RDS Multi-AZ 1"></p><ul><li>當進行計畫中的 DB 維護、DB 故障、或是某個 AZ 掛掉了，原本 RDS domain 的指向會立即自動切到備援在另一個 AZ 的 RDS instance 中</li></ul><p><img src="/blog/images/aws/RDS_Multi-AZ-2.png" alt="RDS Multi-AZ 2"></p><ul><li><p>此功能是用在災難還原中，並非用於提昇 DB 效能</p><blockquote><p>要提昇 DB 效能則是需要使用 read replicas 的功能</p></blockquote></li><li><p>如果直接 reboot RDS instance，就會強制觸發 failover 的動作，DB DNS 的指向就會切換到另一個 AZ 的 RDS instance</p></li><li><p>目前支援 Multi-AZ 的資料庫類型有 SQL server、Oracle、MySQL、PostgreSQL、MariaDB</p></li></ul><h2 id="Read-Replicas"><a href="#Read-Replicas" class="headerlink" title="Read Replicas"></a>Read Replicas</h2><ul><li><p>Read Replicas 可以讓生產環境中的 DB，產生出 read-only 的複本用的功能</p></li><li><p>在有設定 read replicas 的 RDS 架構中，所有的 write 會集中在某一台 DB 上，資料則會以<strong>非同步</strong>的方式複製到其他 replica DB 中</p></li></ul><p><img src="/blog/images/aws/RDS_Read-Replicas-1.png" alt="RDS Read Replicas 1"></p><ul><li>但前面存取 RDS instance 的程式就必須要跟著調整，讓讀取的行為可以分散落在不同的 DB 上<blockquote><p>若 primary DB 無法正常連線時，AWS 提供的 domain 不會自動協助切換到 secondary DB，程式中必須調整成連線 secondary DB domain</p></blockquote></li></ul><p><img src="/blog/images/aws/RDS_Read-Replicas-2.png" alt="RDS Read Replicas 2"></p><ul><li><p>Read Replicas 功能通常會用在讀取繁重的業務場景下，用以提昇效能；但也可以考慮使用 <a href="https://aws.amazon.com/tw/elasticache/">Elasticache</a></p></li><li><p>Read Replicas 目前支援 MySQL、PostgreSQL、MariaDB、Oracle、Aurora 等資料庫</p><blockquote><p>目前只有 SQL server 不支援 read replicas</p></blockquote></li><li><p>主要是用在 scaling 以增加讀取效能，非災難還原</p></li><li><p>要使用 read replicas 的功能，必須開啟 automatic backup</p></li><li><p>最多可以有五個 DB 複本</p></li><li><p>可以為複本 RDS instance 再設定 read replica 副本(但要考慮延遲問題)</p></li><li><p>每個 read replica 都會有自己的 DNS endpoint</p></li><li><p>read replica <strong>可以跨 AZ</strong>，也可以<strong>跨 region</strong></p></li><li><p>設定 Multi-AZ 的 RDS instance 也可以設定 read replicas</p></li><li><p>read replica 本身可以將自己提昇為 primary DB，但這樣一來同步複本的功能就會消失</p></li><li><p><strong>data replication 過程中產生的資料傳輸是免費的</strong></p></li></ul><h1 id="Aurora"><a href="#Aurora" class="headerlink" title="Aurora"></a>Aurora</h1><h2 id="Aurora-是什麼"><a href="#Aurora-是什麼" class="headerlink" title="Aurora 是什麼?"></a>Aurora 是什麼?</h2><p>首先來看看 AWS 對 Aurora 的定義：</p><blockquote><p>Amazon Aurora is a MySQL and PostgreSQL-compatible relational database built for the cloud, that combines the performance and availability of traditional enterprise databases with the simplicity and cost-effectiveness of open source databases.</p></blockquote><blockquote><p>Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases. It provides the security, availability, and reliability of commercial databases at 1/10th the cost. Amazon Aurora is fully managed by Amazon Relational Database Service (RDS), which automates time-consuming administration tasks like hardware provisioning, database setup, patching, and backups.</p></blockquote><blockquote><p>Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 64TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).</p></blockquote><p>從上面的敘述可以看出 Aurora 服務的特性：</p><ul><li><p>提供相容於 MySQL &amp; PostgreSQL 的關聯式資料庫服務</p></li><li><p>對於傳統關聯式資料庫的服務的需求，Aurora 提供了另一個高效能(倍數成長)、高可用、簡單、使用成本低的選擇</p></li><li><p>使用者可省下許多處理管理工作的時間，例如：佈建硬體、資料庫設定、更新、備份…等工作</p></li><li><p>提供許多進階管理功能，例如：分散式、容錯、self-healing、資料復原、持續備份、跨 AZ 的複本設定</p></li></ul><h2 id="Aurora-服務的特色"><a href="#Aurora-服務的特色" class="headerlink" title="Aurora 服務的特色"></a>Aurora 服務的特色</h2><ul><li><p>資料庫開始的容量為 10GB，最大可到 64TB；儲存空間可以自動擴展</p></li><li><p><strong>每個 AZ 會有兩份資料備份，搭配最低三個 AZ 的設定，資料一共會有 6 份的備份</strong></p></li><li><p>Aurora 可自動將資料複製為兩份，且不會影響 write availability；最多可將資料複製成三份，且不會影響 read availability</p></li><li><p>Aurora 透過不斷的掃描 data block &amp; 磁碟，自動修復錯誤，藉以提供 storage self-healing 功能</p></li><li><p>Aurora 會自動開啟備份機制，也可以直接對 Aurora DB instance 進行 snapshot，甚至還可以將 snapshot 分享給 AWS 的其他帳號</p></li><li><p>不論是備份 or snapshot 都不會影響 Aurora 的運作效能</p></li><li><p>若是服務初期階段，workload 很低且無法預測，可以使用 Aurora Serverless 來將費用降到最低；此服務也可以在服務需求變大時，自動的 scale up</p></li></ul><h2 id="RDS-Replica-Types"><a href="#RDS-Replica-Types" class="headerlink" title="RDS Replica Types"></a>RDS Replica Types</h2><p>RDS 提供三種複本(replica)類型：</p><ul><li><p>Aurora Replicas (目前只有此版本支援 Automated Failover)</p></li><li><p>MySQL Read Replicas</p></li><li><p>PostgreSQL Read Replica</p></li></ul><p>以下是 Aurora replica &amp; MySQL replica 的簡單比較：</p><table><thead><tr><th>功能</th><th>Aurora Replicas</th><th>MySQL Replicas</th></tr></thead><tbody><tr><td>複本數量</td><td>最大可到 15</td><td>最大可到 5</td></tr><tr><td>複製類型</td><td>非同步(延遲以毫秒計)</td><td>非同步(延遲以秒計)</td></tr><tr><td>對 Primary Instance 效能影響</td><td>低</td><td>高</td></tr><tr><td>複製地點</td><td>In-region(也可以 Cross-region)</td><td>Cross-region</td></tr><tr><td>Failover 功能</td><td>Yes(資料不會掉)</td><td>Yes(有可能掉數分鐘的資料)</td></tr><tr><td>自動 Failover</td><td>Yes</td><td>No</td></tr><tr><td>支援使用者自訂的 replication delay</td><td>No</td><td>Yes</td></tr><tr><td>支援複本與 primary 不同的 data or schema</td><td>No</td><td>Yes</td></tr></tbody></table><h2 id="啟動-Aurora-服務需要提供的資訊"><a href="#啟動-Aurora-服務需要提供的資訊" class="headerlink" title="啟動 Aurora 服務需要提供的資訊"></a>啟動 Aurora 服務需要提供的資訊</h2><ul><li><p>要選擇 MySQL or PostgreSQL與對應的版本</p></li><li><p>資料庫要存在 Region(primary AZ + replica AZ) 中或是 Global(primary region + replica region)，目前僅支援 MySQL</p></li><li><p>若是穩定的 workload，選擇 <code>One Write and multiple readers</code>；若屬於剛開始起步或是無法預測的 workload，選擇 <code>Serverless</code></p></li><li><p>資料庫名稱、帳號、密碼</p></li><li><p>DB instance class</p></li><li><p>是否進行 Multi-AZ 佈署 (Aurora 可提供 HA &amp; 自動 failover 的功能)</p></li><li><p>所使用的 VPC 網路</p></li></ul><h1 id="DynamoDB"><a href="#DynamoDB" class="headerlink" title="DynamoDB"></a>DynamoDB</h1><h2 id="DynamoDB-是什麼"><a href="#DynamoDB-是什麼" class="headerlink" title="DynamoDB 是什麼?"></a>DynamoDB 是什麼?</h2><p>首先來看看 AWS 對 DynamoDB 的定義：</p><blockquote><p>Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It’s a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second.</p></blockquote><p>從上面的敘述可以看出 DynamoDB 服務的特性：</p><ul><li><p>儲存 key/value &amp; document 類型的資料庫</p></li><li><p>即使資料量很大，也不太會影響到存取的速度</p></li><li><p>還可以啟用 memory cache 來取得更快的速度</p></li><li><p>可以跨 region, 同時也包含了備份 &amp; 還原的功能</p></li></ul><h2 id="DynamoDB-基礎知識"><a href="#DynamoDB-基礎知識" class="headerlink" title="DynamoDB 基礎知識"></a>DynamoDB 基礎知識</h2><ul><li><p>資料儲存在 SSD 中</p></li><li><p>會將資料分佈在不同的 AZ 中</p></li><li><p>由於資料寫入後需要同步在 cluster 中的不同機器上，因此需要一點時間，才可以取得最新的資料</p></li><li><p>若資料寫入 DynamoDB 不需要在一秒內存取到最新的資料，那可以選擇 <code>Eventually Consistent Read</code></p></li><li><p>若資料寫入 DynamoDB 後需要在一秒內可以存取到最新的資料，則選擇 <code>Strongly Consistent Read</code></p></li></ul><h1 id="Redshift"><a href="#Redshift" class="headerlink" title="Redshift"></a>Redshift</h1><p>Redshift 是 AWS 提供的資料倉儲服務，可以用來儲存大量的傳統資料，進行 OLAP 相關工作，作為 business intelligence 之用。</p><p>而服務本身包含以下幾點特色：</p><h2 id="使用設定"><a href="#使用設定" class="headerlink" title="使用設定"></a>使用設定</h2><ul><li><p>若是設定 single node，可用容量為 160GB</p></li><li><p>若是設定為 multi node，則包含 <code>Leader node</code> &amp; <code>Compute node</code></p><ul><li><code>Leader node</code>：管理 client 連線 &amp; 接收 request</li><li><code>Compute node</code>：儲存資料，執行查詢 &amp; 相關計算，最多可以有 128 個 compute node</li></ul></li></ul><h2 id="進階功能"><a href="#進階功能" class="headerlink" title="進階功能"></a>進階功能</h2><ul><li><p>Redshift 可以將傳入的資料進行高效率的壓縮，節省儲存空間</p></li><li><p>在資料寫入的過程中，可以自動選取最有效率的壓縮方式</p></li><li><p>Redshift 不需要 Index &amp; materialized view，因此比起傳統 RDBMS，需要的儲存空間就相對低很多</p></li><li><p>MPP(Massively Parallel Processing) 功能可以將資料 &amp; 查詢需求平均分散在不同的 compute node 上</p></li></ul><h2 id="資訊安全"><a href="#資訊安全" class="headerlink" title="資訊安全"></a>資訊安全</h2><ul><li><p>資料傳輸的過程會以 SSL 加密</p></li><li><p>任何 REST 相關的工作都會以 AES-256 加密進行</p></li><li><p>可整合 HSM or AWS KMS 作為金鑰管理方式 </p></li></ul><h2 id="備份-1"><a href="#備份-1" class="headerlink" title="備份"></a>備份</h2><ul><li><p>預設資料保存一天，最多存放 35 天</p></li><li><p>Redshift 會將存入的資料保存三份(原本的、在另外一台 compute node 的複本、在 S3 的備份)</p></li><li><p>Redshift 可以非同步的將 snapshot 傳到 S3 中進行儲存</p></li></ul><h1 id="Elasticache-1"><a href="#Elasticache-1" class="headerlink" title="Elasticache"></a>Elasticache</h1><h2 id="Elasticache-是什麼"><a href="#Elasticache-是什麼" class="headerlink" title="Elasticache 是什麼?"></a>Elasticache 是什麼?</h2><p>首先來看看 AWS 對 Elasticache 的定義：</p><blockquote><p>Amazon ElastiCache allows you to seamlessly set up, run, and scale popular open-Source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p></blockquote><p>從上面的敘述可以看出 Elasticache 服務的特性：</p><ul><li><p>用來儲存應用中常被存取的資料，具有高速低延遲的特性</p></li><li><p>資料儲存在記憶體中，速度非常的快</p></li><li><p>常被用來作為快取、session 儲存、即時資料分析 … 等用途</p></li></ul><h2 id="Elasticache-Engine-Type"><a href="#Elasticache-Engine-Type" class="headerlink" title="Elasticache Engine Type"></a>Elasticache Engine Type</h2><p>Elasticache 提供兩種 Engine Type，分別是 <code>Redis</code> &amp; <code>Memcached</code>，以下是兩者的比較表：</p><table><thead><tr><th>需求</th><th>Memcached</th><th>Redis</th></tr></thead><tbody><tr><td>加速 DB 存取的快取功能</td><td>Yes</td><td>Yes</td></tr><tr><td>可水平擴展</td><td>Yes</td><td>Yes</td></tr><tr><td>多執行緒效能</td><td>Yes</td><td>No</td></tr><tr><td>可存放進階的資料結構</td><td>No</td><td>Yes</td></tr><tr><td>資料集合排序功能(Ranking/Sorting)</td><td>No</td><td>Yes</td></tr><tr><td>訂閱/通知(Pub/Sub)功能</td><td>No</td><td>Yes</td></tr><tr><td>可持久性(Persistence)</td><td>No</td><td>Yes</td></tr><tr><td>Backup &amp; Restore</td><td>No</td><td>Yes</td></tr></tbody></table><p>簡單來說：</p><ul><li><p>若是要簡單且立即可用的快取功能，那就選 Memcached</p></li><li><p>若是希望功能強大一點，那就選 Redis</p></li></ul><h2 id="應考重點"><a href="#應考重點" class="headerlink" title="應考重點"></a>應考重點</h2><ul><li><p>Elasicache 主要用來增加 DB &amp; web application 的效能</p></li><li><p>Redis 支援 Multi-AZ</p></li><li><p>Redis 可使用 Backup &amp; Restore 功能</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/rds/?nc1=h_ls">Amazon Relational Database Service (RDS) – AWS</a></p></li><li><p><a href="https://aws.amazon.com/dynamodb/">Amazon DynamoDB - Overview</a></p></li><li><p><a href="https://aws.amazon.com/tw/rds/aurora/">Amazon Aurora – 專為雲端建立的關聯式資料庫 – AWS</a></p></li><li><p><a href="https://aws.amazon.com/elasticache/?nc1=h_ls">Amazon ElastiCache- In-memory data store and cache</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> RDS </tag>
            
            <tag> Aurora </tag>
            
            <tag> DynamoDB </tag>
            
            <tag> Redshift </tag>
            
            <tag> Elasticache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - VPC(Virtual Private Cloud) Part 1</title>
      <link href="/blog/AWS/AWS-CSA-associate-VPC-part1/"/>
      <url>/blog/AWS/AWS-CSA-associate-VPC-part1/</url>
      
        <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="What-is-VPC"><a href="#What-is-VPC" class="headerlink" title="What is VPC?"></a>What is VPC?</h2><p>要了解一個產品，直接看官方的定義是最快的，因此來看一下 AWS 原廠如何介紹 VPC：</p><blockquote><p>Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 in your VPC for secure and easy access to resources and applications.</p></blockquote><blockquote><p>You can easily customize the network configuration of your Amazon VPC. For example, you can create a public-facing subnet for your web servers that have access to the internet. You can also place your backend systems, such as databases or application servers, in a private-facing subnet with no internet access. You can use multiple layers of security, including security groups and network access control lists, to help control access to Amazon EC2 instances in each subnet.</p></blockquote><p>根據上面的介紹，可以歸納出幾個重點：</p><ul><li><p>可以在 AWS region 中建立一個獨立 &amp; 可完全控制的網路環境(包含 IP、subnet、routing rules、Gateway … 等等)</p></li><li><p>在 VPC 中可以根據需求，同時定義 public subnet(通常提供 web service) &amp; private network(通常提供 backend service)</p></li><li><p>可透過 security group &amp; Network ACLs 來加強安全性</p></li><li><p>甚至可以將地端環境 &amp; AWS VPC 建立一個 VPN 連線，成為一個 Hybrid cloud 的架構</p></li></ul><h2 id="VPC-網路示意"><a href="#VPC-網路示意" class="headerlink" title="VPC 網路示意"></a>VPC 網路示意</h2><p><img src="/blog/images/aws/VPC_Example-2.png" alt="VPC Example 2"></p><p>以下是幾個 VPC 網路的特點：</p><ul><li><p>在 Region 中</p></li><li><p>可以跨 AZ (因為 AZ 是屬於 datacenter 等級，因此也表示跨 datacenter)</p></li><li><p>AWS 會在 VPC 中提供一個 DNS server，因此在 VPC 內部的 instance 都可以透過 hostname 互連</p><blockquote><p>可透過修改 VPC 的 DNS 選項來調整，也可以運行自己的 DNS server</p></blockquote></li></ul><h2 id="VPC-標準範例介紹"><a href="#VPC-標準範例介紹" class="headerlink" title="VPC 標準範例介紹"></a>VPC 標準範例介紹</h2><p>以下介紹一個標準的 VPC 架構的規劃：</p><p><img src="/blog/images/aws/VPC_Example-1.png" alt="VPC Example 1"></p><h3 id="General"><a href="#General" class="headerlink" title="General"></a>General</h3><ul><li><p>紅色外框的部份表示 AWS Region 的範圍，上圖的範例是在 <code>us-east-1</code></p></li><li><p>黃色外框的部份則是 VPC 範圍</p></li></ul><h3 id="Gateway-amp-Router-amp-Route-Table"><a href="#Gateway-amp-Router-amp-Route-Table" class="headerlink" title="Gateway &amp; Router &amp; Route Table"></a>Gateway &amp; Router &amp; Route Table</h3><ul><li><p>要存取 VPC 的方式，可透過 <code>Internet Gateway</code>(來自網際網路) 或是 <code>Virtual Private Gateway</code>(可能來自地端的 VPN 連線)</p></li><li><p>透過兩個 gateway 進來的 traffic 第一個位置都是 <code>Router</code></p></li><li><p>Router 會根據 traffic source 不同，並套用不同的 routing table rules</p></li><li><p>接著 traffic 會根據 routing rules 被導向不同的 Network ACL 進行流量的過濾</p></li></ul><h3 id="Network-ACL"><a href="#Network-ACL" class="headerlink" title="Network ACL"></a>Network ACL</h3><ul><li><p>Network ACL 是第一道防線(可以想像成 firewall)，用來根據需求決定哪些 traffic 允許(或是拒絕)進到內部的服務 or VM</p></li><li><p>Network ACL 是 stateless，因此 inbound/outbound rules 都必須正確的定義</p></li><li><p>可在 Network ACL 中設定黑名單，阻擋特定的 traffic 進到內部</p></li></ul><h3 id="Security-Group"><a href="#Security-Group" class="headerlink" title="Security Group"></a>Security Group</h3><ul><li><p>用來作為 Network ACL 之後，保護 EC2 instance 的第二道防線</p></li><li><p>Security Group 為 stateful，因此 inbound rule 中允許的規則，會自動開放對應的 outbound rule</p><blockquote><p>即使移除 <strong>allow outbound traffic</strong> 的規則，也都還是會通</p></blockquote></li></ul><h3 id="public-private-subnet"><a href="#public-private-subnet" class="headerlink" title="public/private subnet"></a>public/private subnet</h3><ul><li><p>外部進來的流量可以存取 public subnet 中的 Instance (因為有與 Internet Gateway 繫結)</p></li><li><p>private subnet 中的 Instance 是無法被外界存取，對外連線也是禁止的</p><blockquote><p>因為 private subnet 預設的 route table 僅有 local route</p></blockquote></li><li><p>private subnet 若是需要連到 internet，則必須仰賴 NAT gateway(or instance) 來達成</p></li><li><p>public subnet 中的 Instance 是唯一有能力存取 private subnet 中 Instance 的地方</p><blockquote><p>private subnet 通常是存放 DB、backend servivce … 等後端服務的地方</p></blockquote></li><li><p>若真的有需要存取 private subnet 中的服務，可在 public subnet 中放一台 bastion node(俗稱<strong>跳板機</strong>)來進行</p></li></ul><h3 id="IP-範圍的設定"><a href="#IP-範圍的設定" class="headerlink" title="IP 範圍的設定"></a>IP 範圍的設定</h3><p>目前 VPC 支援下面三個 IP 範圍可供使用者使用：</p><ul><li><p>10.0.0.0 ~ 10.255.255.255 (10.0.0.0/8)</p></li><li><p>172.16.0.0 ~ 172.32.255.255 (172.16.0.0/12)</p></li><li><p>192.168.0.0 ~ 192.168.255.255 (192.168.0.0/16)</p></li></ul><blockquote><p>若需要正確的計算 IP 範圍，可以使用 <a href="https://cidr.xyz/">CIDR.xyz</a> 網站。</p></blockquote><h3 id="補充示意圖"><a href="#補充示意圖" class="headerlink" title="補充示意圖"></a>補充示意圖</h3><p>了解上面的概念後，接著看下面這張 VPC 網路示意圖，就不會覺得很突兀了：</p><p><img src="/blog/images/aws/VPC_Example-3.png" alt="VPC Example 1"></p><h2 id="深入認識-VPC"><a href="#深入認識-VPC" class="headerlink" title="深入認識 VPC"></a>深入認識 VPC</h2><h3 id="使用者可以用-VPC-作到哪些事情"><a href="#使用者可以用-VPC-作到哪些事情" class="headerlink" title="使用者可以用 VPC 作到哪些事情?"></a>使用者可以用 VPC 作到哪些事情?</h3><ul><li><p>可以將 EC2 instance 放到你所指定的任何 subnet 中</p></li><li><p>可以為每個 subnet 指定想要的 IP 範圍(但僅限定於上面那三個 private IP 的範圍)</p></li><li><p>可以調整 subnet 之間互相通訊用的 routing rules (route table)</p></li><li><p>建立 Internet Gateway 並與 VPC 相連，提供 VPC 連網的能力</p></li><li><p>可以透過 VPN 將地端 &amp; 雲端的網路環境連接起來</p></li><li><p>對 AWS resource 提供更多安全相關的管理 &amp; 控制</p></li><li><p>設定 Security Group(for <strong>Instance</strong>) &amp; Network ACLs(for <strong>Subnet</strong>)</p></li></ul><h3 id="Default-VPC-的特性"><a href="#Default-VPC-的特性" class="headerlink" title="Default VPC 的特性"></a>Default VPC 的特性</h3><ul><li><p>default VPC 是很簡單好用的，可以讓使用者直接佈署 instance 在裡面 </p></li><li><p>在 default VPC 中，每個 subnet 都有一組可以連外的 routing 設定，因此所有 subnet 都預設具備連網的能力</p></li><li><p>每個 subnet 都會有一個 Internet Gateway 存在並與其相連，因此都是屬於 public subnet</p></li><li><p>每個 EC2 instance 都會同時有 public/private IP</p></li></ul><h3 id="Internet-Gateway"><a href="#Internet-Gateway" class="headerlink" title="Internet Gateway"></a>Internet Gateway</h3><ul><li><p>讓 VPC 中的 instance 與外面的 Internet 通訊的橋樑</p></li><li><p>有自動水平擴展、並帶有 redundant &amp; HA 特性，總之 AWS 會確保 Internet Gateway 穩定的運行</p></li><li><p>沒有對外頻寬的限制</p></li><li><p>會自動幫你的 instance 作 NAT，mapping 到一個 public ip address(From Elastic IP)</p></li><li><p><strong>每個 default VPC 都已經有連接一個 Internet Gateway</strong></p></li><li><p>每個 VPC 一次只能跟一個 Internet Gateway 繫結</p></li><li><p>若是 VPC 中已經有任何 AWS resource(例如：EC2/RDS instance)，就無法移除 Internet Gateway 的繫結</p></li><li><p>若是 VPC 中的 AWS resource 需要對外連網的能力，一定要有 Internet Gateway 才辦得到</p></li></ul><h3 id="Route-Table"><a href="#Route-Table" class="headerlink" title="Route Table"></a>Route Table</h3><ul><li><p>預設情況下，VPC 內部的 traffic 在不同的 subnet 之間是會通的，依靠的就是所謂的 <code>local route</code></p></li><li><p>local route 是預設就會存在，且無法修改的</p></li><li><p>VPC 中可以設定多組 route table，但已經與任何的 subnet 繫結就無法被刪除</p></li><li><p>若要調整 VPC 中的 routing 設定，建議作法是保留 default route table，然後新增其他的 route table 設定</p></li></ul><h3 id="VPC-Peering"><a href="#VPC-Peering" class="headerlink" title="VPC Peering"></a>VPC Peering</h3><ul><li><p>可以使用 private IP 將兩個 VPC 連接起來，使用上就像是在同一個 private network 中</p></li><li><p>VPC 的連結不限定要同一個帳號下的 VPC，也可以跟其他帳號的 VPC 對接</p></li><li><p>VPC peering 是可以跨 region 的</p></li><li><p>peering 是種星狀的連結設定，無法進行 <strong>Transitive Peering</strong></p><blockquote><p>A 與 B 相連，B 與 C 相連 =&gt; 但 A 無法與 C 通訊 (需要額外進行 A &amp; C 的相連設定)</p></blockquote></li></ul><h2 id="應考重點整理"><a href="#應考重點整理" class="headerlink" title="應考重點整理"></a>應考重點整理</h2><ul><li><p>可以將 VPC 視為在 AWS 中的一個 logical datacenter</p></li><li><p>VPC 中包含了 <code>IGW(Internet Gateway)</code>、<code>Virtual Private Gateway</code>、<code>Route Table</code>、<code>Network ACLs</code>、<code>Subnet</code>、<code>Security Group</code> 這幾個重要元素</p></li><li><p>1 subnet = 1 AZ</p><blockquote><p>subnet 是無法跨 AZ 的，不同的 AZ 一定是不同的 subnet</p></blockquote></li><li><p>security group 是 stateful；而 Network ACLs 則是 stateless</p></li><li><p>VPC 無法做 transitive peering，VPC 之間若有通訊的需求，都必須做好連結的設定</p></li></ul><h1 id="實作重點"><a href="#實作重點" class="headerlink" title="實作重點"></a>實作重點</h1><h2 id="了解-Default-VPC"><a href="#了解-Default-VPC" class="headerlink" title="了解 Default VPC"></a>了解 Default VPC</h2><ul><li><p>AWS 會為每個使用者在每一個 region 中建立一個預設的 VPC，並在每個 region 中的 AZ 都建立一個 subnet(不同 AZ，IP 範圍肯定不一樣)</p></li><li><p>同時也會有預設的 route table &amp; Internet Gateway 的設定，主要是為了提供連網的能力</p></li></ul><h2 id="建立-VPC"><a href="#建立-VPC" class="headerlink" title="建立 VPC"></a>建立 VPC</h2><ul><li><p><strong>Tenancy</strong>：若是希望 workload 可以跑在專屬的硬體上，可以選擇 <code>Dedicated</code>，但這要花不少錢；一般都會選擇 <code>Default</code>(共用)</p></li><li><p>自行建立的 VPC 時，不會自動建立 subnet &amp; IGW(Internet Gateway)，必須自己額外建立</p></li><li><p>但 AWS 會幫忙建立 default route table、security group &amp; Network ACLs … 等設定</p></li></ul><p>剛建立好的 VPC 會變成下面的架構：</p><p><img src="/blog/images/aws/VPC_Creation-1.png" alt="VPC arch 1"></p><h2 id="建立-Subnet"><a href="#建立-Subnet" class="headerlink" title="建立 Subnet"></a>建立 Subnet</h2><p>建立 subnet 需要設定以下資訊：</p><ul><li><p><strong>Name</strong>(option)：可用來作為識別用</p></li><li><p><strong>VPC</strong>：指定 subnet 要新增在哪個 VPC 中</p></li><li><p><strong>VPC CIDRs</strong>：此區域會顯示當初建立 VPC 時所指定的 IP range</p></li><li><p><strong>Availability Zone</strong>：指定 subnet 所在的 AZ；但需要注意的是，<strong>不同使用者看到的相同名稱 AZ，不一定是同一個 AZ</strong></p><blockquote><p>這是 AWS 為了確保資源設定可以平均分散在不同 AZ 的設計</p></blockquote></li></ul><p><strong>IPv4(IPv6) CIDR block</strong>：subnet 所要使用的 IP 範圍 (IPv6 可以不指定)</p><p>其他注意事項：</p><ul><li><p><code>Auto-assign public IP</code> 是預設關閉的</p></li><li><p>一個 class C 的 subnet 中只有 251 個 IP 可用 (AWS 保留了幾個 IP，詳細規劃可參考<a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html">官網文件</a>)</p></li></ul><p>當 subnet 建立完成後，VPC 架構圖就會變成如下：</p><p><img src="/blog/images/aws/VPC_Creation-2.png" alt="VPC arch 2"></p><h2 id="為-Subnet-提供連網能力"><a href="#為-Subnet-提供連網能力" class="headerlink" title="為 Subnet 提供連網能力"></a>為 Subnet 提供連網能力</h2><p>連網能力的設定需要從兩個方面下手，分別是：</p><ul><li><p>Internet Gateway</p></li><li><p>Route Table</p></li></ul><h3 id="Internet-Gateway-1"><a href="#Internet-Gateway-1" class="headerlink" title="Internet Gateway"></a>Internet Gateway</h3><ol><li><p>建立 Internet Gateway (只需要指定 name)</p></li><li><p>指定 VPC 並進行 attach</p><ul><li>一個 VPC 只可以 attach 到一個 Internet Gateway，但 AWS 會確保 Internet Gateway 的 HA</li><li>當有 resource 在 VPC 上運作時，是無法將 Internet Gateway 從 VPC 上 deattach</li></ul></li></ol><h3 id="Route-Table-1"><a href="#Route-Table-1" class="headerlink" title="Route Table"></a>Route Table</h3><blockquote><p>用來決定網路流量被導向何處</p></blockquote><ol><li><p>新增 route table，指定所綁定的 VPC</p></li><li><p>新增 <code>Routes</code> 設定，設定 <code>0.0.0.0/0</code>(IPv6) &amp; <code>::/0</code>(IPv6)，並將 Target 設定為 Internet Gateway</p></li><li><p>設定 <code>Subnet Associations</code>，將新的 route table 與 subnet 綁定，如此一來 subnet 就會具備連網能力了</p></li></ol><h3 id="注意事項"><a href="#注意事項" class="headerlink" title="注意事項"></a>注意事項</h3><ul><li><p>在 Subnet Association 中，subnet 不會主動與特定的 route table 關聯，但<strong>會自動與 main route table 關聯</strong></p></li><li><p>若是 main route table 中包含了連外的規則設定，那表示所有的 subnet 預設都會變成 public，會有安全性的疑慮</p></li><li><p>比較正確的作法是確保 main route table 僅處理內部流量</p></li></ul><p>當 Internet Gateway &amp; Route Table 設定好之後，VPC 架構就會變成以下的樣子：</p><p><img src="/blog/images/aws/VPC_Creation-3.png" alt="VPC arch 3"></p><h2 id="Public-Private-之間如何互連"><a href="#Public-Private-之間如何互連" class="headerlink" title="Public/Private 之間如何互連?"></a>Public/Private 之間如何互連?</h2><ul><li><p>從上面的架構圖可以看出，唯一可以管制 subnet 之間的流量方式，就剩下 <code>Security Group</code></p></li><li><p>透過在 private subnet 中設定 security group，允許特定協定 &amp; port 的流量可以進來</p></li><li><p>private subnet 連網能力可以透過 NAT gateway 來提供</p></li></ul><h2 id="應考重點"><a href="#應考重點" class="headerlink" title="應考重點"></a>應考重點</h2><ul><li><p>當建立 VPC 後，同時會建立預設的 Route Table, Network ACLs, Seciruty Group</p></li><li><p>但建立 VPC 後，AWS 不會在 VPC 中建立任何 subnet，也不會建立 Internet Gateway</p></li><li><p>在 console 中看到的 AZ(例如：<code>us-east-1a</code>)，跟別人看到的相同 AZ 名稱，實際上不一定是相同的</p></li><li><p>每個 subnet 會有 5 個 IP 保留給 AWS 管理使用</p></li><li><p>一個 VPC 只能有一個 Internet Gateway</p></li><li><p>Security Group 無法跨 VPC 套用</p></li></ul><h1 id="NAT-Network-Address-Translation"><a href="#NAT-Network-Address-Translation" class="headerlink" title="NAT(Network Address Translation)"></a>NAT(Network Address Translation)</h1><p>NAT 在這裡的目的很簡單，就是為了讓 private subnet 具備連網的能力，但又不想將其暴露出來，希望維持在 private 環境中。</p><p>因此在環境中(<strong>必須位於 public subnet 中</strong>)可以加入 <code>NAT gateway</code>，於是架構變成如下所示：</p><p><img src="/blog/images/aws/VPC_NAT-gateway.png" alt="VPC NAT gateway"></p><p>但其實你也可以自己建立一個 NAT 服務來提供連網能力，這樣的作法叫做 <code>NAT instance</code>，以下是兩個不同方式的簡單介紹。</p><h2 id="NAT-instance"><a href="#NAT-instance" class="headerlink" title="NAT instance"></a>NAT instance</h2><p>這是一個完全自己動手建立 NAT 的概念，在使用上有一些事項需要注意：</p><ul><li><p>必須建立一個 EC2 instance，且關閉 <code>Source &amp; Destination Check</code>，才可以轉發來自其他 instance 的封包</p></li><li><p>必須放在 public subnet 中，本身才會有連網能力</p></li><li><p>private subnet 中要設定 route 規則，將連外的流量導向 NAT instance</p></li><li><p>NAT instance 可以承載的流量取決於 instance 的大小，需要加大流量的話也需要把 instance scale out</p></li><li><p>若要達成 HA，需要搭配 ASG(Auto Scaling Group)，並設定多台 instance &amp; 橫跨不同的 AZ，並且還需要額外的 script 處理 failover 的需求</p><blockquote><p>總之很麻煩….你肯定不會想這麼做….</p></blockquote></li><li><p>必須額外設定 security group 規則</p></li></ul><h2 id="NAT-Gateway"><a href="#NAT-Gateway" class="headerlink" title="NAT Gateway"></a>NAT Gateway</h2><ul><li><p>在 AZ 中是有 redundant 的配置</p></li><li><p>對外頻寬可以從 5Gbps ~ 45Gbps</p></li><li><p>不需要更新，也不用跟任何的 security group 綁定</p></li><li><p>會自動被派發 public IP</p></li><li><p><strong>若要使用 NAT gateway，還是要設定 route table 將要對外的流量導向 NAT gateway (基本上 NAT Gateway 只會接收來自 private subnet 的流量)</strong></p></li><li><p>不需要做 Source/Destination Check</p></li><li><p>建議若是 resource 橫跨 AZ，就不要使用單一 AZ 的 NAT gateway，這樣就會造成 single point of faliure；比較好的作法是讓每個 AZ 都有一個 NAT gateway，而該 AZ 中的 resource 就使用該 AZ 中的 NAT gateway</p></li></ul><h1 id="Network-Security"><a href="#Network-Security" class="headerlink" title="Network Security"></a>Network Security</h1><h2 id="Network-ACLs"><a href="#Network-ACLs" class="headerlink" title="Network ACLs"></a>Network ACLs</h2><p><img src="/blog/images/aws/VPC_Creation-3.png" alt="VPC arch 3"></p><p>根據上圖，Network ACLs 在 VPC 中，是在 security group 之前可以進行流量過濾的手段，以下將一些使用上的特性進行整理：</p><ul><li><p>VPC 會有一個 default network ACL，允許所有 inbound &amp; outbound 的流量</p></li><li><p>自訂的 network ACL，預設是拒絕所有 inbound &amp; outbound 流量</p></li><li><p>每個 subnet 都必須與一個 network ACL 進行關聯(<strong>也只能一個</strong>)，若是沒有設定，就會自動與 default(main) network ACL 關聯</p></li><li><p>要進行黑名單的管制(例如：阻擋特定來源 ip 流量)，只能用 network ACL，無法使用 security group(只能開白名單)</p></li><li><p>可以將同一個 network ACL 與多個 subnet 關聯；但一個 subnet 只能與一個 network ACL 關聯</p></li><li><p>每個 network ACL 中的規則，每一條都會有 number，而這個 number 用來決定執行的順序(數字越小越優先執行)</p></li><li><p>Network ACL 中的 inbound rule &amp; outbound rule 是分開的，雙向都要明確設定正確才會達到想要的效果</p></li><li><p>Network ACL 是 stateless =&gt; 允許 inbound 的流量，並不會自動產生對應的 outbound rule</p></li></ul><blockquote><p>重點!! <strong>Network ACLs 是用來管理進出 subnet 的流量；而 Security Group 則是用來管理進出 instance 的流量</strong></p></blockquote><h2 id="Security-Group-1"><a href="#Security-Group-1" class="headerlink" title="Security Group"></a>Security Group</h2><ul><li><p>屬於 instance level 的設定</p></li><li><p>stateful，因此設定了 inbound allow 後，就會自帶 outbound allow</p></li><li><p>只能設定 allow rules</p></li></ul><h2 id="網路無法連外"><a href="#網路無法連外" class="headerlink" title="網路無法連外?"></a>網路無法連外?</h2><p>這部份很有可能是 Network ACL 中 outbound 的設定有缺少造成的，可按照以下方式設定：</p><p><img src="/blog/images/aws/VPC_NetworkACL-outbound-1.png" alt="VPC Network ACL outbound rule - 1"></p><ul><li>outbound 要允許 TCP port 1024~65535 的 traffic，否則 VM 會無法連網</li></ul><p><img src="/blog/images/aws/VPC_NetworkACL-outbound-1.png" alt="VPC Network ACL outbound rule - 2"></p><ul><li>也可以直接允許所有 outbound 的 TCP traffic </li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/tw/vpc/">Amazon Virtual Private Cloud (VPC)</a></p></li><li><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html">NAT - Amazon Virtual Private Cloud</a></p></li><li><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html">Network ACLs - Amazon Virtual Private Cloud</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> VPC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Terraform] Module 設計上的思考與原則</title>
      <link href="/blog/DevOps/terraform-develop-production-grade-modules/"/>
      <url>/blog/DevOps/terraform-develop-production-grade-modules/</url>
      
        <content type="html"><![CDATA[<h1 id="Module-設計上越小越好"><a href="#Module-設計上越小越好" class="headerlink" title="Module 設計上越小越好"></a>Module 設計上越小越好</h1><p>開發小的 module，而非大的，專注在單一且簡單的部份，大的 module 有不少缺點，列舉如下：</p><ul><li><p>module 太大，執行會很慢，會伴隨著很大的 state &amp; 檢查，執行 <code>terraform plan</code> 可能會花上個數分鐘都有可能</p></li><li><p>若是把整個 infra 都寫進同一個 module，任何要修改的開發者都給予權限，而這權限就可以存取所有資源，這表示每個人必須是 admin，這完全了違反最小權限原則</p></li><li><p>任何一個小修改都可能會毀了很大一部份的現存系統</p></li><li><p>module 不僅會讓人難以了解，很難協助做 review，且難以測試</p></li></ul><h1 id="設計可組合使用-Module"><a href="#設計可組合使用-Module" class="headerlink" title="設計可組合使用 Module"></a>設計可組合使用 Module</h1><p>為了設計出可組合搭配使用的 module，在設計時可以考慮使用類似一般程式語言的思考方式，將一個 module 視為一個有 input &amp; output 的 function，而 module 只是中間處理 input 資訊後，輸出 output 給使用者，甚至是其他的 module。</p><p>因此幾個設計重點如下：</p><ul><li><p>避免從外面讀取 state 資訊，而是改成由 input 取得 </p></li><li><p>避免將 state 資訊寫到外部，而是將需要回傳的資訊透過 output 傳出</p></li></ul><blockquote><p>上面兩個原則主要是為了避免 side effect，同時也讓 module 本身內容更為合理、更容易測試並重複使用</p></blockquote><ul><li>透過將 small module 組合的方式，來開發更為複雜的 module</li></ul><h1 id="設計可測試的-Module"><a href="#設計可測試的-Module" class="headerlink" title="設計可測試的 Module"></a>設計可測試的 Module</h1><p>Module 被開發出來，確保可用是很重要的，大概有幾個方向可以參考：</p><ul><li><p>為 module 撰寫相關的 RAEDME 說明，包含用途 &amp; 使用方法</p></li><li><p>開發使用 module 的範例程式，若是可以是一個可直接執行的 script 最好(讓使用者不用寫太多的程式就可以執行測試)，可以作為確認可執行的參考</p></li><li><p>為範例程式撰寫相關的 README 說明</p></li><li><p>開發測試程式，也可以是一個作為 module 使用範例的內容</p></li><li><p>如果 module 設計上有考慮到在各種不同的情境下的處理，那就可以多寫幾個範例來作為說明，以下圖為例：</p></li></ul><p><img src="/blog/images/devops/terraform_module-example-structure.png" alt="Terraform module example structure"></p><blockquote><p>同一個 <code>asg-rolling-deploy</code> module 就可以有各式不同的應用，可以搭配單一 EC2 instance，也可以加上 load balancer，或是自訂 tag</p></blockquote><ul><li><p>在開始開發 module 前，可以先試著寫 example，藉由範例思考 module 應該如何被使用，並嘗試描繪出 module 該有的 input &amp; output，嘗試從使用者的角度來思考 module 應該如何被設計，在開發 module 時的方向就會明確的多</p><blockquote><p>其實這樣的思考方式也是 TDD(Test-Driven Development) 的精神</p></blockquote></li><li><p>在 module 中要鎖定 terraform &amp; provider 的版本，因為 terraform 不同版本間並不相容，以下是範例：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">terraform &#123;</span><br><span class="line">  required_version &#x3D; &quot;&gt;&#x3D; 0.12, &lt; 0.13&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">provider &quot;aws&quot; &#123;</span><br><span class="line">  region &#x3D; &quot;us-east-2&quot;</span><br><span class="line"></span><br><span class="line">  # AWS provider 2.0 以上的版本皆可</span><br><span class="line">  version &#x3D; &quot;~&gt; 2.0&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>若是針對 production 環境，可以考慮鎖定更細的版本號</p></blockquote><h1 id="Release-Modules"><a href="#Release-Modules" class="headerlink" title="Release Modules"></a>Release Modules</h1><p>當 module 開發 &amp; 測試到一個穩定階段，準備要釋出時，有幾項重點原則可以參考：</p><ul><li><p>為每一個穩定的版本加上 tag，搭配 git，使用者就可以指定要使用哪個版本的 module</p></li><li><p>有落實版本控管的 module，即使版本更新發生問題了，也可以很快的恢復到上一個版本</p></li><li><p>若是泛用性很高的 module，也可以考慮發布到 <a href="https://registry.terraform.io/">Terraform Registry</a> 開源給大家使用，目前 Terraform Registry 已經有很多現成的 module 可用了</p></li><li><p>要在 Terraform Registry 開源自己所開發的 module，其實是有不少規範要遵循的，例如：命名規格、檔案結構、版本控管 …. 等等</p></li><li><p>若要直接使用 Terraform Registry 上的 module，不一定要完整指定 Git Repo ＆ version 來達成，可以類似以下比較簡單的宣告方式：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 使用範本</span><br><span class="line">module &quot;&lt;Name&gt;&quot; &#123;</span><br><span class="line">    source  &#x3D; &quot;&lt;OWNER&gt;&#x2F;&lt;REPO&gt;&#x2F;&lt;PROVIDER&gt;&quot;</span><br><span class="line">    version &#x3D; &quot;&lt;VERSION&gt;&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 實際使用範例</span><br><span class="line">module &quot;vault&quot; &#123;</span><br><span class="line">    source  &#x3D; &quot;hashicorp&#x2F;vault&#x2F;aws&quot;</span><br><span class="line">    version &#x3D; &quot;0.12.2&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://www.terraformupandrunning.com/">Terraform: Up and Running</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Terraform </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Terraform </tag>
            
            <tag> IaC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Terraform] 使用上的小技巧整理</title>
      <link href="/blog/DevOps/terraform-tips-and-tricks/"/>
      <url>/blog/DevOps/terraform-tips-and-tricks/</url>
      
        <content type="html"><![CDATA[<h1 id="規劃-Provider-Plugin-Cache-存放位置"><a href="#規劃-Provider-Plugin-Cache-存放位置" class="headerlink" title="規劃 Provider Plugin Cache 存放位置"></a>規劃 Provider Plugin Cache 存放位置</h1><p>當執行 <code>terraform init</code> 時，terraform 會下載 provider plugin 並放在目前的目錄中，但如果專案中有相當多個目錄，就表示這樣的下載行為會被執行很多次，且會浪費不少空間。</p><p>因此為了解決這個問題，可以搭配 <a href="https://www.terraform.io/docs/commands/cli-config.html">CLI configuragion file</a> 一起使用，只要在 <code>$HOME</code> 中放入 <code>.terraformrc</code> 檔案(only for non-Windows OS)，內容如下：(<strong>指定目錄必須先建立好</strong>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugin_cache_dir &#x3D; &quot;$HOME&#x2F;.terraform.d&#x2F;plugin-cache&quot;</span><br></pre></td></tr></table></figure><p>這樣每次進行 <code>terraform init</code> 的時候，plugin cache 就會被放到同樣的地方囉!</p><blockquote><p>若是設定環境變數 <code>TF_PLUGIN_CACHE_DIR</code> 也是可以有相同效果</p></blockquote><blockquote><p>另外比較需要注意的是，plugin cache 目錄不能與 third party plugin 目錄放在相同地方，會有問題，目前 terraform 還無法處理這樣的規劃安排</p></blockquote><h1 id="Terraform-的使用限制"><a href="#Terraform-的使用限制" class="headerlink" title="Terraform 的使用限制"></a>Terraform 的使用限制</h1><h2 id="count-amp-for-each-無法與-resource-output-搭配使用"><a href="#count-amp-for-each-無法與-resource-output-搭配使用" class="headerlink" title="count &amp; for_each 無法與 resource output 搭配使用"></a>count &amp; for_each 無法與 resource output 搭配使用</h2><p>原因是因為 terraform 在 plan 階段就必須預測 infra 建立後的樣子，但 resource output 卻必須等待 resource 產生後才可以取得，因此無法在 plan 階段預測結果，因此會失敗。</p><h2 id="count-amp-for-each-無法用在-module-設定中"><a href="#count-amp-for-each-無法用在-module-設定中" class="headerlink" title="count &amp; for_each 無法用在 module 設定中"></a>count &amp; for_each 無法用在 module 設定中</h2><p>以下的設定會失敗：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">module &quot;webserver_cluster&quot; &#123;</span><br><span class="line">  source &#x3D; &quot;..&#x2F;..&#x2F;..&#x2F;..&#x2F;modules&#x2F;services&#x2F;webserver-cluster&quot;</span><br><span class="line"></span><br><span class="line">  count &#x3D; 3</span><br><span class="line"></span><br><span class="line">  ....</span><br><span class="line">  ....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Zero-Downtime-Deployment-有其限制"><a href="#Zero-Downtime-Deployment-有其限制" class="headerlink" title="Zero Downtime Deployment 有其限制"></a>Zero Downtime Deployment 有其限制</h1><p>使用 <strong>ASG(Auto Scaling Group)</strong> 搭配 <code>create_before_destroy</code> policy 雖然可以達到 zero-download 的佈署，但並不會考慮到 auto scaling policy 的部份，因為 ASG 被產生後，只會使用 min_size 中所設定的值。</p><p>有兩個 workaround 可以解這個問題：</p><ol><li><p>將 <strong>aws_autoscaling_schedule</strong> 從 <code>0 9 * * *</code> 改成 <code>0-59 9-17 * * *</code>，若目前環境已經達到 schedule 的要求則不會變更，若是沒有，則一分鐘內就會套用規則</p></li><li><p>開發 script，使用 <a href="https://www.terraform.io/docs/providers/external/data_source.html">External Data Source</a> 的機制，搭配 <code>desired_capacity</code> 參數，也可以達成相同效果；但這樣會造成 terraform code 可攜性降低，也會變得較難維護</p></li></ol><h1 id="看起來可執行的-plan-還是有可能會壞掉"><a href="#看起來可執行的-plan-還是有可能會壞掉" class="headerlink" title="看起來可執行的 plan 還是有可能會壞掉"></a>看起來可執行的 plan 還是有可能會壞掉</h1><p>這通常是因為除了 terraform 之外，可能還使用了 CLI 之類的工具進行了資源上的管理，因此在佈署資源時產生了衝突。</p><p>而這樣的問題有兩種解法：</p><ol><li><p>全部用 terraform</p><blockquote><p>讓 terraform state 可以完全符合現況</p></blockquote></li><li><p>若已經有現存的 infra 存在，則使用 import 命令產生 state</p></li></ol><h1 id="重構-Refactoring-要很小心"><a href="#重構-Refactoring-要很小心" class="headerlink" title="重構(Refactoring) 要很小心"></a>重構(Refactoring) 要很小心</h1><p>重構在一般程式語言的開發中是很常見的，但在 terraform 中可不是這樣子，在有些情況下，terraform 會出現非預期的結果，例如：</p><ul><li><p>修改某個 resource name</p></li><li><p>修改 resource identifier</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 將 instance 改為 cluster_instance</span><br><span class="line">resource &quot;aws_security_group&quot; &quot;instance&quot; &#123;.....&#125;</span><br></pre></td></tr></table></figure><p>然後 terraform 就會把原本的 resource 移除，重新建立一個新的，但這並非我們所想要的結果。</p><p>因此可用以下方式避免這個問題的發生：</p><ul><li><p>先用 <code>terraform plan</code> 確認將要發生的事情，確保沒有任何 resource 會被不小心移除</p><blockquote><p>特別是不小心改到無法變動的 resource parameter 時，就一定會造成 resource 被移除再重建，請務必小心</p></blockquote></li><li><p>若是真的要進行 resource replace 的動作，可透過 <code>create_before_destroy</code> 先產生可用的 resource，再移除不需要的</p></li><li><p>調整 identifier 也同時需要調整 terraform state</p><blockquote><p>建議不要手動修改 terraform state 內容，可搭配 <code>terraform state mv &lt;ORIGINAL_REF&gt; &lt;NEW_REG&gt;</code> 的方式進行調整</p></blockquote></li></ul><h1 id="Eventual-Consistency"><a href="#Eventual-Consistency" class="headerlink" title="Eventual Consistency"></a>Eventual Consistency</h1><p>terraform 與 cloud provider API 是以非同步的方式進行通訊，因此即使 API 立即回傳了訊息，這訊息也不見得可以馬上拿來使用。</p><p>舉例來說，當建立一個 AWS EC2 instance，API 很快的回傳 <strong>201 Created</strong>，但實際上要馬上存取到相關資訊可能還是不行的，可能是以下原因造成的：</p><ul><li><p>實際上 EC2 instance 並未完全建立成功</p></li><li><p>相關的資訊還沒有完全在整個 AWS region 中同步</p></li></ul><p>因此，若是在 CI/CD 的自動化流程中，有透過 terraform 產生 resource，並立即要拿來使用的情境，中間就必須設定一些等待時間，或是持續的監測是否能取到所需要的資訊，才能確保自動化流程才可以每一次都順利完成。</p>]]></content>
      
      
      <categories>
          
          <category> Terraform </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Terraform </tag>
            
            <tag> IaC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - S3(Simple Storage Service) Part 3</title>
      <link href="/blog/AWS/AWS-CSA-associate-S3-part3/"/>
      <url>/blog/AWS/AWS-CSA-associate-S3-part3/</url>
      
        <content type="html"><![CDATA[<h1 id="Transfer-Acceleration"><a href="#Transfer-Acceleration" class="headerlink" title="Transfer Acceleration"></a>Transfer Acceleration</h1><p>S3 transfer acceleration 是利用 AWS 佈署在全世界的 edge server 來達到上傳加速的功能，有以下幾點特性：</p><ul><li><p>使用者上傳檔案的 endpoint 會從 region 變成 edge server，因此必須透過另一個不同的 endpoint 上傳檔案</p></li><li><p>檔案傳到 edge server 後，就是透過 AWS 的骨幹網路回到 s3 bucket 所在的 region 內</p></li><li><p>啟用後，最多需要 20 mins 的生效時間</p></li><li><p>啟動此功能後，AWS 會產生一個 <code>bucketname.s3-accelerate.amazonaws.com</code> 的網域名稱，透過此網域名稱，使用者就可以將檔案上傳到離自己最近的 edge server</p><blockquote><p>為什麼一組域名就可以? AWS 會自行根據使用者查詢 DNS 的來源，回應一個最靠近使用者的 edge server，這是每個 CDN provider 有的功能</p></blockquote></li></ul><p><img src="/blog/images/aws/S3_Transfer-Acceleration.png" alt="S3 Transfer Acceleration"></p><p>此外，AWS 也提供了一個<a href="https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html">測速的網頁</a>，使用者可以自行評估須不需要開啟 S3 Transfer Acceleration 功能。</p><h1 id="CloudFront"><a href="#CloudFront" class="headerlink" title="CloudFront"></a>CloudFront</h1><p>CloudFront 就是 AWS 透過佈建在全世界各地的 edge server 所提供的 CDN 服務，有以下幾個重點知識需要了解：</p><ul><li><p>AWS 在全世界各地建立了 edge server，確保大多數的使用者都有一個相對源頭教近的地方可以取得資料</p></li><li><p><strong>Edge Location</strong>：實際就是快取靜態資源的地方(edge server)，跟 Region &amp; AZ 兩個是不同的概念</p></li><li><p><strong>Origin</strong>：這就是靜態資源的原始儲存位置，當使用者對 edge server 要求檔案，但 edge server 沒有時，edge server 就會回源到此處取得所需的檔案</p><blockquote><p>Edge server 會將回源取到的資料進行 cache，下一位使用者來存取時，就不需要再度回源了<br><img src="/blog/images/aws/S3_CloudFront.png" alt="S3 CloudFront"></p></blockquote></li><li><p><strong>Distribution</strong>：這其實就是 CDN 服務本身所提供的 endpoint，搭配其智慧化的 DNS 解析能力，可以讓使用者取得離自己最近的 edge server 紀行存取，因此可以視為 edge location collection；而 AWS CloudFront 目前支援兩種資源的發佈</p><ul><li><p><strong>Web Distribution</strong>: 通常是一般網站使用的靜態資源，像是 image, css, javascript, music … 都屬於此類；也包含透過 HTTP 協議傳輸的視訊協定，例如：HTTP Live Streaming(HLS), Dynamic Adaptive Streaming over HTTP (DASH), Microsoft Smooth Streaming (MSS), or HTTP Dynamic Streaming (HDS) …. 等等。</p></li><li><p><strong>RTMP</strong>: 用於視訊串流類的服務中(但這通常會根據服務型態不同，還需要額外考慮視訊延遲的問題)</p><blockquote><p>CloudFront 將會在 2020 年底停止 RTMP 的支援</p></blockquote></li></ul></li><li><p>Edge Locations 並非只是用來拉取資料，也會有寫入資料的應用場景(例如：<strong>S3 Transger Acceleration</strong>)</p></li><li><p>快取在 edge location 的靜態資源，會有 TTL(Time to Live) 的屬性，不會永久存在</p></li><li><p>為了讓使用者可以很快取得最新的資料，可以執行 CDN 清理的工作或是讓快取失效(invalidate)，但因為需要重新回源，因此會有額外費用產生</p></li></ul><h2 id="進階設定"><a href="#進階設定" class="headerlink" title="進階設定"></a>進階設定</h2><ul><li><code>Restrict Viewer Access(Use Signed URLs or Signed Cookies)</code>：開啟此功能，可以限定使用者必須要登入後取得特定域名的 cookie 才可以存取 CDN 上的快取資源</li></ul><h1 id="Snowball-系列服務"><a href="#Snowball-系列服務" class="headerlink" title="Snowball 系列服務"></a>Snowball 系列服務</h1><p>Snowball 是設計用來協助使用者移動大量資料用，不論是從本地端到 AWS，或是從 AWS 回到本地端，都可以利用此服務。</p><h2 id="Snowball-Edge"><a href="#Snowball-Edge" class="headerlink" title="Snowball Edge"></a>Snowball Edge</h2><ul><li><p>Snowball 是個安全且兼顧的硬體裝置，被設計用來移動大量資料，不論是從本地端到 AWS，或是 AWS 到本地端都可；簡單來說，就是被設計用來做資料遷移和邊緣運算裝置，可簡單稱為 <code>Snowball Edge</code></p></li><li><p>為了加強資料移動的安全性，除了有堅固的外殼外，Snowball 還帶有加密功能 &amp; 內建 TPM；並且還設計了資料抹除的功能，確保資料移動完畢後可以完全清除</p></li><li><p>與透過現有網際網路來傳輸資料相比，透過 Snowball 有便宜、安全 … 等優點</p></li><li><p>Snowball Edge 可以分為 <code>Snowball Edge Storage Optimized</code> &amp; <code>Snowball Edge Compute Optimized</code> 兩類，可以讓使用者在移動大量資料時，也可以根據需求進行一定程度的運算工作(例如：EC2 &amp; Lambda function)</p></li><li><p>多台 Snowball 裝置還可以組成 cluster，變成一個臨時的大型設備</p></li></ul><h2 id="Snowmobile"><a href="#Snowmobile" class="headerlink" title="Snowmobile"></a>Snowmobile</h2><div class="video-container"><iframe src="https://www.youtube.com/embed/8vQmTZTq7nw" frameborder="0" loading="lazy" allowfullscreen></iframe></div><ul><li><p>Snowmobile 本身是個 數百 PB ~ EB 等級的資料移動服務，這樣量級的資料量，需要拖車來運送了….</p></li><li><p>Snowmobile 採用了多層安全保護，包含專門安全人員、GPS 追蹤、警示監控、24 小時全年無休的視訊監視，以及運輸期間選擇性的安全護衛車隊(帶保鏢的意思….)</p></li></ul><h1 id="Storage-Gateway"><a href="#Storage-Gateway" class="headerlink" title="Storage Gateway"></a>Storage Gateway</h1><h2 id="什麼是-Storage-Gateway"><a href="#什麼是-Storage-Gateway" class="headerlink" title="什麼是 Storage Gateway ?"></a>什麼是 Storage Gateway ?</h2><p>AWS Storage Gateway 是一種混合雲端儲存服務，有以下兩個特性：</p><ul><li><p>是一個連結地端(on-premise) &amp; 雲端(AWS)的工具，可以是一個 software appliance(以 VM 的形式存在，目前支援 VMware ESXi &amp; Microsoft Hyper-V)，也可以是一台硬體</p></li><li><p>提供非常簡易且安全的方式，協助使用者將資料移到 AWS 儲存服務(S3, EBS … etc)中</p></li></ul><p>其實最終的目的就是為了讓使用者可以大幅簡化資料移到 AWS 進行儲存的工作。</p><h2 id="Storage-Gateway-如何處理不同類型的資料"><a href="#Storage-Gateway-如何處理不同類型的資料" class="headerlink" title="Storage Gateway 如何處理不同類型的資料 ?"></a>Storage Gateway 如何處理不同類型的資料 ?</h2><p>使用者購買 storage 進行資料的儲存，雖然同樣是存放資料，但跟資料的使用方式、用途、情境，其實還可以細分為以下三類：</p><ul><li><p><code>一般檔案</code>：此類型檔案通常會以 <strong>NFS</strong> or <strong>SMB</strong> 的形式提供存取</p></li><li><p><code>Volume</code>：此類檔案則是由 block storage 來提供，通常會以 <strong>iSCSI</strong> 的方式提供存取，使用上就像一個硬碟一樣</p></li><li><p><code>Tape</code>：這類的檔案通常都是因為法令關係需要將資料封存一段時間而存在於磁帶中</p></li></ul><p>因此，為了處理上述的不同的資料類型，因此 AWS 也提供了三種不同的 Storage Gateway 來處理這些資料，分別是 <code>File Gateway</code>、<code>Volume Gateway</code>、<code>Tape Gateway</code>。</p><h3 id="File-Gateway"><a href="#File-Gateway" class="headerlink" title="File Gateway"></a>File Gateway</h3><p><img src="/blog/images/aws/S3_StorageGateway-FileGateway.png" alt="Storage Gateway - File Gateway"></p><ul><li><p>會將檔案以 object 的形式存放在 S3 bucket 中</p></li><li><p>本地端可透過 NFS 的方式進行儲存</p></li><li><p>與檔案相關的 ownership, permission, timestamp 等資訊，則會存放在 S3 object user-metadata 中</p></li><li><p>一旦將資料移動到 S3 後，就可以使用 S3 提供的 versioning, lifecycle management, cross-region replication … 等功能</p></li></ul><h3 id="Volume-Gateway"><a href="#Volume-Gateway" class="headerlink" title="Volume Gateway"></a>Volume Gateway</h3><p>Volume Gateway 提供了 <code>iSCSI</code> 的方式，讓資料可透過**單一磁碟(volume)**為單位的角度來進行資料的儲存與管理；寫入 volume 的資料會以非同步的方式透過 volume snapshot 的方式進行儲存，也因為是 block device 的關係，因此每次的變更備份都是只有處理變更的 block 部份而已。</p><p>而根據 volume 使用情境的不同，Volume Gateway 還可以分成以下兩種：</p><h4 id="Stored-Volume"><a href="#Stored-Volume" class="headerlink" title="Stored Volume"></a>Stored Volume</h4><p><img src="/blog/images/aws/S3_StorageGateway-VolumeGateway-StoredVolume.png" alt="Storage Gateway - Stored Volume"></p><ul><li><p>資料主要儲存在本地端(對本地端的應用程式相對延遲低)，非同步備份到 AWS</p></li><li><p>實際寫進 volume 的資料會先存在於本地端的儲存設備中</p></li><li><p>資料備份會以非同步的方式，並以 EBS(Elastic Block Volume) snapshot 的形式存放於 S3 中 </p></li><li><p>目前支援的 volume 大小範圍為 1GB ~ 16 TB</p></li></ul><h4 id="Cached-Volume"><a href="#Cached-Volume" class="headerlink" title="Cached Volume"></a>Cached Volume</h4><p><img src="/blog/images/aws/S3_StorageGateway-VolumeGateway-CachedVolume.png" alt="Storage Gateway - Cached Volume"></p><ul><li><p>資料其實是存放在 S3 中</p></li><li><p>但為了讓本地端使用上可以更為快速，會將常存取的資料 cache 在本地端</p><blockquote><p>透過此方式，可以讓地端設備不需要購置大容量的儲存設備，因為資料其實大部份都是存放在 S3，本地端只存放常用的一小部份資料</p></blockquote></li><li><p>支援的 volume 大小範圍為 1GB ~ 32TB</p></li></ul><h3 id="Tape-Gateway"><a href="#Tape-Gateway" class="headerlink" title="Tape Gateway"></a>Tape Gateway</h3><ul><li><p>用來解決磁帶備份 &amp; 保存的問題</p></li><li><p>以 <code>iSCSI</code> device 的形式提供給使用者進行資料存放</p></li><li><p>需要與現有的磁帶備份軟體(例如：NetBackup, Backup Exec, Veeam … 等)搭配使用</p></li><li><p>會將資料非同步的回傳至 S3，並搭配 Glacier 將成本降低</p></li></ul><h1 id="Athena-vs-Macie"><a href="#Athena-vs-Macie" class="headerlink" title="Athena vs Macie"></a>Athena vs Macie</h1><p>這兩個服務提供了使用者直接對儲存在 S3 的資料進行查詢 &amp; 分析的能力。</p><h2 id="Athena"><a href="#Athena" class="headerlink" title="Athena"></a>Athena</h2><ul><li><p>Athena 是種互動是的查詢服務，以 serverless 的方式運行</p></li><li><p>可透過標準的 SQL 對 S3 中的資料進行查詢，不需要設置複雜的 ETL(Extract/Transform/Load) 程序</p></li><li><p>實際查詢行為發生時才需要付費</p></li><li><p>通常用來查詢 Log(例如：ELB log, S3 access log)、產生報表</p></li></ul><h2 id="Macie"><a href="#Macie" class="headerlink" title="Macie"></a>Macie</h2><ul><li><p>同樣也是針對儲存在 S3 的資料進行分析查詢(也可以分析 CloudTrail logs)</p></li><li><p>透過機器學習 &amp; 自然語言處理(NLP, Natural Language Processing)，自動探索、分類和保護 AWS 中的敏感資料</p></li><li><p>可辨識個人識別資訊(PII, Personal Identification Information) 或智慧財產等敏感資料</p></li><li><p>很適合與 PCI-DSS 標準進行整合，並用來防止 ID 被盜取的問題</p></li><li><p>也可用來分析 CloudTrail log 來發現可疑的 API 存取行為</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></li></ul><h2 id="S3"><a href="#S3" class="headerlink" title="S3"></a>S3</h2><ul><li><p><a href="https://docs.aws.amazon.com/zh_tw/AmazonS3/latest/dev/transfer-acceleration.html">Amazon S3 Transfer Acceleration - Amazon Simple Storage Service</a></p></li><li><p><a href="https://aws.amazon.com/s3/faqs/">Amazon Simple Storage Service (S3) FAQs</a> <a href="https://aws.amazon.com/tw/s3/faqs/">(中文)</a></p></li></ul><h2 id="CloudFront-1"><a href="#CloudFront-1" class="headerlink" title="CloudFront"></a>CloudFront</h2><ul><li><p><a href="https://docs.aws.amazon.com/zh_tw/AmazonS3/latest/dev/website-hosting-cloudfront-walkthrough.html">以 Amazon CloudFront 加速您的網站 - Amazon Simple Storage Service</a></p></li><li><p><a href="https://aws.amazon.com/cloudfront/faqs/?nc=sn&loc=6">FAQs | CDN, Zone Apex, Edge Cache | Amazon CloudFront</a></p></li></ul><h2 id="Snowball"><a href="#Snowball" class="headerlink" title="Snowball"></a>Snowball</h2><ul><li><p><a href="https://aws.amazon.com/tw/snowball/">AWS Snowball | 安全邊緣運算和離線資料傳輸 | Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/tw/snowball/faqs/">AWS Snowball 常見問答集</a></p></li></ul><h2 id="Snowmobile-1"><a href="#Snowmobile-1" class="headerlink" title="Snowmobile"></a>Snowmobile</h2><ul><li><p><a href="https://aws.amazon.com/tw/snowmobile/">AWS Snowmobile | EB 級資料傳輸 | Amazon Web Serivces</a></p></li><li><p><a href="https://aws.amazon.com/tw/snowmobile/faqs/">https://aws.amazon.com/tw/snowmobile/faqs/</a></p></li></ul><h2 id="Storage-Gateway-1"><a href="#Storage-Gateway-1" class="headerlink" title="Storage Gateway"></a>Storage Gateway</h2><ul><li><p><a href="https://aws.amazon.com/tw/storagegateway">AWS Storage Gateway - Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/tw/storagegateway/volume/">AWS Storage Gateway – 磁碟區閘道</a></p></li></ul><h2 id="Athena-amp-Macie"><a href="#Athena-amp-Macie" class="headerlink" title="Athena &amp; Macie"></a>Athena &amp; Macie</h2><ul><li><p><a href="https://aws.amazon.com/tw/athena/">Amazon Athena - 無伺服器互動式查詢服務 - Amazon Web Services</a></p></li><li><p><a href="https://aws.amazon.com/tw/macie/">Amazon Macie | 探索、分類和保護敏感資料 | Amazon Web Services (AWS)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> S3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - S3(Simple Storage Service) Part 2</title>
      <link href="/blog/AWS/AWS-CSA-associate-S3-part2/"/>
      <url>/blog/AWS/AWS-CSA-associate-S3-part2/</url>
      
        <content type="html"><![CDATA[<h1 id="安全與加密"><a href="#安全與加密" class="headerlink" title="安全與加密"></a>安全與加密</h1><h2 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h2><ul><li><p>預設所有的 Bucket 建立時，都是 <strong>PRIVATE</strong> 無法被公開存取的</p></li><li><p>可以開啟將所有對 S3 Bucket 存取的行為全部 log 下來的功能，而這些存取的 log 可以存到另一個 S3 Bucket(可以是同一個帳號 or 不同帳號)  </p></li><li><p>要管理 S3 bucket 的安全，可透過 <code>IAM Policy</code>、<code>S3 Bucket Policy</code>、<code>S3 ACL</code> 三種方式來達成</p></li><li><p>只要上面三個管控機制中，有任何一個設定了 deny，就無法存取；若是什麼也沒設定，也同樣無法存取</p><blockquote><p>只有明確的有允許存取的定義，且沒有任何 deny 的設定，才可以正確的存取指定的 S3 resource<br><img src="/blog/images/aws/S3_Access-Authorization-Process.png" alt="S3 resource access authorization process"></p></blockquote></li></ul><h3 id="IAM-Policy"><a href="#IAM-Policy" class="headerlink" title="IAM Policy"></a>IAM Policy</h3><ul><li><p>IAM policy 用來<strong>指定哪些操作(PUT/DELETE/UPDATE…etc)可以被使用在哪些 AWS resource 上</strong></p></li><li><p>IAM policy 可以與 IAM user/group/role 綁定，而這些被綁定的對象，則稱為 <code>principal</code></p></li></ul><blockquote><p>簡單來說，<strong>IAM policy 就是用來定義 principal 可以在你的 AWS 環境上做哪些事情</strong></p></blockquote><ul><li><p>可以沿用管控其他 AWS resource 的經驗同樣的套用到 S3 bucket 的管理上</p></li><li><p>從稽核(audit)的角度，希望可以回答<strong>特定使用者可以在 AWS 環境中做什麼事情</strong>這類的問題，適合使用 IAM policy</p></li></ul><h3 id="S3-Bucket-Policy"><a href="#S3-Bucket-Policy" class="headerlink" title="S3 Bucket Policy"></a>S3 Bucket Policy</h3><ul><li><p>顧名思義，S3 Bucket Policy 就是用來設計只與 S3 bucket 綁定用的(為了更細膩的安全管控需求)，bucket 中的所有 object 都會套用相同的 policy 設定</p></li><li><p>與 IAM 相同，<strong>S3 Bucket Policy 就是用來定義 principal 可以對你的 S3 bucket 做哪些事情</strong></p></li><li><p>這裡的 principal 指的是 user, account, service 或是其他 entity (<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/s3-bucket-user-policy-specifying-principal-intro.html">參考文件</a>)</p></li><li><p>S3 Bucket Policy 中同時包含了權限範圍 &amp; principal 的資訊</p><blockquote><p>IAM policy 中沒有 principal 的設定，因為在 IAM 中，principle 是以一種 entity 的形式與 IAM policy 綁定後來發揮效用 </p></blockquote></li><li><p>principal 以外的特殊情況限定，則是透過 <code>Condition</code> 欄位進行設定(例如：限制來源 IP、限制 HTTP referer)</p></li><li><p>適合想要簡單管控 cross account 存取的場景</p></li><li><p>從稽核(audit)的角度，希望可以回答<strong>誰可以存取特定的 S3 bucket</strong>這類的問題，適合使用 S3 Bucket Policy</p></li></ul><h3 id="S3-ACL"><a href="#S3-ACL" class="headerlink" title="S3 ACL"></a>S3 ACL</h3><ul><li><p>S3 ACL 是屬於傳統的安全管控機制，AWS 官方是建議盡量使用 IAM or S3 Bucket Policy</p></li><li><p>S3 ACL 的規則可以細到 object level，但同樣也可以設定為 bucket level</p></li><li><p>可管控哪些 AWS 帳號可以存取指定的 S3 resource，或是直接對 public 開放</p></li><li><p>object ACL 可以讓使用者產生提供所有人存取 object 用的 public URL link</p></li></ul><h2 id="Encryption"><a href="#Encryption" class="headerlink" title="Encryption"></a>Encryption</h2><p>AWS S3 可以為每個 object(最細可以到 object 為單位) 進行資料的加密，這對於非常注重資料安全性的使用者是個很有用的功能；一旦加密功能開啟後，資料被加密，即使有人進入了 AWS 資料中心將帶有資料的硬碟取走，也沒辦法取得正確的檔案內容。</p><p>加密的方式有兩種：</p><h3 id="在傳輸中進行加密"><a href="#在傳輸中進行加密" class="headerlink" title="在傳輸中進行加密"></a>在傳輸中進行加密</h3><p>這也就是標準的 HTTPS，透過 SSL/TLS 來完成，基本上 S3 本身就是透過 HTTPS 存取，這個部份已經是早就存在的加密方式了。</p><h3 id="在終端進行加密"><a href="#在終端進行加密" class="headerlink" title="在終端進行加密"></a>在終端進行加密</h3><p>終端可分為兩個部份，分別是 <code>server side</code> &amp; <code>client side</code>：</p><ul><li><p>**Server Side Encryption(SSE)**：加密金鑰會由 AWS 管理，而加密金鑰可透過以下幾種方式取得：</p><ul><li>由 S3 管理的 key (<code>SSE-S3</code>)</li><li>透過 AWS Key Management Service 來管理金鑰 (<code>SSE-KMS</code>)</li><li>Server Side Encryption with Customer Provided Keys (<code>SSE-C</code>)</li></ul></li><li><p><strong>Client Side Encryption</strong>：</p><ul><li>由使用者自行加密後，再將加密後的資料傳到 S3 上</li><li>在檔案下載的場景下，使用者也還是必須自行解密</li></ul></li></ul><h1 id="版本控制-Version-Control"><a href="#版本控制-Version-Control" class="headerlink" title="版本控制(Version Control)"></a>版本控制(Version Control)</h1><p>版本控制(version control)同樣也是 S3 增加資料安全性的一種方式，而 S3 版本控制有以下幾個特性需要了解：</p><ul><li><p>object 的所有版本都被保留，<strong>包含所有的寫入資訊甚至是刪除的資訊</strong>都會被完整保留 (<strong>同樣也要支付多倍的費用來儲存不同的版本</strong>)</p></li><li><p>可以作為一個強大的備份手段使用</p></li><li><p>一旦版本控制啟用，就無法關閉，僅能暫停而已(已經因為 versioning 功能產生出來的舊版本資料都會依然保留著)</p></li><li><p>可以與其他資料儲存服務(例如：Glacier)搭配，並設定相關生命週期規則(Lifecycle policy)來進行進階的管理</p></li><li><p>可將刪除版本的動作綁定 MFA，強制使用者執行刪除動作前進行認證，進一步提高資料的安全性(<strong>只能由 root 帳號啟用此功能</strong>)</p><blockquote><p>刪除 object 前必須提供 token or security code 來完成</p></blockquote></li><li><p>若要做 cross region replication，source bucket 的 versioning 功能需要開啟才行</p></li></ul><h1 id="生命週期管理-Lifecycle-Management"><a href="#生命週期管理-Lifecycle-Management" class="headerlink" title="生命週期管理(Lifecycle Management)"></a>生命週期管理(Lifecycle Management)</h1><p>AWS S3 生命週期管理功能有以下幾個重點：</p><ul><li><p>可以根據預先定義好的規則(時間週期)，自動協助使用者將存放在 S3 的 object 在不同的 storage tier 中移動，讓檔案儲存的方式以更自動化且節省成本的方式進行，</p><blockquote><p>類似地端儲存中，是將資料從 <code>hot</code>(frequently    accessed) -&gt; <code>warm</code>(less    frequently    accessed) -&gt; <code>cold</code>(long-term    backup    or    archive) 的概念)</p></blockquote></li><li><p>可與版本控管(version control)的機制結合</p></li><li><p>結合了版本控管的機制後，就可以針對現有版本(current version)與先前所有版本(previous versions)進行更細粒度的管理，甚至對資料儲存成本有更進一步的優化</p></li><li><p>若要自動移除資料，也可以設定 <code>expiration</code> 相關的 policy 來達成</p></li><li><p>預設這個功能是關閉的</p></li></ul><p><img src="/blog/images/aws/S3_Lifecycle-Policy-example.png" alt="S3 resource access authorization process"></p><p>以下是幾種常見的生命週期規則設定：</p><ul><li><p>建立日期超過 30 天的檔案，自動移動到 <code>Standard - IA</code> tier 中</p></li><li><p>超過 60 天的檔案，再從 <code>Standard - IA</code> tier 移動到 <code>Glacier</code> tier</p></li><li><p>超過 90 天的檔案，可以永久刪除</p><blockquote><p>若資料有移到 Glacier，要把刪除的日期設為 90 天以上，因為 Glacier 費用低消是 90 days</p></blockquote></li></ul><p>比較需要注意的地方是，上述的規則是以<code>檔案建立時間</code>為基準計算，但有可能檔案建立了很久，依然有大量的存取；因此或許應該考量的是<code>檔案存取時間</code>可能相對的較為合理點，但目前 AWS 並沒有提供這功能，因此這邊先列入後續追蹤。</p><h1 id="Event-Notification"><a href="#Event-Notification" class="headerlink" title="Event Notification"></a>Event Notification</h1><p><img src="/blog/images/aws/S3_Event-Notification.png" alt="S3 Event Notification"></p><ul><li><p>Event Notification 是當 S3 bucket 有事件(event)發生時，主動往外送出通知</p></li><li><p>常見的事件有：(上圖可以看到目前支援的完整事件項目)</p><ul><li>object 相關操作 (PUT/POST/COPT/CompleteMultiPartUpload)</li><li>RRSObject Lost</li></ul></li><li><p>通知目前可以送到 <code>SNS</code>、<code>SQS</code>、<code>Lambda</code> 三個服務做後續進一步的處理</p></li></ul><h1 id="Cross-Region-Replication"><a href="#Cross-Region-Replication" class="headerlink" title="Cross Region Replication"></a>Cross Region Replication</h1><ul><li><p>要啟用 cross region replication 的功能，source &amp; destination bucket 都必須要啟用 versioning 才可以</p></li><li><p>設定 replication 前已經存在的 object 不會自動被同步，只有後續上傳的 object &amp; 版本資訊會被同步</p></li><li><p>任何與 object 相關的 metadata or ACL 被變更時，都會觸發 replication 的工作執行</p></li><li><p>開啟 object 的 public access 屬性也同樣會被複製到 destination bucket，<strong>但必須兩個 bucket 都先打開 public access 才可以</strong></p></li><li><p>必須設定 IAM policy，用以提供 S3 合適的權限進行 replication 工作</p></li><li><p>單一 bucket 無法同步到多個 region，但可透過 <code>region1_bucket -&gt; region2_bucket -&gt; region3_bucket</code> 的方式達成相同效果 (daisy chain 似乎也不行)</p></li><li><p>Delete marker 本身是不會被跨 region 複製的，因此刪除 source bucket 的資料的行為並不會複製到另一個 region 的 destination bucket</p></li><li><p>在 source bucket 刪除特定的版本 or delete marker 的行為，都不會被複製到另一個 region 的 destination bucket</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/">IAM Policies and Bucket Policies and ACLs! Oh, My! (Controlling Access to S3 Resources) | AWS Security Blog</a></p></li></ul><h2 id="Version-Control"><a href="#Version-Control" class="headerlink" title="Version Control"></a>Version Control</h2><ul><li><a href="https://docs.aws.amazon.com/zh_tw/AmazonS3/latest/dev/Versioning.html">使用版本控制 - Amazon Simple Storage Service</a></li></ul><h2 id="Lifecycle-Management"><a href="#Lifecycle-Management" class="headerlink" title="Lifecycle Management"></a>Lifecycle Management</h2><ul><li><a href="https://docs.aws.amazon.com/zh_tw/AmazonS3/latest/dev/object-lifecycle-mgmt.html">物件生命週期管理 - Amazon Simple Storage Service</a></li></ul><h2 id="Cross-Region-Replication-1"><a href="#Cross-Region-Replication-1" class="headerlink" title="Cross Region Replication"></a>Cross Region Replication</h2><ul><li><a href="https://docs.aws.amazon.com/zh_tw/AmazonS3/latest/dev/replication.html">複寫 - Amazon Simple Storage Service</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> S3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS CSA Associate 學習筆記 - S3(Simple Storage Service) Part 1</title>
      <link href="/blog/AWS/AWS-CSA-associate-S3-part1/"/>
      <url>/blog/AWS/AWS-CSA-associate-S3-part1/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-S3"><a href="#What-is-S3" class="headerlink" title="What is S3?"></a>What is S3?</h1><p>首先來看看 S3(<strong>AWS Simple Storage Service</strong>) 在 AWS 原廠網站上的定義：</p><blockquote><p>Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9’s) of durability, and stores data for millions of applications for companies all around the world.</p></blockquote><p>基本上可以歸納出幾個重點：</p><ul><li><p>要找個安全的地方存放檔案，S3 是個很好的選擇 (有 11 個 9 個可用度，自己維護 storage 要到這種可用度幾乎是不可能啦….)</p></li><li><p>S3 是屬於 object-based storage，與 block-based 類型的 storage 是不同的</p></li><li><p>各種以 file-based(可將 object 視為一個 file) 的應用，都可以與 S3 進行整合</p></li></ul><h1 id="關於-S3-個科普知識"><a href="#關於-S3-個科普知識" class="headerlink" title="關於 S3 個科普知識"></a>關於 S3 個科普知識</h1><h2 id="General"><a href="#General" class="headerlink" title="General"></a>General</h2><ul><li><p>S3 bucket 雖然是屬於特定 region，但在 AWS console 會以 global 的方式顯示(表示 console 中會顯示所有 region 的 bucket)</p></li><li><p>S3 是 object storage，儲存在上面就是一般的檔案</p></li><li><p>單一檔案大小的限制為 <code>0 bytes</code> ~ <code>5 TB</code>，整體的儲存空間無限制</p></li><li><p>檔案一律存在 bucket 中 (Bucket 裏面無法再放一個 bucket，但可以放 folder)</p></li><li><p><strong>單一帳號預設最大上限可存放 100 個 buckets，但可以通知 AWS 協助放大上限</strong></p></li><li><p>S3 裡面的每個 bucket 都會有一個全球獨一無二的 DNS 名稱(ex: <code>https://YOUR_UNIQUE_BUCKET_NAME.s3.amazonaws.com</code>)；若是要指定到特定的 region，則可能會是 <code>https://YOUR_UNIQUE_BUCKET_NAME.eu-west-1.amazonaws.com</code>；但無論如何，使用者只要記住 bucket name 即可</p><blockquote><p>因此可以做為 static website hosting 之用 (Everyone-read 的權限必須有設定好)</p></blockquote></li><li><p>若希望 S3 resource 可以在不同的 domain 之間互相分享，要把 <code>CORS</code> 設定開啟，允許其他 domain 來的 request</p></li><li><p>成功上傳檔案到 S3 後，會收到 <code>HTTP 200</code> 的回應</p></li></ul><h2 id="AWS-S3-給使用者的保證"><a href="#AWS-S3-給使用者的保證" class="headerlink" title="AWS S3 給使用者的保證"></a>AWS S3 給使用者的保證</h2><ul><li><p>S3 平台本身有 99.99% 可用度</p></li><li><p>AWS 平台保證有 99.9% 的可用度 (但可靠度要看最小的，所以還是會以 99.9% 為主)</p></li><li><p>對於儲存在 S3 的檔案，AWS 保證 11 個 9 的可靠度(資料遺失的可能性極低)</p><blockquote><p>為避免人為不小心刪除的狀況發生，最好的方法還是建議把 versioning, cross-regsion replication, MFA 刪除等機制啟用</p></blockquote></li></ul><h2 id="其他特性"><a href="#其他特性" class="headerlink" title="其他特性"></a>其他特性</h2><ul><li><p>支援多種 storage tier</p><blockquote><p>可與檔案生命週期管理政策進行搭配</p></blockquote></li><li><p>檔案生命週期管理</p><blockquote><p>可以設定前 30 天在正常的 <strong>standard</strong> tier, 接著 30 天移到另外一個 IA(Infrequently Accessed) tier, 90 天後進行 archive(移到 Glacier)</p></blockquote></li><li><p>版本控管</p></li><li><p>檔案加密 </p></li><li><p>MFA Delete</p><blockquote><p>重要檔案可以加上 MFA(多因素認證) 的流程進行確認，才能實際刪除檔案 </p></blockquote></li><li><p>可透過 <strong>Access Control Lists</strong> &amp; <strong>Bucket Policies</strong> 來提升存取檔案的安全性</p><blockquote><p>剛建立好的 bucket or 上傳的 object 所預設的權限僅限於自己可以存取(private &amp; inaccessible)，完全沒有預設對外開放的規則</p></blockquote></li></ul><h2 id="Object-中包含什麼資訊"><a href="#Object-中包含什麼資訊" class="headerlink" title="Object 中包含什麼資訊?"></a>Object 中包含什麼資訊?</h2><p>S3 is object based. 每個 object 都包含以下資訊：</p><ul><li><p><strong>Key</strong>: object name (檔案會依照字母順序排序，新增時要考量這個問題)</p></li><li><p><strong>Value</strong>: 基本上就是此檔案的資料本身(一堆 byte 的組合)</p></li><li><p><strong>Version ID</strong>: 作為版本控管之用</p></li><li><p><strong>Metadata</strong>: 額外用來記錄 object 相關資訊的資料(ex: 上傳檔案的時間、最後變更的時間…etc)</p><blockquote><p>使用者也可以自訂客製化的 metadata，藉此來為 object 標註不同的屬性值</p></blockquote></li><li><p><strong>Subresources</strong></p><ul><li>Access Control Lists (用來做細部的存取控管)</li><li>Torrent (S3 支援 bittorrent protocol)</li></ul></li></ul><h2 id="不是真的-folder-的-S3-folder"><a href="#不是真的-folder-的-S3-folder" class="headerlink" title="不是真的 folder 的 S3 folder"></a>不是真的 folder 的 S3 folder</h2><ul><li><p>S3 本身的設計是種 flat 結構，並不是以 hierarchy 的方式組織 &amp; 存放資料</p></li><li><p>為了讓使用者方便使用S3，S3 支援了 <code>folder</code> 的概念，但這其實只是一種 group 的概念</p></li><li><p>S3 是透過為 object 加上 key-name prefix 的方式達成的(此部份使用者不可見)</p></li></ul><h1 id="在-S3-上的檔案一致性如何呈現"><a href="#在-S3-上的檔案一致性如何呈現" class="headerlink" title="在 S3 上的檔案一致性如何呈現?"></a>在 S3 上的檔案一致性如何呈現?</h1><ul><li><p>若是新增檔案(透過 <code>PUTS</code> 的方式新增)，當檔案寫入後馬上就可以讀取到</p></li><li><p>若是更新(<code>overwrite PUTS</code>) or 刪除檔案(<code>DELETE</code>)，則是 Eventual Consistency，這樣的變更需要花點時間才會完全套用(propagate)到所有的硬體設施中</p></li></ul><h1 id="S3-Storage-Class"><a href="#S3-Storage-Class" class="headerlink" title="S3 Storage Class"></a>S3 Storage Class</h1><p>S3 根據使用者存取的頻率與需求，提供不同的 storage class 供使用者選擇：</p><blockquote><p>調整單位可以細到 object level，而非 bucket level</p></blockquote><h2 id="Standard"><a href="#Standard" class="headerlink" title="Standard"></a>Standard</h2><ul><li><p>存取速度最快</p></li><li><p>提供 99.99% 的可用率</p></li><li><p>提供跨 AZ 的 11 個 9 個可靠度</p></li><li><p>會自動將資料儲存在多個 AZ 中，會同時有三個備份，因此在兩個實體設施損毀的情況下，還是可以取得資料</p></li></ul><h3 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h3><blockquote><p><a href="https://aws.amazon.com/tw/s3/storage-classes/#General_purpose">物件儲存類別 – Amazon S3 Storage Class(Standard)</a></p></blockquote><h2 id="IA-Infrequently-Accessed"><a href="#IA-Infrequently-Accessed" class="headerlink" title="IA(Infrequently Accessed)"></a>IA(Infrequently Accessed)</h2><ul><li><p>存取頻率相較 standard 較低，但需要的時候還是可以馬上取得</p></li><li><p>提供 99.5% 的可用率</p></li><li><p>提供單一 AZ 的 11 個 9 個可靠度</p></li><li><p>費用比 standard 便宜</p></li><li><p>存取速度比 standard 慢</p></li></ul><h3 id="參考資料-1"><a href="#參考資料-1" class="headerlink" title="參考資料"></a>參考資料</h3><blockquote><p><a href="https://aws.amazon.com/tw/s3/storage-classes/#Infrequent_access">物件儲存類別 – Amazon S3 Storage Class(IA)</a></p></blockquote><h2 id="One-Zone-IA"><a href="#One-Zone-IA" class="headerlink" title="One Zone - IA"></a>One Zone - IA</h2><ul><li><p>2008 年所新增的 storage class</p></li><li><p>沒有提供 multiple AZ 的資料保護</p></li><li><p>價格比 IA 更為便宜</p></li></ul><h3 id="參考資料-2"><a href="#參考資料-2" class="headerlink" title="參考資料"></a>參考資料</h3><ul><li><p><a href="https://aws.amazon.com/tw/about-aws/whats-new/2018/04/announcing-s3-one-zone-infrequent-access-a-new-amazon-s3-storage-class/">宣佈 S3 One Zone-Infrequent Access，一種新的 Amazon S3 儲存類別</a></p></li><li><p><a href="https://aws.amazon.com/tw/s3/storage-classes/#__">物件儲存類別 – Amazon S3 Storage Class(One Zone - IA)</a></p></li></ul><h2 id="Intelligent-Tiering"><a href="#Intelligent-Tiering" class="headerlink" title="Intelligent Tiering"></a>Intelligent Tiering</h2><ul><li><p>AWS 會自動協助使用者進行優化，將不常存取的檔案移動到較為便宜的 access tier，常用的則會留在 standard tier</p></li><li><p>透過人工智慧分析，自動將使用成本最佳化，且不會造成效能的影響，也可以減輕管理上的負擔</p></li></ul><blockquote><p>但最多幫使用者將資料移動到 IA tier 的費率，無法更低了(例如：<strong>One Zone-IA</strong> or <strong>Glacier</strong>)</p></blockquote><h3 id="參考資料-3"><a href="#參考資料-3" class="headerlink" title="參考資料"></a>參考資料</h3><ul><li><a href="https://aws.amazon.com/tw/s3/storage-classes/#Unknown_or_changing_access">物件儲存類別 – Amazon S3 Storage Class(Intelligent Tiering)</a></li></ul><h2 id="Glacier"><a href="#Glacier" class="headerlink" title="Glacier"></a>Glacier</h2><ul><li><p>設計用來封存資料用(例如：稽核用的 log)，但<strong>不應該拿來備份用的</strong></p></li><li><p>存取時間會需要數分鐘到數小時不等，因此分成 <code>Expedited</code>(1<del>5 mins)、<code>Standard</code>(3</del>5 hours)、<code>Bulk</code>(5~12 hours) 三個等級，單位收費也不同</p></li><li><p>提供跨 AZ 的 11 個 9 個可靠度</p></li><li><p>一個 AZ 全毀的情況下還是可以取得資料 </p></li><li><p>儲存費用很低</p></li></ul><h3 id="參考資料-4"><a href="#參考資料-4" class="headerlink" title="參考資料"></a>參考資料</h3><ul><li><a href="https://aws.amazon.com/tw/s3/storage-classes/#Archive">物件儲存類別 – Amazon S3 Storage Class(Glacier)</a></li></ul><h2 id="Glacier-Deep-Archive"><a href="#Glacier-Deep-Archive" class="headerlink" title="Glacier Deep Archive"></a>Glacier Deep Archive</h2><ul><li><p>存取時間會需要 12 小時以上</p></li><li><p>提供跨 AZ 的 11 個 9 個可靠度</p></li><li><p>如果要保存好幾年的資料，可以考慮使用這個 tier</p></li><li><p>費用最便宜的選項</p></li></ul><h3 id="參考資料-5"><a href="#參考資料-5" class="headerlink" title="參考資料"></a>參考資料</h3><ul><li><a href="https://aws.amazon.com/tw/s3/storage-classes/#____">物件儲存類別 – Amazon S3 Storage Class(Glacier Deep Archive)</a></li></ul><p><img src="/blog/images/aws/S3_Storage-Classes-comparision.png" alt="S3 Storage Class comparison"></p><h1 id="S3-的收費標準"><a href="#S3-的收費標準" class="headerlink" title="S3 的收費標準"></a>S3 的收費標準</h1><p>介紹了 S3 的眾多特性，總是要了解使用 S3 服務時，在哪些時候 AWS 會進行收費，目前收費的情況會發生在以下幾個部份：</p><ul><li><p>資料儲存的費用 (儲存越多當然就收費越多，單位費用會根據使用的 access tier 而不同，例如：standard 會比 IA 貴)</p></li><li><p>對 S3 發出 request (對 S3 進行 HTTP request 是會計費的)</p></li><li><p>儲存管理相關的費用(例如：analysis, tagging, inventory check)</p></li><li><p>資料傳輸的費用</p><blockquote><p>資料存入 S3 免費，但資料傳出則要付費；往其他地方傳則要付費，即使是不同 region 之間互傳也是需要付費</p></blockquote></li><li><p>啟用加速傳輸(<a href="https://docs.aws.amazon.com/zh_tw/AmazonS3/latest/dev/transfer-acceleration.html">Transfer Acceleration</a>)</p><blockquote><p>此功能是利用 AWS 分佈在全球的 edge server，讓使用者透過 edge server 傳輸資料，並透過 AWS 佈建在全球的骨幹網路，以最佳化的傳輸路徑快速的存入 S3 storage</p></blockquote></li><li><p>跨 Region 的備份</p><blockquote><p>多一份的備份，資料更安全，自然也會需要額外的費用</p></blockquote></li></ul><h1 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h1><ul><li><p>S3 是 object-based storage，用來上傳一般檔案用</p></li><li><p>單一檔案的大小限制在 0 bytes ~ 5 TB</p></li><li><p>只要荷包夠深，沒有限制可使用的儲存空間</p></li><li><p>檔案儲存在 Bucket 中</p></li><li><p>S3 的名稱必須是全球唯一的</p></li><li><p>因為是 object-based storage，因此不適合拿來安裝 OS 或是資料庫服務用</p></li><li><p>檔案上傳成功會回傳 HTTP 200</p></li><li><p>可以透過開啟 MFA Delete，可以多一層資料刪除前的保護</p></li><li><p>S3 object 包含了 key, value(檔案內容本身), version ID, Metadata, Subresource(ACL, Torrent) … 等資訊</p></li><li><p>檔案上傳後就可以馬上讀取到</p></li><li><p>若是更新 or 刪除檔案，則需要一段時間才會取得最新結果</p></li><li><p>S3 有眾多的 storage class 可以選用，使用者可以根據實際需求進行不同 storage class 的搭配以降低費用；若是懶的管理可以考慮選用 <code>Intelligent Tiering</code>，AWS 會根據實際使用狀況，最佳化使用成本 (但要長期備份的資料還是要自己移到 <code>Glacier</code> or <code>Glacier Deep Archive</code>)</p></li><li><p>用來控制 Bucket 存取權限有兩個手段，分別是 <code>bucket ACL</code>(控制存取 bucket 本身的權限) &amp; <code>bucket policies</code>(控制 bucket 與其他 AWS service 互相存取的權限)</p></li><li><p>S3 FAQ 很重要，考前必讀!</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://aws.amazon.com/tw/s3/storage-classes/">物件儲存類別(Storage Class) – Amazon S3</a></p></li><li><p><a href="https://aws.amazon.com/tw/s3/faqs/">Amazon Simple Storage Service (S3) – 雲端儲存 – AWS</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> S3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS 學習筆記 - IAM(Identity and Access Management)</title>
      <link href="/blog/AWS/AWS-CSA-associate-IAM/"/>
      <url>/blog/AWS/AWS-CSA-associate-IAM/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-IAM"><a href="#What-is-IAM" class="headerlink" title="What is IAM?"></a>What is IAM?</h1><p>首先來看看 IAM(<strong>AWS Identity and Access Management</strong>) 在 AWS 原廠網站上的定義：</p><blockquote><p>AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources.</p></blockquote><p>IAM 的主要目的如下：</p><ul><li><p>用來管理使用者的權限等級，限制使用者可以存取的 AWS 資源範圍</p></li><li><p>IAM 是以目前流行的 RBAC(Role-Based Access Control) 設計的，可以建立 User/Group，並將權限給予到 Group 等級</p></li><li><p>IAM 服務是免費的</p></li></ul><h1 id="IAM-所提供的功能"><a href="#IAM-所提供的功能" class="headerlink" title="IAM 所提供的功能"></a>IAM 所提供的功能</h1><p>IAM 提供以下眾多的功能：</p><ul><li><p>統一管理 AWS 帳號</p></li><li><p>可以將 AWS resource 使用權限分享給其他使用者</p></li><li><p>可以針對每個帳號可存取的資源權限進行很細部的控制，例如<strong>限制某人只能對 DynamoDB 進行唯讀的存取</strong></p></li><li><p>提供 Identity Federation 功能，可與其他服務(例如：Active Directory, Facebook, Linkedin etc)整合，方便帳號管理</p></li><li><p>多因素認證(Multifactor Authentication)，為帳號密碼之外，額外增加一個隨時變動的密碼</p><blockquote><p>AWS 建議為每個帳號都設定 multifactor authentication</p></blockquote></li><li><p>可為 user/device/service(例如：手機裝置) …等對象提供暫時的存取權限，一段時間後權限即關閉</p></li><li><p>可自訂 password rotation policy，強制密碼一段時間後要變更</p></li><li><p>為了確保使用者可以安全使用 AWS 服務，IAM 與 AWS 眾多服務都有良好的整合</p></li><li><p>支援 PCI DSS(Payment Card Industry Data Security Standards) Compliance，方便整合外部支付的服務，提昇線上支付的安全性</p><blockquote><p>PCI DSS(支付卡產業資料安全標準)是在整合外部付費服務之用，為了提升線上支付的安全性</p></blockquote></li></ul><h1 id="IAM-的核心概念"><a href="#IAM-的核心概念" class="headerlink" title="IAM 的核心概念 "></a>IAM 的核心概念 </h1><p>IAM 的核心概念包含以下四項：</p><ul><li><p><strong>Users</strong>：指的就是使用者，也可以泛指使用資源的人 or 對象</p></li><li><p><strong>Group</strong>：一群 <code>Users</code> 的集合，在 Group 中的 User 都會繼承 Group 所擁有的權限</p></li><li><p><strong>Roles</strong>：這個概念就是用來與實際的權限綁定所設計出來的，例如：<code>UpdateApp</code>，並指定 RDS &amp; S3 的讀寫權限</p></li><li><p><strong>Policies</strong>：實際將權限綁定到 User/Group/Role(統稱為 <code>principal</code>) 的關鍵就是 Policy 了(<strong>指定哪些操作(PUT/DELETE/UPDATE…etc)可以被使用在哪些 AWS resource 上</strong>)，這是一個使用 JSON 格式所定義的文件，裡面會清楚描述可使用哪些 AWS resource &amp; 可使用的權限，以下是一個例子：</p></li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;Version&quot;</span>: <span class="string">&quot;2012-10-17&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Allow&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Action&quot;</span>: <span class="string">&quot;s3:*&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="IAM-Policy"><a href="#IAM-Policy" class="headerlink" title="IAM Policy"></a>IAM Policy</h1><ul><li><p>清楚定義的 deny policy 的效果會蓋掉所有清楚定義的 allow policy</p></li><li><p>可以透過下列 policy document 快速封鎖特定的 IAM user 所有權限</p></li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;Version&quot;</span>: <span class="string">&quot;2012-10-17&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;Statement&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Deny&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Action&quot;</span>: <span class="string">&quot;*&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>AWS 有三個重要且預先建立好的 policy template：</p><ul><li><code>Administrator access</code>：可以存取所有 AWS resource</li><li><code>Power user access</code>：有 admin 存取權限，但無法做 user/group 管理</li><li><code>Read only access</code>：只能檢視 AWS resource</li></ul></li><li><p>一個 IAM user 可以同時與多個 IAM policy 綁定</p></li><li><p><strong>IAM policy 是無法與 AWS resource/service 綁定的</strong>，它是用來指定給 IAM user/group 用的</p></li></ul><h1 id="IAM-User-Group"><a href="#IAM-User-Group" class="headerlink" title="IAM User/Group"></a>IAM User/Group</h1><ul><li><p>每個 IAM User 都可以給予不同的權限</p></li><li><p>IAM Group 可以一次賦予多個使用者相同的權限(<code>設定權限</code> -&gt; <code>指定加入到 Group 的 User</code>)</p></li></ul><h1 id="IAM-Role"><a href="#IAM-Role" class="headerlink" title="IAM Role"></a>IAM Role</h1><ul><li><p>IAM Role 是為了讓 IAM User/Group、AWS Resource(例如：EC2 instance)、臨時需要存取 AWS 資源的外部帳號 … 等取得權限用的機制</p></li><li><p>IAM Policy 無法直接套用在 AWS service 上 (因此若是要賦予 EC2 instance 權限，那只能使用 IAM Role)</p></li><li><p><strong>以上述範例來說，也可以使用 AWS credential(Access ID &amp; Secret Key) 來達成相同效果，但完全不建議這麼做，會造成管理上很大負擔</strong></p></li><li><p><strong>每一個 EC2 instance 只能與一個 IAM Role 的設定綁定</strong>；因此在複雜環境下，Role 權限設定要謹慎評估</p></li><li><p>外部帳號(不是 IAM User)需要存取權限，就必須透過整合 SAML provider(例如：AD) 的方式，搭配 IAM Role 來取得所需要的權限</p></li></ul><h2 id="Role-Type"><a href="#Role-Type" class="headerlink" title="Role Type"></a>Role Type</h2><p><code>Role Type</code> 的概念很重要，這是用來決定要將權限賦予到那一種 entity(實體)上，首先要先有以下概念：</p><ul><li><p>IAM Role 會與 IAM Policy 綁定，透過此方式來讓 Role 有特定的 AWS resource 存取權限</p></li><li><p>IAM Role 使用在特定的 entity 上，並非一般的 IAM User &amp; Group</p></li></ul><p>有了以上概念後，接著繼續介紹 IAM Role Type 有以下三種：</p><ol><li><p><code>AWS Service Role</code>：AWS 相關服務，例如：EC2、Lambda、Redshift … etc</p></li><li><p><code>Role for Cross-Account Access</code>：用來設定跨帳號存取權限用</p></li><li><p><code>Role for Identity Provider Access</code>：若是帳號與外部系統(例如：透過 SAML)整合，就可以使用此種 role type 設定外部帳號權限</p></li></ol><h1 id="IAM-Security-Token-Service-STS"><a href="#IAM-Security-Token-Service-STS" class="headerlink" title="IAM Security Token Service (STS)"></a>IAM Security Token Service (STS)</h1><ul><li><p>用來取得臨時存取 AWS resource 用的權限(以 token 的形式提供)的服務</p></li><li><p>這個暫存的 credential 是有效期的，可以根據需求設定從幾分鐘到幾個小時，過期了就會自動失效</p></li><li><p>credential 只能透過 STS API call 取得(無法從 AWS console 設定取得)</p></li><li><p>取得的 credential 包含 <code>session token</code>、<code>access key ID</code>、<code>secret access key</code> 三個部份</p></li><li><p>可與 Identity Federation 搭配；也可以與用在設定 Cross-Account Access &amp; AWS service 權限時的 IAM Role 搭配</p></li></ul><h2 id="使用-STS-的好處"><a href="#使用-STS-的好處" class="headerlink" title="使用 STS 的好處"></a>使用 STS 的好處</h2><ul><li><p>對於暫時需要 AWS resource 存取權限的應用程式，不用特定產生 credential</p></li><li><p>不用先建立一個 IAM identity(例如：IAM User/Group)，因為此服務是基於 <strong>IAM Role</strong> &amp; <strong>Identity Federation</strong> 所搭配而成的</p></li><li><p>過期自動廢棄，不需要人工作業</p></li></ul><h1 id="IAM-API-Keys"><a href="#IAM-API-Keys" class="headerlink" title="IAM API Keys"></a>IAM API Keys</h1><ul><li><p>若是有透過程式(非 AWS console)存取 AWS resource 的需求，就需要 <code>API Access Key</code>，例如：</p><ul><li>AWS CLI</li><li>Windows PowerShell</li><li>AWS SDKs</li><li>直接送到 AWS resource 的 HTTP request</li></ul></li><li><p>產生後只會在一開始顯示一次，後來再也看不到了，沒紀錄到就要重新產生</p></li><li><p>因為 API Access Key 必須與 IAM User 綁定，來取得對應的存取權限(就端看該 user 與什麼 IAM Policy 綁定)</p></li><li><p>要是產生新的 API Access Key，就建議把舊的廢止</p></li><li><p>千萬不要把 API Access Key &amp; Secret Access Key 放到 EC2 instance 中，可能會造成未來安全性上的漏洞(建議改用指定 IAM Role 的方式)</p></li></ul><h1 id="實作筆記"><a href="#實作筆記" class="headerlink" title="實作筆記"></a>實作筆記</h1><h2 id="IAM-is-global-universal"><a href="#IAM-is-global-universal" class="headerlink" title="IAM is global(universal)"></a>IAM is global(universal)</h2><p>從 management console 進入 IAM 的功能頁面後，Region 的部份會變成 <code>global</code>，表示 IAM 只需要設定一次，這個設定就可以用來套用到使用者在全球所有 region 中的 resource</p><h2 id="啟動-MFA"><a href="#啟動-MFA" class="headerlink" title="啟動 MFA"></a>啟動 MFA</h2><p><img src="/blog/images/aws/IAM_MFA-options.png" alt="IAM MFA options"></p><ul><li><p>建議啟動 MFA(Multi-Factor Authentication)，用來增加 root account 的安全性</p><blockquote><p>MFA 選項有三個，選擇 <code>Virtual MFA device</code>(Hardware 是要花錢買的) 可以與常見的 <code>Google Authenticator</code> or <code>Authy</code> app 搭配使用，透過掃描 barcode 的方式，手機上會一直出現 random 的啟用碼(要等一下)，輸入兩個就可以用來啟用 AWS IAM MFA 了</p></blockquote></li><li><p>root account 是一開始建立 AWS 所使用的帳號，擁有存取所有 AWS resource 的權限</p></li><li><p>使用 root account 建立其他擁有較小權限的帳號，並用其他帳號登入，會是相對較為安全的作法</p></li></ul><h2 id="IAM-users-sign-in-link"><a href="#IAM-users-sign-in-link" class="headerlink" title="IAM users sign-in link"></a>IAM users sign-in link</h2><p><img src="http://etutorialsworld.com/wp-content/uploads/2016/05/72.22BAWS2BIAM2BDashboard-1.png" alt="IAM users sign-in link"></p><ol><li><p>這是用來提供給其他使用者存取 AWS resource 之用，並非 root account，需要注意一下!</p></li><li><p>網址是動態產生的，可以透過 <strong>Customize</strong> 的 link 設定別名以方便記憶</p></li><li><p>IAM user 的登入入口跟 root account 是不一樣的</p></li></ol><h2 id="新增-IAM-Users"><a href="#新增-IAM-Users" class="headerlink" title="新增 IAM Users"></a>新增 IAM Users</h2><ul><li><p>所有帳號在 AWS 的有效範圍都是 Global 的，沒辦法為特定的 Region 開啟帳號</p></li><li><p>建立 IAM user 時，可以根據需求建立；若是透過程式(API/SDK/CLI)存取 AWS，勾選 <code>Programmaric access</code>(需要 “<strong>access key ID</strong>“ &amp; “<strong>secret access key</strong>“)；如果是要透過 AWS console 存取 AWS，勾選 <code>AWS Management Console access</code>(需要密碼)<br><img src="/blog/images/aws/IAM_User-access-types.png" alt="IAM User access types"></p></li><li><p>在建立 Group 頁面中，指定權限的部份，若是看到有橘色方塊的項目，就表示此權限為 AWS 預先定義好的權限(AWS managed policy)；而 <strong>Type</strong> 為 <code>Job Function</code> 的部份，其實就是 AWS 預先為各種不同的管理職能，整理好的 AWS managed policy 的集合(減輕管理者設定全線上的負擔)，因此可將 Job Function 視為 AWS managed policy colleciton<br><img src="/blog/images/aws/IAM_CreateGroup_AWS-managed-policy.png" alt="IAM Create Group - AWS Managed policy"></p></li><li><p>policy 並非 group 專屬，也可以 attach 到單一 user</p></li><li><p>每個權限有其對應的 JSON 格式設定，若是未來要使用程式化的方式定義 IAM role 的權限，可以透過此方式很方便的取得正確的定義<br><img src="/blog/images/aws/IAM_CreateGroup_permission-json.png" alt="IAM Create Group - Permission JSON payload example"></p></li></ul><h2 id="新增-Role"><a href="#新增-Role" class="headerlink" title="新增 Role"></a>新增 Role</h2><ul><li><p>目前支援的 Role Type 有四種，分別是 <code>AWS services</code>, <code>Another AWS account</code>, <code>Web Identity</code>, <code>SAML 2.0 federation</code>，可能之後還會增加</p></li><li><p>以 <code>AWS service</code> type 為例，可用來指定 AWS 上面的特定 service 為 trusted entity(也可以視為用來存取其他 AWS resource 的來源端，例如：EC2)，並指定 trusted entity 可以擁有其他特定資源的存取權限</p><blockquote><p>這可以設定的非常細，例如：只讓 EC2 對 S3 完全存取，無其他 service 的存取權限</p></blockquote></li></ul><p>其他的部份可以參考<a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">官網文件(IAM Roles - AWS Identity and Access Management)</a>，有非常詳細的說明。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><ul><li><p>IAM 有效範圍是 Global，目前還不支援 for 特定的 Region</p></li><li><p><code>root account</code> 是建立 AWS 帳號時所用的帳號，擁有存取所有 AWS resource 的權限</p></li><li><p>新建立的使用者預設是沒有任何權限的，都需要額外添加</p></li><li><p>要透過程式 or CLI 存取 AWS 的使用者必須要有 <code>Access Key ID</code> &amp; <code>Secret Access Key</code> (只能看一次，因此產生的當下要妥善保存)</p></li><li><p>root account 一定要啟用 MFA 以提高帳號安全性</p></li><li><p>可建立客製化的 password rotation policy</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/aws-certified-solutions-architect-associate/">AWS Certified Solutions Architect: Associate Certification Exam | Udemy</a></p></li><li><p><a href="https://docs.aws.amazon.com/iam/index.html">AWS Identity and Access Management Documentation</a></p></li><li><p><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html">IAM Roles - AWS Identity and Access Management</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AWS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> CSA </tag>
            
            <tag> IAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Consul 入門</title>
      <link href="/blog/ServiceMesh/consul-getting-started/"/>
      <url>/blog/ServiceMesh/consul-getting-started/</url>
      
        <content type="html"><![CDATA[<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p><img src="/blog/images/service-mesh/consul-arch.png" alt="Kubernetes Persistent Volume Provisioning"></p><ul><li><p><code>RPC</code>：Remote Procedure Call，這是 client &amp; server 之間進行 request/response 的機制</p></li><li><p><code>Agent</code>：agent 是一個長時間運行在 Consul cluster 中的一個 daemon，可以 server or client 兩種模式運行，每個 agent 都可以運行 DNS &amp; HTTP interface，並負責檢查確保 cluster 的 service 的一致性</p></li><li><p><code>Client</code>：負責將所有的 RPC 轉到 server 去，並在背景中一部份參與 LAN gossip pool，工作量不大，因此消耗的資源 &amp; 網路頻寬就很少</p></li><li><p><code>Server</code>：同樣也是 agent，但是要負責的工作就多很多，包含參與 Raft quorum，維護 cluster 狀態，回應 RPC query，與其他 datacenter 進行 WAN gossip，將 query 轉發給 leader 或是其他 datacenter</p></li><li><p><code>Datacenter</code>：具有獨立網路、低延遲、高頻寬的一個 infra 環境，官網定義中，把 AWS 中同一個 region 但在不同 AZ 的 環境都當作是一個 datacenter </p></li><li><p><code>Consensus</code>：(待補…)</p></li><li><p><code>Gossip</code>：(待補…)</p></li><li><p><code>LAN Gossip</code>：(待補…)</p></li><li><p><code>WAN Gossip</code>：(待補…)</p></li></ul><p>詳細的原理說明，可以參考此篇文章(<a href="http://ljchen.net/2019/01/04/consul%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">Consul原理解析 | ljchen’s Notes</a>)，寫的相當清楚。</p><h1 id="Installing-Consule"><a href="#Installing-Consule" class="headerlink" title="Installing Consule"></a>Installing Consule</h1><p>這部份沒太特別，可直接下載<a href="https://releases.hashicorp.com/consul/">官網的 binary</a>，解壓縮後放進 <code>/usr/bin</code> 即可。</p><h1 id="Run-the-Consul-Agent"><a href="#Run-the-Consul-Agent" class="headerlink" title="Run the Consul Agent"></a>Run the Consul Agent</h1><p>這部份大概有以下幾個重點：</p><ol><li><p>要使用 Consul，首先要先運行一個 agent 起來才行，而 agent 的運行可以分為 server mode &amp; client mode</p></li><li><p>一般為了 HA 的目的，在 data center 中可以設置 3 or 5 個 server mode agent，若是要開發測試用，一個 server mode agent 即可</p></li><li><p>除了 server mode agent 之外，其他的 agent 都是以 client mode 運行，而 client 是個非常輕量的 process，負責作 service registration, health check, forward query to server 等工作，而 cluster 中的每一個 node 都必須運行一個 consul client</p></li></ol><h2 id="Start-the-Agent"><a href="#Start-the-Agent" class="headerlink" title="Start the Agent"></a>Start the Agent</h2><p>以下透過 docker-compose，啟動一個開發模式的 agent：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line">  <span class="attr">telemetry:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">bridge</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">consul:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">consul</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">&quot;consul&quot;</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">&quot;agent -dev -client 0.0.0.0&quot;</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8300:8300&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8400:8400&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8500:8500&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8600:8600/udp&quot;</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">telemetry</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br></pre></td></tr></table></figure><p>啟動之後就可以來查詢一下 member status：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br><span class="line">Node          Address         Status  Type    Build  Protocol  DC   Segment</span><br><span class="line">34e223d8f010  127.0.0.1:8301  alive   server  1.5.1  2         dc1  &lt;all&gt;</span><br></pre></td></tr></table></figure><p>上面的資訊是由 Gossip protocol 所提供，因此在特定時間點上，不同的 local agent 可能看到的內容會稍微有差異(但最後會一致)，若是要查詢最正確的 cluster 資訊，則是要透過 HTTP API call，由 Consul servers 來回答：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 透過 HTTP API 查詢</span></span><br><span class="line">$ curl localhost:8500/v1/catalog/nodes</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;ID&quot;</span>: <span class="string">&quot;e7d2d640-d118-0b7d-a37b-ab017689e0b1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Node&quot;</span>: <span class="string">&quot;34e223d8f010&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Address&quot;</span>: <span class="string">&quot;127.0.0.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Datacenter&quot;</span>: <span class="string">&quot;dc1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;TaggedAddresses&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;lan&quot;</span>: <span class="string">&quot;127.0.0.1&quot;</span>,</span><br><span class="line">            <span class="string">&quot;wan&quot;</span>: <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Meta&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;consul-network-segment&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;CreateIndex&quot;</span>: 9,</span><br><span class="line">        <span class="string">&quot;ModifyIndex&quot;</span>: 10</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以透過 DNS interface 查詢</span></span><br><span class="line">$ dig @127.0.0.1 -p 8600 34e223d8f010.node.consul</span><br><span class="line"></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.7-Ubuntu &lt;&lt;&gt;&gt; @127.0.0.1 -p 8600 34e223d8f010.node.consul</span><br><span class="line">; (1 server found)</span><br><span class="line">;; global options: +cmd</span><br><span class="line">;; Got answer:</span><br><span class="line">;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 36094</span><br><span class="line">;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 2</span><br><span class="line">;; WARNING: recursion requested but not available</span><br><span class="line"></span><br><span class="line">;; OPT PSEUDOSECTION:</span><br><span class="line">; EDNS: version: 0, flags:; udp: 4096</span><br><span class="line">;; QUESTION SECTION:</span><br><span class="line">;34e223d8f010.node.consul.    IN    A</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">34e223d8f010.node.consul. 0    IN    A    127.0.0.1</span><br><span class="line"></span><br><span class="line">;; ADDITIONAL SECTION:</span><br><span class="line">34e223d8f010.node.consul. 0    IN    TXT    <span class="string">&quot;consul-network-segment=&quot;</span></span><br><span class="line"></span><br><span class="line">;; Query time: 7 msec</span><br><span class="line">;; SERVER: 127.0.0.1<span class="comment">#8600(127.0.0.1)</span></span><br><span class="line">;; WHEN: Fri Jun 21 09:56:14 CST 2019</span><br><span class="line">;; MSG SIZE  rcvd: 105</span><br></pre></td></tr></table></figure><h1 id="Registering-Services"><a href="#Registering-Services" class="headerlink" title="Registering Services"></a>Registering Services</h1><p>由於 Consul 是個可以用來協助進行 service discovery 的工具，因此如何向 consul 註冊 &amp; 查詢 service 是很重要的!</p><h2 id="Definining-a-Service"><a href="#Definining-a-Service" class="headerlink" title="Definining a Service"></a>Definining a Service</h2><p>重新調整一下 docker-compose 設定，並設定可自動載入外部的設定檔：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line">  <span class="attr">telemetry:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">bridge</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">consul:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">consul</span></span><br><span class="line">    <span class="attr">container_name:</span> <span class="string">&quot;consul&quot;</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">&quot;agent -dev -ui -config-dir=/etc/consul/consul.d -client 0.0.0.0&quot;</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./consul:/etc/consul/consul.d</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8300:8300&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8400:8400&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8500:8500&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8600:8600/udp&quot;</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">telemetry</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br></pre></td></tr></table></figure><p>接著在 config-dir 中放入以下幾個 json 檔案：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;service&quot;</span>: &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;web&quot;</span>, <span class="attr">&quot;tags&quot;</span>: [<span class="string">&quot;rails&quot;</span>], <span class="attr">&quot;port&quot;</span>: <span class="number">80</span>&#125;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;service&quot;</span>:&#123;<span class="attr">&quot;id&quot;</span>:<span class="string">&quot;server-1&quot;</span>,<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;server&quot;</span>,<span class="attr">&quot;tags&quot;</span>:[<span class="string">&quot;server-1&quot;</span>,<span class="string">&quot;S5B&quot;</span>,<span class="string">&quot;rack5&quot;</span>,<span class="string">&quot;u-13&quot;</span>],<span class="attr">&quot;address&quot;</span>:<span class="string">&quot;10.5.92.11&quot;</span>,<span class="attr">&quot;port&quot;</span>:<span class="number">443</span>&#125;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;service&quot;</span>:&#123;<span class="attr">&quot;id&quot;</span>:<span class="string">&quot;server-2&quot;</span>,<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;server&quot;</span>,<span class="attr">&quot;tags&quot;</span>:[<span class="string">&quot;server-2&quot;</span>,<span class="string">&quot;S5B&quot;</span>,<span class="string">&quot;rack5&quot;</span>,<span class="string">&quot;u-15&quot;</span>],<span class="attr">&quot;address&quot;</span>:<span class="string">&quot;10.5.92.12&quot;</span>,<span class="attr">&quot;port&quot;</span>:<span class="number">443</span>&#125;&#125;</span><br></pre></td></tr></table></figure><p>啟動 container 後，consul 會從指定的 config-dir 自動載入 service 相關的檔案，可以在 <a href="http://localhost:8500/">UI</a> 上看到 service 列表：</p><p><img src="/blog/images/service-mesh/consul-service-1.png" alt="Consul service list"></p><h2 id="Querying-Services"><a href="#Querying-Services" class="headerlink" title="Querying Services"></a>Querying Services</h2><p>看到 service 出現後，可以怎麼使用呢? </p><p>先撇開這問題不談，先說 consul 本身除了提供 HTTP API 的查詢外，也可以透過 DNS 查詢：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 name 作為 domain prefix 查詢</span></span><br><span class="line">$ dig @127.0.0.1 -p 8600 server.service.consul</span><br><span class="line">...(略)</span><br><span class="line">server.service.consul.    0    IN    A    10.5.92.12</span><br><span class="line">server.service.consul.    0    IN    A    10.5.92.11</span><br><span class="line">...(略)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 id 作為 domain prefix 查詢</span></span><br><span class="line">$ dig @127.0.0.1 -p 8600 server-1.server.service.consul</span><br><span class="line">...(略)</span><br><span class="line">server-1.server.service.consul.    0 IN    A    10.5.92.11</span><br><span class="line">...(略)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 tag 作為 domain prefix 查詢</span></span><br><span class="line">$ dig @127.0.0.1 -p 8600 rack5.server.service.consul</span><br><span class="line">...(略)</span><br><span class="line">rack5.server.service.consul. 0    IN    A    10.5.92.11</span><br><span class="line">rack5.server.service.consul. 0    IN    A    10.5.92.12</span><br><span class="line">...(略)</span><br></pre></td></tr></table></figure><p>從上面可以看出，使用者可以將其他服務的 DNS 指向 consul，並透過 <code>name</code>, <code>id</code>, <code>tag</code>，向 consul 詢問到正確的 service list，因此<strong>系統架構中的其他 service 就不用再考慮 ip address 這個層面的問題了</strong>!</p><p>當然也可以透過 HTTP API 查詢：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://localhost:8500/v1/catalog/service/server</span><br><span class="line">[&#123;<span class="string">&quot;ID&quot;</span>:<span class="string">&quot;2f13387f-7475-7706-1186-1fbf30c28507&quot;</span>,<span class="string">&quot;Node&quot;</span>:<span class="string">&quot;d51a6166c1d5&quot;</span>,<span class="string">&quot;Address&quot;</span>:<span class="string">&quot;127.0.0.1&quot;</span>,<span class="string">&quot;Datacenter&quot;</span>:<span class="string">&quot;dc1&quot;</span>,<span class="string">&quot;TaggedAddresses&quot;</span>:&#123;<span class="string">&quot;lan&quot;</span>:<span class="string">&quot;127.0.0.1&quot;</span>,<span class="string">&quot;wan&quot;</span>:<span class="string">&quot;127.0.0.1&quot;</span>&#125;,<span class="string">&quot;NodeMeta&quot;</span>:&#123;<span class="string">&quot;consul-network-segment&quot;</span>:<span class="string">&quot;&quot;</span>&#125;,<span class="string">&quot;ServiceKind&quot;</span>:<span class="string">&quot;&quot;</span>,<span class="string">&quot;ServiceID&quot;</span>:<span class="string">&quot;server-1&quot;</span>,<span class="string">&quot;ServiceName&quot;</span>:<span class="string">&quot;server&quot;</span>,<span class="string">&quot;ServiceTags&quot;</span>:[<span class="string">&quot;server-1&quot;</span>,<span class="string">&quot;S5B&quot;</span>,<span class="string">&quot;rack5&quot;</span>,<span class="string">&quot;u-13&quot;</span>],<span class="string">&quot;ServiceAddress&quot;</span>:<span class="string">&quot;10.5.92.11&quot;</span>,<span class="string">&quot;ServiceWeights&quot;</span>:&#123;<span class="string">&quot;Passing&quot;</span>:1,<span class="string">&quot;Warning&quot;</span>:1&#125;,<span class="string">&quot;ServiceMeta&quot;</span>:&#123;&#125;,<span class="string">&quot;ServicePort&quot;</span>:443,<span class="string">&quot;ServiceEnableTagOverride&quot;</span>:<span class="literal">false</span>,<span class="string">&quot;ServiceProxyDestination&quot;</span>:<span class="string">&quot;&quot;</span>,<span class="string">&quot;ServiceProxy&quot;</span>:&#123;&#125;,<span class="string">&quot;ServiceConnect&quot;</span>:&#123;&#125;,<span class="string">&quot;CreateIndex&quot;</span>:10,<span class="string">&quot;ModifyIndex&quot;</span>:10&#125;,&#123;<span class="string">&quot;ID&quot;</span>:<span class="string">&quot;2f13387f-7475-7706-1186-1fbf30c28507&quot;</span>,<span class="string">&quot;Node&quot;</span>:<span class="string">&quot;d51a6166c1d5&quot;</span>,<span class="string">&quot;Address&quot;</span>:<span class="string">&quot;127.0.0.1&quot;</span>,<span class="string">&quot;Datacenter&quot;</span>:<span class="string">&quot;dc1&quot;</span>,<span class="string">&quot;TaggedAddresses&quot;</span>:&#123;<span class="string">&quot;lan&quot;</span>:<span class="string">&quot;127.0.0.1&quot;</span>,<span class="string">&quot;wan&quot;</span>:<span class="string">&quot;127.0.0.1&quot;</span>&#125;,<span class="string">&quot;NodeMeta&quot;</span>:&#123;<span class="string">&quot;consul-network-segment&quot;</span>:<span class="string">&quot;&quot;</span>&#125;,<span class="string">&quot;ServiceKind&quot;</span>:<span class="string">&quot;&quot;</span>,<span class="string">&quot;ServiceID&quot;</span>:<span class="string">&quot;server-2&quot;</span>,<span class="string">&quot;ServiceName&quot;</span>:<span class="string">&quot;server&quot;</span>,<span class="string">&quot;ServiceTags&quot;</span>:[<span class="string">&quot;server-2&quot;</span>,<span class="string">&quot;S5B&quot;</span>,<span class="string">&quot;rack5&quot;</span>,<span class="string">&quot;u-15&quot;</span>],<span class="string">&quot;ServiceAddress&quot;</span>:<span class="string">&quot;10.5.92.12&quot;</span>,<span class="string">&quot;ServiceWeights&quot;</span>:&#123;<span class="string">&quot;Passing&quot;</span>:1,<span class="string">&quot;Warning&quot;</span>:1&#125;,<span class="string">&quot;ServiceMeta&quot;</span>:&#123;&#125;,<span class="string">&quot;ServicePort&quot;</span>:443,<span class="string">&quot;ServiceEnableTagOverride&quot;</span>:<span class="literal">false</span>,<span class="string">&quot;ServiceProxyDestination&quot;</span>:<span class="string">&quot;&quot;</span>,<span class="string">&quot;ServiceProxy&quot;</span>:&#123;&#125;,<span class="string">&quot;ServiceConnect&quot;</span>:&#123;&#125;,<span class="string">&quot;CreateIndex&quot;</span>:11,<span class="string">&quot;ModifyIndex&quot;</span>:11&#125;]</span><br></pre></td></tr></table></figure><h2 id="Updating-Services"><a href="#Updating-Services" class="headerlink" title="Updating Services"></a>Updating Services</h2><p>理解上面的查詢方式後，那如何更新呢? 有兩種方式：</p><ol><li><p>修改原本的 configuration file，並送一個 SIGHUP 訊號給 consul agent，就會自己 reload 並套用了</p></li><li><p>透過 HTTP API 修改</p></li></ol><p>而關於 <code>service</code>，有哪些屬性可以拿來設定呢? 可參考 <a href="https://www.consul.io/api/agent/service.html#parameters-2">Consul 官網的 API 文件</a> 得到答案。</p><h1 id="Registering-Health-Checks"><a href="#Registering-Health-Checks" class="headerlink" title="Registering Health Checks"></a>Registering Health Checks</h1><p>要實作 service discovery 的功能，有一個很重要的部份就是 health check；為了避免 client 連線到無法提供正常服務的 service，做好 service health check 這件事情就是必須的。</p><h2 id="Defining-Checks"><a href="#Defining-Checks" class="headerlink" title="Defining Checks"></a>Defining Checks</h2><p>首先，為了啟動 service health check 的功能，啟動 Consule agent 時必須將 <code>enable_script_checks</code> 設定為 <strong>true</strong>；另外，要增加 check 的設定，可以在 json 設定檔中定義，也可以使用 HTTP API call 來達成。</p><p>若要了解 check 的詳細設定，可以參考<a href="https://www.consul.io/docs/agent/checks.html">官網文件</a>，以下用幾個範例簡介：</p><h3 id="ping-json"><a href="#ping-json" class="headerlink" title="ping.json"></a>ping.json</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;check&quot;</span>: &#123;<span class="attr">&quot;name&quot;</span>: <span class="string">&quot;ping&quot;</span>, <span class="attr">&quot;args&quot;</span>: [<span class="string">&quot;ping&quot;</span>, <span class="string">&quot;-c1&quot;</span>, <span class="string">&quot;google.com&quot;</span>], <span class="attr">&quot;interval&quot;</span>: <span class="string">&quot;30s&quot;</span>&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="server-1-json"><a href="#server-1-json" class="headerlink" title="server-1.json"></a>server-1.json</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;service&quot;</span>:&#123;<span class="attr">&quot;id&quot;</span>:<span class="string">&quot;server-1&quot;</span>,<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;server&quot;</span>,<span class="attr">&quot;tags&quot;</span>:[<span class="string">&quot;server-1&quot;</span>,<span class="string">&quot;S5B&quot;</span>,<span class="string">&quot;rack5&quot;</span>,<span class="string">&quot;u-13&quot;</span>],<span class="attr">&quot;address&quot;</span>:<span class="string">&quot;10.5.92.11&quot;</span>,<span class="attr">&quot;port&quot;</span>:<span class="number">443</span>,<span class="attr">&quot;check&quot;</span>:&#123;<span class="attr">&quot;id&quot;</span>:<span class="string">&quot;api&quot;</span>,<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;HTTPs IPMI portal check&quot;</span>,<span class="attr">&quot;http&quot;</span>:<span class="string">&quot;https://10.5.92.11&quot;</span>,<span class="attr">&quot;tls_skip_verify&quot;</span>:<span class="literal">true</span>,<span class="attr">&quot;method&quot;</span>:<span class="string">&quot;GET&quot;</span>,<span class="attr">&quot;header&quot;</span>:&#123;<span class="attr">&quot;x-foo&quot;</span>:[<span class="string">&quot;bar&quot;</span>,<span class="string">&quot;baz&quot;</span>]&#125;,<span class="attr">&quot;interval&quot;</span>:<span class="string">&quot;10s&quot;</span>,<span class="attr">&quot;timeout&quot;</span>:<span class="string">&quot;5s&quot;</span>&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="server-2-json"><a href="#server-2-json" class="headerlink" title="server-2.json"></a>server-2.json</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;service&quot;</span>:&#123;<span class="attr">&quot;id&quot;</span>:<span class="string">&quot;server-2&quot;</span>,<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;server&quot;</span>,<span class="attr">&quot;tags&quot;</span>:[<span class="string">&quot;server-2&quot;</span>,<span class="string">&quot;S5B&quot;</span>,<span class="string">&quot;rack5&quot;</span>,<span class="string">&quot;u-15&quot;</span>],<span class="attr">&quot;address&quot;</span>:<span class="string">&quot;10.5.92.12&quot;</span>,<span class="attr">&quot;port&quot;</span>:<span class="number">443</span>,<span class="attr">&quot;check&quot;</span>:&#123;<span class="attr">&quot;args&quot;</span>:[<span class="string">&quot;curl&quot;</span>,<span class="string">&quot;10.5.92.12&quot;</span>],<span class="attr">&quot;interval&quot;</span>:<span class="string">&quot;10s&quot;</span>&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><p>當 consul 取得以上設定後，可以從 UI 畫面中看到以下內容：</p><p>Health Check Overview：<br><img src="/blog/images/service-mesh/consul-service-healthcheck-1.png" alt="Consul service list"></p><p>Service Health Check Overview：<br><img src="/blog/images/service-mesh/consul-service-healthcheck-2.png" alt="Consul service list"></p><p>Service Level Health Check Status：<br><img src="/blog/images/service-mesh/consul-service-healthcheck-3.png" alt="Consul service list"></p><p>Node Level Health Check Status：<br><img src="/blog/images/service-mesh/consul-service-healthcheck-4.png" alt="Consul service list"></p><h2 id="Checking-Health-Status"><a href="#Checking-Health-Status" class="headerlink" title="Checking Health Status"></a>Checking Health Status</h2><p>當加入 Health Check 的機制後，若是某個 service 的狀態不健康了，透過上面提到的 service 查詢，就不會回應該 service 的資訊，也就是說：</p><blockquote><p>Consul 只會回應使用者目前健康狀態檢查通過的 service 列表</p></blockquote><p>這是在 service discovery 這個主題中非常重要的功能，此外，若是透過 CLI 可以用 <code>curl http://localhost:8500/v1/health/state/critical</code> 來查詢有問題的 service。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="http://ljchen.net/2019/01/04/consul%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">Consul原理解析 | ljchen’s Notes</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Consul </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Consul </tag>
            
            <tag> ServiceMesh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[維運管理] Cloud Native 時代的維運實踐</title>
      <link href="/blog/OperationManagement/Operation-Management-CloudNative-Era/"/>
      <url>/blog/OperationManagement/Operation-Management-CloudNative-Era/</url>
      
        <content type="html"><![CDATA[<h1 id="為什麼要選擇上雲"><a href="#為什麼要選擇上雲" class="headerlink" title="為什麼要選擇上雲?"></a>為什麼要選擇上雲?</h1><ul><li><p>Cloud Computing 發展到今天，無論是在技術、服務層面，還是在商業層面都已經相對比較成熟</p></li><li><p>大多數的新創公司在 infrastructure 上的策略一定是 public cloud，已經極少再有自建或託管 IDC 的情況</p></li><li><p>但若是原有系統規模已經有一定程度的公司，要搬遷上雲就必須進行全面考量，包含 infra 的變化，業務的平穩過度，運維模式的轉變，成本管控的調整，以及眾多的細節問題</p></li></ul><h2 id="電商案例探討"><a href="#電商案例探討" class="headerlink" title="電商案例探討"></a>電商案例探討</h2><ul><li><p>對於電商系統，例行的促銷活動(例如：雙11)已經成為常態，而這樣的活動在技術層面，也代表系統要在短時間內應對遠遠超過日常的峰值流量，可能是平時的十幾倍，甚至是上百倍，因此系統要有足夠的容量可以支援</p></li><li><p>從技術和架構層面來優化，可以提升可承載的容量；但是無論如何優化，充足硬體資源可以提供 scale out 才是前提條件</p></li><li><p>對於自建 infra 的公司，只能透過預先採購設備 or 跟廠商談租用硬體的方式，來取得足夠的硬體資源；但促銷過後，這些硬體資源使用率就變得極低，成本投入效益相當低</p></li><li><p>特殊需求(例如：大數據分析時)臨時需要大量的硬體資源時，在資源調度上的難度並不低，通常只能挑業務離峰時間進行</p></li><li><p>當業務發展到一定規模時，在底層技術層面上也要很大程度的投入，但通常這樣的投入相較於 public cloud 廠商，效益並不好，尤其是以業務維生的公司更是如此(當然技術創新類的公司例外…..)</p><blockquote><p>簡單來說，技術層面就是讓 public cloud 廠商來負責，因為他們有大量的底層技術專業人才支撐這些研發，專注在各類服務整合 &amp; 業務即可，不要讓底層技術問題變成了業務發展的阻礙</p></blockquote></li></ul><h2 id="技術發展趨勢"><a href="#技術發展趨勢" class="headerlink" title="技術發展趨勢"></a>技術發展趨勢</h2><ul><li><p>從軟體架構發展趨勢來看：<code>Bare Metal</code> -&gt; <code>VM</code> -&gt; <code>Container</code> -&gt; <code>Serverless</code></p><blockquote><p>上述的發展就是為了不斷的提昇資源利用率而發展出來的</p></blockquote></li><li><p>Public Cloud 上的服務不斷推陳出新，像是全託管 DB、Container、人工智慧、IoT、更彈性的網路管理….等，因此如果想要利用新技術為業務帶來更多的可能性，擁抱 Cloud Computing 是最好的選擇</p></li><li><p>未來人工智慧的發展和應用，必然會依附於 Cloud Computing</p></li></ul><h1 id="Hybrid-Cloud-將是未來的主流"><a href="#Hybrid-Cloud-將是未來的主流" class="headerlink" title="Hybrid Cloud 將是未來的主流"></a>Hybrid Cloud 將是未來的主流</h1><ul><li><p>隨著 public cloud 服務越來越豐富，對 public cloud 的應用也不再僅僅限於資源層面，而更多地體現在 cloud service 層面</p></li><li><p>CDN 其實就是 cloud service 最早被應用的典型形態</p></li><li><p>有些公司因為在 public cloud 蓬勃發展之前就已經建設了自有的技術體系和架構，所以在選擇上雲的過程中，就需要有個過渡過程，這個過程就是 hybrid cloud 需求存在的應用場景</p></li><li><p>即使不上雲，我們的數據在自己的機房裡就一定 100% 安全嗎？</p></li><li><p>不管如何選擇和使用，我們一定還是要以滿足業務需求為出發點，脫離了這一點，單純追求技術深度和複雜度是沒有意義的</p></li><li><p>利用 cloud computing 的優勢，擁抱變化，才能夠為我們的業務發展和創新帶來更多的可能性</p></li></ul><h1 id="面對應用層的雲架構解決方案"><a href="#面對應用層的雲架構解決方案" class="headerlink" title="面對應用層的雲架構解決方案"></a>面對應用層的雲架構解決方案</h1><ul><li><p>Cloud Native(雲原生)的概念，目的是幫企業提供在 cloud 上業務 end to end 的技術解決方案，全面提升軟體交付效率，降低運維成本</p></li><li><p>基於上述理念，cloud native 解決方案就會包括多雲和跨雲平台的管理、監控、發佈，以及基礎的 DB、緩存和消息隊列…等各種服務</p></li><li><p>CNCF 的項目優勢在於，它們是與 Kubernetes 整合 &amp; 配套的，可以很方便的應用於 K8S 生態中</p></li><li><p>目前 Kubernetes 已實際上成為業界容器編排方面的標準，且被廣泛應用，所以各大雲廠商，無論公有雲和私有雲，都會主動支援 Kubernetes 在雲計算體系中的落地</p></li></ul><h2 id="可預見的技術發展趨勢"><a href="#可預見的技術發展趨勢" class="headerlink" title="可預見的技術發展趨勢"></a>可預見的技術發展趨勢</h2><ul><li><p>很多獨立的技術產品，正在向雲生態靠攏，選擇跟 public cloud 合作，爭取讓產品進入到某個雲生態中，並提供相應的雲上解決方案和技術支持</p></li><li><p>在 cloud native 的理念中，跟業務無直接關係且相對通用的技術在不斷地被標準化，而且標準化層面越來越高</p></li><li><p>技術每被標準化一層，原來繁瑣低效率的工作就少一些，技術標準化的層面越高，技術門檻就會變得越低</p></li><li><p>對於技術人員來說，需要開始轉換成思考如何找到適合業務解決方案的技術並落地實現，而不再只是專注於技術層面的造輪子</p></li><li><p>對於維運人員來說，要瞭解技術發展趨勢，應該成為技術架構的管理者，從效率、成本、穩定性這幾個方面來檢驗架構是否合理，並為架構朝著更加健康的方向發展保駕護航</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://time.geekbang.org/column/intro/63">趙成的運維體系管理課 - 極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Operation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[維運管理] 穩定性實踐 &amp; 故障管理</title>
      <link href="/blog/OperationManagement/Operation-Management-Stability-and-Failure/"/>
      <url>/blog/OperationManagement/Operation-Management-Stability-and-Failure/</url>
      
        <content type="html"><![CDATA[<h1 id="穩定性實踐：限流-amp-降級"><a href="#穩定性實踐：限流-amp-降級" class="headerlink" title="穩定性實踐：限流 &amp; 降級"></a>穩定性實踐：限流 &amp; 降級</h1><ul><li><p>當面對極端的業務場景時，瞬時的業務流量會帶來龐大的壓力，可能還會大大超出系統可負載的容量</p></li><li><p>無限制的通過 resource scale out 來提升系統可負載容量為無論從<strong>技術</strong> or <strong>成本</strong>投入角度，這樣做都是不划算的，也是不現實的</p></li><li><p>因此可以透過<code>限流降級</code>的手段，以保障承諾容量下的系統穩定；為某些功能加上 feature gate，在必要時可以關閉特定功能，在業務量達到高峰時，只保障核心業務功能，非核心業務功能就關閉</p></li></ul><h2 id="限流-amp-降級是什麼"><a href="#限流-amp-降級是什麼" class="headerlink" title="限流 &amp; 降級是什麼?"></a>限流 &amp; 降級是什麼?</h2><ul><li><p><strong>限流</strong>：根據某個應用或基礎部件的某些核心指標，如 <code>QPS</code> 或<code>並發數量</code>，來決定是否將後續的請求進行攔截。</p><blockquote><p>假設一秒的 QPS threshold 為 200，超過即攔截 &amp; 回應錯誤訊息</p></blockquote></li><li><p><strong>降級</strong>：通過判斷某個 application or component 的服務狀態是否正常，來決定是否繼續提供服務。</p><blockquote><p>例如：在某段時間內，Response Time 超過 50ms 的次數大於 N 次，則會短暫的關閉服務，等待一段時間(e.g. 5s~10s)後重新開啟</p></blockquote></li></ul><h2 id="如何進行限流"><a href="#如何進行限流" class="headerlink" title="如何進行限流?"></a>如何進行限流?</h2><p>最常見的就是從 traffic 入口端就作限流了! 例如：</p><ul><li><p>Nginx 限流：作為最常被使用的 reverse proxy，可以透過以 QPS, 並發數量、CPU 使用率作為限流的指標</p></li><li><p>API Gateway：同樣也是要監控要作為限流的指標，自動進行 traffic routing policy 的調整</p></li></ul><p>其他應用層 &amp; infra 層的限流，就需要跟開發同仁一同配合才比較容易完成，有機會研究到再來寫….</p><h2 id="實現限流-amp-降級的困難之處"><a href="#實現限流-amp-降級的困難之處" class="headerlink" title="實現限流 &amp; 降級的困難之處"></a>實現限流 &amp; 降級的困難之處</h2><p>困難之處主要是兩個部份：</p><ul><li><p>Technique Stack 的統一</p><blockquote><p>這牽涉到大量標準化建設的工作，需要與開發同仁一同進行技術選型、定義規範 …. 等等，若 technique stack 不統一，限流 &amp; 降級就幾乎不可能落地</p></blockquote></li><li><p>對於每個應用的限流降級策略可以精準掌握 &amp; 配置</p><blockquote><p>這需要對業務 &amp; 應用有深入了解，並經過複雜的測試才能評估出一個正確的數字；而且重要的事情是，限流 &amp; 降級是一個動態 &amp; 不斷完善的過程，因為資源動態管理絕對不是可以一步到位的</p></blockquote></li></ul><p>由於改善是不斷持續的，所以當整體系統規模達到一定程度時，當已經無法使用人力來評估所有事情時，就必須引入機器學習 or 人工智慧這一類的技術來協助了!</p><h1 id="穩定性實踐：開關和備案"><a href="#穩定性實踐：開關和備案" class="headerlink" title="穩定性實踐：開關和備案"></a>穩定性實踐：開關和備案</h1><p>上一個部份提到的限流降級的方案，是針對服務層面的，也就是對服務本身進行限流 &amp; 降級；而另外一個可從業務角度來出發的，則是<code>開關</code> &amp; <code>備案</code>。</p><h2 id="什麼是開關-amp-備案"><a href="#什麼是開關-amp-備案" class="headerlink" title="什麼是開關 &amp; 備案?"></a>什麼是開關 &amp; 備案?</h2><h3 id="開關"><a href="#開關" class="headerlink" title="開關"></a>開關</h3><p>開關，這個概念更多是業務和功能層面的，主要是針對單個功能的啟用和停止進行控制(可透過 <code>feature gate</code> 的方式來做)，或者將功能狀態在不同版本之間進行切換。</p><p>從業務層面舉例，假設以一個雙11促銷的例子來說，只保留交易鏈路的核心功能，先暫時關閉非必要功能，例如：商品評論….這樣的作法就是業務功能開關的實現。</p><p>從功能層舉例，一般系統架構中為了提昇效能，可能會有 cache tier 的存在，但如果因為 cache tier 發生問題，可以透過功能開關的方式，bypass cache tier，讓資料直接轉進資料庫。</p><h3 id="備案"><a href="#備案" class="headerlink" title="備案"></a>備案</h3><p>所謂的<code>備案</code>，是為讓應用或業務進入到某種特定狀態，所預先設計的複雜場景(or 方案)，而這個場景通常會透過<code>開關</code>、<code>限流</code>和<code>降級策略</code>的組合技來實現。</p><p>例如：例如上面雙11促銷的例子，除了關閉商品評論外，可能還會需要關閉商品收藏提示、商品推薦…等功能</p><p>這一類的調整通常不是很單純的，而在不同的情況(or 場景)下，用來應對的 <code>開關</code>、<code>限流</code>、<code>降級策略</code>的組合可能都會完全不同；因此透過<code>備案</code>的設計，可以針對特定的事件一次進行<code>開關</code>、<code>限流</code>和<code>降級策略</code>的相關處理，在管理維護上就會容易很多!</p><p>因此<code>備案</code>的管理也是保持整體系統穩定性很重要的一環。</p><h1 id="穩定性實踐：全鏈路追蹤系統"><a href="#穩定性實踐：全鏈路追蹤系統" class="headerlink" title="穩定性實踐：全鏈路追蹤系統"></a>穩定性實踐：全鏈路追蹤系統</h1><p>隨著微服務和分散式架構的導入，各類和基礎組件形成了網狀的分佈式呼叫(調用)關係，這種複雜的調用關係就大大增加了問題定位、瓶頸分析、容量評估以及限流降級等穩定性保障工作的難度，因此才會產生了<code>全鏈路追蹤</code>這一類的 solution。</p><h2 id="如何產生請求鏈路"><a href="#如何產生請求鏈路" class="headerlink" title="如何產生請求鏈路"></a>如何產生請求鏈路</h2><p>全鏈路追蹤的核心技術關鍵就是 <code>TraceID</code>，當 request 從接入層進來時，這個 TraceID 就要被創建出來；或者是通過 Nginx 插件方式創建放到 http 的 header 裡面；或者是通過 RPC 服務化框架生成；在後續的 request 中，TraceID 必須能夠被框架傳遞到下一個被呼叫的功能。</p><blockquote><p>有了這個 TraceID，就可以將一個完整的請求鏈路給串聯起來了</p></blockquote><h2 id="問題定位和排查"><a href="#問題定位和排查" class="headerlink" title="問題定位和排查"></a>問題定位和排查</h2><p>全鏈路跟蹤系統，要解決的首要問題就是在複雜的服務調用關係中快速準確地定位問題，大概有以下兩類：</p><ul><li><p>瓶頸分析</p></li><li><p>異常錯誤定位</p></li></ul><h2 id="服務運行狀態分析"><a href="#服務運行狀態分析" class="headerlink" title="服務運行狀態分析"></a>服務運行狀態分析</h2><p>透過收集大量個請求 &amp; 調用數據後，可以有效的分析出下面幾類訊息：</p><ul><li><p>服務運行的品質狀況</p></li><li><p>應用和服務依賴關係</p><blockquote><p>依賴關係資訊是作為未來進行<code>容量壓測</code>和<code>限流降級</code>這兩個工作所必要的資訊，也可以作為 scale out 的依據</p></blockquote></li><li><p>依賴關係的服務質量</p></li></ul><h2 id="業務全息"><a href="#業務全息" class="headerlink" title="業務全息"></a>業務全息</h2><p>一般來說，全鏈路跟蹤的應用通常是在技術層面，用來定位“應用或服務”的問題，或者是應用或服務間的依賴關係等等。</p><p>但其實也很適合用來作為業務鏈路的分析，當業務在不同的階段發生問題時，同樣都是透過像是 <code>TrackID + 訂單 ID + 用戶 ID ... 等</code>這幾類訊息來加入業務相關問題的排查。</p><h1 id="故障管理：理解-定級-定責、應急"><a href="#故障管理：理解-定級-定責、應急" class="headerlink" title="故障管理：理解, 定級, 定責、應急"></a>故障管理：理解, 定級, 定責、應急</h1><h2 id="“SRE：Google-運維解密”-給我們的啟示"><a href="#“SRE：Google-運維解密”-給我們的啟示" class="headerlink" title="“SRE：Google 運維解密” 給我們的啟示"></a>“SRE：Google 運維解密” 給我們的啟示</h2><ul><li><p>SRE 的崗位職責在很大程度上就是應對故障</p></li><li><p>理解一個系統應該如何工作並不能使人成為專家，只能靠調查系統為何不能正常工作才行</p></li></ul><h2 id="面對故障應有的態度"><a href="#面對故障應有的態度" class="headerlink" title="面對故障應有的態度"></a>面對故障應有的態度</h2><ul><li><p>系統正常，只是該系統無數異常情況下的一種特例</p></li><li><p>故障，是一種常態，任何一個軟體系統都避免不了；當業務量越大，系統越複雜，問題和故障就越多，出現故障是必然的</p></li><li><p>因為我們無法杜絕故障，所以必須 Design for Failure；應該考慮的是，怎麼讓系統更健壯，在一般的問題面前依然可以穩定提供服務，甚至故障後也可以快速恢復</p></li><li><p>上面提到的<code>限流</code>、<code>降級</code>、<code>開關</code>、<code>備案</code> … 等手段，本質並非完全避免故障，而是為了能夠更有效面對故障</p></li><li><p>一般大型往站在<code>故障隔離</code>、<code>快速恢復</code>、<code>容錯切換</code>這些部份都做的很好，即使小問題發生也不會影響主要的服務</p></li></ul><h2 id="故障發生後的調整"><a href="#故障發生後的調整" class="headerlink" title="故障發生後的調整"></a>故障發生後的調整</h2><ul><li><p>永遠不要將注意力放在故障本身上，一定要將注意力放到故障背後的技術和管理問題上去；深究原因，其實故障發生的原因，通常是技術和管理上的問題累積到一定程度後爆發出來的</p></li><li><p>故障後的反思；人員技術不到位? 人為操作太多? 自動化不夠完善? 發佈後無法快速 rollback?</p></li><li><p>發生故障無法快速恢復的反思; 監控不到位? 告警太多導致人員麻痺? 定位問題的效率低? 故障隔離不夠確實? 故障備案無法落實?</p></li><li><p>任何一個故障的原因都可以歸納到具體的技術和管理問題上，通常在回頭審視並聚焦在故障案例上的過程中，都可以得到一個個非常具體的改善方案</p></li><li><p>反問自己：下次出現類似問題，怎麼才能更快的發現問題，更快的恢復業務？即使這一次的故障處理已經做得非常好了，下次是否可以有更進一步的改進？</p></li></ul><h2 id="面對故障的管理態度"><a href="#面對故障的管理態度" class="headerlink" title="面對故障的管理態度"></a>面對故障的管理態度</h2><ul><li><p>管理者要先自我反省，一昧究責對解決問題沒有幫助</p></li><li><p>故障發生就表示整體上(不論系統 or 管理制度)有存在不完善的地方或漏洞，認真思考，找出來並修正</p></li><li><p>強調技術解決問題，而不是單純地靠增加管理流程和檢查環節來解決問題，技術手段暫時無法滿足的，可以靠管理手段來輔助</p></li><li><p>定責的過程，是找出根因，針對不足找出改進措施，落實責任人；定責的目的，是責任到人，並且負責人能夠確實認識到自己的不足之處，能夠主導改進措施的落地</p></li><li><p>絕大多數的嚴重故障都是因為無意識或意識薄弱導致的，並不是因為單純的技術能力不足等技術因素</p></li><li><p>如果大家意識到位，能夠謹慎小心，絕大多數因為人為疏失造成的低級錯誤都是可以避免的</p></li><li><p>管理者一定要對故障有一定的容忍度，因為員工努力做事的積極性一旦被打擊，變得畏首畏尾，也就不會有什麼技術進步和突破了</p></li><li><p>如果定責跟績效強掛鉤，團隊內成員就有極大可能陷入恐慌、質疑、挑戰以致最終相互不信任的局面</p></li><li><p>管理者除了關注故障本身之外，還要考慮得更加全面一些，要關注到人的感受，關注事情的前因後果，只有這樣，在管理執行過程中才會讓員工感受到尊重和信任</p></li><li><p>對故障要有正確和理性的認識，既不能放任不管，也不要談之色變；同時我們也需要科學的管理方式，跟業務結合，制定出對應的故障等級和定級定責制度</p></li><li><p>在日常要做好各種備案和模擬演練，當故障真實發生時，能夠做到冷靜處理和高效地組織協調</p></li></ul><h2 id="故障緊急處理-amp-復盤"><a href="#故障緊急處理-amp-復盤" class="headerlink" title="故障緊急處理 &amp; 復盤"></a>故障緊急處理 &amp; 復盤</h2><h3 id="故障緊急處理"><a href="#故障緊急處理" class="headerlink" title="故障緊急處理"></a>故障緊急處理</h3><ul><li><p>第一原則：優先恢復業務，而不是定位問題</p></li><li><p>業務恢復備案的執行不能僅僅在故障發生時才執行，而是應該把故障模擬和恢復演練放在平時</p></li><li><p>關於故障模擬，可以試試看 Netflix 的 <code>Chaos Engineering</code></p></li><li><p>組織中要有”技術支援”的角色；當故障發生時，對內，要有效組織技術團隊的集中和協作；對外，負責對接業務部門同步相關訊息，同時消除各方對技術團隊和故障處理人員的干擾</p></li><li><p>故障緊急處理要注重平時的演練，注意建設各種工具和平台，同時要盡可能地考慮和模擬各種故障場景</p></li></ul><h3 id="故障復盤"><a href="#故障復盤" class="headerlink" title="故障復盤"></a>故障復盤</h3><ul><li><p>復盤的目的是為了從故障中學習，找到我們技術和管理上的不足，然後不斷改進</p></li><li><p>切忌將復盤過程和目的搞成追究責任或實施懲罰，這對於團隊氛圍和員工積極性的打擊是非常大的</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://time.geekbang.org/column/intro/63">趙成的運維體系管理課 - 極客時間</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Operation </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Operation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rancher Architecture Overview</title>
      <link href="/blog/Rancher/Rancher-architecture-overview/"/>
      <url>/blog/Rancher/Rancher-architecture-overview/</url>
      
        <content type="html"><![CDATA[<h1 id="Rancher-Server-Architecture"><a href="#Rancher-Server-Architecture" class="headerlink" title="Rancher Server Architecture"></a>Rancher Server Architecture</h1><p><img src="/blog/images/rancher/rancher-arch-overview.png" alt="Rancher Architecture Overview"></p><p>從上面的示意圖可以看出幾點：</p><ul><li><p>Rancher server 可以同時管理多個下游的 k8s cluster (可以是透過 RKE 佈署的 k8s，也可以是 AWS EKS)</p></li><li><p>使用者可以透過 Authentication Proxy，對多個下游 k8s cluster 進行管理</p></li><li><p>Rancher 本身也是一個小型的 k8s cluster，可以安裝在單一節點上(測試環境)或是帶有 HA 的 k8s cluster(生產環境) 中</p></li><li><p>Rancher server 所運行的節點務必與所管理的 k8s cluster 分開</p></li></ul><h1 id="Rancher-server-如何與下游的-k8s-cluster-通訊"><a href="#Rancher-server-如何與下游的-k8s-cluster-通訊" class="headerlink" title="Rancher server 如何與下游的 k8s cluster 通訊"></a>Rancher server 如何與下游的 k8s cluster 通訊</h1><p>以下是個簡單的示意圖：</p><p><img src="/blog/images/rancher/how-rancher-communicate.png" alt="How Rancher Communicate"></p><p>上圖中有幾大重要的部份：</p><h2 id="Authentication-Proxy"><a href="#Authentication-Proxy" class="headerlink" title="Authentication Proxy"></a><a href="https://rancher.com/docs/rancher/v2.x/en/overview/architecture/#1-the-authentication-proxy">Authentication Proxy</a></h2><ul><li><p>透過 Rancher server 進行 request 的使用者，會經過 Authentication Proxy 的認證後，由 Rancher server 將 request 轉到要管理的 cluster 上。</p></li><li><p>可以整合 GitHub, ActiveDirectory …等服務進行驗證</p></li><li><p>Rancher server 是透過 service account 來作為管理下游 k8s cluster 的識別</p></li><li><p>預設 Rancher server 會提供一份完整的 kubeconfig 檔案作為透過 Authentication Proxy 來管理下游 cluster 之用</p></li></ul><h2 id="Cluster-Controller"><a href="#Cluster-Controller" class="headerlink" title="Cluster Controller"></a>Cluster Controller</h2><p>每個由 Rancher server 管理的 k8s cluster 都會有一對 Cluster Agent &amp; Controller 來搭配負責進行管理，而 Cluster Controller 負責以下工作：</p><ul><li><p>監控下游 k8s cluster 的變更</p></li><li><p>確保下游的 k8s cluster 有達到 desired state</p></li><li><p>將存取權限設定套用到下游的 cluster &amp; project</p></li><li><p>與 RKE or GKE …. 等服務搭配，進行 provision 的工作</p></li></ul><h2 id="Cluster-Agent-cattle-cluster-agent"><a href="#Cluster-Agent-cattle-cluster-agent" class="headerlink" title="Cluster Agent (cattle-cluster-agent)"></a>Cluster Agent (cattle-cluster-agent)</h2><p>預設每個下游的 k8s cluster 都會有一個 agent，並建立一條 tunnel 連回 Rancher server；若沒有，Cluster Controller 會直接連到 <strong>Node Agent</strong>。</p><p>而 Cluster Agent 負責以下工作：</p><ul><li><p>負責連到由 Rancher 啟動的 k8s cluster 中的 API server</p></li><li><p>為每個 cluster 管理 workload，包含 pod, deployment … 等等</p></li><li><p>將 role &amp; binding …. 等全域設定套用到 cluster 中</p></li><li><p>回報 event、stat、node info, health … 等資訊給 Rancher server</p></li></ul><h2 id="Node-Agent-cattle-node-agent"><a href="#Node-Agent-cattle-node-agent" class="headerlink" title="Node Agent (cattle-node-agent)"></a>Node Agent (cattle-node-agent)</h2><ul><li><p>若 cluster agent 無法使用，cluster 中的某一個 node agnet 就會負責與 Rancher server 通訊。</p></li><li><p>node agnet 是以 DaemonSet 的形式安裝在由 Rancher 佈署的下游 k8s cluster 中</p></li><li><p>負責進行 node level 的相關操作，例如：更新 k8s 版本、還原 etcd snapshot … 等等</p></li></ul><h2 id="Authorized-Cluster-Endpoint"><a href="#Authorized-Cluster-Endpoint" class="headerlink" title="Authorized Cluster Endpoint"></a>Authorized Cluster Endpoint</h2><p>透過 RKE 安裝的 k8s cluster 中會包含一個名稱為 <code>kube-api-auth</code> 的 pod，允許使用者可以不透過 Authentication Proxy 與下游的 k8s cluster 直接進行通訊，主要目的有兩個：</p><ol><li><p>避免 Rancher server 發生故障倒至於無法存取 cluster</p></li><li><p>若 Rancher server 與下游的 k8s cluster 距離很遠，可以透過此 endpoint 降低延遲</p></li></ol><blockquote><p>Rancher server 可以產生讓使用者可以直連下游 k8s cluster 的 kubeconfig (建議此類的 kubeconfig 可以先匯出)</p></blockquote><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://rancher.com/docs/rancher/v2.x/en/overview/architecture/">Architecture - Rancher</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Rancher </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Rancher </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Message Queue 簡介(以 RabbitMQ 為例)</title>
      <link href="/blog/ChatOps/message-queue-concepts/"/>
      <url>/blog/ChatOps/message-queue-concepts/</url>
      
        <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>以下內容是學習線上課程 <a href="https://www.udemy.com/course/rabbitmq-and-messaging-concepts/">RabbitMQ and Messaging Concepts</a> 時所留下的學習筆記。</p><p>由於目前 Message Queue 在系統架設設計時很常出現，到底這類型的元件有什麼特性? 可以為系統帶來什麼優點? 以及其相關的特性 &amp; 運作方式 (以 RabbitMQ 為例)。以下的部份會進行說明解釋。</p><h1 id="Application-整合模式"><a href="#Application-整合模式" class="headerlink" title="Application 整合模式"></a>Application 整合模式</h1><p>在一個系統中，一般不會只有一隻程式在運作，而是會有多隻程式同時負責各種不同的任務，而程式之間難免會有互相傳遞資料進行處理的需求，而這類的需求，以下都統稱為 applcation 的整合。</p><p>而一般常見的 application 整合方式大概可以分為以下幾種：</p><h2 id="Filed-Based-Integration"><a href="#Filed-Based-Integration" class="headerlink" title="Filed Based Integration"></a>Filed Based Integration</h2><p><img src="/blog/images/middleware/message-queue_file-based-integration.png" alt="File Based Integration"></p><ul><li><p>source application 會根據需要處理的工作，產生新的檔案到特定的路徑</p></li><li><p>其他的 process application 則會是一直監控該路徑有沒有新檔案，有新的檔案則取出進行處理</p></li></ul><h2 id="Shared-Database-Integration"><a href="#Shared-Database-Integration" class="headerlink" title="Shared Database Integration"></a>Shared Database Integration</h2><p><img src="/blog/images/middleware/message-queue_shared-db-integration.png" alt="Shared Database Integration"></p><ul><li><p>source application 收到新任務時，或將資訊寫入 DB table 中</p></li><li><p>processor application 則是持續監控 DB 中的特定 table，若有新紀錄則取出進行處理</p></li><li><p>processor application 處理完工作後可能會將狀態回寫到 DB 中</p></li></ul><h2 id="Direct-Connection-Integration"><a href="#Direct-Connection-Integration" class="headerlink" title="Direct Connection Integration"></a>Direct Connection Integration</h2><p><img src="/blog/images/middleware/message-queue_direct-connection-integration.png" alt="Direct Connection Integration"></p><ul><li><p>source application 直接傳遞訊息給 processor application</p></li><li><p>可能透過 TCP/IP 或是 named pipe connection 的方式傳遞資料</p></li><li><p>不限傳遞的資料格式，連線兩端的 application 傳遞的資料格式可自訂，可能是純文字、XML or JSON</p></li></ul><h2 id="透過-Message-Broker-以非同步的方式傳遞訊息Asynchronous-Messaging"><a href="#透過-Message-Broker-以非同步的方式傳遞訊息Asynchronous-Messaging" class="headerlink" title="透過 Message Broker 以非同步的方式傳遞訊息Asynchronous Messaging"></a>透過 Message Broker 以非同步的方式傳遞訊息Asynchronous Messaging</h2><p><img src="/blog/images/middleware/message-queue_message-broker-integration.png" alt="Message Broker Based Integration"></p><ul><li><p>不限傳遞的資料格式</p></li><li><p>需要額外的 Message Queue middleware 的協助，也有可能被稱為 Message Broker 或是 Message Bus</p></li><li><p>Message Broker 搜到訊息後(來自 producer application )會轉發給 consumer application</p></li></ul><blockquote><p>從上面看得出來，其實透過 Message Broker 的方式是以上很多不同方式改良後所產生的結果</p></blockquote><h1 id="使用-Message-Queue-有什麼優點"><a href="#使用-Message-Queue-有什麼優點" class="headerlink" title="使用 Message Queue 有什麼優點?"></a>使用 Message Queue 有什麼優點?</h1><p>了解 Message Queue(Broker) 的簡單架構後，那接下來的問題可能是：到底在系統中加入 Message Queue，具體可以帶來哪些好處 or 優點呢?</p><p>以下列出幾項說明：(<code>publisher</code> 為訊息傳送者，<code>consumer</code> 為訊息接收者)</p><ul><li><p>將 publisher &amp; consumer 進行 decouple 了，因此程式開發人員可以各自專心負責規模較小 &amp; 單純的程式開發工作</p></li><li><p>publisher &amp; consumer 不需要知道雙方的實際的位置(例如：IP address)，只要將資料往 message queue 送就好</p></li><li><p>即使 consumer 短暫的無法提供服務也沒關係，message queue 可以將資料暫存起來，等待 consumer 重新上線時再送過去</p></li><li><p>比起持續 polling 的方式相對有效率的多</p></li><li><p>提供了一個可靠的方式，讓<code>訊息傳遞</code> &amp; <code>工作處理</code>兩件事情可以用非同步的方式進行</p></li><li><p>當單一 consumer 不足以完成所有工作時，可以很容易的增加 consumer 數量進行水平擴展</p></li></ul><p>從上面的優點可以看出，加入了 Message Queue，對於整體系統中各個不同元件的解耦很有幫助，同時了也帶來了水平擴展的可能性。(當然這部份也會牽涉到系統流程面上的設計，並非只透過系統架構就可以完成)</p><h1 id="使用場景範例"><a href="#使用場景範例" class="headerlink" title="使用場景範例"></a>使用場景範例</h1><p>以下就透過兩個範例說明，介紹 Message Queue 在整體系統中可以作為什麼樣的角色。</p><h2 id="Case-1"><a href="#Case-1" class="headerlink" title="Case 1"></a>Case 1</h2><p><img src="/blog/images/middleware/message-queue_case-1.png" alt="Message Queue Case 1"></p><p>從上圖可以看出，當 product 有任何更動時，需要後端資料庫的 search index 時，相關的資訊會先傳進 message queue，然後會有其他 worker(consumer) 接收進行處理。</p><h2 id="Case-2"><a href="#Case-2" class="headerlink" title="Case 2"></a>Case 2</h2><p><img src="/blog/images/middleware/message-queue_case-2.png" alt="Message Queue Case 2"></p><p>在一個電子商務網站，可能會因為以下不同的理由，以<code>非同步</code>的方式寄送 email 給會員：</p><ul><li><p>驗證 E-Mail address</p></li><li><p>重設密碼</p></li><li><p>訂單確認</p></li><li><p>促銷活動</p></li></ul><h1 id="Messaging-System-中的標準組成元素"><a href="#Messaging-System-中的標準組成元素" class="headerlink" title="Messaging System 中的標準組成元素"></a>Messaging System 中的標準組成元素</h1><p>了解 Message Queue 的功能之後，接著說明在一個 Messaging System 會包含的標準元素：</p><ul><li><p><strong>Message</strong>：這是最主要的部份，簡單來說 <strong>message 就是要從一個 application 傳遞到另一個 application 的資料</strong>，可以是很多形式，例如：command、query 或是任何事件資訊；而每個 message 會包含兩個部份，分別是 <code>routing information</code> &amp; <code>payload(實際資料)</code>。</p></li><li><p><strong>Producer/Publisher</strong>：producer 是產生 message 並將其傳到 message broker</p></li><li><p><strong>Consumer/Receiver</strong>：收取來自 message broker 的 message 並進行處理</p></li><li><p><strong>Message Queue</strong>：是個存放來自 producer 的 message list，通常一個 messaging system 中會有多個 queue，而每個 queue 都會有一個識別名稱</p></li><li><p><strong>Message Broker</strong>：將 message 從傳送者轉發到 receiver 的一個中間者</p></li><li><p><strong>Router/Exchange</strong>：根據軟體設定，決定 message 傳遞的路徑，將不同的訊息轉發進不同的 queue 中</p><blockquote><p>Routing 的概念在 RabbitMQ 中以 <code>Exchange</code> 來表示</p></blockquote></li><li><p><strong>Connection</strong>：真實的 TCP 連接，像是 producer &amp; message broker 之間的連接，或是 message broker 與 consumer 之間的連接</p></li><li><p><strong>Channel</strong>：在真實的 TCP 連接中定義出來的 virtual connection，這才是實際上 producer/consumer 與 message broker 之間的連接改念</p></li><li><p><strong>Binding</strong>：Binding 定義了 <code>Exchange</code> &amp; <code>Queue</code> 之間的關係以及訊息 routing 的設定，可能還包含了一些 filter 的設定<br><img src="/blog/images/middleware/message-queue_concept-binding.png" alt="Message Queue Case 2"></p></li></ul><h1 id="細部探究-Message-Queue-amp-Exchange-的屬性資訊"><a href="#細部探究-Message-Queue-amp-Exchange-的屬性資訊" class="headerlink" title="細部探究 Message, Queue &amp; Exchange 的屬性資訊"></a>細部探究 Message, Queue &amp; Exchange 的屬性資訊</h1><p>了解了上面關於 Messaging System 的標準組成元素之後，可以大致了解一個 Message Queue 大概是如何與外部 application 進行溝通運作的；但如果要更探究 Message Queue 內部的運作細節，可以從標準組成元素中的屬性(attribute)來進行了解。</p><blockquote><p>這部份的學習會以 <a href="https://www.rabbitmq.com/">RabbitMQ</a> 為主，因此以下屬性介紹會以 RabiitMQ 為主，但跟其他主流的 Message Queue 應該不會差異太大</p></blockquote><h2 id="Message-的屬性"><a href="#Message-的屬性" class="headerlink" title="Message 的屬性"></a>Message 的屬性</h2><ul><li><p><strong>Routing Key</strong>：用來決定訊息進入到 Messaging System 後如何被轉發的資訊</p></li><li><p><strong>Headers</strong>：key/value 的資訊集合，可用來作為訊息 routing 之後或是傳遞 publisher 想要傳遞的額外訊息</p></li><li><p><strong>Payload</strong>：實際所要傳遞的資料</p></li><li><p><strong>Publishing TimeStamp</strong>(optional)：publisher 所提供的 timestamp 資訊</p></li><li><p><strong>Expiration</strong>：Message 可停留在 Queue 中的存活時間，超過 expiration 的設定則 message 會視為 dead 而不會傳送</p></li><li><p><strong>Delivery Mode</strong>：會有 <code>persistent</code> or <code>trasient</code> 兩個選項，而 persistent 會將 message 寫入 disk 中，即使 RabiitMQ 服務重啟，訊息也都不會遺失，而 trasient 則不會</p></li><li><p><strong>Priority</strong>：message 的優先權(0~255, 這個需要 Queue 支援才可以)</p></li><li><p><strong>Message ID</strong>(optional)：由 publisher 給入，用來識別 message 的 ID 資訊</p></li><li><p><strong>Correlation ID</strong>(optional)：在 RPC 場景中用來匹配 request &amp; response 用的資訊</p></li><li><p><strong>Replay To</strong>(optional)：在 request-response 場景中會使用到的 exchange 或是 queue 的名稱</p></li></ul><h2 id="Queue-的屬性"><a href="#Queue-的屬性" class="headerlink" title="Queue 的屬性"></a>Queue 的屬性</h2><ul><li><p><strong>Name</strong>：唯一的名稱，最多 255 個字元的 UTF-8 字串</p></li><li><p><strong>Durable</strong>：指定在 RabbitMQ 重啟後保留 or 移除 Queue 的依據</p></li><li><p><strong>Auto Delete</strong>：沒有任何訂閱者的情況下是否自動移除</p></li><li><p><strong>Exclusive</strong>：只服務特定一個 connection，一旦該 connection 斷掉就會移除 Queue</p></li><li><p><strong>Max Length</strong>：最多可以停留在 Queue 中的 message 數量，可以指定若超過會從最舊的訊息開始移除或是拒絕新的 message 進入</p></li><li><p><strong>Max Priority</strong>：priority 可設定的最大值</p></li><li><p><strong>Message TTL</strong>：存入到 Queue 中的 Message 的 TTL(若是 Message &amp; Queue 都有 TTL 的設定，會以較低的為主)</p></li><li><p><strong>Dead-letter Exchange</strong>：可用來指定過期 or 被丟棄的 message 被自動傳送到某一個 exchange</p></li><li><p><strong>Binding Configuration</strong>：Queue &amp; Exchange 之間的關聯資訊 (每個 Queue 都必須與某個 Exchange 綁定，確保可以從 Exchange 取得 message)</p></li></ul><h2 id="Exchange-的屬性"><a href="#Exchange-的屬性" class="headerlink" title="Exchange 的屬性"></a>Exchange 的屬性</h2><ul><li><p><strong>Name</strong>：Exchange 的名稱(必須唯一)</p></li><li><p><strong>Type</strong>：Exchange 的運作類型，會是 <code>Fanout</code>, <code>Direct</code>, <code>Topic</code>, <code>Headers</code> 四種之一 </p></li><li><p><strong>Durability</strong>：與 Queue Durability 相同，用來決定在 RabbitMQ 服務重啟後會不會依然存在</p></li><li><p><strong>Auto Delete</strong>：設定是否在沒有任何 Queue 綁定的情況下，就自動移除</p></li><li><p><strong>Internal</strong>：若設定為 Internal，表示僅能接收來自其他 exchange 的 message，無法接收來自 publisher 的 message</p></li><li><p><strong>Alternate Exchange</strong>：指定無法 route 的 message 的去向</p></li><li><p>**Other Arguments(x-arguments)**：Exchange 其他的 metadata，可能會用於其他的 plugin 中</p></li></ul><h1 id="Exchange"><a href="#Exchange" class="headerlink" title="Exchange"></a>Exchange</h1><p>要切入 RabbitMQ 之前，<code>Exchange</code> 的概念是必須先了解的，以下是 RabbitMQ 的內部架構圖： </p><p><img src="/blog/images/middleware/message-queue_concept-binding.png" alt="RabbitMQ Exchange"></p><ul><li><p>Exchange 是 RabbitMQ 系統中負責轉發訊息的元件</p></li><li><p>Message Producer 無法將訊息直接傳到 Queue 中，在 RabbitMQ 中訊息的第一個進入點是 Exchange</p></li><li><p>實際儲存訊息的 Queue 會根據使用者的設定，與不同的 Exchange 進行綁定</p></li><li><p>當 Exchange 收到訊息後，就會轉發到與其綁定的 Queue (可能 0 到多個不等)</p></li><li><p>Exchange 僅能將訊息轉發到與其綁定的 Queue 上</p></li><li><p>Exchange 有四種轉發模式，分別是 <code>Fanout</code>, <code>Direct</code>, <code>Topic</code>, <code>Headers</code></p></li><li><p>至少會有一個預設 Exchange 存在於 RabbitMQ 系統中，稱為 <strong>default exchange</strong>，轉發的模式為 <code>direct</code>；每個新建立的 Queue，若是沒指定 exchane 資訊，就會與預設的綁定</p></li></ul><h1 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h1><p>透過上面關於 Messaging System 的標準組成元素說明，可以了解到一個 Message Queue 在整體系統上所扮演的角色；加上內部組件的屬性(attribute)說明，也大致可以推敲出一個 Messaging System(以 RabbitMQ 為主) 內部運作的狀況。</p><p>因此在實際使用上，我們可以利用 Message Queue 有效的將 application 進行解耦，讓大問題拆分為小問題，並且讓 developer 可以專注在特定的系統功能上，讓 application 面對的，就是 Message Queue 而已。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/rabbitmq-and-messaging-concepts">Free RabbitMQ Tutorial - RabbitMQ and Messaging Concepts | Udemy</a></p></li><li><p><a href="https://www.rabbitmq.com/documentation.html">Documentation: Table of Contents — RabbitMQ</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Message Queue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Message Queue </tag>
            
            <tag> RabbitMQ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[ChatOps] 簡單介紹如何開發一個 Telegram Bot</title>
      <link href="/blog/ChatOps/howto-develop-telegram-chatbot/"/>
      <url>/blog/ChatOps/howto-develop-telegram-chatbot/</url>
      
        <content type="html"><![CDATA[<h1 id="注意事項"><a href="#注意事項" class="headerlink" title="注意事項"></a>注意事項</h1><ul><li><p>要透過 <code>@botfather</code> 來新增 bot(使用 <code>/newbot</code> 命令)</p></li><li><p>要讓 bot 可以在 group 裡面可以回應，要透過 <code>/mybots</code> -&gt; <strong>選擇 bot</strong> -&gt; <code>Bot Settings</code> -&gt; <code>Group Privacy</code> -&gt; 按下 <code>Turn Off</code>(畫面顯示 <strong>Privacy mode is disabled for xxxx</strong> 就表示啟用了)</p></li><li><p>開啟 bot inline query 功能，要透過 <code>/mybots</code> -&gt; <strong>選擇 bot</strong> -&gt; <code>Bot Settings</code> -&gt; <code>Inline Mode</code> -&gt; 按下 <code>Turn on</code>(畫面顯示 <strong>Inline mode is currently enabled for xxxx</strong> 就表示啟用了)</p></li><li><p>要設定 bot 使用提示功能，要透過 <code>/mybots</code> -&gt; <strong>選擇 bot</strong> -&gt; <code>Bot Settings</code> -&gt; <code>Inline Mode</code> -&gt; 按下 <code>Edit inline placeholder</code>，輸入使用提示即可 (例如：<code>&lt;type&gt; &lt;search query&gt;</code>)</p></li><li><p>安裝 <code>nodemon</code>(安裝方法 <code>npm install nodemon -g</code>)，當程式修改完後儲存，系統就會自動重新協助啟動程式</p></li></ul><h1 id="基本觀念"><a href="#基本觀念" class="headerlink" title="基本觀念"></a>基本觀念</h1><h2 id="use"><a href="#use" class="headerlink" title="use"></a>use</h2><p>所有 bot 收到的命令都會經過 <code>use</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bot.use((ctx) &#x3D;&gt; &#123;</span><br><span class="line">    ctx.reply(&quot;You used the bot&quot;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="middleware"><a href="#middleware" class="headerlink" title="middleware"></a>middleware</h2><p>先搞清楚什麼是 <a href="https://telegraf.js.org/#/?id=middleware">middleware</a>，簡單範例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const bot &#x3D; new Telegraf(process.env.BOT_TOKEN)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;這是一個 middleware</span><br><span class="line">bot.use(async (ctx, next) &#x3D;&gt; &#123;</span><br><span class="line">  const start &#x3D; new Date()</span><br><span class="line">  await next()</span><br><span class="line">  const ms &#x3D; new Date() - start</span><br><span class="line">  console.log(&#39;Response time: %sms&#39;, ms)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;這是另一個 middleware</span><br><span class="line">bot.on(&#39;text&#39;, (ctx) &#x3D;&gt; ctx.reply(&#39;Hello World&#39;))</span><br><span class="line"></span><br><span class="line">bot.launch()</span><br></pre></td></tr></table></figure><p><img src="/blog/images/chatbot/telegraf-middleware.png" alt="Telegram Middleware"></p><ul><li><p>當 bot 收到從 telegram channel 來的 request 之後，會經過一連串設定在 bot 上的 middleware(如上圖所示)，最後再回傳給 telegram channel</p></li><li><p>透過 <code>next</code> function 可以將 request 傳遞給下一個 middleware</p></li></ul><h2 id="state"><a href="#state" class="headerlink" title="state"></a>state</h2><p>要在不同的 middleware 之間共享訊息，不需要靠全域變數，透過 <code>[State](https://telegraf.js.org/#/?id=state)</code> 就可以搞定</p><h2 id="context-shortcut"><a href="#context-shortcut" class="headerlink" title="context shortcut"></a>context shortcut</h2><p>透過 context shortcut，可以不用繞一大圈用 telegram API，省略帶入 chatid 的部份</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&#39;start&#39;, ctx &#x3D;&gt; &#123;</span><br><span class="line">  ctx.reply(&quot;Hello World&quot;);</span><br><span class="line"></span><br><span class="line">  bot.telegram.sendMessage(ctx.chat.id, &quot;Hello World&quot;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Echo-Bot"><a href="#Echo-Bot" class="headerlink" title="Echo Bot"></a>Echo Bot</h1><h2 id="help-command"><a href="#help-command" class="headerlink" title="help command"></a>help command</h2><p>若要提供給使用者一個 command 的清單可供參考，可以透過 helper command 的方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">const helpMessage &#x3D; &#96;</span><br><span class="line">Say something to me</span><br><span class="line">&#x2F;start - start the bot</span><br><span class="line">&#x2F;help - command reference</span><br><span class="line">&#96;;</span><br><span class="line"></span><br><span class="line">bot.start((ctx) &#x3D;&gt; &#123;</span><br><span class="line">  ctx.reply(&quot;Hi, I am Echo bot!&quot;);</span><br><span class="line">  ctx.reply(helpMessage);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">bot.help((ctx) &#x3D;&gt; &#123;</span><br><span class="line">  ctx.reply(helpMessage);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="echo"><a href="#echo" class="headerlink" title="echo"></a>echo</h2><p>telegraf 套件中並沒有對使用者輸入進行額外處理，所以要處理較為複雜的 command，就必須要自己開發相關邏輯</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;echo&quot;, (ctx) &#x3D;&gt; &#123;</span><br><span class="line">  let input &#x3D; ctx.message.text;</span><br><span class="line">  let inputArray &#x3D; input.split(&quot; &quot;);</span><br><span class="line">  console.log(inputArray);</span><br><span class="line"></span><br><span class="line">  let message &#x3D; &quot;&quot;;</span><br><span class="line">  if(inputArray.length &#x3D;&#x3D; 1) &#123;</span><br><span class="line">    message &#x3D; &quot;You said &#x2F;echo&quot;;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    inputArray.shift();</span><br><span class="line">    message &#x3D; inputArray.join(&quot; &quot;);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ctx.reply(message);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h2><p>透過 <code>use</code>(所有的 command 都會經過 <code>use</code> middleware) 可以設計出 logging 的機制，讓使用者所有的 command 都會發到另外一個 group channel 做紀錄</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bot.use((ctx, next) &#x3D;&gt; &#123;</span><br><span class="line">    &#x2F;&#x2F;可在 group channel 中傳訊息，並從 ctx.chat.id 中取得 chatid</span><br><span class="line">    if(ctx.updateSubTypes[0] &#x3D;&#x3D; &quot;text&quot;) &#123;</span><br><span class="line">        bot.telegram.sendMessage(-325114827, ctx.from.username + &quot; said &quot; + ctx.message.text);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        bot.telegram.sendMessage(-325114827, ctx.from.username + &quot; sent &quot; + ctx.updateSubTypes[0]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;完成 log 功能後，繼續將 context 往下傳給其他 middleware</span><br><span class="line">    next();</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Media-Bot"><a href="#Media-Bot" class="headerlink" title="Media Bot"></a>Media Bot</h1><h2 id="傳送檔案"><a href="#傳送檔案" class="headerlink" title="傳送檔案"></a>傳送檔案</h2><p>在範例中會以傳送圖片來進行說明，以下說明幾個重點：</p><ul><li><p>Telegram 物件(來自於 <code>(new Telegraf()).telegram</code>)傳送物件是透過 <a href="https://telegraf.js.org/#/?id=sendphoto">sendPhoto</a> 方法來完成</p></li><li><p>在 telegraf 中處理 file 的方式有五種(<a href="https://telegraf.js.org/#/?id=working-with-files">官網說明</a>)</p></li><li><p>在 <code>sendPhoto</code> 中的 <code>File</code> type，就是上述的五種方法其中之一</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;test&quot;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    bot.telegram.sendPhoto(ctx.chat.id, &quot;https:&#x2F;&#x2F;pgw.udn.com.tw&#x2F;gw&#x2F;photo.php?u&#x3D;https:&#x2F;&#x2F;uc.udn.com.tw&#x2F;photo&#x2F;2019&#x2F;11&#x2F;27&#x2F;99&#x2F;7117740.jpg&amp;x&#x3D;0&amp;y&#x3D;0&amp;sw&#x3D;0&amp;sh&#x3D;0&amp;sl&#x3D;W&amp;fw&#x3D;1050&quot;);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;file path</span><br><span class="line">    bot.telegram.sendPhoto(ctx.chat.id, &#123;</span><br><span class="line">        source: &quot;res&#x2F;london.jpg&quot;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;file id</span><br><span class="line">    &#x2F;&#x2F;這個部份可透過 &quot;on&quot; middleware，並自行上傳圖片後，透過 ctx.message.photo.file_id 取得正確的 file id</span><br><span class="line">    &#x2F;&#x2F;並在 sendPhoto 中作為第二個參數使用</span><br><span class="line">    &#x2F;&#x2F;(每一張上傳到 telegram 的圖片都會有獨一無二的 file id)</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="針對使用者訊息進行回覆"><a href="#針對使用者訊息進行回覆" class="headerlink" title="針對使用者訊息進行回覆"></a>針對使用者訊息進行回覆</h2><p>這個部份需要透過 <code>ctx.message.message_id</code> 取得使用者訊息的 ID 並進行回覆</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;newyork&quot;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    bot.telegram.sendPhoto(ctx.chat.id, &#123;</span><br><span class="line">        source: &quot;res&#x2F;newyork.jpg&quot;</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        reply_to_message_id: ctx.message.message_id</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="chat-action"><a href="#chat-action" class="headerlink" title="chat action"></a>chat action</h2><p>透過 <a href="https://core.telegram.org/bots/api#sendchataction">chat action</a> 可以在 telegram 的界面上告知使用者目前 bot 正在進行的動作，可視為是一種狀態資訊。</p><blockquote><p>訊息只會五秒或是更短，一旦訊息成功送給使用者後，chat action 的訊息就會消失</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;newyork&quot;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    bot.telegram.sendChatAction(ctx.chat.id, &quot;upload_photo&quot;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="傳送多張圖片"><a href="#傳送多張圖片" class="headerlink" title="傳送多張圖片"></a>傳送多張圖片</h2><p>要同時傳送多張圖片可透過 <code>[sendMediaGroup](https://telegraf.js.org/#/?id=sendmediagroup)</code> 方法，並將圖片以 array 的形式傳入。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;cities&quot;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    let cities &#x3D; [&#39;res&#x2F;dubai.jpg&#39;, &#39;res&#x2F;hongkong.jpg&#39;, &#39;res&#x2F;london.jpg&#39;, &#39;res&#x2F;newyork.jpg&#39;, &#39;res&#x2F;singapore.jpg&#39;]</span><br><span class="line">    let result &#x3D; cities.map(city &#x3D;&gt; &#123;</span><br><span class="line">        return &#123;</span><br><span class="line">            type: &#39;photo&#39;, </span><br><span class="line">            media: &#123; source: city &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    bot.telegram.sendMediaGroup(ctx.chat.id, result);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="傳送文件"><a href="#傳送文件" class="headerlink" title="傳送文件"></a>傳送文件</h2><p>傳送文件可以同時加上縮圖。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;citieslist&quot;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    bot.telegram.sendDocument(ctx.chat.id, &#123;</span><br><span class="line">        source: &quot;res&#x2F;citieslist.txt&quot;</span><br><span class="line">    &#125;, &#123;</span><br><span class="line">        thumb: &#123; source: &quot;res&#x2F;dubai.jpg&quot; &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="互動式鍵盤"><a href="#互動式鍵盤" class="headerlink" title="互動式鍵盤"></a>互動式鍵盤</h1><h2 id="產生-inline-keyboard"><a href="#產生-inline-keyboard" class="headerlink" title="產生 inline keyboard"></a>產生 inline keyboard</h2><p>要使用 telegram keyboard 功能，就必須要使用 <code>sendMessage</code> 而非單純的 <code>reply</code>(沒有 keyboard 的參數可用) 來產生</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;test&quot;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    ctx.telegram.sendMessage(ctx.chat.id, &#39;Welcome&#39;, &#123;</span><br><span class="line">        reply_markup: &#123;</span><br><span class="line">            inline_keyboard: [</span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &quot;one&quot;, callback_data: &#39;one&#39; &#125;, </span><br><span class="line">                    &#123; text: &quot;one&quot;, callback_data: &#39;one&#39; &#125;</span><br><span class="line">                ], </span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &quot;one&quot;, callback_data: &#39;one&#39; &#125;</span><br><span class="line">                ]</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="點選-keyboard-後進行回應"><a href="#點選-keyboard-後進行回應" class="headerlink" title="點選 keyboard 後進行回應"></a>點選 keyboard 後進行回應</h2><p>回應 keyboard event 必須使用 <code>action</code> 方法，並根據在 keyboard 中設定的 callback data 來設定處理的邏輯</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;test&quot;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    ctx.telegram.sendMessage(ctx.chat.id, &#39;Welcome&#39;, &#123;</span><br><span class="line">        reply_markup: &#123;</span><br><span class="line">            inline_keyboard: [</span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &quot;Click me&quot;, callback_data: &#39;one&#39; &#125;</span><br><span class="line">                ], </span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">bot.action(&#39;one&#39;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    &#x2F;&#x2F; 透過 answerCbQuery 可以用來回應處理邏輯已經完成</span><br><span class="line">    ctx.answerCbQuery(&#39;Job has done!&#39;);</span><br><span class="line">    ctx.reply(&quot;You clicked the button&quot;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="來回選單的開發"><a href="#來回選單的開發" class="headerlink" title="來回選單的開發"></a>來回選單的開發</h2><p>若要開發來回選單的效果，建議透過刪除 message 的方式來進行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">bot.command(&quot;test&quot;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    ctx.telegram.sendMessage(ctx.chat.id, &#39;Main Menu&#39;, &#123;</span><br><span class="line">        reply_markup: &#123;</span><br><span class="line">            inline_keyboard: [</span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &quot;See Fruits List&quot;, callback_data: &#39;fruits&#39; &#125;</span><br><span class="line">                ], </span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &quot;See Meats List&quot;, callback_data: &#39;meats&#39; &#125;</span><br><span class="line">                ]</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">bot.action(&#39;fruits&#39;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    ctx.deleteMessage();</span><br><span class="line">    ctx.telegram.sendMessage(ctx.chat.id, &#39;List of fruits:\n- Apples\n- Oranges\n- Pears&#39;, &#123;</span><br><span class="line">        reply_markup: &#123;</span><br><span class="line">            inline_keyboard: [</span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &quot;Back to menu&quot;, callback_data: &#39;menu&#39; &#125;</span><br><span class="line">                ]</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">bot.action(&#39;menu&#39;, ctx &#x3D;&gt; &#123;</span><br><span class="line">    ctx.deleteMessage();</span><br><span class="line">    ctx.telegram.sendMessage(ctx.chat.id, &#39;Main Menu&#39;, &#123;</span><br><span class="line">        reply_markup: &#123;</span><br><span class="line">            inline_keyboard: [</span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &quot;See Fruits List&quot;, callback_data: &#39;fruits&#39; &#125;</span><br><span class="line">                ], </span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &quot;See Meats List&quot;, callback_data: &#39;meats&#39; &#125;</span><br><span class="line">                ]</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Inline-Query"><a href="#Inline-Query" class="headerlink" title="Inline Query"></a>Inline Query</h1><h2 id="原理-amp-流程說明"><a href="#原理-amp-流程說明" class="headerlink" title="原理 &amp; 流程說明"></a>原理 &amp; 流程說明</h2><p>以下透過 <strong>Telegram Inline Query Flow</strong> 來說明整體開發流程：</p><p><img src="/blog/images/chatbot/telegraf-inline_query_flow.png" alt="Telegram Inline Query Result"></p><p>首先使用者可以在 telegram 界面中透過 <code>@bot [search term]</code> 的方式觸發 inline query。</p><p>接著 inline query 會被送到 bot 端的程式(這個部份就是自己開發的 bot code)進行處理，若是透過 [nodejs telegraf][<a href="https://telegraf.js.org/]">https://telegraf.js.org/]</a> 套件，則以透過 <code>context.inlineQuery(&#39;search_term&#39;, ()=&gt;&#123;&#125;)</code> or <code>context.on(&#39;inline_query&#39;, ()=&gt;&#123;&#125;)</code> 來進行處理。</p><blockquote><p>其中 <code>on</code> 是 <a href="https://telegraf.js.org/#/?id=middleware">middleware</a>，<code>inline_query</code> 則是 <a href="https://telegraf.js.org/#/?id=update-types">update type</a></p></blockquote><p>最後開發者可以根據攔截到的訊息，經過處理之後，回應 <a href="https://core.telegram.org/bots/api#inlinequeryresult">Inline Query Result</a>，而在 telegram 中可使用多達 20 種的 Inline Query Result：</p><p><img src="/blog/images/chatbot/telegraf-inline_query_result.png" alt="Telegram Inline Query Result"></p><blockquote><p>需要注意的是，一般透過 <code>@botfather</code> 新增的 bot 預設是不會啟動 Inline Query Mode，必須在 Bot Settings 中開啟才行。</p></blockquote><h2 id="搜尋範例"><a href="#搜尋範例" class="headerlink" title="搜尋範例"></a>搜尋範例</h2><p>以 <code>InlineQueryResultCachedPhoto</code> 回應 inline query 範例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;使用 middleware &quot;on&quot; + update type &quot;inline_query&quot; 來攔截事件</span><br><span class="line">bot.on(&#39;inline_query&#39;, async ctx &#x3D;&gt; &#123;</span><br><span class="line">    &#x2F;&#x2F;透過 context 的 inlineQuery.query 可以取得使用者輸入的資料</span><br><span class="line">    let query &#x3D; ctx.inlineQuery.query;</span><br><span class="line">    &#x2F;&#x2F;以下則是呼叫 pixabay API 的範例</span><br><span class="line">    let res &#x3D; await axios.get(&#96;https:&#x2F;&#x2F;pixabay.com&#x2F;api&#x2F;?key&#x3D;$&#123;apikey&#125;&amp;q&#x3D;$&#123;query&#125;&#96;);</span><br><span class="line">    </span><br><span class="line">    let data &#x3D; res.data.hits;</span><br><span class="line">    &#x2F;&#x2F;type&#x3D;&#39;photo&#39; 表示回應 InlineQueryResultCachedPhoto</span><br><span class="line">    &#x2F;&#x2F;回應內容可以用 markdown 的格式回應</span><br><span class="line">    let results &#x3D; data.map((item, index) &#x3D;&gt; &#123;</span><br><span class="line">        return &#123;</span><br><span class="line">            type: &#39;photo&#39;, </span><br><span class="line">            id: String(index), </span><br><span class="line">            photo_url: item.webformatURL, </span><br><span class="line">            thumb_url: item.previewURL, </span><br><span class="line">            photo_width: 300, </span><br><span class="line">            photo_height: 200, </span><br><span class="line">            caption: &#96;[Source]($&#123;item.webformatURL&#125;)\n[Large Iamge]($&#123;item.largeImageURL&#125;)&#96;, </span><br><span class="line">            parse_mode: &#39;Markdown&#39;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;透過 context 的 answerInlineQuery 可以直接回應 inline query 而不需要帶上 query id 參數</span><br><span class="line">    ctx.answerInlineQuery(results);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><p>以 <code>InlineQueryResultArticle</code> 回應 inline query 範例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">bot.on(&#39;inline_query&#39;, async ctx &#x3D;&gt; &#123;</span><br><span class="line">    let query &#x3D; ctx.inlineQuery.query;</span><br><span class="line">    let res &#x3D; await axios.get(&#96;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;w&#x2F;api.php?action&#x3D;opensearch&amp;format&#x3D;json&amp;search&#x3D;$&#123;query&#125;&amp;limit&#x3D;5&#96;);</span><br><span class="line"></span><br><span class="line">    let data &#x3D; res.data;</span><br><span class="line">    let allTitles &#x3D; data[1];</span><br><span class="line">    let allLinks &#x3D; data[3];</span><br><span class="line"></span><br><span class="line">    if(allTitles &#x3D;&#x3D; undefined) &#123;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    let results &#x3D; allTitles.map((item, index) &#x3D;&gt; &#123;</span><br><span class="line">        return &#123;</span><br><span class="line">            type: &#39;article&#39;, </span><br><span class="line">            id: String(index), </span><br><span class="line">            title: item, </span><br><span class="line">            input_message_content: &#123;</span><br><span class="line">                message_text: &#96;$&#123;item&#125;\n$&#123;allLinks[index]&#125;&#96;</span><br><span class="line">            &#125;, </span><br><span class="line">            description: allLinks[index]</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    ctx.answerInlineQuery(results);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h2 id="透過-switch-inline-query-current-chat-提供-template-command"><a href="#透過-switch-inline-query-current-chat-提供-template-command" class="headerlink" title="透過 switch_inline_query_current_chat 提供 template command"></a>透過 switch_inline_query_current_chat 提供 template command</h2><p>為了方便使用者，可以透過按鈕的方式進行動作而非純敲鍵盤的方式，可以使用 callback function 中，使用 <code>reply_markup</code> + <code>inline_keyboard</code> + <code>switch_inline_query_current_chat</code> 來達成此目的，以下是個簡單範例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;使用者輸入 &#x2F;start 或是 &#x2F;help 之後，就會出現兩個搜尋按鈕</span><br><span class="line">bot.command([&#39;start&#39;, &#39;help&#39;], ctx &#x3D;&gt; &#123;</span><br><span class="line">    let message &#x3D; &#96;</span><br><span class="line">Welcome to Search Bot!</span><br><span class="line">Use the inline mode below</span><br><span class="line">@godleon_test_bot p &lt;search image&gt;</span><br><span class="line">@godleon_test_bot w &lt;search wiki&gt;</span><br><span class="line">&#96;;</span><br><span class="line">    &#x2F;&#x2F;按下第一個按鈕，會自動在使用者的訊息輸入欄中填入 &quot;@bot_id p&quot;</span><br><span class="line">    &#x2F;&#x2F;按下第一個按鈕，會自動在使用者的訊息輸入欄中填入 &quot;@bot_id w&quot;</span><br><span class="line">    ctx.reply(message, &#123;</span><br><span class="line">        reply_markup: &#123;</span><br><span class="line">            inline_keyboard: [</span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &#39;Search Pixabay Image&#39;, switch_inline_query_current_chat: &#39;p &#39; &#125;</span><br><span class="line">                ], </span><br><span class="line">                [</span><br><span class="line">                    &#123; text: &#39;Search Wiki&#39;, switch_inline_query_current_chat: &#39;w &#39; &#125;</span><br><span class="line">                ]</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h1 id="Polling-Mode-v-s-Webhook"><a href="#Polling-Mode-v-s-Webhook" class="headerlink" title="Polling Mode v.s. Webhook"></a>Polling Mode v.s. Webhook</h1><h2 id="Polling-Mode"><a href="#Polling-Mode" class="headerlink" title="Polling Mode"></a>Polling Mode</h2><p>當一個 telegram bot 被建立時，預設設定使用的是 polling mode，而此模式的行為如下：</p><ol><li><p>bot 程式會不斷的去詢問 telegram server 目前有沒有新的訊息</p></li><li><p>若是沒有新訊息，telegram server 則回應沒有，bot 程式也就不會做任何事情</p></li><li><p>一旦使用者輸入訊息，下次 bot 程式來詢問時，telegram server 就會回應該訊息</p></li><li><p>bot 程式收到新的訊息，進行處理</p></li></ol><p>而 polling mode 有以下幾個優點：</p><ul><li><p>開發方便，開發者只要確定程式運作環境可以連上網路就可以進行開發</p></li><li><p>佈署也同時方便，放在任何可以連上網路的環境即可</p></li></ul><p>但是從上面的流程看來，polling mode 的缺點也顯而易見，<strong>這其實蠻浪費資源的，會對 telegram server 產生很多詢問的 polling</strong>，整體來說並不是一個有效率的資源使用方法。</p><h2 id="Webhook"><a href="#Webhook" class="headerlink" title="Webhook"></a>Webhook</h2><p>然而上面的問題可以改用 webhook 來解決，而 webhook 模式行為就很簡單：</p><ol><li><p>bot 不會主動去問 telegram server 任何訊息</p></li><li><p>當 telegram server 收到訊息後，主動通知 bot</p></li><li><p>bot 收到訊息後進行處理</p></li></ol><p>可以看出上面的流程比起 polling mode 有相當大的改善，但使用 webhook 需要提供一個 telegram server 可以存取到的 public URL，但這個部份也不難，可以考慮以下項目：</p><ul><li><p><a href="https://aws.amazon.com/tw/api-gateway/">AWS API Gateway</a></p><blockquote><p>若是 bot 程式需要取用外部公有資源(例如上面範例的 pixabay &amp; wiki)，那就適合放在這裡</p></blockquote></li><li><p><a href="https://ngrok.com/">ngrok</a></p><blockquote><p>若是要存取的資源都在內部，那就用這個吧(ngrok 可以提供一個 public URL，並從 ngrok server 建立一個 network tunnel 到內部程式)，對於開發者開發也方便</p></blockquote></li></ul><p>而開啟 <a href="https://core.telegram.org/bots/api#setwebhook">bot wehbook mode</a>，則是要使用以下 template URL 來完成：</p><blockquote><p><a href="https://api.telegram.org/bot">https://api.telegram.org/bot</a><token>/setWebhook?url=<endpoint></p></blockquote><p>假如設定如下：</p><ul><li><p>Bot token：<code>808328592:AAFFUBCK2g7olBWpncLlfsrLh1AQee_sIzk</code></p></li><li><p>public URL：<code>https://3bao3o4fy3.execute-api.us-east-1.amazonaws.com/v1</code></p></li></ul><p>那設定 telegram bot webhook 就可以使用下列網址：</p><blockquote><p><a href="https://api.telegram.org/bot808328592:AAFFUBCK2g7olBWpncLlfsrLh1AQee_sIzk/setWebhook?url=https://3bao3o4fy3.execute-api.us-east-1.amazonaws.com/v1">https://api.telegram.org/bot808328592:AAFFUBCK2g7olBWpncLlfsrLh1AQee_sIzk/setWebhook?url=https://3bao3o4fy3.execute-api.us-east-1.amazonaws.com/v1</a></p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.udemy.com/course/build-telegram-bots-with-javascript-the-complete-guide">Build Telegram Bots with JavaScript: The Complete Guide | Udemy</a></p></li><li><p><a href="https://telegraf.js.org/">Telegraf: Modern Telegram Bot Framework for Node.js</a></p></li><li><p><a href="https://core.telegram.org/bots">Telegram Bots: An introduction for developers</a></p></li><li><p><a href="https://core.telegram.org/api">Telegram APIs</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> ChatOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
            <tag> ChatOps </tag>
            
            <tag> Telegram </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Elasticsearch] 分散式特性 &amp; 分散式搜尋的機制</title>
      <link href="/blog/Elasticsearch/Elasticsearch-distributed-mechanism/"/>
      <url>/blog/Elasticsearch/Elasticsearch-distributed-mechanism/</url>
      
        <content type="html"><![CDATA[<h1 id="集群分佈式模型及選主與腦裂問題"><a href="#集群分佈式模型及選主與腦裂問題" class="headerlink" title="集群分佈式模型及選主與腦裂問題"></a>集群分佈式模型及選主與腦裂問題</h1><h2 id="分散式特性"><a href="#分散式特性" class="headerlink" title="分散式特性"></a>分散式特性</h2><p>Elasticsearch 的分散式架構帶來以下優點：</p><ul><li><p>可以水平擴展儲存空間，支援 PB 等級的資料儲存</p><blockquote><p>可以根據 request &amp; data 增加的需求進行 scale out；資料分散儲存，因此在 storage 的部份同樣也是可以 scale out 的</p></blockquote></li><li><p>提供系統高可用性(HA)，當某些節點停止服務時，整個 cluster 的服務不會受影響</p><ul><li>Service HA：若有 node 停止服務，整個 cluster 還是可以提供服務</li><li>Data HA：若有 node 掛掉，資料不會遺失</li></ul></li></ul><p>關於設定 Elasticsearch cluster：</p><ul><li><p>不同的 cluster 透過不同的名字區分，預設為 <code>elasticsearch</code></p></li><li><p>cluster name 可以透過設定檔修改，也可以在啟動指令中指定 <code>-E cluster.name=[CLUSTER_NAME]</code> 進行設定</p></li></ul><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><ul><li><p>Node 就是一個 Elasticsearch 的 Java process；基本上一台機器上可以同時運行多個 Elasticsearch process，但 production 使用建議還是只要一個就好</p></li><li><p>每個 node 都有名稱，可透過設定檔配置，也可以在啟動時透過 <code>-E node.name=[NODE_NAME]</code> 進行設定</p></li><li><p>每個 node 啟動之後都會分配一個 UID，並儲存在 <code>/usr/share/elasticsearch/data</code> 目錄下</p></li></ul><h2 id="Coordinating-Node"><a href="#Coordinating-Node" class="headerlink" title="Coordinating Node"></a>Coordinating Node</h2><ul><li><p>處理 request 的 node 稱為 <strong>Coordinating Node</strong>，其功能是將 request 轉發到正確的 node 上(例如：建立 index 的 request 要轉發給 master node)</p></li><li><p>所有的 node 都預設是 Coordinating Node</p></li><li><p>可以透過將其他 node type 都設定為 <code>false</code>，這樣就可以讓特定的 node 變成 dedicated coordinating node</p></li></ul><h2 id="Data-Node"><a href="#Data-Node" class="headerlink" title="Data Node"></a>Data Node</h2><ul><li><p>可以保存資料的 node，每個 node 啟動後都會預設是 data node，可以透過設定 <code>node.data: false</code> 停用 data node 功能</p></li><li><p>用來保存分片資料，實現資料的 scalibility (由 master node 決定如何把分片分發到不同的 data node 上)</p></li><li><p>透過增加 data node 可以解決資料水平擴展 &amp; 解決單點故障導致資料遺失的問題</p></li></ul><h2 id="Master-Node"><a href="#Master-Node" class="headerlink" title="Master Node"></a>Master Node</h2><ul><li><p>master node 用來處理以下工作：</p><ul><li><p>處理建立/刪除 index 的 request，並實際執行</p></li><li><p>決定每個 shard 要被分配到哪個 data node 上</p></li><li><p>維護 &amp; 更新 cluster state</p></li></ul></li><li><p>master node 配置的 best practice：</p><ul><li><p>master node 很重要，佈署上要考慮避免單點故障的狀況發生</p></li><li><p>為 cluster 設置多個 master node，且必須是 dedicated master node</p></li></ul></li></ul><ul><li><p>Master Eligible Node &amp; 選舉流程</p><ul><li><p>cluster 中可以設定多個 master eligible node，當 master node 發生問題時，這些 master eligible node 就會開始選舉流程，選出下一個 msater node</p></li><li><p>每個 node 啟動時就預設是一個 master eligible node，可以透過設定 <code>node.master: false</code> 取消此預設設定</p></li><li><p>當 cluster 中的第一個 master eligible node 啟動時，就會把自己選舉為 master node</p></li></ul></li></ul><h2 id="Cluster-State"><a href="#Cluster-State" class="headerlink" title="Cluster State"></a>Cluster State</h2><ul><li><p>cluster state 維護了一個 cluster 中必要的訊息，包含以下內容：</p><ul><li><p>所有的 node 資訊</p></li><li><p>所有的 index &amp; 相對應的 mapping/setting 配置</p></li><li><p>shard 的路由資訊</p></li></ul></li><li><p>每個 node 上都保存了 cluster state</p></li><li><p>只有 master 才可以修改 cluster state 並負責同步給其他 node (若是每個 node 都可以修改 cluster state，很有可能會導致 cluster state 不一致的狀況發生)</p></li></ul><h2 id="配置不同的-node-type"><a href="#配置不同的-node-type" class="headerlink" title="配置不同的 node type"></a>配置不同的 node type</h2><p>以下是要設定不同的 node type 時所會用到的設定參數：</p><table><thead><tr><th>Node Type</th><th>配置參數</th><th>預設值</th></tr></thead><tbody><tr><td>Master Eligible</td><td><code>node.master</code></td><td>true</td></tr><tr><td>Data</td><td><code>node.data</code></td><td>true</td></tr><tr><td>Ingest</td><td><code>node.ingest</code></td><td>true</td></tr><tr><td>Dedicated Coordinating</td><td><code>無</code></td><td>設置上面三個參數皆為 false</td></tr><tr><td>Machine Learning</td><td><code>node.ml</code></td><td>true (需要 enable x-pack)</td></tr></tbody></table><h1 id="Shard-amp-Cluster-的故障轉移"><a href="#Shard-amp-Cluster-的故障轉移" class="headerlink" title="Shard &amp; Cluster 的故障轉移"></a>Shard &amp; Cluster 的故障轉移</h1><p><img src="/blog/images/Elasticsearch/es_shard-allocation.png" alt="Elasticsearch - shard allocation status"></p><h2 id="Primary-Shard-提昇系統儲存容量"><a href="#Primary-Shard-提昇系統儲存容量" class="headerlink" title="Primary Shard (提昇系統儲存容量)"></a>Primary Shard (提昇系統儲存容量)</h2><ul><li><p>shard 是 Elasticsearch 分散式儲存的基礎，包含 primary shard &amp; replica shard</p></li><li><p>primary shard 功能是將一份被索引後的資料，分散到多個 data node 上存放，實現儲存方面的水平擴展</p></li><li><p>primary shard 的數量在建立 index 時就會指定，後續是無法修改的，若要修改就必須要進行 reindex</p></li></ul><h2 id="Replica-Shard-提高資料可用性"><a href="#Replica-Shard-提高資料可用性" class="headerlink" title="Replica Shard (提高資料可用性)"></a>Replica Shard (提高資料可用性)</h2><ul><li><p>replica shard 用來提供資料可用性，當 primary shard 遺失時，replica shard 就可以被 promote 成 primary shard 來保持資料完整性</p></li><li><p>replica shard 數量可以動態調整，讓每個 data node 上都有完整的資料</p></li><li><p>replica shard 可以一定程度的提高讀取(查詢)的效能</p></li></ul><blockquote><p>若不設定 replica shard，一旦有 data node 故障導致 primary shard 遺失，資料可能就無法恢復了</p></blockquote><blockquote><p>ES 7.0 開始，primary shard 預設為 <code>1</code>，replica shard 預設為 <code>0</code></p></blockquote><h2 id="Shard-的規劃-amp-設定"><a href="#Shard-的規劃-amp-設定" class="headerlink" title="Shard 的規劃 &amp; 設定"></a>Shard 的規劃 &amp; 設定</h2><ul><li><p>primary shard 數量設定太小會遇到以下問題：</p><ul><li>若 index 資料增加很快時，cluster 無法通過增加 node 數量對 index 進行資料擴展</li><li>單一 shard 資料太大，導致資料重新分配耗時</li></ul></li><li><p>primary shard 數量設定太大會遇到以下問題：</p><ul><li>導致每個 shard 容量很小，讓一個 data node 上有過多 shard 而影響效能</li><li>影響搜尋時的相關性算分，會讓統計結果失準</li></ul></li><li><p>replica shard 若設定過多，會降低 cluster 整體的寫入效能</p></li></ul><blockquote><p>replica shard 必須和 primary shard 被分配在不同的 data node 上；但所有的 primary shard 可以在同一個 data node 上</p></blockquote><h2 id="如何判斷-Cluster-目前的健康狀態"><a href="#如何判斷-Cluster-目前的健康狀態" class="headerlink" title="如何判斷 Cluster 目前的健康狀態?"></a>如何判斷 Cluster 目前的健康狀態?</h2><h3 id="cluster-status"><a href="#cluster-status" class="headerlink" title="cluster status"></a>cluster status</h3><p>透過 <code>[GET _cluster/health](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html)</code> 可以取得目前 cluster 的健康狀態：</p><p><img src="/blog/images/Elasticsearch/es_cluster-health-statue.png" alt="Elasticsearch - cluster health status"></p><ul><li><p><code>Green</code>：表示 primary &amp; replica shard 都可以正常分配</p></li><li><p><code>Yellow</code>：表示 primary shard 可以正常分配，但 replica shard 分配有問題</p></li><li><p><code>Red</code>：有 primary shard 無法正常分配</p><blockquote><p>例如：當 data node 磁碟空間已經超過 85% 時，此時建立 index 就會出現無法分配 primary shard 的問題</p></blockquote></li></ul><h3 id="shard-status"><a href="#shard-status" class="headerlink" title="shard status"></a>shard status</h3><p>透過 <code>[GET _cat/shards](https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-shards.html)</code> 可以取得目前的 shard 狀態：</p><p><img src="/blog/images/Elasticsearch/es_cat-shard-api.png" alt="Elasticsearch - shard status"></p><p>從上圖可以看到以下訊息：</p><ul><li><p>那些 shard 是 primary(<code>p</code>)，那些是 replica(<code>r</code>) </p></li><li><p>shard 的分佈情況 (位於那一個 ndoe 上)</p></li><li><p>每個 shard 包含的 document 數量 &amp; 佔用的空間</p></li></ul><h2 id="cluster-amp-shard-設定情境-amp-範例"><a href="#cluster-amp-shard-設定情境-amp-範例" class="headerlink" title="cluster &amp; shard 設定情境 &amp; 範例"></a>cluster &amp; shard 設定情境 &amp; 範例</h2><h3 id="single-node-cluster"><a href="#single-node-cluster" class="headerlink" title="single node cluster"></a>single node cluster</h3><p><img src="/blog/images/Elasticsearch/es_cluster-statue-1.png" alt="Elasticsearch - single node cluster"></p><ul><li><p>由於 cluster 中只有單一個 node，因此全部都會被 primary shard 佔據</p></li><li><p>replica shard 無法被分配，因此 cluster 健康狀態為黃色</p></li></ul><h3 id="增加第二個-node"><a href="#增加第二個-node" class="headerlink" title="增加第二個 node"></a>增加第二個 node</h3><p><img src="/blog/images/Elasticsearch/es_cluster-statue-2.png" alt="Elasticsearch - add second node to cluster"></p><ul><li><p>增加第二個 node，因此 replica shard 就可以被正常分配</p></li><li><p>cluster 健康狀態就會轉為綠色</p></li><li><p>目前 cluster 已經具備 failover 的能力了</p></li></ul><h3 id="增加第三個-node"><a href="#增加第三個-node" class="headerlink" title="增加第三個 node"></a>增加第三個 node</h3><p><img src="/blog/images/Elasticsearch/es_cluster-statue-3.png" alt="Elasticsearch - add third node to cluster"></p><ul><li><p>此時 primary shard &amp; replica shard 都會再重新分配，力求資料可以平均分散</p></li><li><p>由 master node 決定每個 shard 會被分配到那一個 data node</p></li><li><p>透過持續增加 node，可以提高 cluster 的計算能力</p></li></ul><h3 id="Failover"><a href="#Failover" class="headerlink" title="Failover"></a>Failover</h3><p><img src="/blog/images/Elasticsearch/es_cluster-failover-1.png" alt="Elasticsearch - cluster failover"></p><blockquote><p>1 index, 3 primary shard, 1 replica shard</p></blockquote><ul><li><p>上面的部份是正常的時候，下面是表示 node 1(master node) 發生故障</p></li><li><p>node 2 透過選舉的過程變成了 master node</p></li><li><p>node 1 上的 P0 跟 R1 已經不可用</p></li><li><p>node 3 上的 R0 會提昇 P0 成用來取代 Node 1 上的 P0</p></li><li><p>為了要滿足 replica shard 的設定，node 2(目前的 master) 重新將 R0 &amp; R1 分配到剩餘的 data node 上</p></li><li><p>cluster 狀態重新變成綠色</p></li></ul><blockquote><p>若是擔心 reboot 機器造成 failover 動作開始執行，可以設定將 replication 延遲一段時間後再執行，避免無謂的 data copy 動作</p></blockquote><h2 id="值得一看的-Q-amp-A-資料"><a href="#值得一看的-Q-amp-A-資料" class="headerlink" title="值得一看的 Q&amp;A 資料"></a>值得一看的 Q&amp;A 資料</h2><p>1：選主的過程中可能存在問題的場景？</p><blockquote><p>選主的過程應該很短，這個期間，如果有創建index或者分片reallocation有可能會出錯。</p></blockquote><p>2：故障轉移期間可能會出現問題的場景？</p><blockquote><p>故障轉移期間，如果只是黃色變綠，應該不影響讀寫，因為副本會提升為主分片。集群變紅，代表有主分片丟失，這個時候會影響讀寫。</p></blockquote><p>3： 故障轉移，資料重新分配，消耗性能的避免方式？</p><blockquote><p>例如一個主分片不可用了。只要設置了副本分片，其中一個副本分片立即會將自己提升為主分片。同時會將自己的資料分配到一個新的replica上，有時候，我們只是重啟一台機器，可以讓這個reallocation的動作延遲一段時間再做，從而避免無謂的資料拷貝。</p></blockquote><p>4：故障轉移可能存在資料丟失的場景嘛？</p><blockquote><p>node如果丟失，如果沒有落盤。就有丟失的可能。如果節點重新回來，會從translog中恢覆沒有寫入的資料</p></blockquote><h1 id="Document-分散式儲存"><a href="#Document-分散式儲存" class="headerlink" title="Document 分散式儲存"></a>Document 分散式儲存</h1><h2 id="document-儲存在-shard-中"><a href="#document-儲存在-shard-中" class="headerlink" title="document 儲存在 shard 中"></a>document 儲存在 shard 中</h2><ul><li><p>document 會儲存在特定的 primary shard &amp; replica shard 中，例如 document(id=1) 儲存在 P0 &amp; R0 shard 上</p></li><li><p>將 document 進行分散式儲存的可能解決方案：</p><ul><li><p>random / round robin 的方式，但此方法在 shard 數量大時，需要多次查詢才能找到 document</p></li><li><p>維護 document &amp; shard 之間的 mapping 關係，但當 document 數量大時，維護成本就會變得很高</p></li><li><p>透過即時運算，以 document id 為基礎，計算出要去那一個 shard 儲存 &amp; 取得 document</p></li></ul></li></ul><h2 id="document-到-shard-的路由計算方式"><a href="#document-到-shard-的路由計算方式" class="headerlink" title="document 到 shard 的路由計算方式"></a>document 到 shard 的路由計算方式</h2><blockquote><p><code> shard = hash(_routing) % number_of_primary_shard</code></p></blockquote><ul><li><p>hash algorithm 確保 document 均勻的分散到 primary shard 中</p></li><li><p>預設的 <code>_routing</code> value 是 document id</p></li><li><p>可以自行指定 routing value，藉此讓 document 都分配到特定的 shard 上</p></li><li><p>也是因為上述路由設計機制的原因，導致於設定 index settings 之後，primary shard 數量無法隨意變更</p></li></ul><h2 id="更新-document-流程"><a href="#更新-document-流程" class="headerlink" title="更新 document 流程"></a>更新 document 流程</h2><p><img src="/blog/images/Elasticsearch/es_distribute-shard-update.png" alt="Elasticsearch - flow of updating a document"></p><ol><li><p>使用者送出一個 update document request 到 master node(同時也是 coordinating node)</p></li><li><p>coordinating node 透過 hash 計算出 document 存放的 shard 在哪裡</p></li><li><p>將 update request 轉發到正確的 data node 上</p></li><li><p>data node 取得 update request，會先將 document 刪除</p></li><li><p>再重新索引新的 document，完成標準的 update 操作</p></li><li><p>將更新成功訊息回傳給 coordinating node</p></li><li><p>coordinating node 回傳操作成功訊息給使用者</p></li></ol><h2 id="刪除-document-流程"><a href="#刪除-document-流程" class="headerlink" title="刪除 document 流程"></a>刪除 document 流程</h2><p><img src="/blog/images/Elasticsearch/es_distribute-shard-delete.png" alt="Elasticsearch - flow of deleting a document"></p><ol><li><p>使用者送出一個 delete document request 到 master node(同時也是 coordinating node)</p></li><li><p>coordinating node 透過 hash 計算出 document 存放的 primary shard 在哪裡，並將 request 轉發到正確的 data node 上</p></li><li><p>data node 刪除 shard 中指定的 document</p></li><li><p>data node 送出 delete replica request 到儲存 replica shard 的 data node 上 (透過每個 node 上的 cluster state 才可以找到 routing 資訊)</p></li><li><p>replica shard 中的資料被刪除，data node 會回傳成功訊息</p></li><li><p>儲存 primary shard 的 data node 將刪除成功的訊息回傳給 coordinating node</p></li><li><p>coordinating node 回傳操作成功訊息給使用者</p></li></ol><h2 id="值得一看的-Q-amp-A-資料-1"><a href="#值得一看的-Q-amp-A-資料-1" class="headerlink" title="值得一看的 Q&amp;A 資料"></a>值得一看的 Q&amp;A 資料</h2><ul><li>請問，視頻中更新和刪除文檔的請求，首先會發送到master節點嗎，還是通過前置的負載均衡工具分發到某一個節點？</li></ul><blockquote><p>視頻中發送到9200，我也沒在開發環境中指定dedicated的節點。所以這個節點既是master也是data，當然肯定也是coordinating節點。</p></blockquote><blockquote><p>在生產環境，你可以設置dedicate的 coordinate節點，發查詢到這些節點。不建議直接發送請求到master節點，雖然也會工作，但是大量請求發送到master，會有潛在的性能問題</p></blockquote><h1 id="Shard-amp-Life-cycle"><a href="#Shard-amp-Life-cycle" class="headerlink" title="Shard &amp; Life cycle"></a>Shard &amp; Life cycle</h1><h2 id="shard-內部原理"><a href="#shard-內部原理" class="headerlink" title="shard 內部原理"></a>shard 內部原理</h2><ul><li><p>shard 是 ES 中最小的工作單元</p></li><li><p>shard 是一個 Lucene 的 index</p></li></ul><p>關於一些 Elasticsearch 的相關問題：</p><ul><li><p>Elasticsearch 的搜尋是如何作到接近即時的 ?</p></li><li><p>Elasticsearch 如何確保臨時的停電不會造成資料遺失 ?</p></li><li><p>為什麼刪除 document 後，不會馬上釋放空間 ?</p></li></ul><h2 id="Inverted-Index-的不可變動性"><a href="#Inverted-Index-的不可變動性" class="headerlink" title="Inverted Index 的不可變動性"></a>Inverted Index 的不可變動性</h2><ul><li><p>Inverted Index 使用 immutable design，一旦產生出來就無法變更</p></li><li><p>然而不可變動的特性會帶來以下好處：</p><ul><li><p>不需要考慮同時多個 document 寫入的問題，因此避免了 lock 機制所帶來的效能問題</p></li><li><p>一旦資料進入到 cache，就會留在裡面；只要 cache 夠大，大部分 request 就不會有 dick access，藉此大幅提昇讀取性能</p></li><li><p>cache 容易產生 &amp; 維護</p></li><li><p>資料可以被壓縮</p></li></ul></li><li><p>不可變動的特性帶來的缺點 =&gt; 若需要讓一個新的 document 可以被搜尋，需要重建 index</p></li></ul><h2 id="Index-Refresh"><a href="#Index-Refresh" class="headerlink" title="Index Refresh"></a>Index Refresh</h2><p><img src="/blog/images/Elasticsearch/es_index-refresh.png" alt="Elasticsearch - Index Refresh"></p><ul><li><p>Elasticsearch 寫入 document 時，會先寫入稱為 <code>Index Buffer</code> 的儲存空間</p></li><li><p>到特定時間點(or 滿足特定條件)時，就會將 Index Buffer 中的內容寫入 <code>Segment</code>，而這寫入的動作就稱為 <code>Refresh</code>，但預設不會執行 <strong>fsync</strong> 操作</p><blockquote><p>預設一秒一次，可以透過設定 <code>index.refresh_interval</code> 進行調整</p></blockquote></li><li><p>當 document 被 refresh 進入到 segment 之後，就可以被搜尋到了</p></li><li><p>若系統有大量的資料寫入，就會產生很多 segment</p></li><li><p>當 Index Buffer 被佔滿時，也會觸發 refresh 動作，預設值是 JVM 的 10%</p></li></ul><h2 id="Transaction-Log"><a href="#Transaction-Log" class="headerlink" title="Transaction Log"></a>Transaction Log</h2><p><img src="/blog/images/Elasticsearch/es_transaction-log.png" alt="Elasticsearch - Transaction Log"></p><ul><li><p>segment 寫入磁碟的過程相對耗時，因此藉由 cache，在進行 refresh 時先將 segment 寫入 cache 以開放查詢</p></li><li><p>但使用 cache 可能就會有資料遺失的問題，因此為了保證資料不會遺失，就有了 transaction log 的設計</p></li><li><p>將 document 進行索引時，同時也會寫入 transaction log，且預設都會寫入磁碟中</p><blockquote><p>每個 shard 都會有對應的 transaction log</p></blockquote></li><li><p>Elasticsearch 進行 refresh 時，index buffer 會被清空，但 transaction log 則不會</p></li></ul><h2 id="Flush"><a href="#Flush" class="headerlink" title="Flush"></a>Flush</h2><p><img src="/blog/images/Elasticsearch/es_flush.png" alt="Elasticsearch - Flush"></p><ul><li><p>執行 refresh，將 index buffer 清空</p></li><li><p>執行 fsync，將在 cache 中的 segment 全部寫入磁碟中</p></li><li><p>清空 transaction log</p></li><li><p>由於操作對資源消耗相對大，因此預設 30 分鐘執行一次</p></li><li><p>當 transaction log 滿了(預設為 512 MB)，也會觸發 flush 的操作</p></li></ul><h2 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h2><ul><li><p>當 flush 工作陸陸續續完成後，segment 上的資料會寫入磁碟中，因此就會造成很多空的 segment，而 merge 就可以協助定期將 segment 合併</p><blockquote><p>可以減少 segment 數量 &amp; 將被刪除的 document 從磁碟中真正的移除掉</p></blockquote></li><li><p>Elasticsearch 會定期自動執行 merge 工作，但若是要強制執行，可以呼叫 <code>POST [INDEX_NAME]/_forcemerge</code> 執行</p></li></ul><h2 id="值得一看的-Q-amp-A-資料-2"><a href="#值得一看的-Q-amp-A-資料-2" class="headerlink" title="值得一看的 Q&amp;A 資料"></a>值得一看的 Q&amp;A 資料</h2><ol><li><p>客戶端發起資料寫入請求，對你寫的這條資料根據_routing規則選擇發給哪個Shard。</p><ul><li>確認Index Request中是否設置了使用哪個Filed的值作為路由參數，</li><li>如果沒有設置，則使用Mapping中的配置，</li><li>如果mapping中也沒有配置，則使用_id作為路由參數，然後通過_routing的Hash值選擇出Shard，最後從集群的Meta中找出出該Shard的Primary節點。</li></ul></li><li><p>寫入請求到達Shard後，先把資料寫入到內存（buffer）中，同時會寫入一條日誌到translog日誌文件中去。</p><ul><li>當寫入請求到shard後，首先是寫Lucene，其實就是創建索引。</li><li>索引創建好後並不是馬上生成segment，這個時候索引資料還在緩存中，這裡的緩存是lucene的緩存，並非Elasticsearch緩存，lucene緩存中的資料是不可被查詢的。</li></ul></li><li><p>執行refresh操作：從內存buffer中將資料寫入os cache(操作系統的內存)，產生一個segment file文件，buffer清空。</p><ul><li>寫入os cache的同時，建立倒排索引，這時資料就可以供客戶端進行訪問了。</li><li>默認是每隔1秒refresh一次的，所以es是准實時的，因為寫入的資料1秒之後才能被看到。</li><li>buffer內存佔滿的時候也會執行refresh操作，buffer默認值是JVM內存的10%。</li><li>通過es的restful api或者java api，手動執行一次refresh操作，就是手動將buffer中的資料刷入os cache中，讓資料立馬就可以被搜索到。</li><li>若要優化索引速度, 而不注重實時性, 可以降低刷新頻率。</li></ul></li><li><p>translog會每隔5秒或者在一個變更請求完成之後，將translog從緩存刷入磁盤。</p><ul><li>translog是存儲在os cache中，每個分片有一個，如果節點宕機會有5秒資料丟失，但是性能比較好，最多丟5秒的資料。。</li><li>可以將translog設置成每次寫操作必須是直接fsync到磁盤，但是性能會差很多。</li><li>可以通過配置增加transLog刷磁盤的頻率來增加資料可靠性，最小可配置100ms，但不建議這麼做，因為這會對性能有非常大的影響。</li></ul></li><li><p>每30分鐘或者當tanslog的大小達到512M時候，就會執行commit操作（flush操作），將os cache中所有的資料全以segment file的形式，持久到磁盤上去。</p><ul><li>第一步，就是將buffer中現有資料refresh到os cache中去。</li><li>清空buffer 然後強行將os cache中所有的資料全都一個一個的通過segmentfile的形式，持久到磁盤上去。</li><li>將commit point這個文件更新到磁盤中，每個Shard都有一個提交點(commit point), 其中保存了當前Shard成功寫入磁盤的所有segment。</li><li>把translog文件刪掉清空，再開一個空的translog文件。</li><li>flush參數設置：</li><li>index.translog.flush_threshold_period:</li><li>index.translog.flush_threshold_size:</li><li>#控制每收到多少條資料後flush一次</li><li>index.translog.flush_threshold_ops:</li></ul></li><li><p>Segment的merge操作：</p><ul><li>隨著時間，磁盤上的segment越來越多，需要定期進行合併。</li><li>Es和Lucene 會自動進行merge操作，合併segment和刪除已經刪除的文檔。</li><li>我們可以手動進行merge：POST index/_forcemerge。一般不需要，這是一個比較消耗資源的操作</li></ul></li></ol><blockquote><p>當資料從hot移動到warm，官方建議手工執行一下_forcemerge</p></blockquote><p><img src="/blog/images/Elasticsearch/es_qa_chap40-1.png" alt="Elasticsearch - Q&amp;A Chap40 - 1"></p><p><img src="/blog/images/Elasticsearch/es_qa_chap40-1.png" alt="Elasticsearch - Q&amp;A Chap40 - 2"></p><h1 id="剖析-Distributed-Search-分佈式查詢-及相關性算分"><a href="#剖析-Distributed-Search-分佈式查詢-及相關性算分" class="headerlink" title="剖析 Distributed Search(分佈式查詢)及相關性算分"></a>剖析 Distributed Search(分佈式查詢)及相關性算分</h1><h2 id="Distributed-Search-的運作機制"><a href="#Distributed-Search-的運作機制" class="headerlink" title="Distributed Search 的運作機制"></a>Distributed Search 的運作機制</h2><ul><li>Elasticsearch 的搜尋會分為兩個階段，分別是 <code>Query</code> &amp; <code>Fetch</code>，也就是 <code>Query-then-Fetch</code></li></ul><h2 id="Query-then-Fetch-範例"><a href="#Query-then-Fetch-範例" class="headerlink" title="Query-then-Fetch 範例"></a>Query-then-Fetch 範例</h2><p>以下面的搜尋為例：</p><p><img src="/blog/images/Elasticsearch/es_distrbuted-search.png" alt="Elasticsearch - Distributed Search"></p><h3 id="Query-階段"><a href="#Query-階段" class="headerlink" title="Query 階段"></a>Query 階段</h3><ul><li><p>使用者送出 search 到 Elasticsearch，Coordinating Node 會在六個 primary &amp; replica shard 中隨機挑選三個 shard 並送出 request 給 data node</p></li><li><p>被選中的 shard 執行查詢 &amp; 排序，返回 <code>From + Size</code> 個<strong>排序後</strong>的 document ID &amp; 排序值給 Coordinating Node</p><blockquote><p>上圖中是 P0, P1, P2 被選中 (<strong>目前只有取得 document ID，沒有內容</strong>)</p></blockquote></li></ul><h3 id="Fetch-階段"><a href="#Fetch-階段" class="headerlink" title="Fetch 階段"></a>Fetch 階段</h3><ul><li><p>Coordinating Node 會將 Query 階段中從每個 shard 取得的 document ID 重新排序，並根據 <code>From</code> &amp; <code>Size</code> 重新選出 document ID list</p></li><li><p>以 multiple GET 的方式，從對應的 shard 取得詳細的 document 資訊</p></li></ul><h2 id="Query-then-Fetch-的潛在問題"><a href="#Query-then-Fetch-的潛在問題" class="headerlink" title="Query-then-Fetch 的潛在問題"></a>Query-then-Fetch 的潛在問題</h2><p>效能問題：</p><ul><li><p>每個 shard 需要查詢的 document 數量 = <code>from</code> + <code>size</code></p></li><li><p>最後 Coordinating Node 需要處理 <code>number_of_shard * (from + size)</code> 數量的 document</p></li><li><p>若是遇到深度分頁的情況，效能會變很差</p></li></ul><p>相關性算分：</p><ul><li>每個 shard 都基於自己 shard 上的資料進行相關度計算；若是在資料量少，shard 數量越大會導致算分越不準確</li></ul><h2 id="如何解決算分不準的問題"><a href="#如何解決算分不準的問題" class="headerlink" title="如何解決算分不準的問題 ?"></a>如何解決算分不準的問題 ?</h2><ul><li><p>資料量不大時，可以將 primary shard 數量設定為 1</p><blockquote><p>但若資料足夠大時，只要 document 可以平均分散在多個 shard 上，結果就不會有太大偏差</p></blockquote></li><li><p>使用 CFS Query Then Fetch</p></li><li><p>在搜尋的 URL 中指定參數 <code>_search?search_type=dfs_query_then_fetch</code></p></li><li><p>這會到各 shard 中蒐集完整的 TF &amp; IDF 的資料，再重新匯總並算分，但這會消耗太多 CPU &amp; memory，不建議使用</p></li></ul><h2 id="值得一看的-Q-amp-A-資料-3"><a href="#值得一看的-Q-amp-A-資料-3" class="headerlink" title="值得一看的 Q&amp;A 資料"></a>值得一看的 Q&amp;A 資料</h2><p><img src="/blog/images/Elasticsearch/es_qa_chap40-1.png" alt="Elasticsearch - Q&amp;A Chap41 - 1"></p><h1 id="排序及-Doc-Values-amp-Fielddata"><a href="#排序及-Doc-Values-amp-Fielddata" class="headerlink" title="排序及 Doc Values &amp; Fielddata"></a>排序及 Doc Values &amp; Fielddata</h1><h2 id="Sorting"><a href="#Sorting" class="headerlink" title="Sorting"></a>Sorting</h2><ul><li><p>Elasticsearch 預設會使用相關性算分對結果進行排序</p></li><li><p>可以透過設定 <code>sorting</code> 參數決定排序的條件</p></li><li><p>若自己給入 sort 條件時但卻不是指定 <code>_score</code>，則算分為 NULL</p></li></ul><p>以下是幾個範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//single field 查詢，並指定 &quot;sort&quot;</span></span><br><span class="line"><span class="comment">//回傳的資料在 _score 部份都會以 null 呈現</span></span><br><span class="line">POST /kibana_sample_data_ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;size&quot;</span>: <span class="number">5</span>,</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match_all&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;sort&quot;</span>: [</span><br><span class="line">    &#123;<span class="attr">&quot;order_date&quot;</span>: &#123;<span class="attr">&quot;order&quot;</span>: <span class="string">&quot;desc&quot;</span>&#125;&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//multiple field 查詢的範例</span></span><br><span class="line"><span class="comment">//因為有指定將 _score 拿來排序，因此 _score 就不會是 null</span></span><br><span class="line"><span class="comment">//查詢中使用 &quot;match_all&quot;，因此所有的算分都會是 1</span></span><br><span class="line">POST /kibana_sample_data_ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;size&quot;</span>: <span class="number">5</span>,</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match_all&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;sort&quot;</span>: [</span><br><span class="line">    &#123;<span class="attr">&quot;order_date&quot;</span>: &#123;<span class="attr">&quot;order&quot;</span>: <span class="string">&quot;desc&quot;</span>&#125;&#125;,</span><br><span class="line">    &#123;<span class="attr">&quot;_doc&quot;</span>:&#123;<span class="attr">&quot;order&quot;</span>: <span class="string">&quot;asc&quot;</span>&#125;&#125;,</span><br><span class="line">    &#123;<span class="attr">&quot;_score&quot;</span>:&#123; <span class="attr">&quot;order&quot;</span>: <span class="string">&quot;desc&quot;</span>&#125;&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>若想要對 text 類型的 field 做排序：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//這樣的查詢會報錯，預設是無法針對 text 類型的 field 做排序</span></span><br><span class="line">POST /kibana_sample_data_ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;size&quot;</span>: <span class="number">5</span>,</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match_all&quot;</span>: &#123;</span><br><span class="line"></span><br><span class="line">    &#125;!</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;sort&quot;</span>: [</span><br><span class="line">    &#123;<span class="attr">&quot;customer_full_name&quot;</span>: &#123;<span class="attr">&quot;order&quot;</span>: <span class="string">&quot;desc&quot;</span>&#125;&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//透過將 field 中的 &quot;fielddata&quot;: true 設定打開</span></span><br><span class="line"><span class="comment">//這樣就可以對 text 的 field 進行排序</span></span><br><span class="line">PUT kibana_sample_data_ecommerce/_mapping</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;properties&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;customer_full_name&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;text&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;fielddata&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">          <span class="attr">&quot;fields&quot;</span> : &#123;</span><br><span class="line">            <span class="attr">&quot;keyword&quot;</span> : &#123;</span><br><span class="line">              <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;keyword&quot;</span>,</span><br><span class="line">              <span class="attr">&quot;ignore_above&quot;</span> : <span class="number">256</span></span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="sorting-的過程"><a href="#sorting-的過程" class="headerlink" title="sorting 的過程"></a>sorting 的過程</h2><ul><li><p>sorting 是針對原始的 field 內容進行，因此 inverted index 無法發揮作用</p></li><li><p>需要使用到 forward index(正排索引)，透過 document ID &amp; field 快速得到 field 原始內容</p></li><li><p>在 Elasticsearch 中有兩種實現 sorting 的方式：</p><ul><li><p>Fielddata</p></li><li><p>Doc Values (列式儲存，對 text 類型無效))</p></li></ul></li></ul><h2 id="Doc-Values-v-s-Field-data"><a href="#Doc-Values-v-s-Field-data" class="headerlink" title="Doc Values v.s. Field data"></a>Doc Values v.s. Field data</h2><table><thead><tr><th></th><th>Doc Values</th><th>Field data</th></tr></thead><tbody><tr><td>何時建立?</td><td>資料進行索引時，和 inverted index 一起建立</td><td>搜尋時動態建立</td></tr><tr><td>建立位置</td><td>磁碟檔案</td><td>JVM Heap</td></tr><tr><td>優點</td><td>避免佔據大量記憶體</td><td>索引速度快，不佔用額外的磁碟空間</td></tr><tr><td>缺點</td><td>降低索引速度，佔用額外的磁碟空間</td><td>document 過多時，動態建立的消耗大，佔用過多的 JVM Heap</td></tr><tr><td>預設?</td><td>ES 2.x 之後</td><td>ES 1.x 及之前</td></tr></tbody></table><blockquote><p>一般不會開啟 fielddata，因為對 text field 排序通常沒什麼意義，通常僅有在 aggregation 需求時才會開啟 fielddata 的設定</p></blockquote><h2 id="關閉-Doc-Values"><a href="#關閉-Doc-Values" class="headerlink" title="關閉 Doc Values"></a>關閉 Doc Values</h2><p>透過以下的語法，可以關閉 Doc Values：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">PUT test_keyword/_mapping</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;properties&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;user_name&quot;</span>:&#123;</span><br><span class="line">      <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;keyword&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;doc_values&quot;</span>:<span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>Doc Values 預設啟用，可以透過 mapping 設定關閉，優點可以增加索引速度 &amp; 減少硬碟空間的消耗</p></li><li><p>如果要重新打開，需要重建 index</p></li><li><p>若是對於不需要進行 sorting or aggregation 的欄位，可以關閉該欄位的 Doc Values</p></li></ul><h1 id="分頁與遍歷：From-Size-Search-After-amp-Scroll-API"><a href="#分頁與遍歷：From-Size-Search-After-amp-Scroll-API" class="headerlink" title="分頁與遍歷：From, Size, Search After &amp; Scroll API"></a>分頁與遍歷：From, Size, Search After &amp; Scroll API</h1><h2 id="From-amp-Size"><a href="#From-amp-Size" class="headerlink" title="From &amp; Size"></a>From &amp; Size</h2><ul><li>Elasticsearch 預設對搜尋回傳 10 筆紀錄，<code>From</code> 是 document 開始位置，<code>Size</code> 是期望取得 document 的總數</li></ul><h2 id="分散式系統中-Deep-Pagination-深度分頁-的問題"><a href="#分散式系統中-Deep-Pagination-深度分頁-的問題" class="headerlink" title="分散式系統中 Deep Pagination(深度分頁)的問題"></a>分散式系統中 Deep Pagination(深度分頁)的問題</h2><p><img src="/blog/images/Elasticsearch/es_deep-pagination-1.png" alt="Elasticsearch - Deep Pagination"></p><ul><li><p>Elasticsearch 是個分散式系統，資料會分佈在多個 shard 中</p></li><li><p>但假設查詢的 <code>From=990</code> &amp; <code>Size=10</code> 時，Elasticsearch 會進行以下操作：</p><ul><li><p>在每個 shard 中取得 1000 個 document，然後 coordinating node 會整合所有結果，最後透過排序選取前 1000 個 document</p></li><li><p>頁數越深，佔用的 memory 越多，而 Elasticsearch 預設限制到 10000 個 document(可透過修改 <code>index.max_result_window</code> 來調整)</p></li></ul></li></ul><h2 id="使用-search-after-避免-Deep-Pagination-深度分頁-問題"><a href="#使用-search-after-避免-Deep-Pagination-深度分頁-問題" class="headerlink" title="使用 search_after 避免 Deep Pagination(深度分頁)問題"></a>使用 search_after 避免 Deep Pagination(深度分頁)問題</h2><p>為了避免 deep pagination 的問題，Elasticsearch 提供了 <code>search_after</code> 的功能，以下是一個簡單範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//透過 &quot;search_after&quot; 可以明確指定從哪裡開始取得資料</span></span><br><span class="line"><span class="comment">//因此也就避免了 deep pagination 的問題</span></span><br><span class="line">POST users/_search</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;size&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;search_after&quot;</span>: [<span class="number">10</span>, <span class="string">&quot;ZQ0vYGsBrR8X3IP75QqX&quot;</span>],</span><br><span class="line">    <span class="attr">&quot;sort&quot;</span>: [</span><br><span class="line">        &#123;<span class="attr">&quot;age&quot;</span>: <span class="string">&quot;desc&quot;</span>&#125; ,</span><br><span class="line">        &#123;<span class="attr">&quot;_id&quot;</span>: <span class="string">&quot;asc&quot;</span>&#125;    </span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>然而 search_after 會有以下兩項限制：</p><ul><li><p>不支援指定頁數 (也就是表示不能指定 <code>From</code>)</p></li><li><p>只能往下翻頁，無法往回</p></li></ul></li><li><p>搜尋時需要指定上一次的 <code>sort</code> value，如此一來就可以一次一次的搜尋結果進行翻頁</p></li><li><p><code>search_after</code> 是透過唯一排序值的定位方式，將每次要處理的 document 數量都控制在 10(可自訂 <code>size</code>)</p></li></ul><h2 id="Scroll-API"><a href="#Scroll-API" class="headerlink" title="Scroll API"></a>Scroll API</h2><p>Scroll API 也是為了解決深度搜尋的另外一種方式，實踐的方法類似 <code>search_after</code>，作法如下：</p><ul><li><p>搜尋時建立一個快照，作為後續繼續快速搜尋之用，但如果有新的資料寫入後，就沒辦法被查詢到了</p></li><li><p>每次查詢時，要將上次查詢結果中的 scroll id 拿來使用，才可以正確的繼續往下搜尋</p></li></ul><h2 id="不同的搜尋類型-amp-使用場景"><a href="#不同的搜尋類型-amp-使用場景" class="headerlink" title="不同的搜尋類型 &amp; 使用場景"></a>不同的搜尋類型 &amp; 使用場景</h2><ul><li><p><strong>一般搜尋</strong>：需要接近即時的取得最新的部份資料，例如：查詢最新訂單</p></li><li><p><strong>Scroll</strong>：需要全部 document，但過程中的分頁速度要快，例如：導出全部資料</p></li><li><p><strong>Pagination</strong>：使用 <code>from</code> &amp; <code>size</code>，如果要處理 deep pagination 的問題，則使用 <code>search_after</code></p></li></ul><h1 id="處理-Concurrent-讀寫操作"><a href="#處理-Concurrent-讀寫操作" class="headerlink" title="處理 Concurrent 讀寫操作"></a>處理 Concurrent 讀寫操作</h1><h2 id="Concurrent-control-的重要性"><a href="#Concurrent-control-的重要性" class="headerlink" title="Concurrent control 的重要性"></a>Concurrent control 的重要性</h2><p>以下是一個常見的範例，假設在沒有做 concurrent control 的狀況下，下圖的情況就有可能發生，導致實際庫存錯誤：</p><p><img src="/blog/images/Elasticsearch/es_concurrent-1.png" alt="Elasticsearch - Concurrent Read &amp; Write operation"></p><ul><li><p>兩個 web process 同時更新某個 document，如果沒有對 concurrent control，可能會導致更新的資料遺失</p></li><li><p>悲觀鎖定(Pessimistic Locking)：假設有變更衝突的可能性，因此對資源更新的時候會進行 lock，防止衝突，例如：DB row lock</p></li><li><p>樂觀鎖定(Optimistic Locking)</p><ul><li><p>假設更新衝突不會發生，因此不會阻止正在嘗試更新的操作</p></li><li><p>但如果資料在讀寫中被修改，更新會失敗</p></li><li><p>通常由應用程式端來解決這樣的衝突，例如：重新嘗試更新、使用新資料、或是回應錯誤訊息給使用者</p></li><li><p>Elasticsearch 採用的即是 Optimistic Locking</p></li></ul></li></ul><h2 id="Elasticsearch-如何作到-Optimistic-Locking-下的-Concurrent-Control"><a href="#Elasticsearch-如何作到-Optimistic-Locking-下的-Concurrent-Control" class="headerlink" title="Elasticsearch 如何作到 Optimistic Locking 下的 Concurrent Control ?"></a>Elasticsearch 如何作到 Optimistic Locking 下的 Concurrent Control ?</h2><p><img src="/blog/images/Elasticsearch/es_optimistic-locking-1.png" alt="Elasticsearch - Optimistic Locking"></p><ul><li><p>Elasticsearch 中的 document 是不可變更的；更新 document 會將原本的 document 標記為刪除，並增加一個全新的 document，同時 document 的 <code>version</code> field 加 1</p></li><li><p>內部版本控制，可透過 <code>if_seq_no</code> + <code>if_primary_term</code> 來處理</p></li><li><p>外部版本(Elasticsearch 只是用來作為類似 DB backup)時，可使用 <code>version</code> + <code>version_type=external</code> 來處理</p></li></ul><p>以下是個簡單範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">DELETE products</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增 document</span></span><br><span class="line">PUT products/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;title&quot;</span>:<span class="string">&quot;iphone&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;count&quot;</span>:<span class="number">100</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//取得新增的資料內容</span></span><br><span class="line"><span class="comment">//可以看到 &quot;_seq_no&quot; : 0 &amp; &quot;_primary_term&quot; : 1</span></span><br><span class="line">GET products/_doc/1</span><br><span class="line"></span><br><span class="line"><span class="comment">//進行更新操作，並指定 if_seq_no=0 &amp; if_primary_term=1</span></span><br><span class="line"><span class="comment">//可以成功更新</span></span><br><span class="line">PUT products/_doc/1?if_seq_no=1&amp;if_primary_term=1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;title&quot;</span>:<span class="string">&quot;iphone&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;count&quot;</span>:<span class="number">100</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//再一次進行更新操作，同樣指定 if_seq_no=0 &amp; if_primary_term=1</span></span><br><span class="line"><span class="comment">//這次更新就會失敗了，會出現 &#x27;version_conflict_engine_exception&#x27;</span></span><br><span class="line">PUT products/_doc/1?if_seq_no=1&amp;if_primary_term=1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;title&quot;</span>:<span class="string">&quot;iphone&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;count&quot;</span>:<span class="number">102</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//也可透過搭配 version &amp; version_type=external 的方式更新</span></span><br><span class="line"><span class="comment">//這一次可以更新成功</span></span><br><span class="line">PUT products/_doc/1?version=30000&amp;version_type=external</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;title&quot;</span>:<span class="string">&quot;iphone&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;count&quot;</span>:<span class="number">100</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//同樣的 version 再進行更新，就會失敗了</span></span><br><span class="line"><span class="comment">//出現 version_conflict_engine_exception</span></span><br><span class="line">PUT products/_doc/1?version=30000&amp;version_type=external</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;title&quot;</span>:<span class="string">&quot;iphone&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;count&quot;</span>:<span class="number">102</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html">持久化变更 | Elasticsearch: 权威指南 | Elastic</a></p></li><li><p><a href="https://qbox.io/blog/refresh-flush-operations-elasticsearch-guide">Guide to Refresh and Flush Operations in Elasticsearch</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux] 利用未分割的硬碟空間，擴充 LVM 空間</title>
      <link href="/blog/Linux/Linux-extend-lvm-from-unused-space/"/>
      <url>/blog/Linux/Linux-extend-lvm-from-unused-space/</url>
      
        <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>因為剛剛發現監控主機空間不夠了，記得之前是以 LVM 的規劃儲存空間的，因此進行了擴充空間的操作。</p><p>實際情境：</p><ul><li><p>現有的硬碟還有未分割的空間</p></li><li><p>以 LVM 的方式規劃儲存空間，目前 LV 空間已經快要耗盡</p></li></ul><h1 id="Scenario"><a href="#Scenario" class="headerlink" title="Scenario"></a>Scenario</h1><ul><li><p>host 上只有一個 200GB disk，但<strong>當初分割時只使用了約 60GB 的空間，還有 140GB 左右未分割空間</strong></p></li><li><p>PV 已經沒有空間可用</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ pvdisplay </span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               /dev/sda3</span><br><span class="line">  VG Name               ubuntu-vg</span><br><span class="line">  PV Size               &lt;63.00 GiB / not usable 0   </span><br><span class="line">  Allocatable           yes </span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              16127</span><br><span class="line">  Free PE               3327</span><br><span class="line">  Allocated PE          12800</span><br><span class="line">  PV UUID               cs39RZ-Dfac-qED7-KHNz-Liht-QMtu-FvhS04</span><br></pre></td></tr></table></figure><ul><li>VG 可分配的空間也沒很多了</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ vgdisplay </span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               ubuntu-vg</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        1</span><br><span class="line">  Metadata Sequence No  2</span><br><span class="line">  VG Access             <span class="built_in">read</span>/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                1</span><br><span class="line">  Open LV               1</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                1</span><br><span class="line">  Act PV                1</span><br><span class="line">  VG Size               &lt;63.00 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              16127</span><br><span class="line">  Alloc PE / Size       12800 / 50.00 GiB</span><br><span class="line">  Free  PE / Size       3327 / &lt;13.00 GiB</span><br><span class="line">  VG UUID               LRE3Y3-QvmP-mCI0-D0PT-Hk9w-3tqb-129LhN</span><br></pre></td></tr></table></figure><ul><li>LV 當初只有分配了 50 GB 空間</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ lvdisplay </span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/ubuntu-vg/ubuntu-lv</span><br><span class="line">  LV Name                ubuntu-lv</span><br><span class="line">  VG Name                ubuntu-vg</span><br><span class="line">  LV UUID                xWDQsf-VimA-iX00-Kx6G-NzmJ-yaol-XVZr3L</span><br><span class="line">  LV Write Access        <span class="built_in">read</span>/write</span><br><span class="line">  LV Creation host, time ubuntu-server, 2019-08-05 10:16:13 +0800</span><br><span class="line">  LV Status              available</span><br><span class="line">  <span class="comment"># open                 1</span></span><br><span class="line">  LV Size                50.00 GiB</span><br><span class="line">  Current LE             12800</span><br><span class="line">  Segments               1</span><br><span class="line">  Allocation             inherit</span><br><span class="line">  Read ahead sectors     auto</span><br><span class="line">  - currently <span class="built_in">set</span> to     256</span><br><span class="line">  Block device           253:0</span><br></pre></td></tr></table></figure><h1 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h1><p>目標是將所有未分割的空間加入到 PV，讓 VG 可以有更多的空間可以分配給 LV，以下是操作步驟：</p><h2 id="擴充原有-partition，使用剩餘空間"><a href="#擴充原有-partition，使用剩餘空間" class="headerlink" title="擴充原有 partition，使用剩餘空間"></a>擴充原有 partition，使用剩餘空間</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 parted 工具進行擴充</span></span><br><span class="line">$ parted</span><br><span class="line">GNU Parted 3.2</span><br><span class="line">Using /dev/sda</span><br><span class="line">Welcome to GNU Parted! Type <span class="string">&#x27;help&#x27;</span> to view a list of commands.</span><br><span class="line">(parted) <span class="built_in">print</span>                                                            </span><br><span class="line">Model: VMware Virtual disk (scsi)</span><br><span class="line">Disk /dev/sda: 215GB    <span class="comment">#硬碟容量一共有 215GB</span></span><br><span class="line">Sector size (logical/physical): 512B/512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"><span class="comment"># 其中 number 3 只使用了 67.6GB</span></span><br><span class="line">Number  Start   End     Size    File system  Name  Flags</span><br><span class="line"> 1      1049kB  2097kB  1049kB                     bios_grub</span><br><span class="line"> 2      2097kB  1076MB  1074MB  ext4</span><br><span class="line"> 3      1076MB  68.7GB  67.6GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># 執行 partition resize 操作                                                       </span></span><br><span class="line">(parted) resizepart                                                       </span><br><span class="line">Partition number? 3                                                       </span><br><span class="line">End?  [68.7GB]? 215GB   <span class="comment">#使用所有空間</span></span><br><span class="line">(parted) <span class="built_in">print</span>                                                            </span><br><span class="line">Model: VMware Virtual disk (scsi)</span><br><span class="line">Disk /dev/sda: 215GB</span><br><span class="line">Sector size (logical/physical): 512B/512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line">Disk Flags: </span><br><span class="line"><span class="comment"># 此時 number 3 partition 已經使用了所有空間</span></span><br><span class="line">Number  Start   End     Size    File system  Name  Flags</span><br><span class="line"> 1      1049kB  2097kB  1049kB                     bios_grub</span><br><span class="line"> 2      2097kB  1076MB  1074MB  ext4</span><br><span class="line"> 3      1076MB  215GB   214GB</span><br><span class="line"></span><br><span class="line"><span class="comment"># 離開 parted</span></span><br><span class="line">(parted) quit                                                             </span><br><span class="line">Information: You may need to update /etc/fstab.</span><br></pre></td></tr></table></figure><h2 id="擴充-PV-空間"><a href="#擴充-PV-空間" class="headerlink" title="擴充 PV 空間"></a>擴充 PV 空間</h2><p>原本上面的 number 3 partition 已經是現存的 PV，既然 partition 已經變大，接著就可以直接進行 resize 的操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pv 尚未進行 resize 前 </span></span><br><span class="line">$ pvdisplay </span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               /dev/sda3</span><br><span class="line">  VG Name               ubuntu-vg</span><br><span class="line">  PV Size               &lt;63.00 GiB / not usable 0   </span><br><span class="line">  Allocatable           yes </span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              16127</span><br><span class="line">  Free PE               3327</span><br><span class="line">  Allocated PE          12800</span><br><span class="line">  PV UUID               cs39RZ-Dfac-qED7-KHNz-Liht-QMtu-FvhS04</span><br><span class="line"></span><br><span class="line"><span class="comment"># 執行 pvresize</span></span><br><span class="line">$ pvresize /dev/sda3</span><br><span class="line">  Physical volume <span class="string">&quot;/dev/sda3&quot;</span> changed</span><br><span class="line">  1 physical volume(s) resized / 0 physical volume(s) not resized</span><br><span class="line"></span><br><span class="line"><span class="comment"># pv 已經確定空間變大(從 63GB -&gt; 199GB)</span></span><br><span class="line">$ pvdisplay </span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               /dev/sda3</span><br><span class="line">  VG Name               ubuntu-vg</span><br><span class="line">  PV Size               &lt;199.00 GiB / not usable 16.50 KiB</span><br><span class="line">  Allocatable           yes </span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              50943</span><br><span class="line">  Free PE               38143</span><br><span class="line">  Allocated PE          12800</span><br><span class="line">  PV UUID               cs39RZ-Dfac-qED7-KHNz-Liht-QMtu-FvhS04</span><br></pre></td></tr></table></figure><h2 id="檢查-VG-空間"><a href="#檢查-VG-空間" class="headerlink" title="檢查 VG 空間"></a>檢查 VG 空間</h2><p>從下面的執行結果可以看出，包含上面 PV 的 VG 容量也變大了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ vgdisplay </span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               ubuntu-vg</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        1</span><br><span class="line">  Metadata Sequence No  3</span><br><span class="line">  VG Access             <span class="built_in">read</span>/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                1</span><br><span class="line">  Open LV               1</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                1</span><br><span class="line">  Act PV                1</span><br><span class="line">  VG Size               &lt;199.00 GiB     <span class="comment"># 變成 199GB 了</span></span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              50943</span><br><span class="line">  Alloc PE / Size       12800 / 50.00 GiB</span><br><span class="line">  Free  PE / Size       38143 / &lt;149.00 GiB</span><br><span class="line">  VG UUID               LRE3Y3-QvmP-mCI0-D0PT-Hk9w-3tqb-129LhN</span><br></pre></td></tr></table></figure><h2 id="擴充-LV-空間"><a href="#擴充-LV-空間" class="headerlink" title="擴充 LV 空間"></a>擴充 LV 空間</h2><p>接著擴充 LV 的空間：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原本的 LV 空間狀況</span></span><br><span class="line">$ lvdisplay </span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/ubuntu-vg/ubuntu-lv</span><br><span class="line">  LV Name                ubuntu-lv</span><br><span class="line">  VG Name                ubuntu-vg</span><br><span class="line">  LV UUID                xWDQsf-VimA-iX00-Kx6G-NzmJ-yaol-XVZr3L</span><br><span class="line">  LV Write Access        <span class="built_in">read</span>/write</span><br><span class="line">  LV Creation host, time ubuntu-server, 2019-08-05 10:16:13 +0800</span><br><span class="line">  LV Status              available</span><br><span class="line">  <span class="comment"># open                 1</span></span><br><span class="line">  LV Size                50.00 GiB  <span class="comment"># 只有 50GB</span></span><br><span class="line">  Current LE             12800</span><br><span class="line">  Segments               1</span><br><span class="line">  Allocation             inherit</span><br><span class="line">  Read ahead sectors     auto</span><br><span class="line">  - currently <span class="built_in">set</span> to     256</span><br><span class="line">  Block device           253:0</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 將 LV 空間擴充成 100GB</span></span><br><span class="line">$ lvextend -L 100G /dev/ubuntu-vg/ubuntu-lv </span><br><span class="line">  Size of logical volume ubuntu-vg/ubuntu-lv changed from 50.00 GiB (12800 extents) to 100.00 GiB (25600 extents).</span><br><span class="line">  Logical volume ubuntu-vg/ubuntu-lv successfully resized.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新檢視 LV 狀態，空間已經成功的擴充</span></span><br><span class="line">$ lvdisplay </span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/ubuntu-vg/ubuntu-lv</span><br><span class="line">  LV Name                ubuntu-lv</span><br><span class="line">  VG Name                ubuntu-vg</span><br><span class="line">  LV UUID                xWDQsf-VimA-iX00-Kx6G-NzmJ-yaol-XVZr3L</span><br><span class="line">  LV Write Access        <span class="built_in">read</span>/write</span><br><span class="line">  LV Creation host, time ubuntu-server, 2019-08-05 10:16:13 +0800</span><br><span class="line">  LV Status              available</span><br><span class="line">  <span class="comment"># open                 1</span></span><br><span class="line">  LV Size                100.00 GiB     <span class="comment"># 目前是 100GB</span></span><br><span class="line">  Current LE             25600</span><br><span class="line">  Segments               1</span><br><span class="line">  Allocation             inherit</span><br><span class="line">  Read ahead sectors     auto</span><br><span class="line">  - currently <span class="built_in">set</span> to     256</span><br><span class="line">  Block device           253:0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最後要使用 resize2fs 調整的空間才會真正生效</span></span><br><span class="line">$ resize2fs /dev/ubuntu-vg/ubuntu-lv</span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://godleon.github.io/blog/RHCE/RHCE7-RH134-LearningNotes-CH10_ManagingLogicalVolumeManagement%28LVM%29Storage">[RHCE7] RH134 Chapter 10. Managing Logical Volume Management(LVM) Storage 學習筆記 - 小信豬的原始部落</a></p></li><li><p><a href="https://blog.gtwang.org/linux/parted-command-to-create-resize-rescue-linux-disk-partitions/3/">Linux 的 Parted 指令教學：建立、變更與修復磁碟分割區 - 頁3，共3 - G. T. Wang</a></p></li></ul><ul><li><a href="https://serverfault.com/questions/378086/how-to-extend-a-linux-pv-partition-online-after-virtual-disk-growth">virtualization - How to extend a Linux PV partition online after virtual disk growth - Server Fault</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Elasticsearch] 深入搜索</title>
      <link href="/blog/Elasticsearch/Elasticsearch-advanced-search/"/>
      <url>/blog/Elasticsearch/Elasticsearch-advanced-search/</url>
      
        <content type="html"><![CDATA[<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>以下的內容其實不少，先把重點抓到這裡來：</p><ul><li><p>term query 用於數據化結構的查詢</p></li><li><p>Full text 使用 match 查詢</p></li><li><p>bool query 是種複合查詢，可以結合 term query &amp; full text query</p></li><li><p>multi-match + best field = disjunction max query</p></li></ul><h1 id="Term-Query-amp-Full-Text-Query"><a href="#Term-Query-amp-Full-Text-Query" class="headerlink" title="Term Query &amp; Full Text Query"></a>Term Query &amp; Full Text Query</h1><h2 id="Term-Query"><a href="#Term-Query" class="headerlink" title="Term Query"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-term-query.html">Term Query</a></h2><ul><li><p>Term 是表達語意的最小單位，搜尋或是利用自然語言進行處理時都需要處理 term</p></li><li><p>Term Level Query 包含 <strong>Term Query</strong> / <strong>Range Query</strong> / <strong>Exists Query</strong> / <strong>Prefix Query</strong> / <strong>Wildcard Query</strong></p></li><li><p>當 ES 接收到 term query 時，對輸入不會做分詞，並將輸入當作一個整體在 inverted index 中找到精確的詞項，並使用相關度算分的機制來計算分數</p></li><li><p>若是不需要算分，則可以利用 <code>constant score</code> 將查詢轉換成 <code>filter</code>，並利用 cache 來提高效能</p></li></ul><p>以下使用範例說明 term 查詢所需要注意的事項：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新增 index</span></span><br><span class="line">PUT products</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;settings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;number_of_shards&quot;</span>: <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增三筆資料</span></span><br><span class="line">POST /products/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;XHDK-A-1293-#fJ3&quot;</span>,<span class="attr">&quot;desc&quot;</span>:<span class="string">&quot;iPhone&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;KDKE-B-9947-#kL5&quot;</span>,<span class="attr">&quot;desc&quot;</span>:<span class="string">&quot;iPad&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">3</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;JODL-X-1937-#pV7&quot;</span>,<span class="attr">&quot;desc&quot;</span>:<span class="string">&quot;MBP&quot;</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//檢視 index 的 setting &amp; mapping 資訊</span></span><br><span class="line">GET /products</span><br><span class="line"></span><br><span class="line"><span class="comment">//搜尋 &quot;iPhone&quot; =&gt; 找不到資料，因為預設的 index analyzer 分詞後會將每個單字轉小寫</span></span><br><span class="line"><span class="comment">//搜尋 &quot;iphone&quot; =&gt; 改成小寫，順利找到資料</span></span><br><span class="line">POST /products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;desc&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;value&quot;</span>: <span class="string">&quot;iPhone&quot;</span></span><br><span class="line">        <span class="comment">//&quot;value&quot;:&quot;iphone&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//改成在 &quot;desc.keyword&quot; field 含有大寫字母的 &quot;iPohne&quot;，可以找到資料</span></span><br><span class="line"><span class="comment">//因為 keyword field 會完整保留原始資料</span></span><br><span class="line"><span class="comment">//反而使用小寫 &quot;iphone&quot; 是無法找到資料的</span></span><br><span class="line">POST /products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;desc.keyword&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;value&quot;</span>: <span class="string">&quot;iPhone&quot;</span></span><br><span class="line">        <span class="comment">//&quot;value&quot;:&quot;iphone&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//跟上面的範例相同，大寫的搜尋條件無法找到資料</span></span><br><span class="line"><span class="comment">//因為預設的 analyzer 會作小寫處理</span></span><br><span class="line">POST /products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;productID&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;value&quot;</span>: <span class="string">&quot;XHDK-A-1293-#fJ3&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用 keyword field 就可以正確使用大寫的搜尋條件找到資料了</span></span><br><span class="line">POST /products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//&quot;explain&quot;: true,</span></span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;productID.keyword&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;value&quot;</span>: <span class="string">&quot;XHDK-A-1293-#fJ3&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//透過 &quot;constant_score&quot; 將查詢轉換成 filter 來提昇查詢效率</span></span><br><span class="line"><span class="comment">//因為算分過程被忽略，所有搜尋結果都是 1 分</span></span><br><span class="line">POST /products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;constant_score&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;filter&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;productID.keyword&quot;</span>: <span class="string">&quot;XHDK-A-1293-#fJ3&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Full-Text-Query"><a href="#Full-Text-Query" class="headerlink" title="Full Text Query"></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/full-text-queries.html">Full Text Query</a></h2><ul><li><p>Full text query 可以是 match query, match phrase query, query string query … 等等，詳細的列表可以參考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/full-text-queries.html">官網資料</a></p></li><li><p>index &amp; query 都會進行分詞，查詢字串會先傳給一個合適的 analyzer，並生成用來查詢的 term list</p></li><li><p>分詞後的 term list 會被逐一拿來查詢，並將最後結果合併後，為每個 document 計算出一個分數</p></li></ul><p>以下是幾個簡單範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//若只有 query，預設的 operator 是 or，因此會找到多筆資料</span></span><br><span class="line"><span class="comment">//搭配 operator or minimum_should_match，可以讓查詢結果更準確</span></span><br><span class="line">POST movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;title&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">//&quot;operator&quot;: &quot;and&quot;, </span></span><br><span class="line">        <span class="comment">//&quot;minimum_should_match&quot;: 2,</span></span><br><span class="line">        <span class="attr">&quot;query&quot;</span>: <span class="string">&quot;Matrix reloaded&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//也可以使用 &quot;match_phrase&quot; 搭配 &quot;slop&quot; 讓搜尋結果更準確</span></span><br><span class="line">POST movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match_phrase&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;title&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">//&quot;slop&quot;: 1,</span></span><br><span class="line">        <span class="attr">&quot;query&quot;</span>: <span class="string">&quot;Matrix reloaded&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="結構化搜尋"><a href="#結構化搜尋" class="headerlink" title="結構化搜尋"></a>結構化搜尋</h1><h2 id="結構化數據"><a href="#結構化數據" class="headerlink" title="結構化數據"></a>結構化數據</h2><ul><li><p>像是 date, boolean, number … 這一類的數據都是屬於結構化的</p></li><li><p>有些 text 也屬於結構化的，例如：顏色(red, green, blue)、tag(distributed, search)、特定編碼….只要有遵守規定產生的 text，都可以算是結構化的格式</p></li></ul><blockquote><p>而結構化搜尋就是對結構化的數據進行搜尋</p></blockquote><ul><li><p>結構化的 text 可以做精確比對(term query) or 部份比對(prefix query)</p></li><li><p>結構化的搜尋結果只會有 “true” or “false” 兩種值，並且可以根據需求來決定是否做 scoring 的行為</p></li></ul><p>以下是一些簡單範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">#結構化搜索，精確匹配</span><br><span class="line">DELETE products</span><br><span class="line"></span><br><span class="line"><span class="comment">//加入資料</span></span><br><span class="line"><span class="comment">//並不是所有資料都有 date 欄位</span></span><br><span class="line">POST /products/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;price&quot;</span> : <span class="number">10</span>,<span class="attr">&quot;avaliable&quot;</span>:<span class="literal">true</span>,<span class="attr">&quot;date&quot;</span>:<span class="string">&quot;2018-01-01&quot;</span>, <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;XHDK-A-1293-#fJ3&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;price&quot;</span> : <span class="number">20</span>,<span class="attr">&quot;avaliable&quot;</span>:<span class="literal">true</span>,<span class="attr">&quot;date&quot;</span>:<span class="string">&quot;2019-01-01&quot;</span>, <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;KDKE-B-9947-#kL5&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">3</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;price&quot;</span> : <span class="number">30</span>,<span class="attr">&quot;avaliable&quot;</span>:<span class="literal">true</span>, <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;JODL-X-1937-#pV7&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">4</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;price&quot;</span> : <span class="number">30</span>,<span class="attr">&quot;avaliable&quot;</span>:<span class="literal">false</span>, <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;QQPX-R-3956-#aD8&quot;</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查詢 dynamic mapping 後的結果</span></span><br><span class="line">GET products/_mapping</span><br><span class="line"></span><br><span class="line"><span class="comment">//進行 term query，並計算出分數</span></span><br><span class="line">POST products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;profile&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;explain&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;avaliable&quot;</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//進行 term query，但使用 filter context，因此不算分</span></span><br><span class="line">POST products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;profile&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;explain&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;constant_score&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;filter&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;avaliable&quot;</span>: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用 range 查詢 &amp; 搭配 filter context 跳過算分步驟</span></span><br><span class="line">GET products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span> : &#123;</span><br><span class="line">    <span class="attr">&quot;constant_score&quot;</span> : &#123;</span><br><span class="line">      <span class="attr">&quot;filter&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;range&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : &#123;</span><br><span class="line">            <span class="attr">&quot;gte&quot;</span> : <span class="number">20</span>,</span><br><span class="line">            <span class="attr">&quot;lte&quot;</span>  : <span class="number">30</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用特殊字元(y-&gt;年)處理日期相關查詢</span></span><br><span class="line">POST products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span> : &#123;</span><br><span class="line">    <span class="attr">&quot;constant_score&quot;</span> : &#123;</span><br><span class="line">      <span class="attr">&quot;filter&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;range&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;date&quot;</span> : &#123;</span><br><span class="line">            <span class="attr">&quot;gte&quot;</span> : <span class="string">&quot;now-1y&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// =========== multi-value field 的處理 ===========</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//若是 field 中包含多個 text</span></span><br><span class="line">POST /movies/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span> : <span class="string">&quot;Father of the Bridge Part II&quot;</span>,<span class="attr">&quot;year&quot;</span>:<span class="number">1995</span>, <span class="attr">&quot;genre&quot;</span>:<span class="string">&quot;Comedy&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span> : <span class="string">&quot;Dave&quot;</span>,<span class="attr">&quot;year&quot;</span>:<span class="number">1993</span>,<span class="attr">&quot;genre&quot;</span>:[<span class="string">&quot;Comedy&quot;</span>,<span class="string">&quot;Romance&quot;</span>] &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//由於 genere 為 multi-value field</span></span><br><span class="line"><span class="comment">//因此以下搜尋會將 genre 中有包涵 Comedy 的資料全部顯示出來</span></span><br><span class="line">POST movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;constant_score&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;filter&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;genre.keyword&quot;</span>: <span class="string">&quot;Comedy&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="搜索的相關性算分"><a href="#搜索的相關性算分" class="headerlink" title="搜索的相關性算分"></a>搜索的相關性算分</h1><h2 id="相關性-amp-相關性算分"><a href="#相關性-amp-相關性算分" class="headerlink" title="相關性 &amp; 相關性算分"></a>相關性 &amp; 相關性算分</h2><ul><li><p><strong>搜尋的相關性算分</strong>描述一個 document 與查詢語句相符的程度，ES 會對每個符合查詢條件的結果進行算分，並放到 _score field 中</p></li><li><p>分數的是要用在搜尋結果排序中，在 ES5 之前使用 <code>TF-IDF</code> 算分，後面的版本使用 <code>BM 25</code></p></li></ul><h2 id="Term-Frequency-詞頻-amp-逆文檔頻率-Inverse-Document-Frequency-IDF"><a href="#Term-Frequency-詞頻-amp-逆文檔頻率-Inverse-Document-Frequency-IDF" class="headerlink" title="Term Frequency (詞頻) &amp; 逆文檔頻率(Inverse Document Frequency, IDF)"></a>Term Frequency (詞頻) &amp; 逆文檔頻率(Inverse Document Frequency, IDF)</h2><ul><li><p>檢索詞在一篇 document 中出現的頻率 = <code>檢索詞出現的次數 / document 總字數</code></p></li><li><p>衡量查詢 &amp; 結果 document 相關性的簡單方法就是將每個 term 的 TF 相加 = <code>TF(word1) + TF(word2) + TF(word3)</code></p></li><li><p>stop word 出現太多，一般計算分數時不會列入考慮</p></li><li><p>IDF = <code>log(全部 document 數量 / 檢索詞出現過的 document 總數)</code></p></li><li><p><code>TF-IDF</code> 其實就是從 sum(TF) 變成了 sum(TF + IDF)</p></li><li><p>現代的 search engine 大多以 TF-IDF 為基礎再加上大量的優化</p></li></ul><p><img src="/blog/images/Elasticsearch/es_tf-idf-scoring.png" alt="Elasticsearch - TD-IDF"></p><ul><li>從上圖可見，TF-IDF 的評分公式，除了 TD &amp; IDF 之外，還包涵了 <code>boosting</code> &amp; <code>field length(欄位長度)</code> 兩項</li></ul><h2 id="BM-25"><a href="#BM-25" class="headerlink" title="BM 25"></a>BM 25</h2><p><img src="/blog/images/Elasticsearch/es_bm25.png" alt="Elasticsearch - BM 25"></p><ul><li>跟 TF-IDF 相比，當 TF 無限增加時，BM 25 算分會趨近於某個值，不會無限變大</li></ul><h2 id="Boosting-Relevance"><a href="#Boosting-Relevance" class="headerlink" title="Boosting Relevance"></a>Boosting Relevance</h2><ul><li><p>boosting 是可以用來控制相關度的一種方式，可用在 index, field, 或是查詢子條件中</p></li><li><p>當 <code>boost &gt; 1</code> 可提昇相關度，<code>0 &lt; boost &lt; 1</code> 計算分數的權重相對降低, boost &lt; 0，貢獻為負分</p></li></ul><p>以下是一個簡單的範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">POST testscore/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;boosting&quot;</span> : &#123;</span><br><span class="line">      <span class="attr">&quot;positive&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;content&quot;</span> : <span class="string">&quot;elasticsearch&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;negative&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;content&quot;</span> : <span class="string">&quot;like&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;negative_boost&quot;</span> : <span class="number">0.2</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Query-amp-Filtering-與多字符串多字段查詢"><a href="#Query-amp-Filtering-與多字符串多字段查詢" class="headerlink" title="Query &amp; Filtering 與多字符串多字段查詢"></a>Query &amp; Filtering 與多字符串多字段查詢</h1><h2 id="Query-Context-amp-Filter-Context"><a href="#Query-Context-amp-Filter-Context" class="headerlink" title="Query Context &amp; Filter Context"></a>Query Context &amp; Filter Context</h2><ul><li><p>在 Elasticsearch 中，有 Query &amp; Filter 兩種不同的 Context (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html">官網文件</a>)</p></li><li><p>Query Context 會對相關性進行算分</p></li><li><p>Filter Context 不會進行算分，但可利用 cache 來取得更好的效能</p></li></ul><h2 id="Bool-Query"><a href="#Bool-Query" class="headerlink" title="Bool Query"></a>Bool Query</h2><ul><li>一個 bool query 是一個 or 多個查詢子句所組成</li></ul><table><thead><tr><th>子句</th><th>效果</th></tr></thead><tbody><tr><td><code>must</code></td><td>(<strong>Query Context</strong>) 必須符合，對算分有貢獻</td></tr><tr><td><code>should</code></td><td>(<strong>Query Context</strong>) 選擇性符合，對算分有貢獻</td></tr><tr><td><code>must_not</code></td><td>(<strong>Filter Context</strong>) 必須不能符合，對算分無貢獻</td></tr><tr><td><code>filter</code></td><td>(<strong>Filter Context</strong>) 必須符合，對算分無貢獻</td></tr></tbody></table><ul><li>bool query 中的每一個查詢子句得到的分數都會被合併成總和的相關性評分</li></ul><p>關於查詢的語法，有以下幾點需要注意：</p><ul><li><p>子查詢可以以任意的順序出現</p></li><li><p>可用 list 的方式在一個子查詢中加入多個查詢</p></li><li><p>若 bool query 中沒有 must 條件，那 should </p></li></ul><h2 id="範例"><a href="#範例" class="headerlink" title="範例"></a>範例</h2><h3 id="包含多個子查詢的-Bool-Query"><a href="#包含多個子查詢的-Bool-Query" class="headerlink" title="包含多個子查詢的 Bool Query"></a>包含多個子查詢的 Bool Query</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新增多筆資料</span></span><br><span class="line">POST /products/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;price&quot;</span> : <span class="number">10</span>,<span class="attr">&quot;avaliable&quot;</span>:<span class="literal">true</span>,<span class="attr">&quot;date&quot;</span>:<span class="string">&quot;2018-01-01&quot;</span>, <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;XHDK-A-1293-#fJ3&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;price&quot;</span> : <span class="number">20</span>,<span class="attr">&quot;avaliable&quot;</span>:<span class="literal">true</span>,<span class="attr">&quot;date&quot;</span>:<span class="string">&quot;2019-01-01&quot;</span>, <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;KDKE-B-9947-#kL5&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">3</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;price&quot;</span> : <span class="number">30</span>,<span class="attr">&quot;avaliable&quot;</span>:<span class="literal">true</span>, <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;JODL-X-1937-#pV7&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">4</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;price&quot;</span> : <span class="number">30</span>,<span class="attr">&quot;avaliable&quot;</span>:<span class="literal">false</span>, <span class="attr">&quot;productID&quot;</span> : <span class="string">&quot;QQPX-R-3956-#aD8&quot;</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//bool query，使用多個子查詢</span></span><br><span class="line">POST /products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span> : &#123;</span><br><span class="line">      <span class="comment">//Query Context</span></span><br><span class="line">      <span class="attr">&quot;must&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span> : &#123; <span class="attr">&quot;price&quot;</span> : <span class="string">&quot;30&quot;</span> &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="comment">//Filter Context</span></span><br><span class="line">      <span class="attr">&quot;filter&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span> : &#123; <span class="attr">&quot;avaliable&quot;</span> : <span class="string">&quot;true&quot;</span> &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="comment">//Filter Context</span></span><br><span class="line">      <span class="attr">&quot;must_not&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;range&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;price&quot;</span> : &#123; <span class="attr">&quot;lte&quot;</span> : <span class="number">10</span> &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="comment">//Query Context</span></span><br><span class="line">      <span class="attr">&quot;should&quot;</span> : [</span><br><span class="line">        &#123; <span class="attr">&quot;term&quot;</span> : &#123; <span class="attr">&quot;productID.keyword&quot;</span> : <span class="string">&quot;JODL-X-1937-#pV7&quot;</span> &#125; &#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;term&quot;</span> : &#123; <span class="attr">&quot;productID.keyword&quot;</span> : <span class="string">&quot;XHDK-A-1293-#fJ3&quot;</span> &#125; &#125;</span><br><span class="line">      ],</span><br><span class="line">      <span class="attr">&quot;minimum_should_match&quot;</span> :<span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="複雜的-Bool-Query"><a href="#複雜的-Bool-Query" class="headerlink" title="複雜的 Bool Query"></a>複雜的 Bool Query</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">POST /products/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;must&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;price&quot;</span>: <span class="string">&quot;30&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="comment">//在 bool query 的子查詢中再塞進一個 bool query</span></span><br><span class="line">      <span class="attr">&quot;should&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;must_not&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">                <span class="attr">&quot;avaliable&quot;</span>: <span class="string">&quot;false&quot;</span></span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ],</span><br><span class="line">      <span class="attr">&quot;minimum_should_match&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="不同的算分標準"><a href="#不同的算分標準" class="headerlink" title="不同的算分標準"></a>不同的算分標準</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">POST /animals/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;should&quot;</span>: [</span><br><span class="line">        &#123; <span class="attr">&quot;term&quot;</span>: &#123; <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;brown&quot;</span> &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;term&quot;</span>: &#123; <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;red&quot;</span> &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;term&quot;</span>: &#123; <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;quick&quot;</span>   &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;term&quot;</span>: &#123; <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;dog&quot;</span>   &#125;&#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">POST /animals/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;should&quot;</span>: [</span><br><span class="line">        &#123; <span class="attr">&quot;term&quot;</span>: &#123; <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;quick&quot;</span> &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;term&quot;</span>: &#123; <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;dog&quot;</span>   &#125;&#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="comment">//將同類型的條件分類在另外一個 bool query 中</span></span><br><span class="line">          <span class="comment">//會讓相關性分數的計算更加準確</span></span><br><span class="line">          <span class="attr">&quot;bool&quot;</span>:&#123;</span><br><span class="line">            <span class="attr">&quot;should&quot;</span>:[</span><br><span class="line">               &#123; <span class="attr">&quot;term&quot;</span>: &#123; <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;brown&quot;</span> &#125;&#125;,</span><br><span class="line">                 &#123; <span class="attr">&quot;term&quot;</span>: &#123; <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;brown&quot;</span> &#125;&#125;,</span><br><span class="line">            ]</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="搭配-boosting-進行查詢"><a href="#搭配-boosting-進行查詢" class="headerlink" title="搭配 boosting 進行查詢"></a>搭配 boosting 進行查詢</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">DELETE news</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增多筆資料</span></span><br><span class="line">POST /news/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;content&quot;</span>:<span class="string">&quot;Apple Mac&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;content&quot;</span>:<span class="string">&quot;Apple iPad&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">3</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;content&quot;</span>:<span class="string">&quot;Apple employee like Apple Pie and Apple Juice&quot;</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//若是想要搜尋的是與 apple computer 相關的資訊</span></span><br><span class="line"><span class="comment">//最後一筆與食物相關的訊息會變成搜尋結果第一筆，因為 apple 出現次數最多</span></span><br><span class="line">POST news/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;must&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;match&quot;</span>:&#123;<span class="attr">&quot;content&quot;</span>:<span class="string">&quot;apple&quot;</span>&#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//限制食物相關訊息不能出現</span></span><br><span class="line">POST news/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;must&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;match&quot;</span>:&#123;<span class="attr">&quot;content&quot;</span>:<span class="string">&quot;apple&quot;</span>&#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;must_not&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;match&quot;</span>:&#123;<span class="attr">&quot;content&quot;</span>:<span class="string">&quot;pie&quot;</span>&#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//希望 apple 關鍵字的訊息都出現</span></span><br><span class="line"><span class="comment">//但透過 boosting 的設定讓食物相關訊息分數較低</span></span><br><span class="line">POST news/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;boosting&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;positive&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;content&quot;</span>: <span class="string">&quot;apple&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;negative&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;content&quot;</span>: <span class="string">&quot;pie&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;negative_boost&quot;</span>: <span class="number">0.5</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="單字符串多字段查詢：Disjunction-Max-Query"><a href="#單字符串多字段查詢：Disjunction-Max-Query" class="headerlink" title="單字符串多字段查詢：Disjunction Max Query"></a>單字符串多字段查詢：Disjunction Max Query</h1><p>一般在進行 single string multiple field 的查詢時，預設的分數計算方式過於簡單，往往會出現非使用者所預期的情況，例如以下範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">DELETE blogs</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增多筆資料</span></span><br><span class="line">POST /blogs/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Quick brown rabbits&quot;</span>, <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;Brown rabbits are commonly seen.&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Keeping pets healthy&quot;</span>, <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;My quick brown fox eats rabbits on a regular basis.&quot;</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//預期應該會是 id=2 的 document 相關性比較高</span></span><br><span class="line"><span class="comment">//但結果卻是 id=1 的相關度較高</span></span><br><span class="line"><span class="comment">//因為 id=1 的 document 在 title &amp; body 都有 brown</span></span><br><span class="line">POST /blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;bool&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;should&quot;</span>: [</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Brown fox&quot;</span> &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;Brown fox&quot;</span> &#125;&#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//改用 dis_max 後，會綜合判斷所有符合 document 的分數 &amp; 相關 field</span></span><br><span class="line"><span class="comment">//找到最符合的評分後再返回</span></span><br><span class="line"><span class="comment">//因此 id=2 的 document 符合程度較高，就會被排在前面</span></span><br><span class="line">POST /blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;dis_max&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;queries&quot;</span>: [</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Brown fox&quot;</span> &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;Brown fox&quot;</span> &#125;&#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因此從上面的範例來看，透過 <strong>Disjunction Max Query</strong> 可以找到最符合查詢條件的結果。</p><p>但 Disjunction Max Query 也並非萬靈丹，因為其算分方式也是有盲點的，例如以下範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用 Disjunction Max Query，會讓以下搜尋結果的分數都相同</span></span><br><span class="line">POST blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;dis_max&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;queries&quot;</span>: [</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Quick pets&quot;</span> &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;Quick pets&quot;</span> &#125;&#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//透過加上 tie_breaker 的方式，改變計算分數的方法</span></span><br><span class="line"><span class="comment">//讓實際出現較多查詢條件的 document 取得較高的算分</span></span><br><span class="line">POST blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;dis_max&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;queries&quot;</span>: [</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Quick pets&quot;</span> &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;Quick pets&quot;</span> &#125;&#125;</span><br><span class="line">      ],</span><br><span class="line">      <span class="attr">&quot;tie_breaker&quot;</span>: <span class="number">0.2</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>關於 <code>Disjunction Max Query</code> &amp; <code>tie_breaker</code> 的搭配，可以參考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-dis-max-query.html">官方文件</a>說明。</p><h1 id="單字符串多字段查詢：Multi-Match"><a href="#單字符串多字段查詢：Multi-Match" class="headerlink" title="單字符串多字段查詢：Multi Match"></a>單字符串多字段查詢：Multi Match</h1><p>一般 single string multiple field 的查詢，會發生在三個場景：</p><ul><li><code>Best Field</code> (預設 type)</li></ul><blockquote><p>每個 field 之間會互相競爭，搜尋時會把評分最高的 field 中的資料回傳(可以額外加上 <code>tie_breaker</code> or <code>minumum_should_match</code> 來調整搜尋精準度)</p></blockquote><ul><li><code>Most Field</code></li></ul><blockquote><p>通常會使用在英文的內容時，通常會在 main field 中設定 English Analyzer，並加入同義詞來試圖符合更多的 document，並適時的 text 中加入 sub field 搭配 standard Analyzer 盡量保留原始訊息，以提供更精確的搜尋結果</p></blockquote><ul><li><code>Cross Field</code></li></ul><blockquote><p>某些訊息需要同時在多個 field 查詢時，可以做一種類似多(single string -&gt; multi words)對多(multi field)的搜尋</p></blockquote><p>首先使用以下幾個範例來說明 <strong>Best Field</strong>：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">DELETE blogs</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增多筆資料</span></span><br><span class="line">POST /blogs/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Quick brown rabbits&quot;</span>, <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;Brown rabbits are commonly seen.&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Keeping pets healthy&quot;</span>, <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;My quick brown fox eats rabbits on a regular basis.&quot;</span> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//Disjunction Max Query 搜尋的盲點</span></span><br><span class="line"><span class="comment">//兩個 document 都會取得相同的分數</span></span><br><span class="line">POST blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;dis_max&quot;</span>: &#123;</span><br><span class="line">      <span class="comment">//&quot;tie_breaker&quot;: 0.2,</span></span><br><span class="line">      <span class="attr">&quot;queries&quot;</span>: [</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;Quick pets&quot;</span> &#125;&#125;,</span><br><span class="line">        &#123; <span class="attr">&quot;match&quot;</span>: &#123; <span class="attr">&quot;body&quot;</span>:  <span class="string">&quot;Quick pets&quot;</span> &#125;&#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//multi_match 卻沒搭配 tie_break or minimum_should_match</span></span><br><span class="line"><span class="comment">//效果跟 dis_max 是相同的</span></span><br><span class="line"><span class="comment">//但適時的加上 tie_break or minimum_should_match 可以提高搜尋的準確度</span></span><br><span class="line">POST blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">      <span class="comment">//&quot;tie_breaker&quot;: 0.2,</span></span><br><span class="line">      <span class="comment">//&quot;minimum_should_match&quot;: &quot;20%&quot;, </span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;best_fields&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;query&quot;</span>: <span class="string">&quot;Quick pets&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;fields&quot;</span>: [<span class="string">&quot;title&quot;</span>,<span class="string">&quot;body&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">DELETE titles</span><br><span class="line"></span><br><span class="line"><span class="comment">//設定 analyzer 為 english</span></span><br><span class="line">PUT /titles</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;properties&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;title&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;analyzer&quot;</span>: <span class="string">&quot;english&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增多筆資料</span></span><br><span class="line">POST titles/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;My dog barks&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;I see a lot of barking dogs on the road &quot;</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//搜尋結果跟預期不同</span></span><br><span class="line"><span class="comment">//由於 english analyzer 會將 barking dogs 分詞成 bark &amp; dog 兩個字</span></span><br><span class="line"><span class="comment">//因此上面兩個 document 計算出的分數都是相同的</span></span><br><span class="line">GET titles/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;barking dogs&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>從上面可以看出 best field 在某些時候的確會有一些盲點，因此我們可以試著使用以下方法來調整：</p><ul><li><p>額外增加一個 <code>std</code> field，並使用 standard analyzer</p></li><li><p>將 <strong>query.multi_match.type</strong> 改成 <code>most_fields</code></p></li></ul><blockquote><p>也可以根據需求適時加入 boosting 設定</p></blockquote><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">DELETE /titles</span><br><span class="line"></span><br><span class="line"><span class="comment">//額外增加一個 &quot;std&quot; field，並使用 standard analyzer</span></span><br><span class="line">PUT /titles</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;properties&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;title&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;analyzer&quot;</span>: <span class="string">&quot;english&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;fields&quot;</span>: &#123;<span class="attr">&quot;std&quot;</span>: &#123;<span class="attr">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>,<span class="attr">&quot;analyzer&quot;</span>: <span class="string">&quot;standard&quot;</span>&#125;&#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增資料</span></span><br><span class="line">POST titles/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">1</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;My dog barks&quot;</span> &#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span>: &#123; <span class="attr">&quot;_id&quot;</span>: <span class="number">2</span> &#125;&#125;</span><br><span class="line">&#123; <span class="attr">&quot;title&quot;</span>: <span class="string">&quot;I see a lot of barking dogs on the road &quot;</span> &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//將 query.multi_match.type 改成 &quot;most_fields&quot;</span></span><br><span class="line">GET /titles/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;query&quot;</span>:  <span class="string">&quot;barking dogs&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;type&quot;</span>:   <span class="string">&quot;most_fields&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;fields&quot;</span>: [ <span class="string">&quot;title&quot;</span>, <span class="string">&quot;title.std&quot;</span> ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//也可以根據需要加入 boosting 的設定(範例中的 ^10 就是)</span></span><br><span class="line">GET /titles/_search</span><br><span class="line">&#123;</span><br><span class="line">   <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;query&quot;</span>:  <span class="string">&quot;barking dogs&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;type&quot;</span>:   <span class="string">&quot;most_fields&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;fields&quot;</span>: [ <span class="string">&quot;title^10&quot;</span>, <span class="string">&quot;title.std&quot;</span> ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接著如果我們需要針對更複雜的資料(例如：地址)，並同時在多個 field 中做搜尋，則可以藉由 <code>cross_fields</code> 來取得更精確的搜尋結果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新增一筆地址資料</span></span><br><span class="line">PUT address/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;street&quot;</span>: <span class="string">&quot;5 Poland Street&quot;</span>, </span><br><span class="line">  <span class="attr">&quot;city&quot;</span>: <span class="string">&quot;London&quot;</span>, </span><br><span class="line">  <span class="attr">&quot;country&quot;</span>: <span class="string">&quot;United Kingdom&quot;</span>, </span><br><span class="line">  <span class="attr">&quot;postcode&quot;</span>: <span class="string">&quot;W1V 3DG&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//透過 cross_field&quot; 進行 multi field 資料搜尋</span></span><br><span class="line"><span class="comment">//若是 type 改為 &quot;most_fields&quot; 搭配 AND operator 就會找不到資料</span></span><br><span class="line">POST address/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;query&quot;</span>: <span class="string">&quot;Poland Kingdom W1V&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;cross_fields&quot;</span>, </span><br><span class="line">      <span class="attr">&quot;operator&quot;</span>: <span class="string">&quot;and&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;fields&quot;</span>: [<span class="string">&quot;street&quot;</span>, <span class="string">&quot;city&quot;</span>, <span class="string">&quot;country&quot;</span>, <span class="string">&quot;postcode&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="全文檢索案例"><a href="#全文檢索案例" class="headerlink" title="全文檢索案例"></a>全文檢索案例</h1><h2 id="思考-amp-分析"><a href="#思考-amp-分析" class="headerlink" title="思考 &amp; 分析"></a>思考 &amp; 分析</h2><ul><li><p><code>Full text query</code> or <code>Term query</code></p></li><li><p>針對搜尋的條件 &amp; field，定義合適的 analyzer 是很重要的</p></li><li><p>開發時需要對不同的 analyzer 做測試</p></li><li><p>透過調整 mapping 設定，並改寫搜尋條件，來交叉評估不同的搜尋結果以取得期望的結果</p></li></ul><h2 id="相關性測試"><a href="#相關性測試" class="headerlink" title="相關性測試"></a>相關性測試</h2><ul><li><p>相關性測試並非簡單工作，必須了解原理 &amp; 多做分析 &amp; 多做調整</p></li><li><p>可能會需要多 analyzer &amp; boosting … 等參數做各式各樣的調整</p></li><li><p>每個環境跟需求對應到的設定 &amp; 搜尋條件也都不會相同，不會有 silver bullet 般的設定可以一次解決所有問題</p></li></ul><h2 id="監控-amp-理解用戶行為"><a href="#監控-amp-理解用戶行為" class="headerlink" title="監控 &amp; 理解用戶行為"></a>監控 &amp; 理解用戶行為</h2><ul><li><p>相關性測試其實是最後一步，不要進行過度的相關性測試是很重要的</p></li><li><p>必須嘗試監控用戶的搜尋結果，並試著理解使用者的使用行為</p><ul><li><p>例如：在後台實作功能，查詢使用者的哪些查詢是沒有回傳任何結果的</p></li><li><p>衡量使用者對於實際搜尋結果的點擊率</p></li></ul></li></ul><h1 id="使用-Search-Template-和-Index-Alias-查詢"><a href="#使用-Search-Template-和-Index-Alias-查詢" class="headerlink" title="使用 Search Template 和 Index Alias 查詢"></a>使用 Search Template 和 Index Alias 查詢</h1><h2 id="Search-Template"><a href="#Search-Template" class="headerlink" title="Search Template"></a>Search Template</h2><ul><li><p>主要的功能在於 de-couple <code>程式</code>與<code>查詢用的 DSL</code> 兩者之間的關係，讓專業的工程師可以各司其職的完成工作(開發人員/查詢工程師/效能調校工程師)</p></li><li><p>程式設計師不需要了解查詢的優化細節，只要使用 ES 工程師提供的 Search Template 即可</p></li><li><p>ES 工程師則可以專注在效能的調校 &amp; 優化 search template 工作上</p></li></ul><p>以下是一個簡單範例：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新增 search template</span></span><br><span class="line">POST _scripts/tmdb</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;script&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;lang&quot;</span>: <span class="string">&quot;mustache&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;source&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;_source&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;title&quot;</span>,<span class="string">&quot;overview&quot;</span></span><br><span class="line">      ],</span><br><span class="line">      <span class="attr">&quot;size&quot;</span>: <span class="number">20</span>,</span><br><span class="line">      <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;query&quot;</span>: <span class="string">&quot;&#123;&#123;q&#125;&#125;&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;fields&quot;</span>: [<span class="string">&quot;title&quot;</span>,<span class="string">&quot;overview&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//檢視 search index 的內容</span></span><br><span class="line">GET _scripts/tmdb</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用 search template 查詢，帶入預先定義的參數 q</span></span><br><span class="line">POST tmdb/_search/template</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;id&quot;</span>:<span class="string">&quot;tmdb&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;params&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;q&quot;</span>: <span class="string">&quot;basketball with cartoon aliens&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//刪除 search template</span></span><br><span class="line">DELETE _scripts/tmdb</span><br></pre></td></tr></table></figure><h2 id="Index-Alias"><a href="#Index-Alias" class="headerlink" title="Index Alias"></a>Index Alias</h2><p><code>index alias</code> 就跟 Linux 中的 <code>alias</code> 指令一樣，是作為設定別名的用途。</p><p>而通常為 index 設定別名是有其需要的，例如每天建立一個新的 index，但在程式中每天都根據日期去產生一個字串來作為 index 來查詢，其實挺麻煩的；此時透過設定一個名稱為 <strong>latest_index</strong> 並指到每天最新的 index 的方式，問題就迎刃而解了。</p><p>以下是一個簡單範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">PUT movies-2019/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;name&quot;</span>:<span class="string">&quot;the matrix&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;rating&quot;</span>:<span class="number">5</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT movies-2019/_doc/2</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Speed&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;rating&quot;</span>:<span class="number">3</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//建立 index alias</span></span><br><span class="line">POST _aliases</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;actions&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;add&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;index&quot;</span>: <span class="string">&quot;movies-2019&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;alias&quot;</span>: <span class="string">&quot;movies-latest&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//對 index alias 進行查詢</span></span><br><span class="line">POST movies-latest/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//另外建立一個 index alias</span></span><br><span class="line">POST _aliases</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;actions&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;add&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;index&quot;</span>: <span class="string">&quot;movies-2019&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;alias&quot;</span>: <span class="string">&quot;movies-lastest-highrate&quot;</span>,</span><br><span class="line">        <span class="comment">//index alias 設定中還可以額外設定 filter </span></span><br><span class="line">        <span class="attr">&quot;filter&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;range&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;rating&quot;</span>: &#123;</span><br><span class="line">              <span class="attr">&quot;gte&quot;</span>: <span class="number">4</span></span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//對 index alias 進行查詢</span></span><br><span class="line">POST movies-lastest-highrate/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="綜合排序：Function-Score-Query-優化算分"><a href="#綜合排序：Function-Score-Query-優化算分" class="headerlink" title="綜合排序：Function Score Query 優化算分"></a>綜合排序：Function Score Query 優化算分</h1><h2 id="Scoring-amp-Sorting"><a href="#Scoring-amp-Sorting" class="headerlink" title="Scoring &amp; Sorting"></a>Scoring &amp; Sorting</h2><ul><li><p>Elasticsearch 預設會以 document 的相關度算分進行排序</p></li><li><p>可以指定一個 or 多個 field 進行排序</p></li><li><p>但預設情況下，無法對相關度 or 排序進行更多細部的控制調整</p></li></ul><h2 id="Function-Score-Query"><a href="#Function-Score-Query" class="headerlink" title="Function Score Query"></a>Function Score Query</h2><p>可以在查詢結束後，對每一個符合的 document 進行重新算分，並根據新的分數來進行排序。</p><p>Elasticsearch 中已經包含了以下幾種用來算分的函數：(非全部，比較常用的，詳細資訊可參考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html">官網</a>)</p><ul><li><p><code>Weight</code>：為每一個 document 設定一個簡單不被規範化的權重</p></li><li><p><code>Field Value Factor</code>：可指定特定的 field 來影響算分過程，例如指定<strong>點讚數</strong>作為算分的條件之一</p></li><li><p><code>Random Score</code>：隨機算分</p></li><li><p><code>Decay Function</code>：以某個 field 的值作為基準，距離越遠得分越高</p></li><li><p><code>Script Score</code>：自己寫 script 來自定算分邏輯</p></li></ul><p>以下用一個實際範例來說明使用方法：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">DELETE blogs</span><br><span class="line"></span><br><span class="line"><span class="comment">//加入三筆一模一樣的資料到 blogs index</span></span><br><span class="line"><span class="comment">//唯一的差別只有 votes 數量而已</span></span><br><span class="line"><span class="comment">//若使用預設的 BM25 進行算分，三個 document 會得到相同分數</span></span><br><span class="line">PUT /blogs/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;title&quot;</span>:   <span class="string">&quot;About popularity&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;content&quot;</span>: <span class="string">&quot;In this post we will talk about...&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;votes&quot;</span>:   <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line">PUT /blogs/_doc/2</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;title&quot;</span>:   <span class="string">&quot;About popularity&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;content&quot;</span>: <span class="string">&quot;In this post we will talk about...&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;votes&quot;</span>:   <span class="number">100</span></span><br><span class="line">&#125;</span><br><span class="line">PUT /blogs/_doc/3</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;title&quot;</span>:   <span class="string">&quot;About popularity&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;content&quot;</span>: <span class="string">&quot;In this post we will talk about...&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;votes&quot;</span>:   <span class="number">1000000</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用 function score query，搭配 field_value_factor 來指定重要的 field</span></span><br><span class="line"><span class="comment">//但會發現 votes 值超高的 document 會取得超高的分數</span></span><br><span class="line">POST /blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;function_score&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;query&quot;</span>:    <span class="string">&quot;popularity&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;fields&quot;</span>: [ <span class="string">&quot;title&quot;</span>, <span class="string">&quot;content&quot;</span> ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;field_value_factor&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;votes&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//因此除了 field_value_factor 之外</span></span><br><span class="line"><span class="comment">//可以額外設定 modifier = log1p 來進行平滑曲線設定</span></span><br><span class="line"><span class="comment">//new score = old score * log(1 + votes)</span></span><br><span class="line">POST /blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;function_score&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;query&quot;</span>:    <span class="string">&quot;popularity&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;fields&quot;</span>: [ <span class="string">&quot;title&quot;</span>, <span class="string">&quot;content&quot;</span> ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;field_value_factor&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;votes&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;modifier&quot;</span>: <span class="string">&quot;log1p&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//除了 modifier 之外，還可以額外增加 factor 對平滑曲線做更細膩的控制</span></span><br><span class="line"><span class="comment">//new score = old score * log(1 + factor * votes)</span></span><br><span class="line">POST /blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;function_score&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;query&quot;</span>:    <span class="string">&quot;popularity&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;fields&quot;</span>: [ <span class="string">&quot;title&quot;</span>, <span class="string">&quot;content&quot;</span> ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;field_value_factor&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;votes&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;modifier&quot;</span>: <span class="string">&quot;log1p&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;factor&quot;</span>: <span class="number">0.1</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Boost-Mode-amp-Max-Boost"><a href="#Boost-Mode-amp-Max-Boost" class="headerlink" title="Boost Mode &amp; Max Boost"></a>Boost Mode &amp; Max Boost</h2><p>在 function score query 中同樣也可以加入 boost 的設定，並可以搭配多種不同的 boost mode 來調整計算分數時的基礎：</p><ul><li><p><code>multiply</code>: query score 與 function score 相乘(預設值)</p></li><li><p><code>replace</code>: 忽略 query score，只使用 function score</p></li><li><p><code>sum</code>: query score 與 function score 相加</p></li><li><p><code>avg</code>: query score 與 function score 的平均值</p></li><li><p><code>max</code>: query score 與 function score 兩者中較大的值</p></li><li><p><code>min</code>: query score 與 function score 兩者中較小的值</p></li></ul><p>以下是個簡單範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">POST /blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;function_score&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;multi_match&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;query&quot;</span>:    <span class="string">&quot;popularity&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;fields&quot;</span>: [ <span class="string">&quot;title&quot;</span>, <span class="string">&quot;content&quot;</span> ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="comment">//使用 function score query，搭配 log1p &amp; factor 的設定來做平滑曲線的處理</span></span><br><span class="line">      <span class="attr">&quot;field_value_factor&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;votes&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;modifier&quot;</span>: <span class="string">&quot;log1p&quot;</span> ,</span><br><span class="line">        <span class="attr">&quot;factor&quot;</span>: <span class="number">0.1</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="comment">//還可以額外設定 boost mode &amp; max_boost 來進行搜尋的調校</span></span><br><span class="line">      <span class="attr">&quot;boost_mode&quot;</span>: <span class="string">&quot;sum&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;max_boost&quot;</span>: <span class="number">3</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="一致性的隨機函數"><a href="#一致性的隨機函數" class="headerlink" title="一致性的隨機函數"></a>一致性的隨機函數</h2><ul><li><p>當網站上的廣告需要提高曝光率，又不想要亂槍打鳥時 (希望每個使用者進來看到的廣告是相同的)</p></li><li><p>透過一致性的隨機函數可以讓每個用戶看到不同的隨機排名，但同一個用戶每一次的 request 都會得到一致的回應</p></li></ul><p>以上的需求就可以透過 function score query 中的 <code>Random Score</code> 來達成：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//以隨機但一致的方式進行算分(透過 seed 參數控制)</span></span><br><span class="line">POST /blogs/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;function_score&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;random_score&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;seed&quot;</span>: <span class="number">911119</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Term-amp-Phrase-Suggester"><a href="#Term-amp-Phrase-Suggester" class="headerlink" title="Term &amp; Phrase Suggester"></a>Term &amp; Phrase Suggester</h1><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html">Elastic 官網文件 - Suggester</a></p><h2 id="什麼是搜尋建議"><a href="#什麼是搜尋建議" class="headerlink" title="什麼是搜尋建議?"></a>什麼是搜尋建議?</h2><ul><li><p>搜尋建議是現代化搜尋引擎常見的功能</p></li><li><p>用途是在幫助使用者在輸入搜尋內容的過程中，協助補完搜尋內容，或是進行錯誤的偵測，當然也可以推薦用戶符合更多搜尋條件的建議，因此這樣的作法同樣也可以提高搜尋時找到合適結果的機率</p></li></ul><h2 id="Elasticsearch-Suggester-API"><a href="#Elasticsearch-Suggester-API" class="headerlink" title="Elasticsearch Suggester API"></a>Elasticsearch Suggester API</h2><ul><li><p>搜尋建議的功能在 Elasticsearch 中是透過 Suggester API 來實現的</p></li><li><p>而運作的原理在於將輸入的內容分解為 token，接著在索引的字典裡面尋找相似的 term 並回傳</p></li><li><p>目前 Elasticsearch 一共支援四種 suggester，分別是 <code>Term</code>, <code>Phrase</code>, <code>Complete</code>, <code>Context</code> 四種</p></li><li><p>每個 sueggester 中還可以額外設定好幾個 suggestion mode，分別是：</p><ul><li><p><code>Missing</code>: 如果索引中已經存在，就不提供建議 (表示使用者已經輸入正確的內容)</p></li><li><p><code>Popular</code>: 推薦出現頻率比較高的詞</p></li><li><p><code>Always</code>: 無論是否存在，都提供建議</p></li></ul></li></ul><p>以下是幾個簡單範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">DELETE articles</span><br><span class="line"></span><br><span class="line"><span class="comment">//在 index 中新增多筆資料</span></span><br><span class="line">POST articles/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;body&quot;</span>: <span class="string">&quot;lucene is very cool&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;body&quot;</span>: <span class="string">&quot;Elasticsearch builds on top of lucene&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;body&quot;</span>: <span class="string">&quot;Elasticsearch rocks&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;body&quot;</span>: <span class="string">&quot;elastic is the company behind ELK stack&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;body&quot;</span>: <span class="string">&quot;Elk stack rocks&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123;&#125; &#125;</span><br><span class="line">&#123;  <span class="attr">&quot;body&quot;</span>: <span class="string">&quot;elasticsearch is rock solid&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//進行 term suggestion</span></span><br><span class="line"><span class="comment">//透過檢查 response 中的 &quot;suggest&quot; 區段內容</span></span><br><span class="line"><span class="comment">//可以發現 &quot;lucen&quot; 取得了 &quot;lucene&quot; 的建議</span></span><br><span class="line"><span class="comment">//而 rock 就沒有了，因為 suggest mode 設定為 missing 的關係</span></span><br><span class="line">POST /articles/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;size&quot;</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;body&quot;</span>: <span class="string">&quot;lucen rock&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;suggest&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;term-suggestion&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;lucen rock&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;suggest_mode&quot;</span>: <span class="string">&quot;missing&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;body&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//同樣的 suggest 搜尋，但搭配 suggestion mode 為 &quot;popular&quot;</span></span><br><span class="line"><span class="comment">//此時 rock 就會出現了</span></span><br><span class="line">POST /articles/_search</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;suggest&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;term-suggestion&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;lucen rock&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;suggest_mode&quot;</span>: <span class="string">&quot;popular&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;body&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每個回傳的 suggest 都包含了一個算分，這也代表著相似性(相似性越高，分數越高)，相似性是由 <strong>LEvenshtein Edit Distance</strong> 的計算方法來實作出來的，核心概念就是 <code>一個詞變更了多少字元就可以和另外一個詞一致</code>。</p><p>而相似性的設定部份也可以透過一些參數可以來控制，例如 <code>max_edits</code></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//若是打錯字(範例中的 hocks)，也是可以可以取得搜尋建議</span></span><br><span class="line"><span class="comment">//但是必須要把 prefix_length 那行設定打開</span></span><br><span class="line"><span class="comment">//prefix_length 的功能如下：</span></span><br><span class="line"><span class="comment">// The number of minimal prefix characters that must match in order be a candidate for suggestions. Defaults to 1. Increasing this number improves spellcheck performance. Usually misspellings don’t occur in the beginning of terms. (Old name &quot;prefix_len&quot; is deprecated)</span></span><br><span class="line">POST /articles/_search</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;suggest&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;term-suggestion&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;lucen hocks&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;term&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;suggest_mode&quot;</span>: <span class="string">&quot;always&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;body&quot;</span>,</span><br><span class="line">        <span class="comment">//&quot;prefix_length&quot;:0,</span></span><br><span class="line">        <span class="attr">&quot;sort&quot;</span>: <span class="string">&quot;frequency&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//一個很簡單的 phrase suggester 的範例</span></span><br><span class="line">POST /articles/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;suggest&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;my-suggestion&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;lucne and elasticsear rock hello world &quot;</span>,</span><br><span class="line">      <span class="comment">//要改成 phrase suggester，只要在這裡從 &quot;term&quot; 改為 &quot;phrase&quot; 即可</span></span><br><span class="line">      <span class="attr">&quot;phrase&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;body&quot;</span>,</span><br><span class="line">        <span class="comment">//相較於 term suggester，還額外支援了不少的搜尋參數</span></span><br><span class="line">        <span class="comment">//稍微調整一下兩個參數的值就可以看出會有些不同</span></span><br><span class="line">        <span class="attr">&quot;max_errors&quot;</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="attr">&quot;confidence&quot;</span>:<span class="number">0</span>,</span><br><span class="line">        <span class="attr">&quot;direct_generator&quot;</span>:[&#123;</span><br><span class="line">          <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;body&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;suggest_mode&quot;</span>:<span class="string">&quot;always&quot;</span></span><br><span class="line">        &#125;],</span><br><span class="line">        <span class="attr">&quot;highlight&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;pre_tag&quot;</span>: <span class="string">&quot;&lt;em&gt;&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;post_tag&quot;</span>: <span class="string">&quot;&lt;/em&gt;&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Completion-amp-Context-Suggester"><a href="#Completion-amp-Context-Suggester" class="headerlink" title="Completion &amp; Context Suggester"></a>Completion &amp; Context Suggester</h1><h2 id="Completion-Suggester"><a href="#Completion-Suggester" class="headerlink" title="Completion Suggester"></a>Completion Suggester</h2><ul><li><p>Completion Suggester 提供 auto complete 的功能，因此使用者每輸入一個字，就需要即時發送一個查詢到後端來搜尋符合項目</p></li><li><p>承上，這樣的行為對效能的要求就會相對嚴格；而 Elasticsearch 本身則採用了不同的數據結構來實現這個需求，將 analyzer 處理過後的數據編碼成 FST 和索引一起存放，而整個 FST 會被放到記憶體中，因此存取相對快很多</p></li><li><p>但 FST 只能進行 prefix 的搜尋</p></li></ul><p>要使用 completion suggester，就必須要在 index mapping 的設定中進行相關設定：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PUT articles</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;properties&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;title_completion&quot;</span>:&#123;</span><br><span class="line">        <span class="comment">//需要將 type 設定為 completion</span></span><br><span class="line">        <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;completion&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下是實際的操作範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新增資料</span></span><br><span class="line">POST articles/_bulk</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title_completion&quot;</span>: <span class="string">&quot;lucene is very cool&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title_completion&quot;</span>: <span class="string">&quot;Elasticsearch builds on top of lucene&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title_completion&quot;</span>: <span class="string">&quot;Elasticsearch rocks&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title_completion&quot;</span>: <span class="string">&quot;elastic is the company behind ELK stack&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123; &#125; &#125;</span><br><span class="line">&#123; <span class="attr">&quot;title_completion&quot;</span>: <span class="string">&quot;Elk stack rocks&quot;</span>&#125;</span><br><span class="line">&#123; <span class="attr">&quot;index&quot;</span> : &#123;&#125; &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//使用 completion suggester</span></span><br><span class="line">POST articles/_search?pretty</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;size&quot;</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="attr">&quot;suggest&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;article-suggester&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;prefix&quot;</span>: <span class="string">&quot;e &quot;</span>,</span><br><span class="line">      <span class="comment">//也可以試試看</span></span><br><span class="line">      <span class="comment">//&quot;prefix&quot;: &quot;elk &quot;,</span></span><br><span class="line">      <span class="attr">&quot;completion&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;title_completion&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Context-Suggester"><a href="#Context-Suggester" class="headerlink" title="Context Suggester"></a>Context Suggester</h2><ul><li><p>Context Suggester 是由 ccompletion suggester 擴充而成的</p></li><li><p>可以在搜尋中加入更多上下文的訊息，例如輸入 “<strong>star</strong>“：</p><ul><li><p>若與咖啡相關：建議 “<strong>starbucks</strong>“</p></li><li><p>與電影相關：建議 “<strong>star wars</strong>“</p></li></ul></li></ul><h2 id="如何實作-Context-Suggester"><a href="#如何實作-Context-Suggester" class="headerlink" title="如何實作 Context Suggester ?"></a>如何實作 Context Suggester ?</h2><ul><li><p>可以定義兩種類型的 context</p><ul><li><p><code>Category</code>：任意的 string</p></li><li><p><code>Geo</code>：地理位置訊息</p></li></ul></li><li><p>實作 Context Suggester 的步驟：</p><ul><li><p>設定 mapping</p></li><li><p>將數據進行索引，並且為每個 document 加入 context 訊息</p></li><li><p>結合 context 進行 suggestion 查詢</p></li></ul></li></ul><p>以下是一個簡單的範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">DELETE comments</span><br><span class="line"><span class="comment">//新增 index &amp; 設定 mapping</span></span><br><span class="line">PUT comments</span><br><span class="line">PUT comments/_mapping</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;properties&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;comment_autocomplete&quot;</span>:&#123;</span><br><span class="line">      <span class="comment">//基於 completion suggester 擴充而來</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;completion&quot;</span>,</span><br><span class="line">      <span class="comment">//設定 context，指定類型為 category，並設定 context field &quot;comment_category&quot;</span></span><br><span class="line">      <span class="attr">&quot;contexts&quot;</span>:[&#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span>:<span class="string">&quot;category&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;name&quot;</span>:<span class="string">&quot;comment_category&quot;</span></span><br><span class="line">      &#125;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增 movie category 相關的 document</span></span><br><span class="line">POST comments/_doc</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;comment&quot;</span>:<span class="string">&quot;I love the star war movies&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;comment_autocomplete&quot;</span>:&#123;</span><br><span class="line">    <span class="attr">&quot;input&quot;</span>:[<span class="string">&quot;star wars&quot;</span>],</span><br><span class="line">    <span class="attr">&quot;contexts&quot;</span>:&#123;</span><br><span class="line">      <span class="attr">&quot;comment_category&quot;</span>:<span class="string">&quot;movies&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增 coffee category 相關的 document</span></span><br><span class="line">POST comments/_doc</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;comment&quot;</span>:<span class="string">&quot;Where can I find a Starbucks&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;comment_autocomplete&quot;</span>:&#123;</span><br><span class="line">    <span class="attr">&quot;input&quot;</span>:[<span class="string">&quot;starbucks&quot;</span>],</span><br><span class="line">    <span class="attr">&quot;contexts&quot;</span>:&#123;</span><br><span class="line">      <span class="attr">&quot;comment_category&quot;</span>:<span class="string">&quot;coffee&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//指定 category 來取得建議的關鍵字</span></span><br><span class="line">POST comments/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;suggest&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;MY_SUGGESTION&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;prefix&quot;</span>: <span class="string">&quot;sta&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;completion&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;comment_autocomplete&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;contexts&quot;</span>:&#123;</span><br><span class="line">          <span class="attr">&quot;comment_category&quot;</span>:<span class="string">&quot;coffee&quot;</span></span><br><span class="line">          <span class="comment">//可以嘗試把 category 改成 movies 再試試看</span></span><br><span class="line">          <span class="comment">//&quot;comment_category&quot;:&quot;movies&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="關於-Precision-精準度-amp-Recall-招回率"><a href="#關於-Precision-精準度-amp-Recall-招回率" class="headerlink" title="關於 Precision(精準度) &amp; Recall(招回率)"></a>關於 Precision(精準度) &amp; Recall(招回率)</h2><p>以 <code>Precision(精準度)</code> 來看： (<strong>盡可能返回較少的無關 document</strong>)</p><blockquote><p>Completion &gt; Phrase &gt; Term</p></blockquote><p>以 <code>Recall(招回率)</code> 來看： (<strong>盡量返回較多的相關 document</strong>)</p><blockquote><p>Term &gt; Phrase &gt; Completion</p></blockquote><p>以 performance 來看：</p><blockquote><p>Completion &gt; Phrase &gt; Term</p></blockquote><h1 id="Cross-Cluster-Search"><a href="#Cross-Cluster-Search" class="headerlink" title="Cross Cluster Search"></a>Cross Cluster Search</h1><h2 id="水平擴展的問題"><a href="#水平擴展的問題" class="headerlink" title="水平擴展的問題"></a>水平擴展的問題</h2><ul><li>若只有 single cluster，水平擴展是不能無限增加節點數的</li></ul><blockquote><p>當 cluster metadata(node, index, cluster status) 過大時，會導致更新壓力變大，單一個 active master 會成為效能的瓶頸，導致整個 cluster 無法正常工作</p></blockquote><ul><li>以前透過 <strong>tribe node</strong> 實現 cross node search，但因為問題不少，因此 Elasticsearch 5.3 已經沒有使用 tribe node，而是改用 <code>Cross Cluster Search</code> 功能來取代</li></ul><h2 id="Cross-Cluster-Search-1"><a href="#Cross-Cluster-Search-1" class="headerlink" title="Cross Cluster Search"></a>Cross Cluster Search</h2><ul><li><p>允許任何節點扮演 federated node，以輕量的方式處理搜尋請求</p></li><li><p>不需要以 client node 的形式加入其他的 cluster</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Elasticsearch] 基本概念 &amp; 搜尋入門</title>
      <link href="/blog/Elasticsearch/Elasticsearch-getting-started/"/>
      <url>/blog/Elasticsearch/Elasticsearch-getting-started/</url>
      
        <content type="html"><![CDATA[<h1 id="基本概念：Index、Document-和-REST-API"><a href="#基本概念：Index、Document-和-REST-API" class="headerlink" title="基本概念：Index、Document 和 REST API"></a>基本概念：Index、Document 和 REST API</h1><ul><li><p>Index &amp; Document 是比較偏向開發人員視角，是種邏輯概念</p></li><li><p>Node &amp; Shard 是比較偏向維運人員的視角，是種物理概念</p></li></ul><h2 id="Document"><a href="#Document" class="headerlink" title="Document"></a>Document</h2><ul><li><p>Document 是可以被搜尋數據的最小單位(可能是 log 文件中的一筆紀錄 / 一部電影或唱片的相關訊息 / RDBMS 中的一筆 record)：</p></li><li><p>Document 會被序列化成 JSON(由一堆 Key/Value 的資料組成，並有其資料格式) 格式，保存在 Elasticsearch 中</p></li><li><p>每個 Document 都有一個 UID(Unique ID)，可自己指定或交由 Elasticsearch 自動產生</p></li></ul><h3 id="JSON-document"><a href="#JSON-document" class="headerlink" title="JSON document"></a>JSON document</h3><ul><li><p>包含多個 Key/Value 組合，就像是資料庫中的一筆資料</p></li><li><p>但跟資料庫不一樣的是，JSON 格式靈活不受限，不須預先定義格式</p></li><li><p>每個 Key/Value 的類型(string, number, boolean … etc) 可以自己指定或是由 Elasticsearch 幫忙推算</p></li></ul><h3 id="Metadata"><a href="#Metadata" class="headerlink" title="Metadata"></a>Metadata</h3><p>document metadata 就是描述 document 本身屬性用的資料，通常會包含以下內容：</p><ul><li><p><code>_index</code>：document 所屬的 index 名稱</p></li><li><p><code>_type</code>：document 類型 (例如：<strong>_doc</strong>)</p></li><li><p><code>_id</code>：document ID</p></li><li><p><code>_source</code>：document 的原始 JSON 資料樣貌</p></li><li><p><code>_version</code>：版本訊息 (有這欄位就表示 ES 具有版本控管的能力)</p></li><li><p><code>_score</code>：查詢時的算分結果 (每次的搜尋都會根據 document 對於搜尋內容的相關度進行算分)</p></li></ul><h2 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h2><ul><li><p><code>index</code> 在 ES 中是個邏輯空間的概念，用來儲存 document 的容器 (跟其他領域的 index 用法不太一樣)</p></li><li><p><code>shard</code> 在 ES 中則是個物理空間的的概念，<strong>index 中的資料會分散放在不同的 shard 中</strong></p></li><li><p>index 由以下幾個部份組成：</p><ul><li><code>data</code>：由 document + metadata 所組成</li><li><code>mapping</code>：用來定義每個欄位名稱 &amp; 類型</li><li><code>setting</code>：定義資料是如何存放(例如：replication 數量, 使用的 shard 數量)</li></ul></li><li><p>下圖是 <code>setting</code> 的設定範例：<br><img src="/blog/images/Elasticsearch/es_index-settings.png" alt="Elasticsearch - Index Settings"></p></li><li><p>在 ES 7.0 的版本後，index 在 <code>type</code> 部份只能設定為 <code>_doc</code> (在以前的版本是可以設定不同的 type)</p></li></ul><h2 id="Elasticsearch-與-RDBMS-的比較-amp-取捨"><a href="#Elasticsearch-與-RDBMS-的比較-amp-取捨" class="headerlink" title="Elasticsearch 與 RDBMS 的比較 &amp; 取捨"></a>Elasticsearch 與 RDBMS 的比較 &amp; 取捨</h2><p>以下表格是 Elasticsearch &amp; RDBMS 的對比：(不是完全符合，但概念上是很接近的)</p><table><thead><tr><th><strong>RDBMS</strong></th><th><strong>Elasticsearch</strong></th></tr></thead><tbody><tr><td>Table</td><td>Index</td></tr><tr><td>Row</td><td>Document</td></tr><tr><td>Column</td><td>Field</td></tr><tr><td>Schema</td><td>Mapping</td></tr><tr><td>SQL</td><td>DSL</td></tr></tbody></table><ul><li><p>ES 是 schemaless 的，資料格式可以隨意定，非常適合用來做全文檢索</p></li><li><p>RDBMS 的強項在於處理對於資料事務性(交易)要求特別高的任務</p></li></ul><h2 id="常用搜尋"><a href="#常用搜尋" class="headerlink" title="常用搜尋"></a>常用搜尋</h2><p>在 Kibana Dev Tools 頁面中，可以直接下查詢語法，以下舉幾個與 index 相關的搜尋：</p><h3 id="查詢-index"><a href="#查詢-index" class="headerlink" title="查詢 index"></a>查詢 index</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得指定 index 資訊，包含 mapping &amp; setting ... 等資訊</span></span><br><span class="line">GET kibana_sample_data_ecommerce</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得此 index 中的 document 數量</span></span><br><span class="line">GET kibana_sample_data_ecommerce/_count</span><br></pre></td></tr></table></figure><h3 id="搭配-cat-做搜尋"><a href="#搭配-cat-做搜尋" class="headerlink" title="搭配 _cat 做搜尋"></a>搭配 _cat 做搜尋</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 透過 _cat 查詢 index 相關資訊，搭配正規表示式 </span></span><br><span class="line">GET /_cat/indices/kibana*?v</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_cat-search-1.png" alt="Elasticsearch - _cat search 1"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上過濾條件</span></span><br><span class="line">GET /_cat/indices/kibana*?health=green</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_cat-search-2.png" alt="Elasticsearch - _cat search 2"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用排序</span></span><br><span class="line">GET /_cat/indices?v&amp;s=docs.count:desc</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_cat-search-3.png" alt="Elasticsearch - _cat search 3"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢每個 index 所消耗的 memory 為多少，搭配排序</span></span><br><span class="line">GET /_cat/indices?v&amp;h=i,tm&amp;s=tm:desc</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_cat-search-4.png" alt="Elasticsearch - _cat search 4"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上過濾條件</span></span><br><span class="line">GET /_cat/indices/kibana*?health=green</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_cat-search-2.png" alt="Elasticsearch - _cat search 2"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用排序</span></span><br><span class="line">GET /_cat/indices?v&amp;s=docs.count:desc</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_cat-search-3.png" alt="Elasticsearch - _cat search 3"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢每個 index 所消耗的 memory 為多少，搭配排序</span></span><br><span class="line">GET /_cat/indices?v&amp;h=i,tm&amp;s=tm:desc</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_cat-search-4.png" alt="Elasticsearch - _cat search 4"></p><h1 id="Document-的基本-CRUD-與批次操作"><a href="#Document-的基本-CRUD-與批次操作" class="headerlink" title="Document 的基本 CRUD 與批次操作"></a>Document 的基本 CRUD 與批次操作</h1><h2 id="Document-CRUD"><a href="#Document-CRUD" class="headerlink" title="Document CRUD"></a>Document CRUD</h2><ul><li><code>GET</code>：取得 document<ul><li>語法為 <code>GET _index/_type/Id</code>，例如 <strong>GET users/_doc/1</strong></li><li>document 會有 version control 的功能，因此即使被刪除，version 欄位的值也會不斷增加</li><li><code>_source</code> 欄位包含了 document 的原始訊息</li></ul></li></ul><p><img src="/blog/images/Elasticsearch/es_document-crud-get-example.png" alt="Elasticsearch - document CRUD GET example"></p><ul><li><p><code>Create</code>(<strong>PUT</strong>)：</p><ul><li>建立新的 document，如果 ID 已經存在會發生錯誤</li><li>語法為 <code>PUT _index/_create/[ID]</code> or <code>PUT _index/_doc/[ID]?op_type=create</code>，例如：<strong>PUT users/_create/1</strong> (也可以不帶 ID，就會自動生成)</li></ul></li><li><p><code>Index</code>(<strong>PUT</strong>)：</p><ul><li>如果 ID 不存在，則建立新的 document；若 ID 已經存在，則刪除現存的 document 再建立新的，<strong>version</strong> 的部份會增加</li><li>語法為 <code>PUT _index/_doc/[ID]</code>，例如：<strong>PUT users/_doc/1</strong></li></ul></li><li><p><code>Update</code>(<strong>POST</strong>)：</p><ul><li>document 必須已經存在，更新時只會對 document 中相對應的欄位作增量更新</li><li>語法為 <code>POST _index/_update/[ID]</code>，例如：<strong>POST users/_update/1</strong></li><li>POST 也可以拿來作為新增 document 用</li></ul></li><li><p>呼叫 API 時傳輸的數據不宜過大(預設單一個 document 大小不能超過 100MB)，過大的 document 建議拆成 5~15MB 分次匯入</p></li></ul><h2 id="批次操作"><a href="#批次操作" class="headerlink" title="批次操作"></a>批次操作</h2><p>Elasticsearch 中支援幾種批次操作 API，常用的有以下幾個：</p><h3 id="bulk"><a href="#bulk" class="headerlink" title="_bulk"></a>_bulk</h3><ul><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html">官方文件說明</a></p></li><li><p>讓使用者可以在同一個 API request 中送出多個操作，支援 <strong>Index/Create/Update/Delete</strong>，提昇效率</p></li></ul><h3 id="mget"><a href="#mget" class="headerlink" title="_mget"></a>_mget</h3><ul><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-multi-get.html">官方文件說明</a></p></li><li><p>一次讀取多個不同 index 中特定 ID 的 document</p></li></ul><h3 id="msearch"><a href="#msearch" class="headerlink" title="_msearch"></a>_msearch</h3><ul><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-multi-search.html">官方文件說明</a></p></li><li><p>一次作多個大範圍的搜尋</p></li></ul><ul><li><p>大版本的升級，document 必須重建 index</p></li><li><p>Elasticsearch 預設會提供 dynamic mapping 的功能，因此不用預先設定好 index 結構；但在生產環境中，建議先做好 mapping 的設定後再寫入資料</p></li><li><p>透過 X-Pack security plugin，可以提供 index-level or field-level 的 role based 資料存取控制</p></li></ul><h2 id="API-行為區分"><a href="#API-行為區分" class="headerlink" title="API 行為區分"></a>API 行為區分</h2><ul><li><p>index： 針對整個文檔，既可以新增又可以更新；</p></li><li><p>create：只是新增操作，已有報錯，可以用PUT指定ID，或POST不指定ID；</p></li><li><p>update：指的是部分更新，官方只是說用POST，請求body裡用script或 doc裡包含文檔要更新的部分；</p></li><li><p>delete和read：就是delete和get請求了，比較簡單</p></li></ul><h1 id="Inverted-Index-倒排索引-介紹"><a href="#Inverted-Index-倒排索引-介紹" class="headerlink" title="Inverted Index(倒排索引)介紹"></a>Inverted Index(倒排索引)介紹</h1><ul><li><p>Forward Index(正排索引)：Document ID 到 Document 內容到單詞的關聯</p></li><li><p>Inverted Index(倒排索引)：單詞到 Document ID 的關係</p></li></ul><p><img src="/blog/images/Elasticsearch/es_invert-index-1.png" alt="Elasticsearch - Inverted Index"></p><h2 id="Inverted-Index-組成"><a href="#Inverted-Index-組成" class="headerlink" title="Inverted Index 組成"></a>Inverted Index 組成</h2><ul><li><p>Term Dictionary (單詞辭典)</p><ul><li>為了滿足快速的插入 &amp; 查詢，因此通過 B+ tree or Open Hashing 的方式實現</li><li>記錄 Document 中所有的單詞，記錄單詞到 posting list(倒排列表) 的關聯關係</li></ul></li><li><p>Posting List (倒排列表)：由 posting(倒排索引項組合) 組成，包含以下內容：</p><ul><li>Document ID<ul><li>詞頻 (Term Frequency)：單詞在 document 中出現的次數，用在相關性評分</li><li>位置 (Position)：單詞在 document 的位置，用在搜尋</li><li>偏移 (Offset)：記錄單詞開始 &amp; 結束位置，用於高亮顯示</li></ul></li></ul></li></ul><p><img src="/blog/images/Elasticsearch/es_posting-list-example.png" alt="Elasticsearch - Posting List"></p><ul><li><p>Elasticsearch 的 JSON document 中的單詞都會有自己的倒排索引</p></li><li><p>可以對某些欄位不作索引：</p><ul><li>優點：節省儲存空間</li><li>缺點：該欄位無法被搜尋</li></ul></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/inverted-index.html">倒排索引 | Elasticsearch: 權威指南 | Elastic</a></li></ul><h1 id="通過-Analyzer-進行分詞"><a href="#通過-Analyzer-進行分詞" class="headerlink" title="通過 Analyzer 進行分詞"></a>通過 Analyzer 進行分詞</h1><ul><li><p>Analysis 是將 document 的內容轉換為一系列單詞(term/token) 的過程，也叫<strong>分詞</strong></p></li><li><p>Analysis 是由 Analyzer 來實現，Analyzer 由 <code>Character Filter</code> -&gt; <code>Tokenizer</code> -&gt; <code>Token Filter</code> 三個部份所組成，每個部份都可以自訂</p></li></ul><h2 id="Analyzer-的組成"><a href="#Analyzer-的組成" class="headerlink" title="Analyzer 的組成"></a>Analyzer 的組成</h2><p>Analyzer 是專門處理分詞的組件，由三個部份組成：</p><ul><li><p>Character Filter (針對原始文件進行處理，例如：去除 HTML tag)</p></li><li><p>Tokenizer (根據規則切分 term)</p></li><li><p>Token Filter (將分割後的 term 進行加工，例如：轉小寫、刪除 <a href="https://zh.wikipedia.org/wiki/%E5%81%9C%E7%94%A8%E8%AF%8D">stopwords</a>、增加同義詞)</p></li></ul><p><img src="/blog/images/Elasticsearch/es_analyzer-1.png" alt="Elasticsearch - Analyzer"></p><ul><li>Elasticsearch 內建很多 Analyzer，每個 Analyzer 會由不同的 character filter, tokenizer, token filter 組合而成，使用者也可以自訂 Analyzer</li></ul><h2 id="Elasticsearch-內建的-Analyzer"><a href="#Elasticsearch-內建的-Analyzer" class="headerlink" title="Elasticsearch 內建的 Analyzer"></a>Elasticsearch 內建的 Analyzer</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># character filter: standard (按詞切分)</span></span><br><span class="line"><span class="comment"># tokenizer: 轉小寫</span></span><br><span class="line"><span class="comment"># token filter: N/A (stopword 會保留)</span></span><br><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;analyzer&quot;</span>: <span class="string">&quot;standard&quot;</span>,</span><br><span class="line">  <span class="string">&quot;text&quot;</span>: <span class="string">&quot;2 running Quick brown-foxes leap over lazy dogs in the summer evening.&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># character filter: simple (按詞切分，去除非字母字元)</span></span><br><span class="line"><span class="comment"># tokenizer: 轉小寫</span></span><br><span class="line"><span class="comment"># token filter: N/A (stopword 會保留)</span></span><br><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;analyzer&quot;</span>: <span class="string">&quot;simple&quot;</span>,</span><br><span class="line">  <span class="string">&quot;text&quot;</span>: <span class="string">&quot;2 running Quick brown-foxes leap over lazy dogs in the summer evening.&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># character filter: simple (按詞切分，去除非字母字元)</span></span><br><span class="line"><span class="comment"># tokenizer: 轉小寫</span></span><br><span class="line"><span class="comment"># token filter: 去除 stopwords</span></span><br><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;analyzer&quot;</span>: <span class="string">&quot;stop&quot;</span>,</span><br><span class="line">  <span class="string">&quot;text&quot;</span>: <span class="string">&quot;2 running Quick brown-foxes leap over lazy dogs in the summer evening.&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># character filter: whitespace (單純以空白切分)</span></span><br><span class="line"><span class="comment"># tokenizer: 轉小寫</span></span><br><span class="line"><span class="comment"># token filter: N/A (stopword 會保留)</span></span><br><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;analyzer&quot;</span>: <span class="string">&quot;whitespace&quot;</span>,</span><br><span class="line">  <span class="string">&quot;text&quot;</span>: <span class="string">&quot;2 running Quick brown-foxes leap over lazy dogs in the summer evening.&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># character filter: N/A (不進行分詞)</span></span><br><span class="line"><span class="comment"># tokenizer: keyword (將所有的輸入直接轉成 token)</span></span><br><span class="line"><span class="comment"># token filter: N/A</span></span><br><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;analyzer&quot;</span>: <span class="string">&quot;keyword&quot;</span>,</span><br><span class="line">  <span class="string">&quot;text&quot;</span>: <span class="string">&quot;2 running Quick brown-foxes leap over lazy dogs in the summer evening.&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># character filter: pattern (使用正規表示式分詞)</span></span><br><span class="line"><span class="comment"># tokenizer: 轉小寫</span></span><br><span class="line"><span class="comment"># token filter: N/A (stopword 會保留)</span></span><br><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;analyzer&quot;</span>: <span class="string">&quot;pattern&quot;</span>,</span><br><span class="line">  <span class="string">&quot;text&quot;</span>: <span class="string">&quot;2 running Quick brown-foxes leap over lazy dogs in the summer evening.&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># character filter: english (使用英文辭典分詞)</span></span><br><span class="line"><span class="comment"># tokenizer: 轉小寫</span></span><br><span class="line"><span class="comment"># token filter: # token filter: 去除 stopwords</span></span><br><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;analyzer&quot;</span>: <span class="string">&quot;english&quot;</span>,</span><br><span class="line">  <span class="string">&quot;text&quot;</span>: <span class="string">&quot;2 running Quick brown-foxes leap over lazy dogs in the summer evening.&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Search-API-概覽"><a href="#Search-API-概覽" class="headerlink" title="Search API 概覽"></a>Search API 概覽</h1><ul><li>search API 分成 <code>URI search</code>(使用 GET) &amp; <code>request body search</code>(同時支援 GET &amp; POST，使用 ES 提供的 DSL)</li></ul><p>查詢範圍：</p><table><thead><tr><th>語法</th><th>範圍</th></tr></thead><tbody><tr><td><code>/_search</code></td><td>cluster 上所有的 index</td></tr><tr><td><code>/index1/_search</code></td><td>index1</td></tr><tr><td><code>/index1,index2/_search</code></td><td>index1 + index2</td></tr><tr><td><code>/index*/_search</code></td><td>以 <strong>index</strong> 開頭的 index</td></tr></tbody></table><h2 id="URI-查詢"><a href="#URI-查詢" class="headerlink" title="URI 查詢"></a>URI 查詢</h2><h2 id="衡量相關性"><a href="#衡量相關性" class="headerlink" title="衡量相關性"></a>衡量相關性</h2><p>Information Retrieval</p><ul><li><p>Precision (查準率)：盡可能返回較少的無關 document</p></li><li><p>Recall (查全率)：盡量返回較多的相關 document</p></li><li><p>Ranking：是否能夠依照相關度進行排序?</p></li></ul><p><img src="/blog/images/Elasticsearch/es_precision-and-recall.png" alt="Elasticsearch - Precision &amp; Recall"></p><h1 id="URI-Search詳解"><a href="#URI-Search詳解" class="headerlink" title="URI Search詳解"></a>URI Search詳解</h1><p>URI search 範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GET /movies/_search?q=2012&amp;df=title&amp;sort=year:desc&amp;from=0&amp;size=10&amp;timeout=1s</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><code>q</code>：指定查詢語句，使用 <strong>Query String Syntax</strong></p></li><li><p><code>df</code>：預設查詢的 field，若不指定則會對所有的 field 進行查詢</p></li><li><p><code>sort</code>：排序</p></li><li><p><code>from</code> &amp; <code>size</code> 用於分頁</p></li><li><p><code>profile</code>：可以檢查查詢是如何被執行的</p></li></ul><h2 id="搜尋範例"><a href="#搜尋範例" class="headerlink" title="搜尋範例"></a>搜尋範例</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本查詢</span></span><br><span class="line"><span class="comment"># 明確指定 q, df, sort, from, size, timeout ... 等關鍵字</span></span><br><span class="line"><span class="comment"># 以 TermQuery 的方式查詢</span></span><br><span class="line">GET /movies/_search?q=2012&amp;df=title&amp;sort=year:desc&amp;from=0&amp;size=10&amp;timeout=1s</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;profile&quot;</span>: <span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_search-example-01.png" alt="Elasticsearch - Search Example 1"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 泛查詢 =&gt; 對所有的 term 進行查詢</span></span><br><span class="line"><span class="comment"># 以 DisjunctionMaxQuery 的方式查詢</span></span><br><span class="line">GET /movies/_search?q=2012</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_search-example-02.png" alt="Elasticsearch - Search Example 2"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 title 中尋找 &quot;Beautiful OR Mind&quot;</span></span><br><span class="line"><span class="comment"># 搜尋效果 =&gt; TermQuery(title:beautiful) + DisjunctionMaxQuery(在所有 term 中搜尋 mind)</span></span><br><span class="line"><span class="comment"># search type = BooleanQuery</span></span><br><span class="line">GET /movies/_search?q=title:Beautiful Mind</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_search-example-03.png" alt="Elasticsearch - Search Example 3"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用引號 =&gt; &quot;Beautiful Mind&quot; = &quot;Beautiful AND Mind&quot;</span></span><br><span class="line"><span class="comment"># search type 為 PhraseQuery (同時出現 &amp; 按照順序)</span></span><br><span class="line">GET /movies/_search?q=title:<span class="string">&quot;Beautiful Mind&quot;</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用括號 =&gt; (Beautiful AND Mind) = &quot;Beautiful AND Mind&quot;</span></span><br><span class="line"><span class="comment"># search type 為 BooleanQuery</span></span><br><span class="line">GET /movies/_search?q=title:(Beautiful AND Mind)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_search-example-04.png" alt="Elasticsearch - Search Example 4"></p><p><img src="/blog/images/Elasticsearch/es_search-example-06.png" alt="Elasticsearch - Search Example 6"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用括號 =&gt; (Beautiful Mind) = &quot;Beautiful OR Mind&quot;</span></span><br><span class="line"><span class="comment"># 搜尋效果 =&gt; TermQuery(title:beautiful) + TermQuery(title:mind)</span></span><br><span class="line"><span class="comment"># search type 為 BooleanQuery</span></span><br><span class="line">GET /movies/_search?q=title:(Beautiful Mind)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 title 中尋找 &quot;Beautiful +Mind&quot; (%2 = +)</span></span><br><span class="line"><span class="comment"># 搜尋效果 =&gt; TermQuery(title:beautiful) + TermQuery(+title:mind)</span></span><br><span class="line"><span class="comment"># mind 一定要出現，但 beautiful 不一定要出現</span></span><br><span class="line"><span class="comment"># search type = BooleanQuery</span></span><br><span class="line">GET /movies/_search?q=title:(Beautiful %2BMind)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_search-example-05.png" alt="Elasticsearch - Search Example 5"></p><p><img src="/blog/images/Elasticsearch/es_search-example-07.png" alt="Elasticsearch - Search Example 7"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#正規表示式查詢</span></span><br><span class="line"><span class="comment"># 查詢出任何一個 term 為 b 開頭的 document</span></span><br><span class="line"><span class="comment"># search type = MultiTermQueryConstantScoreWrapper</span></span><br><span class="line">GET /movies/_search?q=title:b*</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_search-example-08.png" alt="Elasticsearch - Search Example 8"></p><blockquote><p>查詢效率低，記憶體消耗大，不建議使用</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模糊查詢 (即使 search term 有輸入錯誤，還是可以查詢)</span></span><br><span class="line"><span class="comment"># &quot;Beautiful Girls&quot;, &quot;Beautiful People&quot;, &quot;Beautiful Thing&quot; ... 都會被搜尋到</span></span><br><span class="line">GET /movies/_search?q=title:beautifl~1</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模糊查詢</span></span><br><span class="line"><span class="comment"># &quot;Lord of the Rings: The Two Towers, The&quot;, &quot;Lord of the Rings, The&quot; ... 都會被搜尋到</span></span><br><span class="line">GET /movies/_search?q=title:<span class="string">&quot;Lord Rings&quot;</span>~2</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;profile&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/blog/images/Elasticsearch/es_search-example-09.png" alt="Elasticsearch - Search Example 9"></p><p><img src="/blog/images/Elasticsearch/es_search-example-10.png" alt="Elasticsearch - Search Example 10"></p><h1 id="Request-Body-與-Query-DSL-簡介"><a href="#Request-Body-與-Query-DSL-簡介" class="headerlink" title="Request Body 與 Query DSL 簡介"></a>Request Body 與 Query DSL 簡介</h1><ul><li>進階的查詢通常只能用 request body 的方式完成</li></ul><h2 id="一般查詢範例"><a href="#一般查詢範例" class="headerlink" title="一般查詢範例"></a>一般查詢範例</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ignore_unavailable=true，可以忽略嘗試訪問不存在的 index “404_idx” 導致的錯誤</span></span><br><span class="line"><span class="comment"># 查詢 movies index</span></span><br><span class="line"><span class="comment"># 開啟 profile</span></span><br><span class="line">POST /movies,404_idx/_search?ignore_unavailable=<span class="literal">true</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;profile&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 from &amp; size 達到分頁效果</span></span><br><span class="line">POST /kibana_sample_data_ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;from&quot;</span>:10,</span><br><span class="line">  <span class="string">&quot;size&quot;</span>:20,</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 對資料排序，使用 sort 參數</span></span><br><span class="line">POST kibana_sample_data_ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;sort&quot;</span>:[&#123;<span class="string">&quot;order_date&quot;</span>:<span class="string">&quot;desc&quot;</span>&#125;],</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># source filtering</span></span><br><span class="line"><span class="comment"># 當某些 document 很大時，僅針對特定的幾個 term 做查詢</span></span><br><span class="line">POST kibana_sample_data_ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;_source&quot;</span>:[<span class="string">&quot;order_date&quot;</span>, <span class="string">&quot;category.keyword&quot;</span>],</span><br><span class="line">  <span class="string">&quot;from&quot;</span>: 10,</span><br><span class="line">  <span class="string">&quot;size&quot;</span>: 5, </span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Scripted-Field-腳本字段-Query"><a href="#Scripted-Field-腳本字段-Query" class="headerlink" title="Scripted Field(腳本字段) Query"></a>Scripted Field(腳本字段) Query</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 透過 ES 中 painless script 算出新的 field value</span></span><br><span class="line"><span class="comment"># 搜尋結果中都會加上 hello 作為結尾</span></span><br><span class="line">GET kibana_sample_data_ecommerce/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;script_fields&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;new_field&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;script&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;lang&quot;</span>: <span class="string">&quot;painless&quot;</span>,</span><br><span class="line">        <span class="string">&quot;source&quot;</span>: <span class="string">&quot;doc[&#x27;order_date&#x27;].value+&#x27;hello&#x27;&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;from&quot;</span>: 10, </span><br><span class="line">  <span class="string">&quot;size&quot;</span>: 5,</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="使用查詢表達式-Match"><a href="#使用查詢表達式-Match" class="headerlink" title="使用查詢表達式 - Match"></a>使用查詢表達式 - Match</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 預設為 &quot;last OR christmas&quot;</span></span><br><span class="line">POST movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;title&quot;</span>: <span class="string">&quot;last christmas&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可透過 operator 改為 &quot;last AND christmas&quot;</span></span><br><span class="line">POST movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;title&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;query&quot;</span>: <span class="string">&quot;last christmas&quot;</span>,</span><br><span class="line">        <span class="string">&quot;operator&quot;</span>: <span class="string">&quot;and&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="使用查詢表達式-Match-Phrase"><a href="#使用查詢表達式-Match-Phrase" class="headerlink" title="使用查詢表達式 - Match Phrase"></a>使用查詢表達式 - Match Phrase</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 必須按照順序出現</span></span><br><span class="line">POST movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;match_phrase&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;title&quot;</span>:&#123;</span><br><span class="line">        <span class="string">&quot;query&quot;</span>: <span class="string">&quot;one love&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加上 slop，中間可以有一個其他的 term 插入</span></span><br><span class="line">POST movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;match_phrase&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;title&quot;</span>:&#123;</span><br><span class="line">        <span class="string">&quot;query&quot;</span>: <span class="string">&quot;one love&quot;</span>,</span><br><span class="line">        <span class="string">&quot;slop&quot;</span>: 1</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Query-String-amp-Simple-Query-String-查詢"><a href="#Query-String-amp-Simple-Query-String-查詢" class="headerlink" title="Query String &amp; Simple Query String 查詢"></a>Query String &amp; Simple Query String 查詢</h1><h2 id="Query-String-Query"><a href="#Query-String-Query" class="headerlink" title="Query String Query"></a>Query String Query</h2><ul><li>類似 URI Query</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可指定 default field(DF)</span></span><br><span class="line"><span class="comment"># 可指定 operrator</span></span><br><span class="line">POST users/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;query_string&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;default_field&quot;</span>: <span class="string">&quot;tool&quot;</span>,</span><br><span class="line">      <span class="string">&quot;query&quot;</span>: <span class="string">&quot;Docker AND Kubernetes&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可指定多個 field</span></span><br><span class="line">POST users/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;query_string&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;fields&quot;</span>:[<span class="string">&quot;tool&quot;</span>,<span class="string">&quot;about&quot;</span>],</span><br><span class="line">      <span class="string">&quot;query&quot;</span>: <span class="string">&quot;(Docker AND Kubernetes) OR (Java AND Elasticsearch)&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Simple-Query-String-Query"><a href="#Simple-Query-String-Query" class="headerlink" title="Simple Query String Query"></a>Simple Query String Query</h2><ul><li><p>類似 Query String, 但會忽略錯誤的語法，並且僅支援部份查詢語法</p></li><li><p>不支援 <strong>AND</strong>/<strong>OR</strong>/<strong>NOT</strong>，會當作 term 來處理</p></li><li><p>term 之間預設的關係是 <code>OR</code>，可指定 <code>operator</code></p></li><li><p>支援使用 <code>+</code> 取代 <code>AND</code>，<code>|</code> 取代 <code>OR</code>，<code>-</code> 取代 <code>NOT</code></p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST users/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;simple_query_string&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;query&quot;</span>: <span class="string">&quot;Docker Kubernetes&quot;</span>,</span><br><span class="line">      <span class="string">&quot;fields&quot;</span>: [<span class="string">&quot;tool&quot;</span>],</span><br><span class="line">      <span class="string">&quot;default_operator&quot;</span>: <span class="string">&quot;AND&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Dynamic-Mapping-和常見字段類型"><a href="#Dynamic-Mapping-和常見字段類型" class="headerlink" title="Dynamic Mapping 和常見字段類型"></a>Dynamic Mapping 和常見字段類型</h1><h2 id="What-is-Mapping"><a href="#What-is-Mapping" class="headerlink" title="What is Mapping ?"></a>What is Mapping ?</h2><ul><li><p>Mapping 類似資料庫中的 schema 定義，用途如下：</p><ul><li><p>定義 index 中每個 term 的名稱</p></li><li><p>定義每個 term 的資料型態，例如：string, Interger, boolean</p></li><li><p>term &amp; inverted index 的相關配置 (要使用哪個 Analyzer，或是不被索引)</p></li></ul></li><li><p>Mapping 會將 JSON document 映射成 Lucene 所需要的扁平格式</p></li><li><p>一個 Mapping 屬於一個 index type</p><ul><li><p>每個 document 都屬於一個 type</p></li><li><p>一個 type 都會有一個 mapping 定義</p></li><li><p>7.0 開始，不需要在 mapping 定義中指定 type 資訊</p></li></ul></li></ul><h2 id="Term-Data-Type"><a href="#Term-Data-Type" class="headerlink" title="Term Data Type"></a>Term Data Type</h2><h3 id="簡單類型"><a href="#簡單類型" class="headerlink" title="簡單類型"></a>簡單類型</h3><ul><li><p>Text / Keyword</p></li><li><p>Date</p></li><li><p>Integer / Floating</p></li><li><p>Boolean</p></li><li><p>IPv4 &amp; IPv6</p></li></ul><h3 id="複雜類型"><a href="#複雜類型" class="headerlink" title="複雜類型"></a>複雜類型</h3><ul><li><p>Object</p></li><li><p>List</p></li></ul><h3 id="特殊類型"><a href="#特殊類型" class="headerlink" title="特殊類型"></a>特殊類型</h3><ul><li><p>geo_point &amp; geo_shape (地理訊息)</p></li><li><p>percolator</p></li></ul><h2 id="What-is-Dynamic-Mapping"><a href="#What-is-Dynamic-Mapping" class="headerlink" title="What is Dynamic Mapping ?"></a>What is Dynamic Mapping ?</h2><ul><li><p>在寫入 Document 時，如果 index 不存在，則會自動建立 index</p></li><li><p>Dynamic Mapping 可以根據 Document 內容，推算出 term data type 並自動建立 mapping，因此不需要手動制定</p></li><li><p>但推算的結果不一定會完全正確(例如：地理位置相關訊息可能會推斷錯誤)</p></li><li><p>term data type 推算錯誤可能會導致某些查詢無法正常使用，例如：range 查詢</p></li></ul><h2 id="範例"><a href="#範例" class="headerlink" title="範例"></a>範例</h2><p>簡單測試 Dynamic Mapping：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 寫入 document，mapping 會動態產生</span></span><br><span class="line">PUT mapping_test/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;firstName&quot;</span>:<span class="string">&quot;Chan&quot;</span>,</span><br><span class="line">  <span class="string">&quot;lastName&quot;</span>: <span class="string">&quot;Jackie&quot;</span>,</span><br><span class="line">  <span class="string">&quot;loginDate&quot;</span>:<span class="string">&quot;2018-07-24T10:29:48.103Z&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 &quot;_mapping&quot; 可以查看 mapping 資訊</span></span><br><span class="line"><span class="comment"># firstName =&gt; text + keyword</span></span><br><span class="line"><span class="comment"># lastName =&gt; text + keyword</span></span><br><span class="line"><span class="comment"># loginDate =&gt; date</span></span><br><span class="line">GET mapping_test/_mapping</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刪除 index</span></span><br><span class="line">DELETE mapping_test</span><br></pre></td></tr></table></figure><p>故意將某些資料類型加上雙引號測試：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># UID 應該為 long，加上雙引號</span></span><br><span class="line"><span class="comment"># isAdmin 應該為 boolean，加上雙引號</span></span><br><span class="line">PUT mapping_test/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;uid&quot;</span> : <span class="string">&quot;123&quot;</span>,</span><br><span class="line">    <span class="string">&quot;isVip&quot;</span> : <span class="literal">false</span>,</span><br><span class="line">    <span class="string">&quot;isAdmin&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">    <span class="string">&quot;age&quot;</span>:19,</span><br><span class="line">    <span class="string">&quot;heigh&quot;</span>:180</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 mapping 設定</span></span><br><span class="line"><span class="comment"># uid =&gt; text + keyword</span></span><br><span class="line"><span class="comment"># isVip =&gt; boolean</span></span><br><span class="line"><span class="comment"># isAdmin =&gt; text + keyword</span></span><br><span class="line"><span class="comment"># age =&gt; long</span></span><br><span class="line"><span class="comment"># height =&gt; long</span></span><br><span class="line">GET mapping_test/_mapping</span><br></pre></td></tr></table></figure><h2 id="修改-Mapping-中的字段類型"><a href="#修改-Mapping-中的字段類型" class="headerlink" title="修改 Mapping 中的字段類型"></a>修改 Mapping 中的字段類型</h2><p>在新增加字段的情況下：</p><ul><li><p>若 dynamic = true，一旦有新增字段的 document 寫入時，mapping 資訊也會同時被更新</p></li><li><p>若 dynamic = false，mapping 不會被更新，新增字段的資料無法被索引，但是資料會出現在 <code>_source</code> 中</p></li><li><p>若 dynamic = strict，寫入 document 的操作會發生錯誤</p></li></ul><p>若是針對已經存在的字段，使用其他字段類型的資料進行寫入操作，是無法變更 mapping 設定的，因為一旦當 reverted index 已經生成後，就無法修改；而若是真的要改變字段類型，則是需要透過 <code>Reindex API</code> 來重建索引。</p><blockquote><p>若是原有字段的數據類型可以隨意修改，這樣會讓原本已經被索引的資料無法被搜尋</p></blockquote><table><thead><tr><th>Dynamic Mapping 設定</th><th>“true”</th><th>“false”</th><th>“strict”</th></tr></thead><tbody><tr><td>document 可索引</td><td>Yes</td><td>Yes</td><td>No (資料寫入會錯誤))</td></tr><tr><td>字段可索引</td><td>Yes</td><td>No</td><td>No</td></tr><tr><td>Mapping 被更新</td><td>Yes</td><td>No (新增字段被丟棄)</td><td>No</td></tr></tbody></table><h2 id="修改-Mapping-範例"><a href="#修改-Mapping-範例" class="headerlink" title="修改 Mapping 範例"></a>修改 Mapping 範例</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 預設 dynamic mapping 開啟，寫入新的 document 進 index 中</span></span><br><span class="line">PUT dynamic_mapping_test/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;newField&quot;</span>:<span class="string">&quot;someValue&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 資料可以被搜尋到，資料也同時出現在 _source 中</span></span><br><span class="line">POST dynamic_mapping_test/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match&quot;</span>:&#123;</span><br><span class="line">      <span class="string">&quot;newField&quot;</span>:<span class="string">&quot;someValue&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 dynamic mapping 設定為 false</span></span><br><span class="line">PUT dynamic_mapping_test/_mapping</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;dynamic&quot;</span>: <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增 anotherField</span></span><br><span class="line">POST dynamic_mapping_test/_doc/10</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;anotherField&quot;</span>:<span class="string">&quot;someValue&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># anotherField 這一筆資料無法被搜索，因為 dynamic mapping 已經被設定為 false</span></span><br><span class="line">POST dynamic_mapping_test/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>:&#123;</span><br><span class="line">    <span class="string">&quot;match&quot;</span>:&#123;</span><br><span class="line">      <span class="string">&quot;anotherField&quot;</span>:<span class="string">&quot;someValue&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 mapping 設定，還是只有原本就存在的 newField 設定</span></span><br><span class="line">get dynamic_mapping_test/_mapping</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 dynamic mapping 設定修改為 strict</span></span><br><span class="line">PUT dynamic_mapping_test/_mapping</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;dynamic&quot;</span>: <span class="string">&quot;strict&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此時寫入操作就會發生錯誤，HTTP Code 400</span></span><br><span class="line">PUT dynamic_mapping_test/_doc/12</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;lastField&quot;</span>:<span class="string">&quot;value&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 index</span></span><br><span class="line">DELETE dynamic_mapping_test</span><br></pre></td></tr></table></figure><h1 id="顯式-Mapping-設置與常見參數介紹"><a href="#顯式-Mapping-設置與常見參數介紹" class="headerlink" title="顯式 Mapping 設置與常見參數介紹"></a>顯式 Mapping 設置與常見參數介紹</h1><h2 id="自定義-mapping"><a href="#自定義-mapping" class="headerlink" title="自定義 mapping"></a>自定義 mapping</h2><p>往 index 送 PUT request 並帶上 mappings 設定即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT movies</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">    //define your mappings here</span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>撰寫 mapping 其實不是這麼容易，除了可以參考 API 來撰寫之外，也可以透過<strong>寫入一些 sample data 到一個臨時的 index，讓 Elasticsearch 自動產生 mapping 定義後，再根據實際需求進行修改</strong>。</p><h2 id="Index-Options"><a href="#Index-Options" class="headerlink" title="Index Options"></a>Index Options</h2><p>Inverted Index 根據要記錄的內容，有四種 index options 可以設定：</p><table><thead><tr><th>Index Option</th><th>Inverted Index 中的記錄內容</th></tr></thead><tbody><tr><td><code>docs</code></td><td>doc id</td></tr><tr><td><code>freqs</code></td><td>doc id + term frequency</td></tr><tr><td><code>positions</code></td><td>doc id + term frequency + term position</td></tr><tr><td><code>offsets</code></td><td>doc id + term frequency + term position + character offsets</td></tr></tbody></table><ul><li><p>type <code>text</code> 預設使用 <code>positions</code>，其他則為 <code>docs</code></p></li><li><p>記錄的資料越多，需要儲存空間越多</p></li></ul><h2 id="選擇性的讓特定-Field-不被索引"><a href="#選擇性的讓特定-Field-不被索引" class="headerlink" title="選擇性的讓特定 Field 不被索引"></a>選擇性的讓特定 Field 不被索引</h2><p>預設情況下每個 field 都會被索引，但若是確定特定的 field 資料不需要被查詢，也可以不被索引：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">PUT users</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;mappings&quot;</span> : &#123;</span><br><span class="line">      <span class="attr">&quot;properties&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;firstName&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;text&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;lastName&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;text&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment">//將 index 設定為 false，ES 就不會索引該 field 的資料</span></span><br><span class="line">        <span class="attr">&quot;mobile&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;text&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;index&quot;</span>: <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 新增資料</span></span><br><span class="line">PUT users/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;firstName&quot;</span>:<span class="string">&quot;Leon&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;lastName&quot;</span>: <span class="string">&quot;Tseng&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;mobile&quot;</span>: <span class="string">&quot;12345678&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 搜尋會發生錯誤</span></span><br><span class="line"><span class="comment">// Cannot search on field [mobile] since it is not indexed.</span></span><br><span class="line">POST /users/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;mobile&quot;</span>:<span class="string">&quot;12345678&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="NULL-value-的處理"><a href="#NULL-value-的處理" class="headerlink" title="NULL value 的處理"></a>NULL value 的處理</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">PUT users</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;mappings&quot;</span> : &#123;</span><br><span class="line">      <span class="attr">&quot;properties&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;firstName&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;text&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;lastName&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;text&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;mobile&quot;</span> : &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;keyword&quot;</span>,</span><br><span class="line">          <span class="comment">//可以透過給一個 &quot;NULL&quot; 字串的方式</span></span><br><span class="line">          <span class="comment">//讓 ES 協助生成一個 keyword field 來搜尋</span></span><br><span class="line">          <span class="attr">&quot;null_value&quot;</span>: <span class="string">&quot;NULL&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增資料1</span></span><br><span class="line">PUT users/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;firstName&quot;</span>:<span class="string">&quot;Leon&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;lastName&quot;</span>: <span class="string">&quot;Tseng&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;mobile&quot;</span>: <span class="literal">null</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增資料2</span></span><br><span class="line">PUT users/_doc/2</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;firstName&quot;</span>:<span class="string">&quot;Leon2&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;lastName&quot;</span>: <span class="string">&quot;Tseng2&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//只會找到資料1</span></span><br><span class="line"><span class="comment">//而且實際存在於 ES 中的是 null 值而非字串 </span></span><br><span class="line">GET users/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;match&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;mobile&quot;</span>:<span class="string">&quot;NULL&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="copy-to-設定"><a href="#copy-to-設定" class="headerlink" title="copy_to 設定"></a>copy_to 設定</h2><p>透過 <code>copy_to</code> 的設定可以新增一個 field，實現類似 <code>_all</code> 的效果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">PUT users</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;properties&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;firstName&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>,</span><br><span class="line">        <span class="comment">//firstName 資料會被複製到 fullName 中</span></span><br><span class="line">        <span class="attr">&quot;copy_to&quot;</span>: <span class="string">&quot;fullName&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;lastName&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>,</span><br><span class="line">        <span class="comment">//lastName 資料會被複製到 fullName 中</span></span><br><span class="line">        <span class="attr">&quot;copy_to&quot;</span>: <span class="string">&quot;fullName&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><code>_all</code> 在 ES 7 後被 <code>copy_to</code> 取代</p></li><li><p>上述範例中，使用者可以針對 <code>fullName</code> 進行搜尋</p></li><li><p><code>fullName</code> 的資料不會出現在 <code>_source</code> 中</p></li></ul><h2 id="Aray-Type"><a href="#Aray-Type" class="headerlink" title="Aray Type"></a>Aray Type</h2><p>目前 Elasticsearch 不支援 array type，但每個 field 都可以包含多個相同類型的數值：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//新增一個帶有 string array 的資料</span></span><br><span class="line">PUT users/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;name&quot;</span>:<span class="string">&quot;twobirds&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;interests&quot;</span>:[<span class="string">&quot;reading&quot;</span>,<span class="string">&quot;music&quot;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//可以正確被搜尋</span></span><br><span class="line">POST users/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;match_all&quot;</span>: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//而 name 的 data type 被設定為 text</span></span><br><span class="line">GET users/_mapping</span><br></pre></td></tr></table></figure><blockquote><p>表示 <code>text</code> type 其實是可以記錄 array type 的資料 (其實其他 type 也是可以記錄 array type 資料，例如：<code>long</code>))</p></blockquote><h1 id="Multiple-Field-特性及-Mapping-中配置自定義-Analyzer"><a href="#Multiple-Field-特性及-Mapping-中配置自定義-Analyzer" class="headerlink" title="Multiple Field 特性及 Mapping 中配置自定義 Analyzer"></a>Multiple Field 特性及 Mapping 中配置自定義 Analyzer</h1><ul><li><p>多個 field 資料中，可透過 <code>keyword</code> field 作精確搜尋</p></li><li><p>若是要做全文搜尋，則是使用 <code>text</code> field 並搭配不同的 <strong>analyzer</strong> &amp; <strong>search analyzer</strong></p></li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">PUT products</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;properties&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;company&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, </span><br><span class="line">        <span class="attr">&quot;fields&quot;</span>: &#123;</span><br><span class="line">          <span class="comment">//利用 keyword field 來實現精確匹配</span></span><br><span class="line">          <span class="attr">&quot;keyword&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;keyword&quot;</span>, </span><br><span class="line">            <span class="attr">&quot;ignore_above&quot;</span>: <span class="number">256</span> </span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, </span><br><span class="line">      <span class="attr">&quot;comment&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>, </span><br><span class="line">        <span class="attr">&quot;fields&quot;</span>: &#123;</span><br><span class="line">          <span class="comment">//針對需要進行全文檢索的欄位，設定不同的 analyzer &amp; search analyzer</span></span><br><span class="line">          <span class="attr">&quot;english_comment&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;analyzer&quot;</span>: <span class="string">&quot;english&quot;</span>,  <span class="comment">//索引用的 analyzer</span></span><br><span class="line">            <span class="attr">&quot;search_analyzer&quot;</span>: <span class="string">&quot;english&quot;</span>  <span class="comment">//搜尋用的 analyzer</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Exact-Values-v-s-Full-Text"><a href="#Exact-Values-v-s-Full-Text" class="headerlink" title="Exact Values v.s. Full Text"></a>Exact Values v.s. Full Text</h2><ul><li><p>Exact Value 包含數字/日期/或是特定的字串(例如：”Apple Store”)，在 Elasticsearch 中的 <code>keyword</code> field 存放的就是這一類的資料</p></li><li><p>Full Text 則是非結構化的資料，在 Elasticsearch 中的 <code>text</code> field 存放的就是這一類的資料</p></li><li><p>Exact Values 不需要做分詞處理，而 Full Text 則會需要分詞處理才能更容易被後續利用</p></li></ul><h2 id="自訂分詞器-Analyzer"><a href="#自訂分詞器-Analyzer" class="headerlink" title="自訂分詞器(Analyzer)"></a>自訂分詞器(Analyzer)</h2><p>Analyzer 是專門處理分詞的組件，由三個部份組成：</p><ul><li><p>Character Filter (針對原始文件進行處理，例如：去除 HTML tag)</p></li><li><p>Tokenizer (根據規則切分 term or token)</p></li><li><p>Token Filter (將分割後的 term 進行加工，例如：轉小寫、刪除 <a href="https://zh.wikipedia.org/wiki/%E5%81%9C%E7%94%A8%E8%AF%8D">stopwords</a>、增加同義詞)</p></li></ul><h3 id="Character-Filter"><a href="#Character-Filter" class="headerlink" title="Character Filter"></a>Character Filter</h3><ul><li><p>可同時設定多的 Character Filter</p></li><li><p>會影響 Tokenizer 的 position &amp; offset 的資訊</p></li><li><p>目前 Elasticsearch 內建提供的 Character Filter 有 HTML strip、Mapping、Patter replace …等等</p></li></ul><h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h3><ul><li><p>將 Character Filter 處理過後的資料，按照一定的規則，切分為詞(term or token)</p></li><li><p>目前 Elasticsearch 內建提供的 tokenizer 有 <code>whitespace</code>/<code>standard</code>/<code>uax_url_email</code>/<code>pattern</code>/<code>keyword</code>(不做任何處理)/<code>path hierarchy</code></p></li><li><p>也可以用 Java 實作自己的 tokenizer</p></li></ul><h3 id="Token-Filter"><a href="#Token-Filter" class="headerlink" title="Token Filter"></a>Token Filter</h3><ul><li><p>將 Tokenizer 輸出的 term(or token) 進行增加、修改、刪除</p></li><li><p>目前 目前 Elasticsearch 內建提供的 Token Filter 有 <code>lowercase</code>/<code>stop</code>/<code>synonym</code> …  等等</p></li></ul><h2 id="範例-1"><a href="#範例-1" class="headerlink" title="範例"></a>範例</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST _analyze</span><br><span class="line">&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//將 HTML tag 去除</span></span><br><span class="line">  <span class="attr">&quot;char_filter&quot;</span>:[<span class="string">&quot;html_strip&quot;</span>],</span><br><span class="line"></span><br><span class="line">  <span class="comment">//keyword 不會進行分詞，會保留全部資料</span></span><br><span class="line">  <span class="attr">&quot;tokenizer&quot;</span>:<span class="string">&quot;keyword&quot;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;&lt;b&gt;hello world&lt;/b&gt;&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">POST _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//會將所有可能的目錄結構的分出來</span></span><br><span class="line">  <span class="comment">//例如：&quot;/user&quot;, &quot;/user/ymruan&quot;, &quot;/user/ymruan/a&quot; .... etc</span></span><br><span class="line">  <span class="attr">&quot;tokenizer&quot;</span>:<span class="string">&quot;path_hierarchy&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;text&quot;</span>:<span class="string">&quot;/user/ymruan/a/b/c/d/e&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">POST _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//會根據 mapping 設定進行字串的取代</span></span><br><span class="line">  <span class="attr">&quot;char_filter&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;mapping&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;mappings&quot;</span> : [ <span class="string">&quot;- =&gt; _&quot;</span>]</span><br><span class="line">      &#125;</span><br><span class="line">  ],</span><br><span class="line"></span><br><span class="line">  <span class="comment">//標準分詞</span></span><br><span class="line">  <span class="attr">&quot;tokenizer&quot;</span>: <span class="string">&quot;standard&quot;</span>,</span><br><span class="line">  </span><br><span class="line">  <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;123-456, I-test! test-990 650-555-1234&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//char filter 替換表情符號</span></span><br><span class="line">POST _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//透過 mapping 將表情符號替換成一般字串</span></span><br><span class="line">  <span class="attr">&quot;char_filter&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;mapping&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;mappings&quot;</span> : [ <span class="string">&quot;:) =&gt; happy&quot;</span>, <span class="string">&quot;:( =&gt; sad&quot;</span>]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;tokenizer&quot;</span>: <span class="string">&quot;standard&quot;</span>,</span><br><span class="line">  </span><br><span class="line">    <span class="attr">&quot;text&quot;</span>: [<span class="string">&quot;I am felling :)&quot;</span>, <span class="string">&quot;Feeling :( today&quot;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//使用空白進行分詞</span></span><br><span class="line">  <span class="attr">&quot;tokenizer&quot;</span>: <span class="string">&quot;whitespace&quot;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="comment">//stop words 會被忽略</span></span><br><span class="line">  <span class="comment">//若改成 [&quot;lowercase&quot;, &quot;stop&quot;]，下面的 &quot;The&quot; 就會被過濾掉了</span></span><br><span class="line">  <span class="attr">&quot;filter&quot;</span>: [<span class="string">&quot;stop&quot;</span>],</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//&quot;The&quot; 會保留著，因為首字母並非小寫</span></span><br><span class="line">  <span class="attr">&quot;text&quot;</span>: [<span class="string">&quot;The rain in Spain falls mainly on the plain.&quot;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="comment">//使用 regular express 進行資料處理</span></span><br><span class="line">  <span class="attr">&quot;char_filter&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;type&quot;</span> : <span class="string">&quot;pattern_replace&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;pattern&quot;</span> : <span class="string">&quot;http://(.*)&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;replacement&quot;</span> : <span class="string">&quot;$1&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;tokenizer&quot;</span>: <span class="string">&quot;standard&quot;</span>,</span><br><span class="line">  </span><br><span class="line">    <span class="attr">&quot;text&quot;</span> : <span class="string">&quot;http://www.elastic.co&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以完全自己定義一個 Analyzer：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">PUT my_index</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;settings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;analysis&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;analyzer&quot;</span>: &#123;</span><br><span class="line">        <span class="comment">//裡面使用的 character filter, tokenizer, token filter 幾乎都是下面自行定義的</span></span><br><span class="line">        <span class="attr">&quot;my_custom_analyzer&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;custom&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;char_filter&quot;</span>: [ <span class="string">&quot;emoticons&quot;</span> ], </span><br><span class="line">          <span class="attr">&quot;tokenizer&quot;</span>: <span class="string">&quot;punctuation&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;filter&quot;</span>: [ <span class="string">&quot;lowercase&quot;</span>, <span class="string">&quot;english_stop&quot;</span> ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, </span><br><span class="line">      <span class="comment">//自訂 character filter =&gt; emoticons</span></span><br><span class="line">      <span class="attr">&quot;char_filter&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;emoticons&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;mapping&quot;</span>, </span><br><span class="line">          <span class="attr">&quot;mappings&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;:) =&gt; _happy_&quot;</span>, </span><br><span class="line">            <span class="string">&quot;:( =&gt; _sad_&quot;</span></span><br><span class="line">          ]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, </span><br><span class="line">      <span class="comment">//自訂 tokenizer =&gt; punctuation</span></span><br><span class="line">      <span class="attr">&quot;tokenizer&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;punctuation&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;pattern&quot;</span>, </span><br><span class="line">          <span class="attr">&quot;pattern&quot;</span>: <span class="string">&quot;[ .,!?]&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, </span><br><span class="line">      <span class="comment">//自訂 token filter =&gt; english_stop</span></span><br><span class="line">      <span class="attr">&quot;filter&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;english_stop&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;stop&quot;</span>, </span><br><span class="line">          <span class="attr">&quot;stopwords&quot;</span>: <span class="string">&quot;_english_&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//驗證上面自訂的 analyzer</span></span><br><span class="line">POST my_index/_analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;analyzer&quot;</span>: <span class="string">&quot;my_custom_analyzer&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;text&quot;</span>: <span class="string">&quot;I&#x27;m a :) person, and you&quot;</span></span><br></pre></td></tr></table></figure><h1 id="Index-Template-和-Dynamic-Template"><a href="#Index-Template-和-Dynamic-Template" class="headerlink" title="Index Template 和 Dynamic Template"></a>Index Template 和 Dynamic Template</h1><ul><li>若 cluster 是用來做 log 管理，每天都產生一個專屬的 index 存放 log，數據管理上較為合理，效能表現也會較好</li></ul><h2 id="Index-Template"><a href="#Index-Template" class="headerlink" title="Index Template"></a>Index Template</h2><p>Index Template 使用來協助使用者設定 mappings &amp; settings 的相關規則，並可透過套用 template 建立新的 index 來取得在 template 中已經存在的設定</p><ul><li><p>index template 只有在建立 index 有用，後續修改 template 不會影響已經存在的 index</p></li><li><p>可以設定多個 index template，這些設定會被合併在一起</p></li><li><p>也可以透過指定 <code>order</code>，來調整 index template 合併的過程</p></li></ul><p>而 index template 是如何在 index 被新增時運作的呢?</p><ol><li><p>先選用 Elasticsearch 中預設的 settings &amp; mappings</p></li><li><p>先套用 order 值低的 index template 中的設定</p></li><li><p>再套用 order 值高的 index template 中的設定，並覆蓋之前的設定</p></li><li><p>如果建立 index 時使用者有指定 settings &amp; mappings，就會覆蓋 index template 的設定</p></li></ol><p>以下透過簡單範例，說明 index template 的建立跟使用時實際的流程：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//建立 default index template</span></span><br><span class="line"><span class="comment">//並設定 order = 0</span></span><br><span class="line">PUT _template/template_default</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;index_patterns&quot;</span>: [<span class="string">&quot;*&quot;</span>],</span><br><span class="line">  <span class="attr">&quot;order&quot;</span> : <span class="number">0</span>,</span><br><span class="line">  <span class="attr">&quot;version&quot;</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">&quot;settings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;number_of_shards&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;number_of_replicas&quot;</span>:<span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//設定 index 為 test 開頭的 index template</span></span><br><span class="line"><span class="comment">//並設定 order = 1</span></span><br><span class="line"><span class="comment">//因此 test 開頭的 index 會優先套用此設定</span></span><br><span class="line">PUT _template/template_test</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;index_patterns&quot;</span> : [<span class="string">&quot;test*&quot;</span>],</span><br><span class="line">    <span class="attr">&quot;order&quot;</span> : <span class="number">1</span>,</span><br><span class="line">    <span class="attr">&quot;settings&quot;</span> : &#123;</span><br><span class="line">        <span class="attr">&quot;number_of_shards&quot;</span>: <span class="number">1</span>,</span><br><span class="line">      <span class="comment">//調整 replica 的數量</span></span><br><span class="line">      <span class="attr">&quot;number_of_replicas&quot;</span> : <span class="number">2</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;mappings&quot;</span> : &#123;</span><br><span class="line">        <span class="comment">//取消日期的偵測</span></span><br><span class="line">      <span class="attr">&quot;date_detection&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">      <span class="comment">//開啟數字的偵測</span></span><br><span class="line">        <span class="attr">&quot;numeric_detection&quot;</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//查詢單一 index template 內容</span></span><br><span class="line">GET /_template/template_default</span><br><span class="line"><span class="comment">//查詢所有 &quot;temp&quot; 開頭的 index template 資訊</span></span><br><span class="line">GET /_template/temp*</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增一個名稱為 testtemplate 的 index，並新增一筆資料</span></span><br><span class="line"><span class="comment">//由於名稱為 test 開頭，因此會套用 template_test 的設定</span></span><br><span class="line">PUT testtemplate/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;someNumber&quot;</span>:<span class="string">&quot;1&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;someDate&quot;</span>:<span class="string">&quot;2019/01/01&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//日期不會被偵測，而被當作一般的 text，而數字則是會偵測為 long</span></span><br><span class="line">GET testtemplate/_mapping</span><br><span class="line"><span class="comment">//shard = 1, replica = 2</span></span><br><span class="line">get testtemplate/_setting</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增一個 index，並自行指定 settings</span></span><br><span class="line"><span class="comment">//自行指定 settings 就會覆蓋 index template 中的所有設定</span></span><br><span class="line">PUT testmy</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;settings&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;number_of_replicas&quot;</span>:<span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//新增資料到 testmy index 中</span></span><br><span class="line">put testmy/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;key&quot;</span>:<span class="string">&quot;value&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//index template 中的 settings 被覆蓋成上面指定的 &quot;number_of_replicas&quot;:5</span></span><br><span class="line">get testmy/_settings</span><br><span class="line"></span><br><span class="line"><span class="comment">//移除 index</span></span><br><span class="line">DELETE testmy</span><br><span class="line">DELETE /_template/template_default</span><br><span class="line">DELETE /_template/template_test</span><br></pre></td></tr></table></figure><h2 id="Dynamic-Template"><a href="#Dynamic-Template" class="headerlink" title="Dynamic Template"></a>Dynamic Template</h2><p>dynamic template 相較於前面提到的 index template，擁有比較彈性的方式來制定 template，例如：</p><ul><li><p>將所有 field 都設定成 keyword，或是關閉 keyword field</p></li><li><p>將 <code>is</code> 開頭的 field 都設定為 boolean</p></li><li><p>將 <code>long</code> 開頭的 field 都設定為 long 類型</p></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//使用內建的自動辨識</span></span><br><span class="line">PUT my_index/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;firstName&quot;</span>:<span class="string">&quot;Ruan&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;isVIP&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//可以發現 firstName &amp; isVIP 都被辨識成 text</span></span><br><span class="line">GET my_index/_mapping</span><br></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//移除原本的 index</span></span><br><span class="line">DELETE my_index</span><br><span class="line"></span><br><span class="line"><span class="comment">//為 index 設定 dynamic template</span></span><br><span class="line">PUT my_index</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;dynamic_templates&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">        <span class="attr">&quot;strings_as_boolean&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;match_mapping_type&quot;</span>:   <span class="string">&quot;string&quot;</span>,</span><br><span class="line">          <span class="comment">//&quot;is&quot; 開頭的 field 設定為 boolean</span></span><br><span class="line">          <span class="attr">&quot;match&quot;</span>:<span class="string">&quot;is*&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;mapping&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;boolean&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;strings_as_keywords&quot;</span>: &#123;</span><br><span class="line">          <span class="attr">&quot;match_mapping_type&quot;</span>:   <span class="string">&quot;string&quot;</span>,</span><br><span class="line">          <span class="comment">//其他的部份則設定為 keyword</span></span><br><span class="line">          <span class="attr">&quot;mapping&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;type&quot;</span>: <span class="string">&quot;keyword&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//firstName 會變成 keyword</span></span><br><span class="line"><span class="comment">//isVIP 會變成 boolean</span></span><br><span class="line">PUT my_index/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;firstName&quot;</span>:<span class="string">&quot;Ruan&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;isVIP&quot;</span>:<span class="string">&quot;true&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET my_index/_mapping</span><br></pre></td></tr></table></figure><p>一個以 path 為基礎，並搭配較為複雜處理的設定：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//移除原本的 index</span></span><br><span class="line">DELETE my_index</span><br><span class="line"></span><br><span class="line"><span class="comment">// 設定 dynamic template，以 path match 為基礎</span></span><br><span class="line">PUT my_index</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;dynamic_templates&quot;</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">&quot;full_name&quot;</span>: &#123;</span><br><span class="line">          <span class="comment">//若符合以下 path 的 match &amp; unmatch 條件</span></span><br><span class="line">          <span class="attr">&quot;path_match&quot;</span>:   <span class="string">&quot;name.*&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;path_unmatch&quot;</span>: <span class="string">&quot;*.middle&quot;</span>,</span><br><span class="line">          <span class="comment">//將資料複製一份到 full_name 欄位</span></span><br><span class="line">          <span class="attr">&quot;mapping&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;type&quot;</span>:       <span class="string">&quot;text&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;copy_to&quot;</span>:    <span class="string">&quot;full_name&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT my_index/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;name&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;first&quot;</span>:  <span class="string">&quot;John&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;middle&quot;</span>: <span class="string">&quot;Winston&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;last&quot;</span>:   <span class="string">&quot;Lennon&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//後面就可以使用額外加入的 full_name 進行搜尋</span></span><br><span class="line"><span class="comment">//full_name 不會出現在 _source 中</span></span><br><span class="line">GET my_index/_search?q=full_name:John</span><br></pre></td></tr></table></figure><h1 id="Elasticsearch-聚合分析簡介"><a href="#Elasticsearch-聚合分析簡介" class="headerlink" title="Elasticsearch 聚合分析簡介"></a>Elasticsearch 聚合分析簡介</h1><h2 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h2><ul><li><p>Elasticsearc 除了一般的搜尋之外，也可透過 aggergation 的方式，提供數據統計分析的功能，可作到很即時的查詢</p></li><li><p>透過 aggregation 可以得到分析 &amp; 總結數據後的總覽，而非單一 document，例如：</p><ul><li><p>某特定區域剩餘的客房數量</p></li><li><p>進行不同價格區間的搜尋，可預定的平價旅館 &amp; 五星級酒店的數量</p></li></ul></li><li><p>效能很好，在 ES 端就可以得到分析結果，不用在 client 端開發分析邏輯</p></li><li><p>許多 Kibana 報表中的元素都可以透過 aggregation 的數據來完成。例如：</p><ul><li><p>公司員工的職位分佈、薪水分佈</p></li><li><p>客戶所在的地理位置分佈</p></li><li><p>訂單的增減狀況</p></li></ul></li></ul><h2 id="Aggregation-Family-聚合總類"><a href="#Aggregation-Family-聚合總類" class="headerlink" title="Aggregation Family (聚合總類)"></a>Aggregation Family (聚合總類)</h2><ul><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html">Bucket Aggregation</a>：滿足特定條件的 document 集合 (類似傳統 SQL 語法中的 <code>Group By</code>)</p></li><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics.html">Metric Aggregation</a>：可透過數學運算，對 document filed 進行統計分析</p><ul><li><p>基於數據集合計算結果；除了支援在 field 上進行計算，也支援在 (painless) script 產生的結果之上進行計算</p></li><li><p>大多數的 metric 是數學計算，只會輸出一個值，例如：<strong>min</strong> / <strong>max</strong> / <strong>sum</strong> / <strong>avg</strong> / <strong>cardinality</strong></p></li><li><p>部份的 metric 支援輸出多個值，例如：<strong>stats</strong> / <strong>percentiles</strong> / <strong>percentile_ranks</strong></p></li></ul></li></ul><p><img src="/blog/images/Elasticsearch/es_aggregation-01.png" alt="Elasticsearch - Bucket &amp; Metric aggregation"></p><ul><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-matrix.html">Matrix Aggregation</a>：支援對多個 field 同時進行操作並提供一個結果矩陣</p></li><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline.html">Pipiline Aggregation</a>: 對其他 aggregation 的結果再一次的進行 aggregation</p></li></ul><h2 id="範例：Bucketing-分桶"><a href="#範例：Bucketing-分桶" class="headerlink" title="範例：Bucketing (分桶)"></a>範例：Bucketing (分桶)</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//根據目的地進行 bucketing 操作</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;size&quot;</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="attr">&quot;aggs&quot;</span>:&#123;</span><br><span class="line">    <span class="attr">&quot;flight_dest&quot;</span>:&#123;</span><br><span class="line">      <span class="comment">//使用 terms 關鍵字，指定要進行 bucketing，使用的是 field DestCountry</span></span><br><span class="line">      <span class="attr">&quot;terms&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;DestCountry&quot;</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="範例：Bucketing-Metric"><a href="#範例：Bucketing-Metric" class="headerlink" title="範例：Bucketing + Metric"></a>範例：Bucketing + Metric</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查看航班目的地的統計資訊，並以目的地為單位，額外增加平均，最高最低價格</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;size&quot;</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="attr">&quot;aggs&quot;</span>:&#123;</span><br><span class="line">    <span class="comment">//先進行 bucket aggregation 操作</span></span><br><span class="line">    <span class="attr">&quot;flight_dest&quot;</span>:&#123;</span><br><span class="line">      <span class="attr">&quot;terms&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;DestCountry&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="comment">//根據上面結果再進行 metric aggregation 操作</span></span><br><span class="line">      <span class="attr">&quot;aggs&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;avg_price&quot;</span>:&#123;</span><br><span class="line">          <span class="attr">&quot;avg&quot;</span>:&#123;</span><br><span class="line">            <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;AvgTicketPrice&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;max_price&quot;</span>:&#123;</span><br><span class="line">          <span class="attr">&quot;max&quot;</span>:&#123;</span><br><span class="line">            <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;AvgTicketPrice&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;min_price&quot;</span>:&#123;</span><br><span class="line">          <span class="attr">&quot;min&quot;</span>:&#123;</span><br><span class="line">            <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;AvgTicketPrice&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="範例：Bucketing-Metric-Matrix"><a href="#範例：Bucketing-Metric-Matrix" class="headerlink" title="範例：Bucketing + Metric + Matrix"></a>範例：Bucketing + Metric + Matrix</h2><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//查看航班目的地的統計資訊，並以目的地為單位，額外增加平均票價 &amp; 目的地天氣的統計資訊</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;size&quot;</span>: <span class="number">0</span>,</span><br><span class="line">  <span class="attr">&quot;aggs&quot;</span>:&#123;</span><br><span class="line">    <span class="attr">&quot;flight_dest&quot;</span>:&#123;</span><br><span class="line">      <span class="attr">&quot;terms&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;DestCountry&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;aggs&quot;</span>:&#123;</span><br><span class="line">        <span class="attr">&quot;stats_price&quot;</span>:&#123;</span><br><span class="line">          <span class="comment">//透過 stats 關鍵字可直接帶出 count/min/max/avg/sum ... 等資訊</span></span><br><span class="line">          <span class="attr">&quot;stats&quot;</span>:&#123;</span><br><span class="line">            <span class="attr">&quot;field&quot;</span>:<span class="string">&quot;AvgTicketPrice&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">        <span class="attr">&quot;weather&quot;</span>:&#123;</span><br><span class="line">          <span class="attr">&quot;terms&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;field&quot;</span>: <span class="string">&quot;DestWeather&quot;</span>,</span><br><span class="line">            <span class="comment">//可限定輸出的資料數量，預設為 10</span></span><br><span class="line">            <span class="attr">&quot;size&quot;</span>: <span class="number">5</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="References-1"><a href="#References-1" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://time.geekbang.org/course/intro/100030501">Elasticsearch核心技術與實戰 | 極客時間</a></p></li><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">Elasticsearch Reference [7.9] | Elastic</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Terraform] Input Variables </title>
      <link href="/blog/DevOps/terraform-input-variables/"/>
      <url>/blog/DevOps/terraform-input-variables/</url>
      
        <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>input variable 通常用來作為 module 的 parameter，而且在 root module 中定義好的 variable，可以根據需求透過 CLI or 環境變數來覆蓋。</p><h1 id="變數宣告"><a href="#變數宣告" class="headerlink" title="變數宣告"></a>變數宣告</h1><p>要宣告一個變數檔，基本上就是新增一個副檔名為 <code>tf</code> 的文字檔，放入類似以下內容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 變數名稱為 &quot;image_id&quot;，型態為 string</span></span><br><span class="line">variable <span class="string">&quot;image_id&quot;</span> &#123;</span><br><span class="line">  <span class="built_in">type</span> = string</span><br><span class="line">  <span class="comment"># 可透過 &quot;description&quot; 參數對變數加上額外說明</span></span><br><span class="line">  description = <span class="string">&quot;The id of the machine image (AMI) to use for the server.&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 變數名稱為 &quot;availability_zone_names&quot;，型態為 list(string)</span></span><br><span class="line">variable <span class="string">&quot;availability_zone_names&quot;</span> &#123;</span><br><span class="line">  <span class="built_in">type</span>    = list(string)</span><br><span class="line">  <span class="comment"># 可指定預設值</span></span><br><span class="line">  default = [<span class="string">&quot;us-west-1a&quot;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 變數名稱為 &quot;docker_ports&quot;，型態為 list(object)</span></span><br><span class="line">variable <span class="string">&quot;docker_ports&quot;</span> &#123;</span><br><span class="line">  <span class="built_in">type</span> = list(object(&#123;</span><br><span class="line">    internal = number</span><br><span class="line">    external = number</span><br><span class="line">    protocol = string</span><br><span class="line">  &#125;))</span><br><span class="line">  default = [</span><br><span class="line">    &#123;</span><br><span class="line">      internal = 8300</span><br><span class="line">      external = 8300</span><br><span class="line">      protocol = <span class="string">&quot;tcp&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基本上變數的名稱可以隨意取(只要遵守<a href="https://www.terraform.io/docs/configuration/syntax.html#identifiers">命名規則</a>即可)，但不可使用 <code>source</code>, <code>version</code>, <code>providers</code>, <code>count</code>, <code>for_each</code>, <code>lifecycle</code>, <code>depends_on</code>, <code>locals</code> … 等關鍵字。</p><h1 id="如何使用-input-variable"><a href="#如何使用-input-variable" class="headerlink" title="如何使用 input variable?"></a>如何使用 input variable?</h1><p>使用方式很簡單，一個簡單的範例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ami 設定中使用變數 image_id</span></span><br><span class="line">resource <span class="string">&quot;aws_instance&quot;</span> <span class="string">&quot;example&quot;</span> &#123;</span><br><span class="line">  instance_type = <span class="string">&quot;t2.micro&quot;</span></span><br><span class="line">  ami           = var.image_id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="變數型態"><a href="#變數型態" class="headerlink" title="變數型態"></a>變數型態</h1><p>input variable 可以有很多型態，以下是目前支援的型態：</p><ul><li><p><code>string</code></p></li><li><p><code>number</code></p></li><li><p><code>bool</code></p></li><li><p><code>list(&lt;TYPE&gt;)</code></p></li><li><p><code>set(&lt;TYPE&gt;)</code></p></li><li><p><code>map(&lt;TYPE&gt;)</code></p></li><li><p><code>object(&#123;&lt;ATTR NAME&gt; = &lt;TYPE&gt;, ... &#125;)</code></p></li><li><p><code>tuple([&lt;TYPE&gt;, ...])</code></p></li></ul><blockquote><p>tuple 中每個元素可以是不同的型態，例如 <code>[&quot;a&quot;, 15, true]</code></p></blockquote><p>詳細的使用規範 &amp; 範例可以參考<a href="https://www.terraform.io/docs/configuration/types.html">官網文件</a>。</p><h1 id="如何傳入-variable-values"><a href="#如何傳入-variable-values" class="headerlink" title="如何傳入 variable values ?"></a>如何傳入 variable values ?</h1><p>定義了 variable 之後，就會面臨到如何傳入 value 的問題。</p><p>以下的說明是將 value 傳入到 root module 中，若是要傳入到 child module 中，則是要在程式中自行處理。</p><h2 id="透過-command-line"><a href="#透過-command-line" class="headerlink" title="透過 command line"></a>透過 command line</h2><p>透過 <code>-var</code> 關鍵字搭配 <code>terraform plan</code> or <code>terraform apply</code> 命令使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ terraform apply -var=<span class="string">&quot;image_id=ami-abc123&quot;</span></span><br><span class="line"></span><br><span class="line">$ terraform apply -var=<span class="string">&#x27;image_id_list=[&quot;ami-abc123&quot;,&quot;ami-def456&quot;]&#x27;</span></span><br><span class="line"></span><br><span class="line">$ terraform apply -var=<span class="string">&#x27;image_id_map=&#123;&quot;us-east-1&quot;:&quot;ami-abc123&quot;,&quot;us-east-2&quot;:&quot;ami-def456&quot;&#125;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p><code>-var</code> 可以在單一指令中使用多次，來傳入多個 variable value</p></blockquote><h2 id="使用變數定義檔-tfvars"><a href="#使用變數定義檔-tfvars" class="headerlink" title="使用變數定義檔(.tfvars)"></a>使用變數定義檔(<code>.tfvars</code>)</h2><p>可以撰寫以 <code>.tfvars</code> or <code>.tfvars.json</code> 為副檔名的變數定義檔，以下是一個簡單的範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">image_id = <span class="string">&quot;ami-abc123&quot;</span></span><br><span class="line"></span><br><span class="line">availability_zone_names = [</span><br><span class="line">  <span class="string">&quot;us-east-1a&quot;</span>,</span><br><span class="line">  <span class="string">&quot;us-west-1c&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>以下是 json 格式的範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;image_id&quot;</span>: <span class="string">&quot;ami-abc123&quot;</span>,</span><br><span class="line"></span><br><span class="line">  <span class="attr">&quot;availability_zone_names&quot;</span>: [<span class="string">&quot;us-west-1a&quot;</span>, <span class="string">&quot;us-west-1c&quot;</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以透過類似以下的語法帶入變數定義檔：</p><blockquote><p>terraform apply -var-file=”testing.tfvars”</p></blockquote><p>而 Terraform 也可以自動將變數定義檔載入，只要符合以下任何一種命名規範即可：</p><ul><li><p>檔案名稱為 <code>terraform.tfvars</code> or <code>terraform.tfvars.json</code></p></li><li><p>任何檔名，副檔名為 <code>.auto.tfvars</code> or <code>.auto.tfvars</code></p></li></ul><blockquote><p>必須放在執行 <code>terraform apply</code> or <code>terraform plan</code> 時所在的目錄中</p></blockquote><h2 id="用-Environment-Variables-傳入"><a href="#用-Environment-Variables-傳入" class="headerlink" title="用 Environment Variables 傳入"></a>用 Environment Variables 傳入</h2><p>除了上面兩種外，Terraform 也會自動去使用環境變數開頭為 <code>TF_VAR_</code> 的環境變數來作為 variable value。</p><p>假設變數名稱為 <code>image_id</code>，只要將變數值設定到 <code>TF_VAR_image_id</code> 環境變數中，terraform 就會自己載入了!</p><h1 id="傳入變數值的優先權"><a href="#傳入變數值的優先權" class="headerlink" title="傳入變數值的優先權"></a>傳入變數值的優先權</h1><p>上面提到幾個方法可以傳入變數值，那如果同時有多個方法被使用，哪個會被優先採用呢?</p><p>基本上順序是這樣的：</p><ol><li><p>首先會使用目錄中的 <code>terraform.tfvars</code> or <code>terraform.tfvars.json</code> 檔案</p></li><li><p>若目錄中有 <code>*.auto.tfvars</code> or <code>*.auto.tfvars.json</code> 檔案，則會覆蓋掉上一個檔案中的變數值</p></li><li><p>若在 command line 中透過 <code>-var</code> or <code>-var-file</code> 指定變數值或是檔案，則會覆蓋掉上面兩個檔案中提供的變數值</p></li></ol><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.terraform.io/docs/configuration/variables.html">Input Variables - Configuration Language - Terraform by HashiCorp</a></p></li><li><p><a href="https://www.terraform.io/docs/configuration/types.html">Type Constraints - Configuration Language - Terraform by HashiCorp</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Terraform </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Terraform </tag>
            
            <tag> IaC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Tekton] Tekton Pipeline 的基本組成元件介紹</title>
      <link href="/blog/DevOps/tekton-pipeline-building-blocks/"/>
      <url>/blog/DevOps/tekton-pipeline-building-blocks/</url>
      
        <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>在 Tekton pipeline 中有以下幾個主要的組成要素，分別是：</p><ul><li><p>PipelineResource</p></li><li><p>Task &amp; ClusterTask</p></li><li><p>TaskRun</p></li><li><p>Pipeline</p></li><li><p>PipelineRun</p></li></ul><h1 id="PipelineResource"><a href="#PipelineResource" class="headerlink" title="PipelineResource"></a>PipelineResource</h1><p><code>PipelineResource</code> 可以作為 task 的 input or output，而每個 task 可以有多個 input &amp; output。</p><h2 id="Syntax"><a href="#Syntax" class="headerlink" title="Syntax"></a>Syntax</h2><p>PipelineResource 的定義中會有以下必要資訊：</p><ul><li><p><code>apiVersion</code>：目前固定是 <code>tekton.dev/v1alpha1</code></p></li><li><p><code>kind</code>：因為這是 CRD，所以是 <code>PipelineResource</code></p></li><li><p><code>metadata</code>：用來辨識此 TaskRun 用的資訊，例如 <strong>name</strong></p></li><li><p><code>sepc</code>：使用 resource 的詳細資訊(例如：路徑、位址)</p></li><li><p><code>type</code>：用來指定 resource type，目前支援 <code>git</code>, <code>pullRequest</code>, <code>image</code>, <code>cluster</code>, <code>storage</code>, <code>cloudevent</code> … 等等</p></li></ul><p>其他選填項目：</p><ul><li><code>params</code>：不同的 resource type 可能會有的不同額外參數資訊</li></ul><h2 id="Resource-Type"><a href="#Resource-Type" class="headerlink" title="Resource Type"></a>Resource Type</h2><p>有了以上概念後，接著要知道的是 <a href="https://github.com/tektoncd/pipeline/blob/master/docs/resources.md" title="PipelineResources">PipelineResources</a> 共有以下幾種類型：</p><ul><li><p>Git Resource</p></li><li><p>Pull Request Resource</p></li><li><p>Image Resource</p></li><li><p>Cluster Resource</p></li><li><p>Storage Resource</p></li><li><p>Cloud Event Resource</p></li></ul><p>以下就針對比較常用的 Git &amp; Image resource 說明，其他的部份可以參考<a href="https://github.com/tektoncd/pipeline/blob/master/docs/resources.md">官網的詳細文件</a>。</p><h3 id="Git-Resource"><a href="#Git-Resource" class="headerlink" title="Git Resource"></a>Git Resource</h3><p>一般的 git repository，作為 task input 時，Tekton 執行 task 前會將程式碼 clone 回來，因此這邊就必須注意 git repository 存取的權限問題，若是 private repository 就要額外提供 credential 資訊才可以正常運作</p><p>以下是一個標準的 Git PipelineResource 的定義：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PipelineResource</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">wizzbang-git</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">url</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">https://github.com/wizzbangcorp/wizzbang.git</span></span><br><span class="line">    <span class="comment"># 可用 branch, tag, commit SHA or ref</span></span><br><span class="line">    <span class="comment"># 沒指定就會拉 master branch</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">revision</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">master</span></span><br><span class="line">      <span class="comment"># value: some_awesome_feature</span></span><br><span class="line">      <span class="comment"># value: refs/pull/52525/head</span></span><br></pre></td></tr></table></figure><h3 id="Image-Resource"><a href="#Image-Resource" class="headerlink" title="Image Resource"></a>Image Resource</h3><p>image resource 就是存放在某個遠端 image registry 的 image，通常作為 task output，表示該 task 過程中會 build image 並上傳到指定的 image resource。</p><p>以下是一個標準 Image PipielineResource 的定義：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PipelineResource</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kritis-resources-image</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">image</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">url</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">gcr.io/staging-images/kritis</span></span><br></pre></td></tr></table></figure><h2 id="使用上的一些小技巧"><a href="#使用上的一些小技巧" class="headerlink" title="使用上的一些小技巧"></a>使用上的一些小技巧</h2><h3 id="調整-resource-掛載的路徑"><a href="#調整-resource-掛載的路徑" class="headerlink" title="調整 resource 掛載的路徑"></a>調整 resource 掛載的路徑</h3><p>實際上工作執行在 container 中，使用 resource 的方法就是會先將其掛載到 container 的檔案系統中。</p><p>使用者可以透過 <code>targetPath</code> 參數來調整 resource 被掛載的路徑，在預設沒有設定 <code>targetPath</code> 的情況下，resource 會被掛載到 <code>/workspace</code> 目錄中，若是有指定則會改到 <code>/workspace/targetPath</code> 目錄中，以下是一個設定 targetPath 的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Task</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">task-with-input</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">        <span class="comment"># 額外指定了 targetPath，會影響實際上在 container 的掛載路徑</span></span><br><span class="line">        <span class="attr">targetPath:</span> <span class="string">go/src/github.com/tektoncd/pipeline</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">unit-tests</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">golang</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;go&quot;</span>]</span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;test&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;./...&quot;</span></span><br><span class="line">      <span class="comment"># 因為設定了 targetPath，因此 workingDir 也要有對應的設定才可以正確執行</span></span><br><span class="line">      <span class="attr">workingDir:</span> <span class="string">&quot;/workspace/go/src/github.com/tektoncd/pipeline&quot;</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GOPATH</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">/workspace/go</span></span><br></pre></td></tr></table></figure><h3 id="在-pipeline-的-task-之間傳遞-resource"><a href="#在-pipeline-的-task-之間傳遞-resource" class="headerlink" title="在 pipeline 的 task 之間傳遞 resource"></a>在 pipeline 的 task 之間傳遞 resource</h3><p>這樣的應用可以透過 <code>paths</code> 設定來達成：</p><ul><li><p>若是在 input resource 設定 paths，表示要改從 paths 指定的路徑取得 resource 內容</p></li><li><p>若是在 output resource 設定 paths，表示要將 resource 複製到 paths 所指定的路徑</p></li></ul><p>通常 <code>paths</code> 的設定常用在 Pipeline 中 Task 之間的 resource 傳遞，以下是個簡單範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Task</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volume-task</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="comment"># 原本是要 push 回 git resource</span></span><br><span class="line">  <span class="attr">outputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build-war</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">objectuser/run-java-jar</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">jar</span></span><br><span class="line">      <span class="attr">args:</span> [<span class="string">&quot;-cvf&quot;</span>, <span class="string">&quot;projectname.war&quot;</span>, <span class="string">&quot;*&quot;</span>]</span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">custom-volume</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/custom</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">TaskRun</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volume-taskrun</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">volume-task</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">resourceRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">java-git-resource</span></span><br><span class="line">  <span class="attr">outputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="comment"># 在 output 設定 paths，會將 output resource 複製到以下路徑中</span></span><br><span class="line">        <span class="comment"># 所以 output resource 會存在於 volume 上</span></span><br><span class="line">        <span class="comment"># 而不是被送回 Git repository</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">/custom/workspace/</span></span><br><span class="line">        <span class="attr">resourceRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">java-git-resource</span></span><br><span class="line">  <span class="comment"># 額外定義一個 volume</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">custom-volume</span></span><br><span class="line">      <span class="attr">emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><h1 id="Task-amp-ClusterTask"><a href="#Task-amp-ClusterTask" class="headerlink" title="Task &amp; ClusterTask"></a>Task &amp; ClusterTask</h1><p><code>Task</code>(&amp; <code>ClusterTask</code>) 中包含了一連串的 step，通常是使用者要用來執行 CI flow，而這些工作會在單一個 pod 中以多個 container 的形式逐一完成。</p><blockquote><p>Task &amp; ClusterTask 兩者的不同在於 Task 是屬於 namespace level，而 ClusterTask 是屬於 cluster level</p></blockquote><p>而在 Task 的定義中，最重要的部份有以下三個項目：</p><ul><li><p>Input</p></li><li><p>Output</p></li><li><p>Steps</p></li></ul><p>以下是一個 task 的標準定義內容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Task</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deploy-using-kubectl</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">source</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">image</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">image</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">path</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">string</span></span><br><span class="line">        <span class="attr">description:</span> <span class="string">Path</span> <span class="string">to</span> <span class="string">the</span> <span class="string">manifest</span> <span class="string">to</span> <span class="string">apply</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">yamlPathToImage</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">string</span></span><br><span class="line">        <span class="attr">description:</span></span><br><span class="line">          <span class="string">The</span> <span class="string">path</span> <span class="string">to</span> <span class="string">the</span> <span class="string">image</span> <span class="string">to</span> <span class="string">replace</span> <span class="string">in</span> <span class="string">the</span> <span class="string">yaml</span> <span class="string">manifest</span> <span class="string">(arg</span> <span class="string">to</span> <span class="string">yq)</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="comment"># step 中可以定義多個執行工作，會依照順序執行</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">replace-image</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">mikefarah/yq</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;yq&quot;</span>]</span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;w&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;-i&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;$(inputs.params.path)&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;$(inputs.params.yamlPathToImage)&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;$(inputs.resources.image.url)&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">run-kubectl</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">lachlanevenson/k8s-kubectl</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;kubectl&quot;</span>]</span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;apply&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;-f&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;$(inputs.params.path)&quot;</span></span><br><span class="line">  <span class="comment"># 此 volume 在 task 中沒有用到，只是一個範例而已</span></span><br><span class="line">  <span class="comment"># 用以表示可以在 task 中定義 volume 並使用</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">example-volume</span></span><br><span class="line">      <span class="attr">emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>以下接著說明上面提到的 <strong>Input</strong> / <strong>Output</strong> / <strong>Steps</strong></p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>若是有需要，可以在 task 中定義 <code>input</code>，而 input 可以有 <code>Parameters</code> &amp; <code>Input resources</code> 兩種：</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><p>這個部份可以當作是簡單的字串類型參數，可以拿來作為程式編譯時的參數 or 用來命名 artifacts 之用；也可以使用 *array** 之類較為複雜的參數。</p><p>以下是一個簡單 Task 的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Task</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">task-with-parameters</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">flags</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">array</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">someURL</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">string</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">my-builder</span></span><br><span class="line">      <span class="attr">args:</span> [<span class="string">&quot;build&quot;</span>, <span class="string">&quot;$(inputs.params.flags)&quot;</span>, <span class="string">&quot;url=$(inputs.params.someURL)&quot;</span>]</span><br></pre></td></tr></table></figure><p>透過以上的 Task template，可以用以下的 TaskRun 來使用：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">TaskRun</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">run-with-parameters</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">task-with-parameters</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">flags</span></span><br><span class="line">        <span class="attr">value:</span> </span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;--set&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;arg1=foo&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;--randomflag&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;--someotherflag&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">someURL</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;http://google.com&quot;</span></span><br></pre></td></tr></table></figure><h3 id="Input-resources"><a href="#Input-resources" class="headerlink" title="Input resources"></a>Input resources</h3><p>這裡的 input resource 即為上面所介紹的 <a href="https://github.com/tektoncd/pipeline/blob/master/docs/resources.md" title="PipelineResources">PipelineResources</a>。</p><p>上面有提到如何定義 PipelineResources，以下是個簡單在 Task 中使用 PipelineResources 的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Task</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deploy-image</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 以下使用到 Git, Image, Cluster 三種 PipelineResource</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dockerimage</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">image</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">testcluster</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">cluster</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">deploy</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">image-wtih-kubectl</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;bash&quot;</span>]</span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;-c&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">kubectl</span> <span class="string">--kubeconfig</span></span><br><span class="line">          <span class="string">/workspace/$(inputs.resources.testCluster.name)/kubeconfig</span> <span class="string">--context</span></span><br><span class="line">          <span class="string">$(inputs.resources.testCluster.name)</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">/workspace/service.yaml&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>基本上 <code>Output</code> 會以 <a href="https://github.com/tektoncd/pipeline/blob/master/docs/resources.md" title="PipelineResources">PipelineResources</a> 的形式存在，在使用上有一些必須注意到的部份：</p><ul><li><p>在一個 Pipeline 中的 output 定義，output resource 會以共享的形式存在於多個 task 之間，並放在 <code>/workspace/output/resource_name/</code> 路徑下</p></li><li><p>由於每個 task 的工作都會在獨立的 pod 完成，因此共享的形式想當然爾就是用 PV &amp; PVC 的方式來達成</p></li><li><p>承上，所以沒有額外 storage 相關設定的情況下，使用者必須確認 k8s cluster 中有預設的 StorageClass 可用</p></li></ul><p>以下是一個簡單的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">outputs:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">storage-gcs</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">gcs</span></span><br><span class="line"><span class="attr">steps:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">objectuser/run-java-jar</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">jar</span>]</span><br><span class="line">    <span class="attr">args:</span></span><br><span class="line">      [<span class="string">&quot;-cvf&quot;</span>, <span class="string">&quot;-o&quot;</span>, <span class="string">&quot;/workspace/output/storage-gcs/&quot;</span>, <span class="string">&quot;projectname.war&quot;</span>, <span class="string">&quot;*&quot;</span>]</span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;FOO&quot;</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;world&quot;</span></span><br></pre></td></tr></table></figure><p>上面的工作完成後，檔案會自動被上傳到 <code>storage-gcs</code> resource 指定的 GCS 上。</p><p>以下的示範則是在一個 task 中的多個 step 之間對 output resource 進行處理：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="comment"># 指定 input resource 掛載到 /workspace/customworkspace 目錄中</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tar-artifact</span></span><br><span class="line">    <span class="attr">targetPath:</span> <span class="string">customworkspace</span></span><br><span class="line">  <span class="comment"># input &amp; output 相同</span></span><br><span class="line">  <span class="comment"># 因此處理完後會將 /workspace/customworkspace 目錄中的檔案上傳到 resource 指定的位置</span></span><br><span class="line">  <span class="attr">outputs:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tar-artifact</span></span><br><span class="line"><span class="attr">steps:</span></span><br><span class="line">  <span class="comment"># 解壓縮來自 input 的檔案(rules_docker-master.tar)到 /workspace/tar-scratch-space/ 目錄中</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">untar</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">ubuntu</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;/bin/bash&quot;</span>]</span><br><span class="line">      <span class="attr">args:</span> [<span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;mkdir -p /workspace/tar-scratch-space/ &amp;&amp; tar -xvf /workspace/customworkspace/rules_docker-master.tar -C /workspace/tar-scratch-space/&#x27;</span>]</span><br><span class="line">  <span class="comment"># 對壓縮檔中的某個檔案進行編輯</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">edit-tar</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">ubuntu</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;/bin/bash&quot;</span>]</span><br><span class="line">      <span class="attr">args:</span> [<span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;echo crazy &gt; /workspace/tar-scratch-space/rules_docker-master/crazy.txt&#x27;</span>]</span><br><span class="line">  <span class="comment"># 將檔案重新打包並放回 /workspace/customworkspace 目錄中</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tar-it-up</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/bin/bash&quot;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;cd /workspace/tar-scratch-space/ &amp;&amp; tar -cvf /workspace/customworkspace/rules_docker-master.tar rules_docker-master&#x27;</span>]</span><br></pre></td></tr></table></figure><h2 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h2><p>在 task 中 <code>steps</code> 的項目，會依照以下方式運作：</p><ul><li><p>可以定義多個項目，都會成為一個獨立的 container 執行</p></li><li><p>同一個 task 中的 steps 定義的項目所對應到的 container 都會存在同一個 pod 內</p></li><li><p>會依照 steps 中定義的順序執行</p></li></ul><p>而 step 的使用方式就像是在 k8s 中 <a href="https://kubernetes.io/docs/concepts/containers/">container</a> 的定義一樣，從上面 input &amp; output 的例子多少可以看出，以下再來一個簡單的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">stepTemplate:</span></span><br><span class="line">  <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;FOO&quot;</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;bar&quot;</span></span><br><span class="line"><span class="attr">steps:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">env</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="bullet">-</span> <span class="string">env</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;FOO&quot;</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;baz</span></span><br></pre></td></tr></table></figure><h1 id="TaskRun"><a href="#TaskRun" class="headerlink" title="TaskRun"></a>TaskRun</h1><p>定義了 task 之後，Tekton 並不會主動執行任何 task，這時候就必須要搭配 TaskRun 才可以讓 task 真正的執行指定工作。</p><h2 id="Syntax-1"><a href="#Syntax-1" class="headerlink" title="Syntax"></a>Syntax</h2><p>TaskRun 的定義中會有以下必要資訊：</p><ul><li><p><code>apiVersion</code>：目前固定是 <code>tekton.dev/v1alpha1</code></p></li><li><p><code>kind</code>：因為這是 CRD，所以是 <code>TaskRun</code></p></li><li><p><code>metadata</code>：用來辨識此 TaskRun 用的資訊，例如 <strong>name</strong></p></li><li><p><code>sepc</code>：一般會放上 <code>taskRef</code>，指定已經定義好的 Task</p></li></ul><p>其他選填資訊：</p><ul><li><p><code>serviceAccount</code>: 讓使用者可以使用自訂的認證資訊來執行工作</p></li><li><p><code>input</code>：用來指定 input parameter or resource</p></li><li><p><code>output</code>：用來指定 output resource</p></li><li><p><code>timeout</code>：指定工作執行的 timeout 時間，若設定為 0 則表示沒有 timeout (預設的 timeout 時間為 <code>60 mins</code>)</p></li><li><p><code>podTemplate</code>：所有的 TaskRun 都是以 Pod 的形式執行，此參數可以在 PodSpec 中額外加入指定的 subset 資訊</p></li></ul><p>以下是一個標準的 TaskRun 定義：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下幾個(apiVersion, kind, metadata, spec)是必要資訊</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">TaskRun</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">build-docker-image-from-git-source-task-run</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">serviceAccount:</span> <span class="string">robot-docker-basic</span></span><br><span class="line">  <span class="comment"># 指定到已經預先定義好的 Task</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">build-docker-image-from-git-source</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-source</span></span><br><span class="line">        <span class="comment"># 指定到已經預先定義好的 PipelineResource</span></span><br><span class="line">        <span class="attr">resourceRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">git-tekton-test</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pathToDockerFile</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">Dockerfile</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pathToContext</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">/workspace/docker-source/examples/microservices/leeroy-web</span></span><br><span class="line">  <span class="attr">outputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">builtImage</span></span><br><span class="line">        <span class="attr">resourceRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">image-tekton-test</span></span><br></pre></td></tr></table></figure><h2 id="Pod-Template"><a href="#Pod-Template" class="headerlink" title="Pod Template"></a>Pod Template</h2><p>透過 Pod Template 的設定，可以提供額外的資訊讓 Task(Pod) 執行時改變某些行為，目前支援的設定如下：</p><ul><li><p><code>nodeSelector</code>：可用來指定 task 要在哪個 node 上執行</p></li><li><p><code>tolerations</code>：允許讓 task 執行上帶有特定 <strong>taint</strong> 設定的 node 上</p></li><li><p><code>affinity</code>：可限制 task 在帶有某些 label 的 node 上執行</p></li><li><p><code>securityContext</code>：額外指定 pod-level 的安全屬性，或像是 <code>runAsUser</code> or <code>selinux</code> 的 container 設定</p></li><li><p><code>volumes</code>：指定可讓 container 掛載的 volume 清單</p></li></ul><p>以下是一個 pod template 的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Task</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myTask</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">write</span> <span class="string">something</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">ubuntu</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;bash&quot;</span>, <span class="string">&quot;-c&quot;</span>]</span><br><span class="line">      <span class="attr">args:</span> [<span class="string">&quot;echo &#x27;foo&#x27; &gt; /my-cache/bar&quot;</span>]</span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-cache</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/my-cache</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">TaskRun</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myTaskRun</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">myTask</span></span><br><span class="line">  <span class="attr">podTemplate:</span></span><br><span class="line">    <span class="comment"># 額外指定 securityContext</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsNonRoot:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 額外指定 volume</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-cache</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">my-volume-claim</span></span><br></pre></td></tr></table></figure><h1 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h1><p>Pipeline 其實可以把它簡單思考為前面 Task 的集合，有順序性的排列，並透過之後介紹的 PipelineRun 來運作。</p><h2 id="Syntax-2"><a href="#Syntax-2" class="headerlink" title="Syntax"></a>Syntax</h2><p>Pipeline 的定義中會有以下必要資訊：</p><ul><li><p><code>apiVersion</code>：目前固定是 <code>tekton.dev/v1alpha1</code></p></li><li><p><code>kind</code>：因為這是 CRD，所以是 <code>Pipeline</code></p></li><li><p><code>metadata</code>：用來辨識此 TaskRun 用的資訊，例如 <strong>name</strong></p></li><li><p><code>sepc</code>：這裡就會透過 <code>tasks</code> 來指定已經定義好的 Task list 了</p></li></ul><p>其他選填項目：</p><ul><li><p><code>resources</code>：設定在 Task 中會使用到的 PipelineResource</p></li><li><p><code>tasks.conditions</code>：用來設定在特定的情況下 Task 才需要執行，詳細內容可參考<a href="https://github.com/tektoncd/pipeline/blob/master/docs/conditions.md">官方文件</a></p></li></ul><h2 id="定義-Tasks-amp-Resource-amp-Parameter"><a href="#定義-Tasks-amp-Resource-amp-Parameter" class="headerlink" title="定義 Tasks &amp; Resource &amp; Parameter"></a>定義 Tasks &amp; Resource &amp; Parameter</h2><p>在 Task 也有 Resource &amp; Parameter 的定義，Input 中可以同時有 Resource &amp; Parameter，但 Output 中僅可以有 Resource。</p><p>那 Pipeline 中的 Resource 就不一樣了，沒有 Input &amp; Output 的差別，就僅僅是 Resource &amp; Parameter 的定義而已，因為實際上需要 Input &amp; Output 的是 Task 本身，Pipeline 中的 Resource &amp; Parameter 定義只是用來滿足 Task 的需要而已，以下是一個簡單範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pipeline</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">list-files-pipeline</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 即為 PipelineResource</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">source-repo</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;path&quot;</span></span><br><span class="line">      <span class="attr">default:</span> <span class="string">&quot;README.md&quot;</span></span><br></pre></td></tr></table></figure><p>而 Parameter 在使用上還有一些額外需要了解的：</p><ul><li><p>每個 Parameter 都需要指定 <code>type</code> 資訊，沒指定就預設為 string</p></li><li><p>若需要同時傳入多個 string，則可以將 type 設定為 <code>array</code></p></li></ul><p>以下是一個 Pipeline &amp; PipelineRun 的組合設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pipeline</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pipeline-with-parameters</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># parameter 定義</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">context</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">string</span></span><br><span class="line">      <span class="attr">description:</span> <span class="string">Path</span> <span class="string">to</span> <span class="string">context</span></span><br><span class="line">      <span class="attr">default:</span> <span class="string">/some/where/or/other</span></span><br><span class="line">  <span class="comment"># 在 pipeline 中要執行的 task</span></span><br><span class="line">  <span class="attr">tasks:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build-skaffold-web</span></span><br><span class="line">      <span class="comment"># 指到之前定義好的 task</span></span><br><span class="line">      <span class="attr">taskRef:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">build-push</span></span><br><span class="line">      <span class="attr">params:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pathToDockerFile</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">Dockerfile</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pathToContext</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;$(params.context)&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PipelineRun</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pipelinerun-with-parameters</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">pipelineRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">pipeline-with-parameters</span></span><br><span class="line">  <span class="comment"># 根據實際狀況，調整 parameter 預設值</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;context&quot;</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;/workspace/examples/microservices/leeroy-web&quot;</span></span><br></pre></td></tr></table></figure><h2 id="from-runAfter-retries-conditions-關鍵字的使用"><a href="#from-runAfter-retries-conditions-關鍵字的使用" class="headerlink" title="from / runAfter / retries / conditions  關鍵字的使用"></a>from / runAfter / retries / conditions  關鍵字的使用</h2><h3 id="from"><a href="#from" class="headerlink" title="from"></a>from</h3><p>有時候我們需要在不同的 task 之間傳遞資訊，這時候就必須使用 <code>from</code>，以下是一個範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 執行順序：&quot;build-app&quot; -&gt; &quot;deploy-app&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build-app</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">build-push</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">outputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">image</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-image</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">deploy-app</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">deploy-kubectl</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">inputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">image</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-image</span></span><br><span class="line">        <span class="comment"># from 的設定通常會指到上一個 Task</span></span><br><span class="line">        <span class="comment"># 而被指到的 Task 會有一個 output 來對應這個 input</span></span><br><span class="line">        <span class="attr">from:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">build-app</span></span><br></pre></td></tr></table></figure><blockquote><p>由於在 <code>deploy-app</code> 有設定 <strong>from</strong> 指到 <code>build-app</code>，因此無論實際上設定檔如何定義順序，執行時都會先以 <code>build-app -&gt; deploy-app</code> 的順序來執行</p></blockquote><h3 id="runAfter"><a href="#runAfter" class="headerlink" title="runAfter"></a>runAfter</h3><p>有時候 Task 之間沒有上到下的 input &amp; output 關係，但又要明確控制執行順序時，可使用 <code>runAfter</code>關鍵字，以下是一個簡單範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 執行順序：&quot;test-app&quot; -&gt; &quot;build-app&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-app</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">make-test</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">inputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-repo</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build-app</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">kaniko-build</span></span><br><span class="line">  <span class="comment"># 確保此 task 會在 &quot;test-app&quot; 結束後再執行</span></span><br><span class="line">  <span class="attr">runAfter:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">test-app</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">inputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-repo</span></span><br></pre></td></tr></table></figure><blockquote><p>由於 runAfter 會明確定義 Task 執行順序，因此以上面的範例來看，即使 “build-app” 擺在 “test-app” 之前也不會改變執行順序</p></blockquote><h3 id="retries"><a href="#retries" class="headerlink" title="retries"></a>retries</h3><p>有時候可能會因為網路問題，或是其他因素造成 Task 無法正確執行，但可能過一陣子就會正常；在這樣的情況下，就會有 Task retry 的需求，以下是設定範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tasks:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build-the-image</span></span><br><span class="line">    <span class="comment"># 此 Task 最多會嘗試執行兩次，兩次都失敗就算失敗</span></span><br><span class="line">    <span class="attr">retries:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">taskRef:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">build-push</span></span><br></pre></td></tr></table></figure><h3 id="conditions"><a href="#conditions" class="headerlink" title="conditions"></a>conditions</h3><p>有時候某些 Task 只有在某些條件滿足的情況下才需要執行，這時候就需要 <a href="https://github.com/tektoncd/pipeline/blob/master/docs/conditions.md">conditions</a> 的設定。</p><p>基本上，conditions 也是一個 CRD，以下是一個 condition 的定義範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Condition</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">file-exists</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;path&quot;</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">string</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="comment"># condition 中要進行的檢查</span></span><br><span class="line">  <span class="attr">check:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">alpine</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;test -f $(resources.workspace.path)/$(params.path)&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pipeline</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">list-files-pipeline</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">source-repo</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;path&quot;</span></span><br><span class="line">      <span class="attr">default:</span> <span class="string">&quot;README.md&quot;</span></span><br><span class="line">  <span class="attr">tasks:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">list-files-1</span></span><br><span class="line">      <span class="attr">taskRef:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">list-files</span></span><br><span class="line">      <span class="attr">conditions:</span></span><br><span class="line">        <span class="comment"># 要滿足以下條件才會執行此 Task</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">conditionRef:</span> <span class="string">&quot;file-exists&quot;</span></span><br><span class="line">          <span class="attr">params:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;path&quot;</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">&quot;$(params.path)&quot;</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">              <span class="attr">resource:</span> <span class="string">source-repo</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">inputs:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">            <span class="attr">resource:</span> <span class="string">source-repo</span></span><br></pre></td></tr></table></figure><h2 id="Ordering"><a href="#Ordering" class="headerlink" title="Ordering"></a>Ordering</h2><p>目前可用來控制 Task 運作順序的，就是 <code>from</code> &amp; <code>runAfter</code> 兩種設定：</p><ul><li><p><code>from</code> 用來指定從上一個 Task output resource 作為目前 Task 的 input resource</p></li><li><p><code>runAfter</code> 用來指定目前的 Task 在某個 Task 執行完成後才開始</p></li></ul><p>透過以上兩種設定，可以將 Pipeline 設計成 DAG(Directed Acyclic Graph) 的流程，以下是官網範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 與 test-app 同時執行</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lint-repo</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">pylint</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">inputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-repo</span></span><br><span class="line"><span class="comment"># 與 lint-repo 同時執行</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-app</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">make-test</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">inputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-repo</span></span><br><span class="line"><span class="comment"># 在 test-app 完成後開始執行</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build-app</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">kaniko-build-app</span></span><br><span class="line">  <span class="attr">runAfter:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">test-app</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">inputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-repo</span></span><br><span class="line">    <span class="attr">outputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">image</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-app-image</span></span><br><span class="line"><span class="comment"># 在 test-app 完成後開始執行</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build-frontend</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">kaniko-build-frontend</span></span><br><span class="line">  <span class="attr">runAfter:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">test-app</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">inputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workspace</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-repo</span></span><br><span class="line">    <span class="attr">outputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">image</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-frontend-image</span></span><br><span class="line"><span class="comment"># 當 build-app &amp; build-frontend 都完成後才會執行</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">deploy-all</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">deploy-kubectl</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">inputs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-app-image</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-app-image</span></span><br><span class="line">        <span class="attr">from:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">build-app</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-frontend-image</span></span><br><span class="line">        <span class="attr">resource:</span> <span class="string">my-frontend-image</span></span><br><span class="line">        <span class="attr">from:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">build-frontend</span></span><br></pre></td></tr></table></figure><p>上面的 pipeline 設定會變成以下流程：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">        |            |</span><br><span class="line">        v            v</span><br><span class="line">     test-app    lint-repo</span><br><span class="line">    /        \</span><br><span class="line">   v          v</span><br><span class="line">build-app  build-frontend</span><br><span class="line">   \          /</span><br><span class="line">    v        v</span><br><span class="line">    deploy-all</span><br></pre></td></tr></table></figure><p>而上面的 Pipeline 設定，要 <code>lint-repo</code> &amp; <code>deploy-all</code> 兩個都執行完後，才算全部完成</p><h1 id="PipelineRuns"><a href="#PipelineRuns" class="headerlink" title="PipelineRuns"></a>PipelineRuns</h1><p>在 Pipeline 中定義了需要執行哪些 Task &amp; 執行順序，但是要讓工作實際上執行起來，就必須要建立 PipelineRun。</p><h2 id="Syntax-3"><a href="#Syntax-3" class="headerlink" title="Syntax"></a>Syntax</h2><p>Pipeline 的定義中會有以下必要資訊：</p><ul><li><p><code>apiVersion</code>：目前固定是 <code>tekton.dev/v1alpha1</code></p></li><li><p><code>kind</code>：因為這是 CRD，所以是 <code>PipelineRun</code></p></li><li><p><code>metadata</code>：用來辨識此 TaskRun 用的資訊，例如 <strong>name</strong></p></li><li><p><code>sepc</code>：這裡就會透過 <code>pipelineRef</code> 來指定已經定義好的 Pipeline 了 (也可以使用 <code>pipelineSpec</code> 並包含 Pipeline 的設定，但不建議，因為這樣做就無法 reuse)</p></li></ul><p>其他選填項目：</p><ul><li><p><code>resources</code>：設定在 PipelineRun 中會使用到的 PipelineResource</p></li><li><p><code>serviceAccount</code>: 讓使用者可以使用自訂的認證資訊來執行工作</p></li><li><p><code>serviceAccounts</code>：可以定義多組 PipelineTask + ServiceAccount 的組合，來設定每個 Task 使用不同的 service account 來執行</p></li><li><p><code>tasks.conditions</code>：用來設定在特定的情況下 Task 才需要執行，詳細內容可參考<a href="https://github.com/tektoncd/pipeline/blob/master/docs/conditions.md">官方文件</a></p></li></ul><p>以下是一個 PipelineRun 的實際範例：(完整程式碼可以參考<a href="https://github.com/tektoncd/pipeline/blob/master/examples/pipelineruns/pipelinerun.yaml">官網範例</a>)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PipelineRun</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-pipeline-run-1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 指定 pipeline</span></span><br><span class="line">  <span class="attr">pipelineRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo-pipeline</span></span><br><span class="line">  <span class="comment"># 指定執行工作時用的 service account</span></span><br><span class="line">  <span class="attr">serviceAccount:</span> <span class="string">&#x27;default&#x27;</span></span><br><span class="line">  <span class="comment"># 這段設定是額外加的，並不在原本的 source code 內</span></span><br><span class="line">  <span class="comment"># 用來表示 Task &quot;build-push&quot; 要用 service account &quot;sa-for-build&quot; 的身份執行</span></span><br><span class="line">  <span class="attr">serviceAccounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">taskName:</span> <span class="string">build-push</span></span><br><span class="line">      <span class="attr">serviceAccount:</span> <span class="string">sa-for-build</span></span><br><span class="line">  <span class="comment"># 指定 pipeline resources</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">source-repo</span></span><br><span class="line">    <span class="attr">resourceRef:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">skaffold-git</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web-image</span></span><br><span class="line">    <span class="attr">resourceRef:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">skaffold-image-leeroy-web</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">app-image</span></span><br><span class="line">    <span class="attr">resourceRef:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">skaffold-image-leeroy-app</span></span><br></pre></td></tr></table></figure><h1 id="結語"><a href="#結語" class="headerlink" title="結語"></a>結語</h1><p>以上內容(PipelineResource, Task, TaskRun, Pipeline, PipelineRun) 是 Tekton 中執行工作的必要元素，實際上執行的 CI/CD 工作都會與這幾個部份有關。</p><p>Tekton 將所有的基本元素拆分成一個一個的 k8s CRD(Custom Resource Definition)，如果是稍微複雜一點的 CI/CD 工作，可能就會需要定義不少個 CRD 才能完成，而且在設計上相對於其他的 CI server(例如：GitLab CI, Drone CI)可能不是這麼直覺；但這樣的設計提供了以下優點：</p><ol><li><p>原生的 k8s 使用經驗，不需要額外學習其他語法</p></li><li><p>定義好的 CRD(PipelineResource, Task, Pipeline) 可以被重複利用</p></li><li><p>原生整合 k8s</p></li></ol><p>若是未來有考慮 workload 都跑在 k8s 上的使用者，在選擇 CI/CD 的工具時或許可以將 Tekton 考慮進行。</p><p>接著可能會面臨到的問題可能是，如果希望作到 GitOps，光是以上項目好像不夠，因為要執行工作，目前看起來似乎都是需要手動的；因此接下來要處理的問題就是，如何在使用者進行了某些行為時，自動觸發產生 PipelineRun 來執行 CI/CD 工作，這還需要額外加上一些 event trigger 的設定才有辦法完成，而這個部份將會由 <a href="https://github.com/tektoncd/triggers">Tekton Trigger</a> 來進行，未來有空的話會繼續探討這個部份。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://github.com/tektoncd/pipeline/tree/master/examples">pipeline/examples at master · tektoncd/pipeline</a></p></li><li><p><a href="https://github.com/tektoncd/pipeline/blob/master/docs/resources.md">pipeline/resources.md at master · tektoncd/pipeline</a></p></li><li><p><a href="https://github.com/tektoncd/pipeline/blob/master/docs/tasks.md">pipeline/tasks.md at master · tektoncd/pipeline</a></p></li><li><p><a href="https://github.com/tektoncd/pipeline/blob/master/docs/tasks.md#examples">Tekton - Task Examples</a></p></li><li><p><a href="https://github.com/tektoncd/pipeline/blob/master/docs/taskruns.md">pipeline/taskruns.md at master · tektoncd/pipeline</a></p></li><li><p><a href="https://github.com/tektoncd/pipeline/tree/master/examples/taskruns">Tekton - TaskRun Examples</a></p></li><li><p><a href="https://github.com/tektoncd/pipeline/blob/master/docs/pipelines.md">pipeline/pipelines.md at master · tektoncd/pipeline</a></p></li></ul><h2 id="Trigger"><a href="#Trigger" class="headerlink" title="Trigger"></a>Trigger</h2><ul><li><a href="https://github.com/tektoncd/triggers">tektoncd/triggers: Event triggering with Tekton!</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Tekton </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Tekton] 使用 Tekton + Kanibo 在 Kubernetes 上產生 container image</title>
      <link href="/blog/DevOps/tekton-kaniko-build-image/"/>
      <url>/blog/DevOps/tekton-kaniko-build-image/</url>
      
        <content type="html"><![CDATA[<h1 id="Ovewview"><a href="#Ovewview" class="headerlink" title="Ovewview"></a>Ovewview</h1><p>Tekton 是個 k8s 原生框架，用來建置 CI/CD 系統，以下是官方說明：</p><blockquote><p>Tekton is a powerful yet flexible Kubernetes-native open-source framework for creating continuous integration and delivery (CI/CD) systems. It lets you build, test, and deploy across multiple cloud providers or on-premises systems by abstracting away the underlying implementation details.</p></blockquote><p>相關的中文介紹可以在網路上找到很多，這邊就不寫了，有興趣的人可以參考以下文章：</p><ul><li><p><a href="https://www.infoq.cn/article/tZ6E1_lhsWeh26C9xUJf">谷歌开源Tekton：Kubernetes原生的CI/CD构建框架-InfoQ</a></p></li><li><p><a href="https://juejin.im/post/5d629c1a5188254628236b69">kubernetes原生CI/CD工具：Tekton探秘与上手实践 - 掘金</a></p></li></ul><p>這邊主要是因為未來要將 workload 往 k8s 上面移動，想說找看看跟 k8s 整合度比較好的新工具來用，就找到了 Tekton，而這工具也是 <a href="https://www.openshift.com/">Red Hat OpenShift</a> 中標配的 CI/CD 工具。</p><h1 id="此篇文章要介紹什麼"><a href="#此篇文章要介紹什麼" class="headerlink" title="此篇文章要介紹什麼?"></a>此篇文章要介紹什麼?</h1><p>這篇文章主要是介紹如何在 k8s 中，透過 <a href="https://github.com/tektoncd/pipeline">Tekton</a> + <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> 來 build container image 後，上傳到 Docker Hub 上。</p><p>原本要 build container image 需要有 root 或是跟 Docker daemon process 直接溝通的能力才有辦法，這樣的作法多少會有一些安全上的疑慮，因此 <code>Kaniko</code> 就是個可以讓使用者在 k8s 中，不需要什麼特別權限也可以 build container image 的工具。</p><p>有了以上需求後，就會衍生出一些問題：</p><ul><li><p>Kaniko 用來 build container image 的流程為何? 需要提供什麼資訊?</p></li><li><p>如何給入 Docker Hub credential</p></li></ul><h1 id="實作過程"><a href="#實作過程" class="headerlink" title="實作過程"></a>實作過程</h1><p>假設 k8s &amp; <a href="https://github.com/tektoncd/pipeline/blob/master/docs/install.md">Tekton Pipelines</a> &amp; <a href="https://github.com/tektoncd/dashboard">Tekton dashboard</a> 都已經已經佈署完成，繼續完成下面的步驟：</p><h2 id="設定-Docker-Hub-credential"><a href="#設定-Docker-Hub-credential" class="headerlink" title="設定 Docker Hub credential"></a>設定 Docker Hub credential</h2><p>首先在本地端先用 <code>docker login</code> 在本地端成功登入 Docker Hub 後，會在家目錄中產生一個 docker config，完整路徑為 <code>~/.docker/config.json</code>，內容大概如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;auths&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;https://index.docker.io/v1&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;auth&quot;</span>: <span class="string">&quot;Z29...........................YzY=&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此時可以用以下指令將 docker config 設定為 k8s secret：</p><blockquote><p>kubectl create secret generic docker-basic –from-file=.dockerconfigjson=~/.docker/config.json –type=kubernetes.io/dockerconfigjson</p></blockquote><h2 id="佈署-Tekton-相關資源"><a href="#佈署-Tekton-相關資源" class="headerlink" title="佈署 Tekton 相關資源"></a>佈署 Tekton 相關資源</h2><p>只要佈署以下的 YAML 就可以完成一個 build container image &amp; push image to Docker Hub 的示範了：</p><p>以下是 ServiceAccount &amp; Resource 的部份：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 需要一個 ServiceAccount 並帶上上面新增的 secret</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">robot-docker-basic</span></span><br><span class="line"><span class="attr">secrets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-basic</span></span><br><span class="line"><span class="attr">imagePullSecrets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-basic</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 設定 Tekton CRD &quot;PipelineResource&quot;</span></span><br><span class="line"><span class="comment"># 並指定其為特定的 Git repository</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PipelineResource</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">git-tekton-test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">revision</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">master</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">url</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">https://github.com/GoogleContainerTools/skaffold</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 設定 Tekton CRD &quot;PipelineResource&quot;</span></span><br><span class="line"><span class="comment"># 指定要使用的 Docker Hub repository</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PipelineResource</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">image-tekton-test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">image</span></span><br><span class="line">  <span class="attr">params:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">url</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">godleon/tekton-test</span></span><br></pre></td></tr></table></figure><p>以下是 Task &amp; TaskRun 的 YAML 的定義：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 設定 Tekton CRD &quot;Task&quot;</span></span><br><span class="line"><span class="comment"># 定義 Task 的詳細步驟(step)，並包含 input, output &amp; 相關參數</span></span><br><span class="line"><span class="comment"># 主要是定義 Kaniko build container image 的相關資訊</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Task</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">build-docker-image-from-git-source</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-source</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pathToDockerFile</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">string</span></span><br><span class="line">        <span class="attr">description:</span> <span class="string">The</span> <span class="string">path</span> <span class="string">to</span> <span class="string">the</span> <span class="string">dockerfile</span> <span class="string">to</span> <span class="string">build</span></span><br><span class="line">        <span class="attr">default:</span> <span class="string">/workspace/docker-source/Dockerfile</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pathToContext</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">string</span></span><br><span class="line">        <span class="attr">description:</span></span><br><span class="line">          <span class="string">The</span> <span class="string">build</span> <span class="string">context</span> <span class="string">used</span> <span class="string">by</span> <span class="string">Kaniko</span></span><br><span class="line">          <span class="string">(https://github.com/GoogleContainerTools/kaniko#kaniko-build-contexts)</span></span><br><span class="line">        <span class="comment"># Kaniko executor image 會將 source code 預設放到 /workspace 目錄中</span></span><br><span class="line">        <span class="attr">default:</span> <span class="string">/workspace/docker-source</span></span><br><span class="line">  <span class="attr">outputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">builtImage</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">image</span></span><br><span class="line">  <span class="attr">steps:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">build-and-push</span></span><br><span class="line">      <span class="comment"># 需要使用 Kaniko executor image</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">gcr.io/kaniko-project/executor</span></span><br><span class="line">      <span class="comment"># 指定 Docker Hub credential 路徑</span></span><br><span class="line">      <span class="comment"># 若有指定帶有 Docker config secret 的 ServiceAccount</span></span><br><span class="line">      <span class="comment"># kaniko executor 會將 Docker config 放到 /builder/home/.docker/ 目錄下</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&quot;DOCKER_CONFIG&quot;</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;/builder/home/.docker/&quot;</span></span><br><span class="line">      <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/kaniko/executor</span></span><br><span class="line">      <span class="comment"># 使用 Kaniko 需要以下三個參數，實際值會從 TaskRun 傳過來</span></span><br><span class="line">      <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--dockerfile=$(inputs.params.pathToDockerFile)</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--destination=$(outputs.resources.builtImage.url)</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--context=$(inputs.params.pathToContext)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># 設定 Tekton CRD &quot;TaskRun&quot;</span></span><br><span class="line"><span class="comment"># 這是以上面的 Task 為 template，傳入參數進行工作</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">tekton.dev/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">TaskRun</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">build-docker-image-from-git-source-task-run</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">serviceAccount:</span> <span class="string">robot-docker-basic</span></span><br><span class="line">  <span class="attr">taskRef:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">build-docker-image-from-git-source</span></span><br><span class="line">  <span class="attr">inputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment"># 指到前面定義好的 Git repository</span></span><br><span class="line">      <span class="comment"># 其中的 name 會變成 /workspace 下的 source code root folder 名稱</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">docker-source</span></span><br><span class="line">        <span class="attr">resourceRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">git-tekton-test</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="comment"># 對應到上面 Task 中定義的參數</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pathToDockerFile</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">Dockerfile</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pathToContext</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">/workspace/docker-source/examples/microservices/leeroy-web</span></span><br><span class="line">  <span class="attr">outputs:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment"># 指到前面定義好的 docker image resource</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">builtImage</span></span><br><span class="line">        <span class="attr">resourceRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">image-tekton-test</span></span><br></pre></td></tr></table></figure><p>將全部的 YAML 都套用後，就可以在 dashboard 上面看到運作的訊息：</p><p><img src="/blog/images/devops/tekton-dashboard.png" alt="Tekton dashboard"></p><p>在 k8s 中 build 好的 container image 也都會送進 Docker Hub 囉!</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://github.com/tektoncd/pipeline">Tekton</a></p></li><li><p><a href="https://github.com/tektoncd/dashboard">Tekton Dashboard</a></p></li><li><p><a href="https://sbr.pm/technical/tekton-usage.html">Tektoncd usage and examples</a></p></li><li><p><a href="https://support.telefonicaopencloud.com/en-us/api/cce/en-us_topic_0035621944.html">Credential 問題的處理 - Creating a Secret</a></p></li><li><p><a href="https://www.ithome.com.tw/news/122484">不需Root權限， Google釋出容器映像檔建立工具Kaniko | iThome</a></p></li><li><p><a href="https://github.com/tektoncd/pipeline/issues/1005">Unable to use docker secrets to push image to dockerhub · Issue #1005 · tektoncd/pipeline</a></p></li><li><p><a href="https://github.com/tektoncd/pipeline/blob/master/docs/auth.md">Tekton pipeline - Authentication</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> DevOps </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Tekton </tag>
            
            <tag> Kanibo </tag>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Prometheus] 學習筆記 - Monitoring Overview</title>
      <link href="/blog/Prometheus/Prometheus-LearningNotes-Monitoring/"/>
      <url>/blog/Prometheus/Prometheus-LearningNotes-Monitoring/</url>
      
        <content type="html"><![CDATA[<h1 id="Monitoring-System-Overview"><a href="#Monitoring-System-Overview" class="headerlink" title="Monitoring System Overview"></a>Monitoring System Overview</h1><ul><li><p>當企業 IT 從傳統的 bare metal(實體機)，轉移到 virtual machine(虛擬機) 甚至是 container(容器) 的過程中，對 monitoring 的需求只會越來越高</p></li><li><p>監控的對象也比起原本多很多，例如：container, distributed system/storage, SDN network, 各種 application，規劃時都需要考量進來</p></li><li><p>相對應 monitoring 所需要的儲存空間也相對龐大，同時也衍生了可以對這些龐大資料進行更進一步的分析、告警、預警…等工作</p></li><li><p>monitoring system 同時也可以作為輔助決策的重要角色，因為不僅可以提供 real-time 的狀態呈現，同時也可以顯示歷史資料，藉以協助進行故障回溯 &amp; 風險預測…等工作</p></li><li><p>從程式開發的角度，可以將監控分為 <code>基礎資源(Infrastructure)</code>、<code>中間層(Middleware)</code>、<code>應用程式監控(Applications)</code> 三層，每一層都有不同的監控指標</p></li></ul><p><img src="/blog/images/prometheus-learning-notes/monitoring-overview-1.png" alt="Monitoring System Overview"></p><ul><li>針對需求決定要收集的監控目標，不需要全部都收</li></ul><h1 id="Infrastructure-Monitoring"><a href="#Infrastructure-Monitoring" class="headerlink" title="Infrastructure Monitoring"></a>Infrastructure Monitoring</h1><p>這部份包含了 <code>Networking</code>、<code>Storage</code>、<code>Computing</code> 三種主要資源的監控</p><h2 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h2><p>這部份包含了以下的監控項目：</p><ol><li><p>Datacenter 內網路流量的監控</p></li><li><p>Networking Topology &amp; Devices 的監控</p></li><li><p>Network Performance 的監控</p></li><li><p>網路攻擊的探測</p></li></ol><ul><li><p>Datacenter 中的網路設備監控，一般都是透過 port mirror 的方式來達成；若是 virtual device 則是可以透過 tcpdump 或是其他工具來完成</p></li><li><p>透過匯總網路封包的 source IP &amp; destination IP 可以用來產生 networking topology</p></li><li><p>網路設備的監控通常會使用 SNMP v3 協定</p></li></ul><h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><ul><li><p>目前 storage 主要分為 cloud storage &amp; distributed storage 兩種</p></li><li><p>cloud storage 是透過儲存設備建立 storage pool，並提供其他的機器統一的 storage interface，例如：iSCSI, NFS</p></li><li><p>distributed storag 則是與機器上的 OS 共存，藉以提供分佈式集群的儲存服務，例如：HDFS</p></li></ul><p>cloud storage 的監控可分為以下三種：</p><ul><li><p><strong>storage performance</strong>：block storage 會著重在 IOPS, Read/Write Latency, Disk Usage；object storage 則是著重在 Inode, Read/Write Speed, Folder permission … 等</p></li><li><p><strong>storage system</strong>：不同的系統有不同的指標，例如 Ceph 則需要監控 OSD, MON, PG … 等資訊</p></li><li><p><strong>storage device</strong>：這部份則是以監控 x86 server 上的儲存相關資源為主</p></li></ul><h2 id="Computing"><a href="#Computing" class="headerlink" title="Computing"></a>Computing</h2><ul><li><p>包含 bare metal, VM, container 的監控</p></li><li><p>可兼容來自於不同廠商的 server</p></li><li><p>可兼容不同的作業系統 (e.g., Windows, Linux … etc)</p></li><li><p>可兼容不同的虛擬化技術 (e.g., VMware, Xen, KVM … etc)</p></li><li><p>監控數據可透過在每個 client 上放 agent 或是從外部透過虛擬化環境的 API 來取得</p></li><li><p>OS 層級通常會監控 CPU、Memory、Network I/O、Disk I/O … 等資訊</p></li><li><p>bare metal server 的監控可以透過 IPMI</p></li></ul><h1 id="Middleware-Monitoring"><a href="#Middleware-Monitoring" class="headerlink" title="Middleware Monitoring"></a>Middleware Monitoring</h1><ul><li><p>常見的 middleware 可分成幾類：<strong>Message Queue</strong>(RabbitMQ, Kafka)，<strong>Web Service</strong>(Tomcat, Jetty), <strong>Buffer</strong>(Redis, Memcached), <strong>Database</strong>(MySQL, PostgreSQL)</p></li><li><p>很多 application 的性能都會直接與 middleware 有關係</p></li><li><p>middleware 的監控可用在 application 的優化，也可以用來在高並發 or 大流量環境時的提供正確的 feedback</p></li><li><p>middleware 的監控會根據不同的軟體有不同的特性資訊需要蒐集，因此沒有統一的標準；比較標準的作法就是分別針對不同的軟體開發不同的 agent，再統一收到監控中心(例如：Prometheus &amp; 眾多的 exporter)</p></li></ul><h1 id="Application-Performance-Monitoring-APM"><a href="#Application-Performance-Monitoring-APM" class="headerlink" title="Application Performance Monitoring (APM)"></a>Application Performance Monitoring (APM)</h1><ul><li><p>APM 主要是針對 application level 的監控</p></li><li><p>監控的項目包含了 application 運行狀態、效能、log、調用鏈(call-chain)跟蹤</p></li><li><p>透過 <code>調用鏈(call-chain)跟蹤</code> 可以建構出完整程式運行的 topology，還可以取得每個元件的執行時間，作為調整效能的重要依據</p></li></ul><p><img src="/blog/images/prometheus-learning-notes/pinpoint-example.png" alt="pinpoint(APM) example"></p><ul><li><p>APM 除了可攔截 method call 的資訊外，還可以攔截 TCP &amp; HTTP request，進而取得花最多時間的 method &amp; SQL command，或是延遲最久的 API … 等訊息</p></li><li><p><strong>商業化的 SaaS APM 甚至可以提供從不同地點發出的模擬請求</strong></p></li><li><p>由於 application 的監控系統也需要關心 host 的狀態，因此會跟 host monitoring 會有重疊</p></li></ul><h1 id="Log-Monitoring"><a href="#Log-Monitoring" class="headerlink" title="Log Monitoring"></a>Log Monitoring</h1><ul><li><p>Log monitoring system 蒐集的主要是文字型的資訊，因此儲存的部份就需要具備全文檢索的能力</p></li><li><p>可用來定位出效能問題，也可以用來做數據統計 &amp; 故障告警等功能</p></li><li><p>目前比較流行的工具組合是：<a href="https://www.fluentd.org/"><code>fluentd</code></a> -&gt; <a href="https://kafka.apache.org/"><code>Kafka</code></a> -&gt; <a href="https://www.elastic.co/products/logstash"><code>Logstash</code></a> -&gt; <a href="https://www.elastic.co/products/elasticsearch"><code>Elasticsearch</code></a> -&gt; <a href="https://www.elastic.co/products/kibana"><code>Kibana</code></a></p></li></ul><h1 id="監控系統的實現"><a href="#監控系統的實現" class="headerlink" title="監控系統的實現"></a>監控系統的實現</h1><ul><li>監控系統通常分為 <strong>數據蒐集</strong>(收集、過濾、匯總、儲存) &amp; <strong>數據處理</strong>(分析、顯示、預警、告警、告警動作觸發) 兩大部份</li></ul><h2 id="數據蒐集"><a href="#數據蒐集" class="headerlink" title="數據蒐集"></a>數據蒐集</h2><ul><li><p>數據蒐集有兩種方式：透過 client 端程式 or 通過標準的 protocol(例如：SNMP、IPMI、JMX … 等等)</p></li><li><p>數據傳輸可以透過 HTTP/Socket 進行傳輸，也可以利用 RabbitMQ/Kafka 等工具作為 buffer 協助傳輸</p></li><li><p>利用 message queue 可以將整體架構進行 decouple，但系統就會與相關工具具有強大的依賴性，且會有單點故障的疑慮</p></li><li><p>儲存數據之前，還需要對資料進行<strong>去重複</strong>(重複上報數據) &amp; <strong>過濾</strong>(並非所有資料都需要)的步驟</p></li><li><p>監控數據的儲存通常使用 TSDB(Time-Series Database)</p></li><li><p>TSDB 具有以下特點：</p><ul><li>寫入一次，多次讀取，通常不會有修改 or 更新的動作</li><li>數據流量相對穩定，不會有突然性爆炸流量的發生</li><li>查詢方式通常是以最近的為主，很少需要查詢到太久之前的資訊(例如：一週以前)</li><li>通常以 LSM tree 來設計(跟 DB 的 B+ tree 是完全不同的)，資料壓縮比高</li><li>需要大容量的資料儲存能力</li></ul></li></ul><h2 id="數據處理"><a href="#數據處理" class="headerlink" title="數據處理"></a>數據處理</h2><ul><li><p>為了提昇查詢歷史資料的速度，必須對資料進行彙整</p></li><li><p>目前比較常用的視覺化查詢工具為 Grafana &amp; Kibana</p></li><li><p>數據分析:指的是對監控數據進行 <code>效能分析</code>、<code>關聯分析</code>、<code>趨勢分析</code></p></li><li><p>效能分析:可按照資源的消耗類型來分類，可能是 high CPU usage 或是 high DISK I/O，不將消耗相同類型資源的 workload 放在一起</p></li><li><p>關聯分析：可透過 source/destination IP 繪製出 network topology；或是在 APM 監控中透過 TraceId 的關聯取得 call chain 的資訊，藉此分析出不同 component 的效能並優化系統</p></li><li><p>趨勢分析：將數據結合各種數學方法，分析數據的週期性，並預測出未來的變化趨勢，並用於資源調度 &amp; 預分配資源上</p></li><li><p>規則告警則是根據監控數據，定義多為度的告警規則</p></li><li><p>規則告警的例子像是：一段時間內觸發告警特定次數就停止、設計告警抑制，假設由 root issue 觸發很多 sub issue 的告警，只要針對 root issue 發告警即可</p></li><li><p>告警動作可以是送訊息 or EMail 通知，也可以是發送 webhook</p></li></ul><h1 id="監控系統的發展趨勢"><a href="#監控系統的發展趨勢" class="headerlink" title="監控系統的發展趨勢"></a>監控系統的發展趨勢</h1><ul><li><p>分散式系統越來越普及，單一指標無法很精確的反應實際系統運行 &amp; 程序當下的狀況</p></li><li><p>傳統監控提供效能、健康狀態、Application … 等監控能力</p></li><li><p>監控系統現在開始朝著自動化 &amp; 智慧化的方向發展，開始會具備機器學習 &amp; 自動化的維運能力</p></li><li><p>智慧監控需要有以下幾種能力：</p><ul><li>根據多種維度的指標分析定位實際問題</li><li>故障 &amp; 風險預測</li><li>智慧處理：告警發生後的問題處理，由智慧系統判斷後自行決定處理方法</li></ul></li></ul><h1 id="Prometheus"><a href="#Prometheus" class="headerlink" title="Prometheus"></a>Prometheus</h1><p>網路上有很多介紹了，簡單介紹可以參考以下文章：</p><ul><li><p><a href="https://www.inwinstack.com/2018/11/14/prometheus-introduction-1/">Prometheus 介紹與基礎入門 (上)</a></p></li><li><p><a href="https://www.inwinstack.com/2018/11/14/prometheus-introduction-2/">Prometheus 介紹與基礎入門 (下) - inwinSTACK</a></p></li></ul><p>以下是其他重點資訊節錄：</p><ul><li><p>Prometheus 以 pull 的方式取得 metric，提供 metric 的 client 不需要知道 Prometheus 在那，因此可完全獨立於監控系統之外</p></li><li><p>一般監控系統以 push 的方式蒐集資料，相對不彈性，也可能因為 push 失敗導致監控系統癱瘓</p></li><li><p>支援靜態設定，也可搭配 ZooKeeper, Consul, Kubernetes … 等方式進行動態的 service discovery</p></li><li><p>本地儲存建議採用 SSD</p></li><li><p>若需要以 push 模式取得資料，可搭配 <a href="https://prometheus.io/docs/practices/pushing/">Pushgateawy</a> 來處理</p></li><li><p>AlertManager 獨立於 Prometheus 之外，當 metric 有觸發了預設在 Prometheus 中的規則時，就會將訊息送到 AlertManager 做後續處理</p></li><li><p>AlertManager 支援多種通知方式，同時也支援 HA 並有 Gossip 的機制在(預防重送)</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://github.com/naver/pinpoint">naver/pinpoint: APM, (Application Performance Management) tool for large-scale distributed systems written in Java.</a></p></li><li><p><a href="https://github.com/apache/skywalking">apache/skywalking: APM, Application Performance Monitoring System</a></p></li><li><p><a href="https://zipkin.io/">OpenZipkin · A distributed tracing system</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Prometheus </tag>
            
            <tag> Telemetry </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 Kubespray 安裝 Kubernetes v1.10 以上版本</title>
      <link href="/blog/Kubernetes/Install-k8s-via-kubespray/"/>
      <url>/blog/Kubernetes/Install-k8s-via-kubespray/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-05-21-Update"><a href="#2019-05-21-Update" class="headerlink" title="2019-05-21 Update"></a>2019-05-21 Update</h1><p>目前已經支援到 <code>v1.15.1</code>，並預設安裝 multus CNI plugin，並以 calico 作為預設使用的 CNI plugin</p><h1 id="Current-Status"><a href="#Current-Status" class="headerlink" title="Current Status"></a>Current Status</h1><p>目前支援到 <code>v1.15.1</code>，如果有需要調整，可以按照下面的說明進行修改。</p><h1 id="準備安裝環境"><a href="#準備安裝環境" class="headerlink" title="準備安裝環境"></a>準備安裝環境</h1><h2 id="安裝-k8s-cluster-用機器"><a href="#安裝-k8s-cluster-用機器" class="headerlink" title="安裝 k8s cluster 用機器"></a>安裝 k8s cluster 用機器</h2><p>在這個安裝過程中，共有 6 個 VM，皆為透過 <code>Ubuntu 18.04 cloud image</code> 所產生，分別有以下幾個 node：</p><ul><li><p>Master Node x 3 (<strong>同時兼任 etcd node</strong>)</p></li><li><p>Worker Node x 3</p></li></ul><blockquote><p>需要確保以上的 VM 可以透過 ssh key 或是帳號密碼登入</p></blockquote><h2 id="準備用來執行安裝指令的機器-bootstrapper"><a href="#準備用來執行安裝指令的機器-bootstrapper" class="headerlink" title="準備用來執行安裝指令的機器 (bootstrapper)"></a>準備用來執行安裝指令的機器 (bootstrapper)</h2><p>這台機器是用來作為執行安裝程序之用，需求如下：</p><ul><li>OS: <strong>Ubuntu 16.04(Trusty)</strong> or <strong>Ubuntu 18.04(Bionic)</strong></li></ul><h1 id="取得安裝程式碼"><a href="#取得安裝程式碼" class="headerlink" title="取得安裝程式碼"></a>取得安裝程式碼</h1><p>首先登入到 bootstrapper，輸入以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update &amp;&amp; sudo apt-get -y install git</span><br><span class="line">$ <span class="built_in">cd</span> /tmp</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/godleon/kubernetes-installation-template.git</span><br></pre></td></tr></table></figure><p>上面的程式其實只是個 wrapper，目的是協助使用者快速產生安裝 Kubespray 用的環境。</p><h1 id="修改安裝環境設定"><a href="#修改安裝環境設定" class="headerlink" title="修改安裝環境設定"></a>修改安裝環境設定</h1><p>接著進入 <strong>/tmp/kubernetes-installation-template</strong> 目錄中</p><blockquote><p>cd /tmp/kubernetes-installation-template</p></blockquote><p>接著根據自己環境的情況，進行後續的修改 &amp; 調整。</p><h2 id="準備存取機器的-SSH-Key"><a href="#準備存取機器的-SSH-Key" class="headerlink" title="準備存取機器的 SSH Key"></a>準備存取機器的 SSH Key</h2><p>將存取機器的 SSH Key(檔名為 <strong>id_rsa</strong>) 放到 <strong>$(pwd)/ssh-privkey</strong> 目錄中</p><blockquote><p>若用帳號 + 密碼存取可忽略此步驟</p></blockquote><h2 id="pwd-kubespray-hosts-ini"><a href="#pwd-kubespray-hosts-ini" class="headerlink" title="$(pwd)/kubespray/hosts.ini"></a>$(pwd)/kubespray/hosts.ini</h2><p>這個檔案定義了要用來安裝 k8s cluster 的機器有哪些，修改時有以下幾點需要注意：</p><ul><li><p>務必輸入機器正確的 hostname</p></li><li><p>下方每個機器的 role(<code>kube-master</code>/<code>etcd</code>/<code>kube-node</code>) 要定義清楚</p></li><li><p>一般來說，master 跟 etcd 會安裝在一起 (大規模佈署才需要分開)</p></li><li><p>Ingress 可以暫時先忽略</p></li></ul><p>以下是根據上面的安裝環境所撰寫的設定範例：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">kube-master0    ansible_ssh_host=[YOUR_SERVER_IP]</span><br><span class="line">kube-master1    ansible_ssh_host=[YOUR_SERVER_IP]</span><br><span class="line">kube-master2    ansible_ssh_host=[YOUR_SERVER_IP]</span><br><span class="line"></span><br><span class="line">kube-worker0    ansible_ssh_host=[YOUR_SERVER_IP]</span><br><span class="line">kube-worker1    ansible_ssh_host=[YOUR_SERVER_IP]</span><br><span class="line">kube-worker2    ansible_ssh_host=[YOUR_SERVER_IP]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="section">[kube-master]</span></span><br><span class="line">kube-master[0:2]</span><br><span class="line"></span><br><span class="line"><span class="section">[etcd]</span></span><br><span class="line">kube-master[0:2]</span><br><span class="line"></span><br><span class="line"><span class="section">[kube-node]</span></span><br><span class="line">kube-worker[0:2]</span><br><span class="line"></span><br><span class="line"><span class="section">[k8s-cluster:children]</span></span><br><span class="line">kube-master</span><br><span class="line">kube-node</span><br></pre></td></tr></table></figure><h2 id="pwd-kubespray-group-vars-all-yml"><a href="#pwd-kubespray-group-vars-all-yml" class="headerlink" title="$(pwd)/kubespray/group_vars/all.yml"></a>$(pwd)/kubespray/group_vars/all.yml</h2><ul><li><p>確認遠端安裝機器的 python 路徑，並修改 <strong>ansible_python_interpreter</strong> 參數(預設是 <code>/usr/bin/python3</code>)</p></li><li><p>確認遠端機器的 OS，並修改 <strong>bootstrap_os</strong> 參數(預設是 <code>ubuntu</code>)</p></li></ul><p>最後要設定安裝用 VM 的存取方式，分為以下兩種狀況：</p><h3 id="使用-SSH-Key-存取"><a href="#使用-SSH-Key-存取" class="headerlink" title="使用 SSH Key 存取"></a>使用 SSH Key 存取</h3><ul><li>確認 SSH Key 為 <strong>$(pwd)/ssh-privkey/id_rsa</strong></li></ul><blockquote><p>這裡需要把存取每個 node 的 SSH private key 放到 <code>$(pwd)/ssh-privkey</code> 目錄中，並命名為 <code>id_rsa</code></p></blockquote><h2 id="使用帳號-密碼存取"><a href="#使用帳號-密碼存取" class="headerlink" title="使用帳號 + 密碼存取"></a>使用帳號 + 密碼存取</h2><ul><li><p>將 <strong>ansible_ssh_pass</strong> 參數的註解取消，並輸入存取密碼</p></li><li><p><strong>ansible_user</strong> 參數請設定為登入帳號</p></li><li><p>移除(or 註解) <strong>ansible_ssh_private_key_file</strong> 參數</p></li></ul><h2 id="pwd-kubespray-group-vars-k8s-cluster-yml"><a href="#pwd-kubespray-group-vars-k8s-cluster-yml" class="headerlink" title="$(pwd)/kubespray/group_vars/k8s-cluster.yml"></a>$(pwd)/kubespray/group_vars/k8s-cluster.yml</h2><p>目前比較重要的安裝預設值如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自訂設定</span></span><br><span class="line"><span class="attr">kubeconfig_localhost:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 若是要啟動特定的 feature gate 可以加入以下設定</span></span><br><span class="line"><span class="attr">kube_feature_gates:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;TTLAfterFinished=true&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#  啟用特定的 admission controller</span></span><br><span class="line"><span class="attr">kube_apiserver_enable_admission_plugins:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">NodeRestriction</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">AlwaysPullImages</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">DefaultStorageClass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># k8s 版本</span></span><br><span class="line"><span class="attr">kube_version:</span> <span class="string">v1.14.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CNI plugin</span></span><br><span class="line"><span class="attr">kube_network_plugin:</span> <span class="string">calico</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 預設安裝 multus CNI plugin，並使用上面定義的 calico 作為預設的 CNI</span></span><br><span class="line"><span class="attr">kube_network_plugin_multus:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># k8s cluster 內部使用的 DNS server</span></span><br><span class="line"><span class="attr">dns_mode:</span> <span class="string">coredns</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Container runtime (也可以是 crio for cri-o)</span></span><br><span class="line"><span class="comment"># 但 cri-o 目前無法使用在 Ubuntu 上，只能用在 CentOS</span></span><br><span class="line"><span class="attr">container_manager:</span> <span class="string">docker</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#... (略)</span></span><br></pre></td></tr></table></figure><p>Kubespray 預設會安裝最新版的 docker，但 k8s 官方建議的穩定版本是 17.03，因此若有調整的需求，可以增加以下設定：</p><blockquote><p>docker_version: “17.03”</p></blockquote><p>其他的部份，使用者可以根據自己佈署的需求調整所相關的參數。</p><h1 id="執行安裝程式"><a href="#執行安裝程式" class="headerlink" title="執行安裝程式"></a>執行安裝程式</h1><p>當以上設定都完成後，就可以開始執行安裝程式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若按照上面的操作，沒有離開原本的目錄的話，應該就是位於下面的目錄中</span></span><br><span class="line">$ <span class="built_in">cd</span> /tmp/kubernetes-installation-template</span><br><span class="line"></span><br><span class="line"><span class="comment"># 開始安裝</span></span><br><span class="line">$ ./start.sh</span><br></pre></td></tr></table></figure><p>整個安裝過程大約需要耗費十來分鐘(端看網路 &amp; VM 運行速度)。</p><p>安裝完之後會額外將 kuebconfig 放到 master node 的使用者家目錄中，因此在 master node 中就可以直接執行 kubectl 的指令對 Kubernetes 進行操作，</p><blockquote><p>若安裝過程失敗，可嘗試再執行一次 <code>./start.sh</code>，應該就會安裝成功了!</p></blockquote><h1 id="操作-Kubernetes"><a href="#操作-Kubernetes" class="headerlink" title="操作 Kubernetes"></a>操作 Kubernetes</h1><p>要操作 k8s 可以直接連到 master node 上直接使用 kubectl 進行操作，也可以使用其他機器進行操作。</p><p>但若要使用非 master node 的機器對 k8s 進行操作，必須先準備好以下環境：</p><ol><li><p>安裝 <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a></p></li><li><p>準備好 kubeconfig (以上例來說，位於 <code>/tmp/kubernetes-installation-template/kubeconfig/admin.conf</code>)，檔案路徑為 <code>~/.kube/config</code></p></li></ol><p>完成後執行簡單的指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME            STATUS    ROLES          AGE       VERSION</span><br><span class="line">kube-master0    Ready     master         1h        v1.11.2</span><br><span class="line">kube-master1    Ready     master         1h        v1.11.2</span><br><span class="line">kube-master2    Ready     master         1h        v1.11.2</span><br><span class="line">kube-worker0    Ready     node           1h        v1.11.2</span><br><span class="line">kube-worker1    Ready     node           1h        v1.11.2</span><br><span class="line">kube-worker2    Ready     node           1h        v1.11.2</span><br></pre></td></tr></table></figure><p>這樣大致上基本的 Kubernetes 就安裝完成了!</p><blockquote><p>若是要取得安裝時所產生的相關憑證檔案，可以到 master node 上的 <code>/etc/kubernetes/ssl</code> 目錄中尋找</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://github.com/kubernetes-incubator/kubespray">kubernetes-incubator/kubespray: Setup a kubernetes cluster</a></p></li><li><p><a href="https://killmebaby.cc/posts/kubernetes-deployment-with-kubespray/">How to deploy Kubernetes with Kubespray - KillMeBaby</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubespray </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Prometheus] Service Discovery &amp; Relabel </title>
      <link href="/blog/Prometheus/Prometheus-Relabel/"/>
      <url>/blog/Prometheus/Prometheus-Relabel/</url>
      
        <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>Relabel 是在 Prometheus 中一個強大的功能，善用 relabel 可以讓資料真正進入到資料庫之前，根據需求完成一些前置處理，了解 relabel 功能怎麼用，在使用 Prometheus 上就可以有許多變化。</p><p>若是使用到 service discovery 的功能就會發現，動態的 instance 要用 static configuration 做到想要的設定是很困難的，而藉由 relabel 的功能就可以根據預先定義好的規則，將資料進行處理。</p><p>以下以 Kubernetes 為例(使用 <code>kubernetes_sd_configs</code>)，做一些 relabel 的示範 &amp; 說明，而在 <strong>kubernetes_sd_configs</strong>，共有以下五種 role 的資料可以動態擷取，分別是：</p><ul><li><p>node</p></li><li><p>service</p></li><li><p>pod</p></li><li><p>endpoints</p></li><li><p>ingress</p></li></ul><h1 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h1><p>一開始設定使用 <code>kubernetes_sd_configs</code> 取得 <code>node</code> 的資訊，設定如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span> <span class="string">(略)</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">kubernetes-nodes-kubelet</span></span><br><span class="line">  <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">node</span></span><br><span class="line">  <span class="attr">scheme:</span> <span class="string">https</span></span><br><span class="line">  <span class="attr">tls_config:</span></span><br><span class="line">    <span class="attr">ca_file:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</span></span><br><span class="line">    <span class="attr">insecure_skip_verify:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">bearer_token_file:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount/token</span></span><br></pre></td></tr></table></figure><p>套用到 Prometheus 之後，發現抓取到的資料似乎不太像是我們想要的，以下的圖是 relable 之前的樣子：</p><p><img src="/blog/images/prometheus/prometheus-k8s-nodes-before-relabel.png" alt="Prometheus - k8s before relabel"></p><p>接著我們要做的是，將原本以 external IP 擷取 node status 的方式，改成透過 internal k8s API server 來取得資訊，而從 k8s cluster 內部要取得 node metric 的方式，是透過以下的網址：</p><blockquote><p><a href="https://kubernetes.default.svc/api/v1/nodes/%60[NODE_NAME]%60/proxy/metrics/cadvisor">https://kubernetes.default.svc:443/api/v1/nodes/`[NODE_NAME]`/proxy/metrics/cadvisor</a></p></blockquote><p>但目前的 endpoint 是 <code>https://10.103.9.x:10250/metrics</code>，因此以下要來進行一些 relabel 的動作：</p><h2 id="修改-Endpoint-IP"><a href="#修改-Endpoint-IP" class="headerlink" title="修改 Endpoint IP"></a>修改 Endpoint IP</h2><p>首先要將 Endpoint 中的 IP 改成 <code>kubernetes.default.svc:443</code>，可以透過以下設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="comment"># 要修改 endpoint 中的 `__address__` value，並非修改 Label 本身</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">kubernetes.default.svc:443</span></span><br></pre></td></tr></table></figure><p>首先必須了解 Endpoint 的值是由 <code>__scheme__</code> + <code>__address__</code> + <code>__metrics_path__</code> 所組成，而上面的設定就是要將 <code>__address__</code> 的部份換成 <code>kubernetes.default.svc:443</code>，因此以上的 relabel 設定就會變成以下結果：</p><p><img src="/blog/images/prometheus/prometheus-k8s-nodes-after-relabel-1.png" alt="Prometheus - k8s after relabel"></p><p>但很明顯，這還沒到結束的程度，Endpoint 的部份還需要更多的加工。</p><blockquote><p><code>/metrics</code> 是預設抓取 metric 的路徑，可以透過 <code>scrape_configs[].metrics_path</code> 調整</p></blockquote><h2 id="修改-metric-path"><a href="#修改-metric-path" class="headerlink" title="修改 metric path"></a>修改 metric path</h2><p>接著要將 metric path 從 <code>/metrics</code> 變成 <code>/api/v1/nodes/[NODE_NAME]/proxy/metrics/cadvisor</code>，但麻煩的事情是，中間 <code>NODE_NAME</code> 的部份會隨著每個 node 而改變，因此這是動態的，那要怎麼解決?</p><p>答案是從現有的 Label metata 中找尋是否有可用的資訊，透過第一張圖可以發現，我們可以用 <code>__meta_kubernetes_node_name</code> 這個 label 取得每個 node 所使用的 node name。</p><p>有了以上的資訊後，可以使用以下設定來修改 metric path：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">    <span class="comment"># 使用 Label &quot;__meta_kubernetes_node_name&quot; 的值</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_node_name</span>]</span><br><span class="line">    <span class="comment"># 使用所有的值，不經過額外處理</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">(.+)</span></span><br><span class="line">    <span class="comment"># 取代 endpoint 中 &quot;__metrics_path__&quot; 的值</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">__metrics_path__</span></span><br><span class="line">    <span class="comment"># $&#123;1&#125; 的部份會由 &quot;__meta_kubernetes_node_name&quot; 的值取代</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">/api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor</span></span><br></pre></td></tr></table></figure><p>透過以上的 relabel 設定後，結果會變成如下：</p><p><img src="/blog/images/prometheus/prometheus-k8s-nodes-after-relabel-2.png" alt="Prometheus - k8s after relabel"></p><h2 id="為資料標記更多-Label"><a href="#為資料標記更多-Label" class="headerlink" title="為資料標記更多 Label"></a>為資料標記更多 Label</h2><p>從上圖中可以看到，在 Label 的部份目前只有一個 instance，因此之後在做資料分析 or 處理時，僅有 instance 可以作為依據來處理，這樣看起來似乎不足；此外 <code>kubernetes_sd_configs</code> 提供了許多 metadata 資訊可供使用，而且很多是富有意義的，例如：</p><ul><li><p><code>__meta_kubernetes_node_label_beta_kubernetes_io_os</code></p></li><li><p><code>__meta_kubernetes_node_label_beta_kubernetes_io_arch</code></p></li><li><p><code>__meta_kubernetes_node_label_kubernetes_io_hostname</code></p></li></ul><p>有了上面的 metadata 資訊，我們就可以額外對資料根據 OS, architecture, hostname 進行分類，為了達到此目的，我們可以透過 <code>labelmap</code> 的方式，為每一筆 time series data 加入新的 label 資訊：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">__meta_kubernetes_node_label_(.+)</span></span><br></pre></td></tr></table></figure><p>以上的設定會將 <code>__meta_kubernetes_node_label_</code> 開頭的 metadata，拆開後加入 Label 中，並會在之後的每一筆擷取到的資料中加上這些 label 的資訊，方便使用者進行分類查詢 or 使用。</p><p>以下是設定 labelmap 後的結果：</p><p><img src="/blog/images/prometheus/prometheus-k8s-nodes-after-relabel-3.png" alt="Prometheus - k8s after relabel"></p><h2 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h2><p>經過上面 3 個 relabel 設定後，我們就可以在 k8s cluster 內部的 prometheus server，透過 k8s API server 來取得每個 node 的狀態，以下是完整 configmap 設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;prometheus-config&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">prometheus.yml:</span> <span class="string">|-</span></span><br><span class="line">    <span class="attr">global:</span></span><br><span class="line">      <span class="attr">scrape_interval:</span> <span class="string">15s</span></span><br><span class="line">      <span class="attr">external_labels:</span></span><br><span class="line">        <span class="attr">monitor:</span> <span class="string">&#x27;codelab-monitor&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">scrape_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">kubernetes-nodes-kubelet</span></span><br><span class="line">      <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">node</span></span><br><span class="line">      <span class="attr">scheme:</span> <span class="string">https</span></span><br><span class="line">      <span class="attr">tls_config:</span></span><br><span class="line">        <span class="attr">ca_file:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</span></span><br><span class="line">        <span class="attr">insecure_skip_verify:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">bearer_token_file:</span> <span class="string">/var/run/secrets/kubernetes.io/serviceaccount/token</span></span><br><span class="line">      </span><br><span class="line">      <span class="attr">relabel_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">        <span class="attr">replacement:</span> <span class="string">kubernetes.default.svc:443</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_node_name</span>]</span><br><span class="line">        <span class="attr">regex:</span> <span class="string">(.+)</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__metrics_path__</span></span><br><span class="line">        <span class="attr">replacement:</span> <span class="string">/api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">__meta_kubernetes_node_label_(.+)</span></span><br></pre></td></tr></table></figure><h1 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h1><p>一開始先開啟 pod scrape 的功能：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">kubernetes-pods</span></span><br><span class="line">  <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">pod</span></span><br></pre></td></tr></table></figure><p>以下的圖是尚未做任何 relabel 設定的結果：</p><p><img src="/blog/images/prometheus/prometheus-k8s-pods-before-relabel.png" alt="Prometheus - k8s pod before relabel"></p><p><img src="/blog/images/prometheus/prometheus-k8s-pods-before-relabel-2.png" alt="Prometheus - k8s pod before relabel"></p><h2 id="只留下需要監控的-Pod"><a href="#只留下需要監控的-Pod" class="headerlink" title="只留下需要監控的 Pod"></a>只留下需要監控的 Pod</h2><p>在 k8s 中有一些 pod 其實不是我們想要關心的，真的想要關心的 pod 會在 YAML configuration 中設定 <code>metadata.annotations.prometheus.io/scrape</code> 設定為 <code>true</code>，prometheus 在擷取 pod 相關資訊時，就會取得 Label <code>__meta_kubernetes_pod_annotation_prometheus_io_scrape</code> 的值為 <strong>true</strong>，因此透過以下設定只取得我們想要留下的 pod 資訊：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_annotation_prometheus_io_scrape</span>]</span><br><span class="line">    <span class="attr">action:</span> <span class="string">&quot;keep&quot;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">&quot;true&quot;</span></span><br></pre></td></tr></table></figure><p>上面的設定會檢查 Label <code>__meta_kubernetes_pod_annotation_prometheus_io_scrape</code> 的值，只有是 <code>true</code> 的 target 才會被保留下來，結果很多 pod 就被過濾(drop)掉了：</p><p><img src="/blog/images/prometheus/prometheus-k8s-pods-after-relabel-1.png" alt="Prometheus - k8s pods after relabel"></p><p>接著還要再移除掉 <code>calico-node</code>，為什麼呢? 因為 calico 有專門口以用來監控的工具 <a href="https://docs.projectcalico.org/v3.5/reference/felix/prometheus">felix</a>，所以這邊就先拿 calico 的部份：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_container_name</span>]</span><br><span class="line">    <span class="attr">action:</span> <span class="string">&quot;drop&quot;</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">&quot;calico-node&quot;</span></span><br></pre></td></tr></table></figure><p>上面的設定會檢查 Label <code>__meta_kubernetes_pod_container_name</code> 的值，如果是 <code>calico-node</code>，就會被過濾掉：</p><p><img src="/blog/images/prometheus/prometheus-k8s-pods-after-relabel-2.png" alt="Prometheus - k8s pods after relabel"></p><h2 id="修改監控用的-port-amp-metrics-path"><a href="#修改監控用的-port-amp-metrics-path" class="headerlink" title="修改監控用的 port &amp; metrics path"></a>修改監控用的 port &amp; metrics path</h2><p>在預設的情況下，prometheus 擷取資料的 endpoint 為 <code>__scheme__</code> + <code>__address__</code> + <code>__metrics_path__</code>，但如果實際要擷取資料的位置不是這 3 個變數的組合呢? 就可以用 relabel 來調整。</p><p>透過 YAML 佈署 k8s pod 時，可以額外加上以下兩種資訊：</p><ul><li><p><code>spec.template.metadata.annotations.prometheus.io/port</code></p></li><li><p><code>spec.template.metadata.annotations.prometheus.io/path</code></p></li></ul><p>就會在 Target 中多出以下 Label 可以用：</p><ul><li><p><code>__meta_kubernetes_pod_annotation_prometheus_io_port</code></p></li><li><p><code>__meta_kubernetes_pod_annotation_prometheus_io_path</code></p></li></ul><p>利用以上的 Label 就可以用來調整實際擷取 metrics 資訊的位置，以下是調整範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__address__</span>, <span class="string">__meta_kubernetes_pod_annotation_prometheus_io_port</span>]</span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">([^:]+)(?::\d+)?;(\d+)</span></span><br><span class="line">    <span class="attr">replacement:</span> <span class="string">$1:$2</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_annotation_prometheus_io_path</span>]</span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">__metrics_path__</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">&quot;(.+)&quot;</span></span><br></pre></td></tr></table></figure><h2 id="為資料標記更多-Label-1"><a href="#為資料標記更多-Label-1" class="headerlink" title="為資料標記更多 Label"></a>為資料標記更多 Label</h2><h3 id="增加-pod-Label-資訊"><a href="#增加-pod-Label-資訊" class="headerlink" title="增加 pod Label 資訊"></a>增加 pod Label 資訊</h3><p>在佈署 pod 的過程中，我們通常會透過 Label(<code>spec.template.metadata.labels</code>) 的方式為 pod 加上一些 metadata，例如：<code>app: prometheus</code>, <code>app: node-exporter</code>，而這樣的資料，在 prometheus 中會自動生成相對應的 metadata Label 可用，以上面的例子來說：</p><ul><li><p><code>app: prometheus</code> -&gt; <code>__meta_kubernetes_pod_label_app=&quot;prometheus&quot;</code></p></li><li><p><code>app: node-exporter</code> -&gt; <code>__meta_kubernetes_pod_label_app=&quot;node-exporter&quot;</code></p></li></ul><p>而我們希望可以在每一筆 time series data 中可以加上這樣的 metadata，因此就可以透過 relabel 的方式來處理：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">    <span class="attr">regex:</span> <span class="string">__meta_kubernetes_pod_label_(.+)</span></span><br></pre></td></tr></table></figure><p>結果變成如下：</p><p><img src="/blog/images/prometheus/prometheus-k8s-pods-after-relabel-3.png" alt="Prometheus - k8s pods after relabel"></p><h3 id="增加-namespace-資訊"><a href="#增加-namespace-資訊" class="headerlink" title="增加 namespace 資訊"></a>增加 namespace 資訊</h3><p>此外，由於 k8s cluster 中資源的 isolation 是用 namespace 實現的，因此我們希望每筆 time series data 可以同時帶著 namespace 的資訊，這時可以從 <code>__meta_kubernetes_namespace</code> 來著手，利用以下設定來完成：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_namespace</span>]</span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">kubernetes_namespace</span></span><br></pre></td></tr></table></figure><p>結果就多了 <code>kubernetes_namespace</code> label：</p><p><img src="/blog/images/prometheus/prometheus-k8s-pods-after-relabel-4.png" alt="Prometheus - k8s pods after relabel"></p><h3 id="增加-pod-name-資訊"><a href="#增加-pod-name-資訊" class="headerlink" title="增加 pod name 資訊"></a>增加 pod name 資訊</h3><p>同上，我們希望可以增加 pod name 資訊(資料來源為 <code>__meta_kubernetes_pod_name</code>)，可用下面的設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_name</span>]</span><br><span class="line">    <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">    <span class="attr">target_label:</span> <span class="string">kubernetes_pod_name</span></span><br></pre></td></tr></table></figure><p>結果就多了 <code>kubernetes_pod_name</code> label：</p><p><img src="/blog/images/prometheus/prometheus-k8s-pods-after-relabel-5.png" alt="Prometheus - k8s pods after relabel"></p><h2 id="結論-1"><a href="#結論-1" class="headerlink" title="結論"></a>結論</h2><p>經過上面的 relabel 設定後，我們就可以在 k8s cluster 內部的 prometheus server，取得我們所需要監控的 pod 資訊，以下是完整 configmap 設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;prometheus-config&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">prometheus.yml:</span> <span class="string">|-</span></span><br><span class="line">    <span class="attr">global:</span></span><br><span class="line">      <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">      <span class="attr">external_labels:</span></span><br><span class="line">        <span class="attr">monitor:</span> <span class="string">&#x27;codelab-monitor&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">scrape_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">kubernetes-pods</span></span><br><span class="line">      <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">pod</span></span><br><span class="line">      <span class="attr">relabel_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_annotation_prometheus_io_scrape</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">&quot;keep&quot;</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_container_name</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">&quot;drop&quot;</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">&quot;calico-node&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__address__</span>, <span class="string">__meta_kubernetes_pod_annotation_prometheus_io_port</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">([^:]+)(?::\d+)?;(\d+)</span></span><br><span class="line">        <span class="attr">replacement:</span> <span class="string">$1:$2</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_annotation_prometheus_io_path</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__metrics_path__</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">&quot;(.+)&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">__meta_kubernetes_pod_label_(.+)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_namespace</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">kubernetes_namespace</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_pod_name</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">kubernetes_pod_name</span></span><br></pre></td></tr></table></figure><h1 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h1><p>在 kubernetes service 的部份，我們希望可以透過 probe 的方式，檢查 service 是否會回應，首先先開啟 k8s 的 service scrape 的功能：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;kubernetes-services&#x27;</span></span><br><span class="line">    <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">service</span></span><br></pre></td></tr></table></figure><p>以下是佈署後的結果結果：</p><p><img src="/blog/images/prometheus/prometheus-k8s-services-before-relabel.png" alt="Prometheus - k8s service before relabel"></p><h2 id="化繁為簡"><a href="#化繁為簡" class="headerlink" title="化繁為簡"></a>化繁為簡</h2><p>此時假如需要監控的 service 有上百個呢? 同時對上百個 service probe 對 prometheus 來說也是挺辛苦的事情，此時就可以在 prometheus 與眾多 service 之間放一個 aggregator 的角色，這裡我們選用的是 <a href="https://github.com/prometheus/blackbox_exporter">Blackbox Prober Exporter</a>。</p><blockquote><p>Blackbox Exporter 允許使用者透過 HTTP、HTTPS、DNS、TCP &amp; ICMP …. 等協定來進行 probe</p></blockquote><p>在 k8s 中套用以下設定以安裝 blackbox exporter：(使用的 namespace 為 <code>kube-system</code>)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">blackbox-exporter</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">blackbox-exporter</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">blackbox</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">9115</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">blackbox-exporter</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">blackbox-exporter</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">blackbox-exporter</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">blackbox-exporter</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">blackbox-exporter</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">prom/blackbox-exporter</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">blackbox-exporter</span></span><br></pre></td></tr></table></figure><p>在設定 blackbox exporter 之前，我們要了解一下 blackbox exporter 的使用方式，首先要先說明架構，原本的架構如下：</p><blockquote><p><code>Prometheus</code> –&gt; <code>Service</code></p></blockquote><p>若是加入了 blackbox exporter 之後，會變成如下：</p><blockquote><p><code>Prometheus</code> –&gt; <code>Blackbox Exporter</code> –&gt; <code>Service</code></p></blockquote><p>而正確的使用的方式呢? 要注意的地方有兩點：</p><ol><li><p>endpoint 要改成 <code>http://[Blackbox_URL]:9115/probe</code></p></li><li><p>要額外給入 target，告知 blackbox exporter 要探測的對象</p></li></ol><p>所以此時 prometheus 的設定會變成如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;kubernetes-services&#x27;</span></span><br><span class="line">    <span class="attr">metrics_path:</span> <span class="string">/probe</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="comment"># 指定 blackbox exporter 使用 http_2xx module</span></span><br><span class="line">      <span class="comment"># 同時也有 http_post_2xx, tcp_connect, ping ... 等 module 可用</span></span><br><span class="line">      <span class="attr">module:</span> [<span class="string">http_2xx</span>]</span><br><span class="line">    <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">service</span></span><br><span class="line">    <span class="attr">relabel_configs:</span></span><br><span class="line">    <span class="comment"># 將 service URL 的資訊新增到 target 中</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__address__</span>]</span><br><span class="line">      <span class="attr">target_label:</span> <span class="string">__param_target</span></span><br><span class="line">    <span class="comment"># 將 endpoint 全部改成 blackbox exporter URL</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">      <span class="attr">replacement:</span> <span class="string">blackbox-exporter:9115</span></span><br></pre></td></tr></table></figure><p>以下是 relabel 後的結果：</p><p><img src="/blog/images/prometheus/prometheus-k8s-services-after-relabel-1.png" alt="Prometheus - k8s services after relabel"></p><h2 id="修正錯誤的-instance"><a href="#修正錯誤的-instance" class="headerlink" title="修正錯誤的 instance"></a>修正錯誤的 instance</h2><p>從上圖可以看出，雖然我們已經將擷取資料的對象從多個 service 改成單一的 blackbox exporter，但同時也把 <code>instance</code> 變成了 blackbox exporter，如此一來 time series data 中就無法判斷資料是來自於哪一個 service，因此可以透過以下設定來修正這個問題：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line"><span class="comment"># 將 instace 的值修改成 target 的值</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__param_target</span>]</span><br><span class="line">  <span class="attr">target_label:</span> <span class="string">instance</span></span><br></pre></td></tr></table></figure><p>以下是 relabel 後的結果：</p><p><img src="/blog/images/prometheus/prometheus-k8s-services-after-relabel-2.png" alt="Prometheus - k8s services after relabel"></p><h2 id="為資料標記更多-Label-2"><a href="#為資料標記更多-Label-2" class="headerlink" title="為資料標記更多 Label"></a>為資料標記更多 Label</h2><p>但僅有 instance label 是不夠的，我們還可以用以下的設定加入更多的 label，以便於後續的資料處理：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">  <span class="attr">regex:</span> <span class="string">__meta_kubernetes_service_label_(.+)</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_namespace</span>]</span><br><span class="line">  <span class="attr">target_label:</span> <span class="string">kubernetes_namespace</span></span><br><span class="line"></span><br><span class="line"><span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_name</span>]</span><br><span class="line">  <span class="attr">target_label:</span> <span class="string">kubernetes_service_name</span></span><br></pre></td></tr></table></figure><p>以下是 relabel 後的結果：</p><p><img src="/blog/images/prometheus/prometheus-k8s-services-after-relabel-3.png" alt="Prometheus - k8s services after relabel"></p><h2 id="僅留下想要監控的-service"><a href="#僅留下想要監控的-service" class="headerlink" title="僅留下想要監控的 service"></a>僅留下想要監控的 service</h2><p>如果不是每個 service 都想要監控呢? 那可以在需要監控的 service YAML 定義加上 <code>metadata.annotations.prometheus.io/probe: &#39;true&#39;</code>，並將以下的 relabel 設定在第一條規則：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">relabel_configs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_annotation_prometheus_io_probe</span>]</span><br><span class="line">  <span class="attr">action:</span> <span class="string">keep</span></span><br><span class="line">  <span class="attr">regex:</span> <span class="string">&#x27;true&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="結論-2"><a href="#結論-2" class="headerlink" title="結論"></a>結論</h2><p>經過上面的 relabel 設定後，我們就可以在 k8s cluster 內部的 prometheus server，取得我們所需要監控的 service 資訊，以下是完整 configmap 設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;prometheus-config&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">prometheus.yml:</span> <span class="string">|-</span></span><br><span class="line">    <span class="attr">global:</span></span><br><span class="line">      <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">      <span class="attr">external_labels:</span></span><br><span class="line">        <span class="attr">monitor:</span> <span class="string">&#x27;codelab-monitor&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">scrape_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;kubernetes-services&#x27;</span></span><br><span class="line">      <span class="attr">metrics_path:</span> <span class="string">/probe</span></span><br><span class="line">      <span class="attr">params:</span></span><br><span class="line">        <span class="attr">module:</span> [<span class="string">http_2xx</span>]</span><br><span class="line">      <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">service</span></span><br><span class="line">      <span class="attr">relabel_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_annotation_prometheus_io_probe</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">keep</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">&#x27;true&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__address__</span>]</span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__param_target</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">        <span class="attr">replacement:</span> <span class="string">blackbox-exporter.monitoring.svc.cluster.local:9115</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__param_target</span>]</span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">instance</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">__meta_kubernetes_service_label_(.+)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_namespace</span>]</span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">kubernetes_namespace</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_name</span>]</span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">kubernetes_service_name</span></span><br></pre></td></tr></table></figure><h1 id="Endpoint-amp-Ingress"><a href="#Endpoint-amp-Ingress" class="headerlink" title="Endpoint &amp; Ingress"></a>Endpoint &amp; Ingress</h1><p>Endpoint &amp; Ingress 在 relabel 部份的作法其實跟上面大同小異，甚至有很多重複的部份，以下僅列出範例的設定方式，使用者可以根據自己的監控需求來進行增減：</p><h2 id="Endpoint"><a href="#Endpoint" class="headerlink" title="Endpoint"></a>Endpoint</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;prometheus-config&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">prometheus.yml:</span> <span class="string">|-</span></span><br><span class="line">    <span class="attr">global:</span></span><br><span class="line">      <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">      <span class="attr">external_labels:</span></span><br><span class="line">        <span class="attr">monitor:</span> <span class="string">&#x27;codelab-monitor&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">scrape_configs:</span>      </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;kubernetes-endpoints&#x27;</span></span><br><span class="line">      <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">endpoints</span></span><br><span class="line">      <span class="attr">relabel_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_annotation_prometheus_io_scrape</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">keep</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="literal">true</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_annotation_prometheus_io_scheme</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__scheme__</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">(https?)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_annotation_prometheus_io_path</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__metrics_path__</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">(.+)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__address__</span>, <span class="string">__meta_kubernetes_service_annotation_prometheus_io_port</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">(.+)(?::\d+);(\d+)</span></span><br><span class="line">        <span class="attr">replacement:</span> <span class="string">$1:$2</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">__meta_kubernetes_service_label_(.+)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_namespace</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">kubernetes_namespace</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_service_name</span>]</span><br><span class="line">        <span class="attr">action:</span> <span class="string">replace</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">kubernetes_name</span></span><br></pre></td></tr></table></figure><h2 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h2><p>Ingress 同樣建議搭配 <a href="https://github.com/prometheus/blackbox_exporter">Blackbox Prober Exporter</a> 一起使用</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;prometheus-config&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">prometheus.yml:</span> <span class="string">|-</span></span><br><span class="line">    <span class="attr">global:</span></span><br><span class="line">      <span class="attr">scrape_interval:</span> <span class="string">5s</span></span><br><span class="line">      <span class="attr">external_labels:</span></span><br><span class="line">        <span class="attr">monitor:</span> <span class="string">&#x27;codelab-monitor&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">scrape_configs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">job_name:</span> <span class="string">&#x27;kubernetes-ingress&#x27;</span></span><br><span class="line">      <span class="attr">kubernetes_sd_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">role:</span> <span class="string">ingress</span></span><br><span class="line">      <span class="attr">relabel_configs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_ingress_scheme</span>,<span class="string">__address__</span>,<span class="string">__meta_kubernetes_ingress_path</span>]</span><br><span class="line">        <span class="attr">regex:</span> <span class="string">(.+);(.+);(.+)</span></span><br><span class="line">        <span class="attr">replacement:</span> <span class="string">$&#123;1&#125;://$&#123;2&#125;$&#123;3&#125;</span></span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">__param_target</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">target_label:</span> <span class="string">__address__</span></span><br><span class="line">        <span class="attr">replacement:</span> <span class="string">blackbox-exporter.example.com:9115</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__param_target</span>]</span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">instance</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">labelmap</span></span><br><span class="line">        <span class="attr">regex:</span> <span class="string">__meta_kubernetes_ingress_label_(.+)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_namespace</span>]</span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">kubernetes_namespace</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">source_labels:</span> [<span class="string">__meta_kubernetes_ingress_name</span>]</span><br><span class="line">        <span class="attr">target_label:</span> <span class="string">kubernetes_name</span></span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://github.com/prometheus/blackbox_exporter">prometheus/blackbox_exporter: Blackbox prober exporter</a></p></li><li><p><a href="https://yunlzheng.gitbook.io/prometheus-book/part-ii-prometheus-jin-jie/exporter/commonly-eporter-usage/install_blackbox_exporter">网络探测：Blackbox Exporter - prometheus-book</a></p></li><li><p><a href="https://blog.frognew.com/2018/02/prometheus-blackbox-exporter.html">使用Prometheus的blackbox_exporter进行网络监控 — 青蛙小白</a></p></li><li><p><a href="https://www.puritys.me/docs-blog/article-30-Javascript-Regular-Expressions-,-%E8%A1%A8%E7%A4%BA%E6%B3%95.html">Javascript Regular Expressions , 表示法</a></p></li><li><p><a href="https://regexr.com/">RegExr: Learn, Build, &amp; Test RegEx</a></p></li><li><p><a href="https://jishu.io/kubernetes/kubernetes-monitoring-with-prometheus/">使用Prometheus完成Kubernetes集群监控 · 极术</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Prometheus </tag>
            
            <tag> Telemetry </tag>
            
            <tag> Service Discovery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Terraform] 入門學習筆記</title>
      <link href="/blog/DevOps/terraform-getting-started/"/>
      <url>/blog/DevOps/terraform-getting-started/</url>
      
        <content type="html"><![CDATA[<h1 id="Installing-Terraform"><a href="#Installing-Terraform" class="headerlink" title="Installing Terraform"></a>Installing Terraform</h1><blockquote><p><a href="https://learn.hashicorp.com/terraform/getting-started/install">官網連結</a></p></blockquote><p>這邊沒什麼太特別，就下載 zip 檔之後，解壓縮到 <code>/usr/bin</code> 目錄中就可以用了。(我使用的是 Ubuntu 18.04 Desktop)</p><p>而放到其他路徑，並修改 <code>$PATH</code> 也是可以的。</p><h1 id="Build-Infrastructure"><a href="#Build-Infrastructure" class="headerlink" title="Build Infrastructure"></a>Build Infrastructure</h1><blockquote><p><a href="https://learn.hashicorp.com/terraform/getting-started/build">官網連結</a></p></blockquote><h2 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h2><p>以下給一個簡單範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">provider <span class="string">&quot;aws&quot;</span> &#123;</span><br><span class="line">  access_key = <span class="string">&quot;ACCESS_KEY_HERE&quot;</span></span><br><span class="line">  secret_key = <span class="string">&quot;SECRET_KEY_HERE&quot;</span></span><br><span class="line">  region     = <span class="string">&quot;ap-northeast-1&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># resource &lt;resource_type&gt; &lt;resource_name&gt;</span></span><br><span class="line">resource <span class="string">&quot;aws_instance&quot;</span> <span class="string">&quot;example&quot;</span> &#123;</span><br><span class="line">  ami             = <span class="string">&quot;ami-0ccdbc8c1cb7957be&quot;</span></span><br><span class="line">  instance_type   = <span class="string">&quot;t2.micro&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p><code>provider</code>：此區塊用來指定所要使用的 provider，上述的例子是 <strong>aws</strong>，而 <a href="https://www.terraform.io/docs/providers/index.html">terraform 支援的 provider 有相當多</a>，而 provider 琳琅滿目，不一定是 infra 類，也有很多是純軟體服務類</p></li><li><p><code>resource</code>：此區塊則是定義透過指定的 provider，所要建立的資源為何，而 resource 可以是一台 VM，也可以是個 app</p></li><li><p><code>&lt;resource_type&gt;</code>：每個 provider 底下有很多不同的 resource type 可以用，上面的範例是 aws_intance，而這個 resource type 也會主動告訴 terraform 所要使用的 provider 為 <strong>aws</strong></p></li><li><p><code>&lt;resource_name&gt;</code>：這就按照個人喜好 or 需求去定義</p></li></ul><h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>這個步驟是執行 <code>terraform init</code> 指令，而 terraform 會根據在當前的目錄下產生一些本地端設定，並根據上面的 configuration 下載相對應的 provider binary，並放到 <code>.terraform</code> 目錄中</p><h2 id="Apply-Changes"><a href="#Apply-Changes" class="headerlink" title="Apply Changes"></a>Apply Changes</h2><p>有了 provider binary 後，就可以執行 <code>terraform apply</code> 來實際產生相對應的資源了；而執行了 <code>terraform apply</code>，terraform 並不會馬上動作，而是會先列出一串執行列表，告訴使用者將會做哪些事情，並詳細列出建立該 resource 所會使用到的參數(資訊)；此外，若是看到 <code>+</code> 符號，表示此 resource 將會被建立。</p><p>terraform 執行完後，會在當前目錄產生 <code>terraform.tfstate</code>，此檔案相當重要，包含了透過 terraform 產生出來的 resource 的詳細資訊，而 terraform 就是依靠此檔案來進行 resource 的追蹤 &amp; 維護。</p><blockquote><p>因為 <strong>terraform.tfstate</strong> 包含了所有 resource 的狀態，因此執行 terraform 指令時，都必須要有這個檔案在，確保 terraform 可以正確的掌握 resource 的狀況，而此需求可以考慮使用 <a href="https://www.terraform.io/docs/state/remote.html">remote state</a> 來達成</p></blockquote><p>透過 <code>terraform show</code> 就可以檢視目前 resource 的詳細狀態</p><h2 id="Provisioning"><a href="#Provisioning" class="headerlink" title="Provisioning"></a>Provisioning</h2><p>這部份則是當 instance 被建立之後，所接著要進行的初始化 or 額外安裝的工作。</p><p>由於上面的範例是透過標準的 AMI 產生出來的 instance，因此還需要使用者額外安裝服務上去，若是透過自訂的 images 來產生的 instance，可以直接將 service 所需要的程式 &amp; library 通通包好，此時可以考慮搭配 <a href="https://www.packer.io/">Packer</a> 來協助建立 image。</p><h1 id="Change-Infrastructure"><a href="#Change-Infrastructure" class="headerlink" title="Change Infrastructure"></a>Change Infrastructure</h1><blockquote><p><a href="https://learn.hashicorp.com/terraform/getting-started/change">官網連結</a></p></blockquote><p>若有變更也相當容易，以上面的例子來說，假設換一個 AMI，就修改 AMI ID 之後，再執行一次 <code>terraform apply</code> 即可，而修改 AMI ID 的話，原本的 instance 會被移除，並使用新的 AMI 重新再建立一個新的 instance。</p><blockquote><p>同樣的，輸入 <code>terraform apply</code> 可以看到執行列表同時有 <code>+</code> &amp; <code>-</code>，表示原本的 instance 會被移除，並重新建立</p></blockquote><h1 id="Destroy-Infrastructure"><a href="#Destroy-Infrastructure" class="headerlink" title="Destroy Infrastructure"></a>Destroy Infrastructure</h1><blockquote><p><a href="https://learn.hashicorp.com/terraform/getting-started/destroy">官網連結</a></p></blockquote><p>這沒什麼訣竅…只要 configuration &amp; <code>terraform.tfstate</code> 資訊有正確，輸入 <code>terraform destroy</code> 即可</p><h1 id="Resource-Dependencies"><a href="#Resource-Dependencies" class="headerlink" title="Resource Dependencies"></a>Resource Dependencies</h1><p>若是要同時建利多個 resource 時，以下面的 configuration 為例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">provider <span class="string">&quot;aws&quot;</span> &#123;</span><br><span class="line">  access_key = <span class="string">&quot;ACCESS_KEY_HERE&quot;</span></span><br><span class="line">  secret_key = <span class="string">&quot;SECRET_KEY_HERE&quot;</span></span><br><span class="line">  region     = <span class="string">&quot;ap-northeast-1&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># resource &lt;resource_type&gt; &lt;resource_name&gt;</span></span><br><span class="line">resource <span class="string">&quot;aws_instance&quot;</span> <span class="string">&quot;example&quot;</span> &#123;</span><br><span class="line">  ami             = <span class="string">&quot;ami-0ccdbc8c1cb7957be&quot;</span></span><br><span class="line">  instance_type   = <span class="string">&quot;t2.micro&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource <span class="string">&quot;aws_eip&quot;</span> <span class="string">&quot;ip&quot;</span> &#123;</span><br><span class="line">  instance = aws_instance.example.id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由於我們指定 EIP 要綁定 instance，因此 terraform 會自動先建立 instance，取得 instance id 後，再建立 EIP。</p><p>透過 <code>aws_instance.example.id</code> 的設定，Terraform 可以自動判定 resource 之間的相依性，而使用者在撰寫 terraform configuration 時，則必須儘可能的將這一類的資訊給定清楚。</p><p>除了上面的範例可以明確讓 terraform 知道 resource dependency 之外，也可以透過 <code>depends_on</code> 關鍵字來強制 resource 按照特定的順序來產生，以下是個範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">provider <span class="string">&quot;aws&quot;</span> &#123;</span><br><span class="line">  access_key = <span class="string">&quot;ACCESS_KEY_HERE&quot;</span></span><br><span class="line">  secret_key = <span class="string">&quot;SECRET_KEY_HERE&quot;</span></span><br><span class="line">  region     = <span class="string">&quot;ap-northeast-1&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource <span class="string">&quot;aws_s3_bucket&quot;</span> <span class="string">&quot;example&quot;</span> &#123;</span><br><span class="line">    bucket = <span class="string">&quot;godleon-learn-terraform&quot;</span></span><br><span class="line">    acl    = <span class="string">&quot;private&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># resource &lt;resource_type&gt; &lt;resource_name&gt;</span></span><br><span class="line">resource <span class="string">&quot;aws_instance&quot;</span> <span class="string">&quot;example&quot;</span> &#123;</span><br><span class="line">    ami             = <span class="string">&quot;ami-0ccdbc8c1cb7957be&quot;</span></span><br><span class="line">    instance_type   = <span class="string">&quot;t2.micro&quot;</span></span><br><span class="line"></span><br><span class="line">    depends_on      = [ aws_s3_bucket.example ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource <span class="string">&quot;aws_eip&quot;</span> <span class="string">&quot;ip&quot;</span> &#123;</span><br><span class="line">  instance = aws_instance.example.id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>按照上面的 configuration，terraform 會按照以下順序建立 resource：</p><ol><li><p>aws_s3_bucket.example</p></li><li><p>aws_instance.example</p></li><li><p>aws_eip.ip</p></li></ol><h1 id="Provision"><a href="#Provision" class="headerlink" title="Provision"></a>Provision</h1><p>若是使用 image-based infrastructure，那就可以忽略 provision 這一段，而轉去看看 <a href="https://www.packer.io/">Packer</a> 是否可以提供更多的協助。</p><p>但如果需要進行一些初始化工作的話，provisioner 可以讓使用者上傳檔案、執行 shell script，或是呼叫其他 configuration management tool 等等。</p><p>以下是一個簡單的 local provisioner 範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">resource <span class="string">&quot;aws_instance&quot;</span> <span class="string">&quot;example&quot;</span> &#123;</span><br><span class="line">    ami             = <span class="string">&quot;ami-0eb48a19a8d81e20b&quot;</span></span><br><span class="line">    instance_type   = <span class="string">&quot;t2.micro&quot;</span></span><br><span class="line">    key_name        = <span class="string">&quot;godleon&quot;</span></span><br><span class="line"></span><br><span class="line">    depends_on = [ aws_s3_bucket.example ]</span><br><span class="line"></span><br><span class="line">    provisioner <span class="string">&quot;local-exec&quot;</span> &#123;</span><br><span class="line">        <span class="built_in">command</span> = <span class="string">&quot;echo <span class="variable">$&#123;aws_instance.example.public_ip&#125;</span> &gt; /tmp/ip_address.txt&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Input-Variables"><a href="#Input-Variables" class="headerlink" title="Input Variables"></a>Input Variables</h1><p>有些敏感 or 重要的資訊可能不適合放在 version control system 作管理，例如：密碼, access key … 等等，而 terraform 也提供了一些方法讓某些資訊可以透過 variable 的方式輸入。</p><h2 id="定義-variables"><a href="#定義-variables" class="headerlink" title="定義 variables"></a>定義 variables</h2><p>我們可以建立一個 <code>variables.tf</code> 的檔案(檔名無所謂，只要副檔名為 tf 即可，terraform 會把當前目錄的 *.tf 全部拉進來用)，內容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">variable <span class="string">&quot;access_key&quot;</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">variable <span class="string">&quot;secret_key&quot;</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以設定預設值</span></span><br><span class="line">variable <span class="string">&quot;region&quot;</span> &#123;</span><br><span class="line">  default = <span class="string">&quot;ap-northeast-1&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有了以上的設定後，在 resource configuration 可以改成如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">provider <span class="string">&quot;aws&quot;</span> &#123;</span><br><span class="line">  access_key    = var.access_key</span><br><span class="line">  secret_key    = var.secret_key</span><br><span class="line">  region        = var.region</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完成以上設定後，在執行 <code>terraform apply</code> 時，會以互動式的方式詢問變數值，但如果不想用互動式的方式指定變數，可以透過以下的語法來指定變數：</p><blockquote><p>terraform apply -var ‘access_key=<YOUR_ACCESS_KEY>‘ -var ‘secret_key=<YOUR_SECRET_KEY>‘</p></blockquote><h2 id="其他傳入-input-variable-的方式"><a href="#其他傳入-input-variable-的方式" class="headerlink" title="其他傳入 input variable 的方式"></a>其他傳入 input variable 的方式</h2><h3 id="使用-variable-file"><a href="#使用-variable-file" class="headerlink" title="使用 variable file"></a>使用 variable file</h3><p>若想要更偷懶讓 terraform 自動載入 variable file，可以在當前目錄建立一個名稱為 <code>terraform.tfvars</code>(或是 <code>*.auto.tfvars</code>) 的檔案，並設定內容如下：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">access_key</span> = <span class="string">&quot;&lt;YOUR_ACCESS_KEY&gt;&quot;</span></span><br><span class="line"><span class="attr">secret_key</span> = <span class="string">&quot;&lt;YOUR_SECRET_KEY&gt;&quot;</span></span><br></pre></td></tr></table></figure><p>若 variable file 想要用其他名稱，那就在執行 <code>terraform apply</code> 時使用 <code>-var-file</code> 參數指定 variable file</p><h3 id="使用環境變數-Environment-Variable"><a href="#使用環境變數-Environment-Variable" class="headerlink" title="使用環境變數(Environment Variable)"></a>使用環境變數(Environment Variable)</h3><p>也可以透過環境變數的方式傳入 input value，terraform 會自動以 <code>TF_VAR_name</code> 的型式讀取目前的環境變數，因此若要傳入 <strong>key1=value1</strong>，只要設定好 <code>TF_VAR_key1 = value1</code> 即可。</p><h2 id="複雜資料結構變數"><a href="#複雜資料結構變數" class="headerlink" title="複雜資料結構變數"></a>複雜資料結構變數</h2><p>terraform 還支援了較為複雜的資料結構變數，分別是 <strong>List</strong> &amp; <strong>Map</strong>，以下是使用範例：</p><h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 宣告 cidrs 變數為 list 格式</span></span><br><span class="line">variable <span class="string">&quot;cidrs&quot;</span> &#123; default = [] &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以明確的宣告其變數型態</span></span><br><span class="line">variable <span class="string">&quot;cidrs&quot;</span> &#123; <span class="built_in">type</span> = list &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 terraform.tfvars 中可用下面方式給入值</span></span><br><span class="line">cidrs = [ <span class="string">&quot;10.0.0.0/16&quot;</span>, <span class="string">&quot;10.1.0.0/16&quot;</span> ]</span><br></pre></td></tr></table></figure><h3 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h3><p>由於在不同的 region，同樣的 AMI 會有不同的 ID，因此可以用 Map 來解決：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 宣告 Map 型態的變數</span></span><br><span class="line">variable <span class="string">&quot;amis&quot;</span> &#123;</span><br><span class="line">  <span class="built_in">type</span> = <span class="string">&quot;map&quot;</span></span><br><span class="line">  default = &#123;</span><br><span class="line">    <span class="string">&quot;us-east-1&quot;</span> = <span class="string">&quot;ami-b374d5a5&quot;</span></span><br><span class="line">    <span class="string">&quot;us-west-2&quot;</span> = <span class="string">&quot;ami-4b32be2b&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用方式</span></span><br><span class="line"><span class="comment"># 其中 var.amis 是 map 變數，var.region 則是 key</span></span><br><span class="line">resource <span class="string">&quot;aws_instance&quot;</span> <span class="string">&quot;example&quot;</span> &#123;</span><br><span class="line">  ami           = <span class="string">&quot;<span class="variable">$&#123;lookup(var.amis, var.region)&#125;</span>&quot;</span></span><br><span class="line">  instance_type = <span class="string">&quot;t2.micro&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Output-Variables"><a href="#Output-Variables" class="headerlink" title="Output Variables"></a>Output Variables</h1><p>執行 terraform 指令時，除了 input 重要外，output 也同樣重要，透過 output 可以讓使用者取得關於 resource 的真正重要資訊(例如：load balancer IP, VPN address … 等等)，特別是當環境很複雜時，設計良好的 output 肯定是必要的。</p><p>而 output 除了會在 <code>terraform apply</code> 執行後顯示，以可以在後續透過 <code>terraform output</code> 來查詢。</p><p>以下範例可以取得 <a href="https://www.terraform.io/docs/providers/aws/d/eip.html">AWS Elastic IP</a> 的 public ip address：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://www.terraform.io/docs/providers/aws/d/eip.html</span></span><br><span class="line">output <span class="string">&quot;ip&quot;</span> &#123;</span><br><span class="line">  value = aws_eip.ip.public_ip</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的範例中，output 名稱為 <code>ip</code>(<strong>名稱必須唯一</strong>)，內容為 AWS Elastic IP 的 public IP address，當執行完 <code>terraform apply</code> 之後，就會自動出現 output，但之後也可以透過以下指令取得單純的內容輸出：</p><blockquote><p>terraform output ip</p></blockquote><h1 id="Modules"><a href="#Modules" class="headerlink" title="Modules"></a>Modules</h1><p>當 infra 越來越大時，原本將所有 resource, variable 等定義檔放在同一個目錄下的方式就顯的很不夠用，且無法重複利用，因此 terraform 就提供了一個稱為 <code>module</code> 的機制來管理這些 configuration，將複雜的 infra 設定拆開成一個個可以獨立管理的小模組。(類似 Ansible Role)</p><h2 id="使用-module"><a href="#使用-module" class="headerlink" title="使用 module"></a>使用 module</h2><p>目前 <a href="https://registry.terraform.io/">Terraform Registry</a> 已經有不少個已經開發完成可以直接使用的 module，若使用者要建立的 infra 中剛好有對應的 module 可以協助建立其中一部份，可以直接拉下來使用。</p><p>以下是一個建立 AWS Consul cluster 的範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">provider <span class="string">&quot;aws&quot;</span> &#123;</span><br><span class="line">  access_key = <span class="string">&quot;AWS ACCESS KEY&quot;</span></span><br><span class="line">  secret_key = <span class="string">&quot;AWS SECRET KEY&quot;</span></span><br><span class="line">  region     = <span class="string">&quot;us-east-1&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">module <span class="string">&quot;consul&quot;</span> &#123;</span><br><span class="line">  <span class="built_in">source</span> = <span class="string">&quot;hashicorp/consul/aws&quot;</span></span><br><span class="line"></span><br><span class="line">  num_servers = <span class="string">&quot;3&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>從以上的內容，加上目前已經學到的 terraform 相關知識，可以歸納出以下重點 &amp; 結論：</p><ul><li><p><code>source</code> 定義的是 module 所在的位置，這邊的定義方式是告訴 terraform 此 module 位於 terraform registry 中，因此必須透過 <code>terraform init</code> 來自動取得</p></li><li><p>由於是 public module，因此<a href="https://registry.terraform.io/modules/hashicorp/consul/aws/0.6.1?tab=inputs">官網可以查詢到可用的 input 有哪些</a></p></li><li><p>每個 module 基本上也都會有一些 output 資訊可以取得(<a href="https://registry.terraform.io/modules/hashicorp/consul/aws/0.6.1?tab=outputs">上面的範例</a>)</p></li><li><p>module 本身也可以來自於 Git repository, HTTP, 或是本地端檔案</p></li><li><p>透過 <code>terraform apply</code> + module 產生出來的 resource 的名稱，都會帶有 module 名稱作為 prefix</p></li><li><p>要將 resource 清除同樣也是可以使用 <code>terraform destroy</code></p></li></ul><h2 id="Module-Output"><a href="#Module-Output" class="headerlink" title="Module Output"></a>Module Output</h2><p>在前面的範例中有指定 input <code>num_servers</code> 的值，而 output 也可以比照原本 terraform 的處理方式比照辦理，由於幾乎每個 module 都會有 output，因此<a href="https://registry.terraform.io/modules/hashicorp/consul/aws?tab=outputs">以上面的 consul module 為例</a>，我們也可以使用 consul module output 來自訂我們想要的 output，範例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得 auto-scaling group 的名稱</span></span><br><span class="line">output <span class="string">&quot;consul_server_asg_name&quot;</span> &#123;</span><br><span class="line">  value = module.consul.asg_name_servers</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接著取得 output 的方式就跟上面相同囉!</p><h1 id="Remote-State-Storage"><a href="#Remote-State-Storage" class="headerlink" title="Remote State Storage"></a>Remote State Storage</h1><p>前面有提到使用 terraform 時很重要的一個課題就是如何管理 state，若是自己私下練習可以放在本機就好，但是當 infra 有很多人同時維護時，可以就需要放在遠端可透過 Internet 存取的地方；terraform 提供了很多 remote state storage babckend 的支援，像是：s3, consul, etcd, terraform cloud … 等等。</p><p>由於目前將 state 放到 terraform cloud 是免費的(此外在 user, workspace 都沒有使用數量的限制，加上官方宣稱使用 <a href="https://www.vaultproject.io/">Vault</a> 作管理加密)，因此這邊就使用 terraform cloud 來做測試。</p><p>首先要取得存取 terraform cloud 的 token，步驟如下：</p><ol><li><p>到 <a href="https://app.terraform.io/signup">terraform cloud</a> 官網註冊新帳號</p></li><li><p>將 terraform 升級到 0.11.13 以上</p></li><li><p>在 <a href="https://app.terraform.io/app/organizations/new">terraform cloud</a> 建立新的 organization</p></li><li><p>到 <a href="https://app.terraform.io/app/settings/tokens">terraform cloud user settings</a> 中建立一個新的 token，並 copy token 內容</p></li><li><p>在本地端建立 <code>~/.terraformrc</code> 檔案，並填入以下內容：</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將上一個步驟的 token 內容取代下方的 REPLACE_ME</span></span><br><span class="line">credentials <span class="string">&quot;app.terraform.io&quot;</span> &#123;</span><br><span class="line">     token = <span class="string">&quot;REPLACE_ME&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="6"><li>在當前目錄下建立一個名稱為 <code>remote-storage-state.tf</code> 的檔案(檔名可隨意，副檔名是 <code>tf</code> 即可)，內容如下：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將 organization 填入</span></span><br><span class="line"><span class="comment"># workspace name 可以隨意取，terraform 會自動在 terraform cloud 上建立下面指定的 workspace </span></span><br><span class="line">terraform &#123;</span><br><span class="line">  <span class="comment"># 其中 &quot;remote&quot; 的意思表示使用 terraform cloud</span></span><br><span class="line">  <span class="comment"># 若是 remote storage 使用 etcdv3，則會是 &quot;etcv3&quot;</span></span><br><span class="line">  backend <span class="string">&quot;remote&quot;</span> &#123;</span><br><span class="line">    organization = <span class="string">&quot;&lt;ORGANIZATION NAME&gt;&quot;</span></span><br><span class="line"></span><br><span class="line">    workspaces &#123;</span><br><span class="line">      name = <span class="string">&quot;&lt;WORKSPACE NAME&gt;&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="7"><li><p>執行 <code>terraform init</code>，取得存取 remote storage 所需要的檔案（若本地端有存在 state 檔案，terraform 會詢問要不要移到 remote storage）</p></li><li><p>接著後續的 <code>terraform apply</code> 就會自動將 state 存到 terraform cloud 了，還會有 version control 的功能喔!</p></li></ol><p>另外一點比較重要的是，如果之後決定要將 state 從 remote 移回本地端，那就移除剛剛上面所新增的 <code>remote-storage-state.tf</code> 檔案，並再執行一次 <code>terraform init</code>，tetrraform 就會詢問要不要將 state 從 remote 移回本地端了!</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://learn.hashicorp.com/terraform/">Terraform Curriculum - HashiCorp Learn</a></p></li><li><p><a href="https://www.terraform.io/docs/modules/index.html">Creating Modules - Terraform by HashiCorp</a></p></li><li><p><a href="https://www.terraform.io/docs/index.html">Documentation - Terraform by HashiCorp</a></p></li><li><p><a href="https://www.terraform.io/intro/examples/index.html">Example Configurations - Terraform by HashiCorp</a></p></li><li><p><a href="https://www.terraform.io/docs/import/index.html">Import - Terraform by HashiCorp</a></p></li><li><p><a href="https://www.terraform.io/docs/configuration/index.html">Configuration Language - Terraform by HashiCorp</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Terraform </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DevOps </tag>
            
            <tag> Terraform </tag>
            
            <tag> IaC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] 如何取得合法可用的權限，讓 pod 與 API server 溝通</title>
      <link href="/blog/Kubernetes/k8s-How-to-access-resource-legally/"/>
      <url>/blog/Kubernetes/k8s-How-to-access-resource-legally/</url>
      
        <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>最近同事在修改 <a href="https://github.com/influxdata/telegraf/blob/release-1.10/plugins/inputs/kube_inventory/README.md">telegraf plugin</a> 要用來取得 Kubernetes cluster level resource status，大概包含以下內容：</p><ul><li><p>daemonsets</p></li><li><p>deployments</p></li><li><p>nodes</p></li><li><p>persistentvolumes</p></li><li><p>persistentvolumeclaims</p></li><li><p>pods (containers)</p></li><li><p>statefulsets</p></li></ul><p>遇到了一些權限上的問題，後來花了點時間解決，趁還沒忘記前作個筆記，或許可以幫助到有需要的人。</p><blockquote><p>以下都是在 RBAC 的環境下所作的操作 &amp; 說明</p></blockquote><h1 id="如何取得合法-amp-可用的權限"><a href="#如何取得合法-amp-可用的權限" class="headerlink" title="如何取得合法 &amp; 可用的權限 ?"></a>如何取得合法 &amp; 可用的權限 ?</h1><p>我們要解決的問題是：</p><blockquote><p>我寫了一支程式，要取得 Kubernetes cluster level 的資訊，要如何取得合法權限 ?</p></blockquote><p>首先，這些資訊要向 API server 取得，因此<strong>要有合法的 token</strong> 才可以向 API server 問到所需要的資訊，但另外一個問題來了：</p><blockquote><p>token 在哪? 要如何取得?</p></blockquote><p>在 Kubernetes 中，token 是存在於 secret 中，而 secret 是存在於 service account 中，但 service account 有多少權限，則是透過 <code>Role</code>, <code>ClusterRole</code>, <code>RoleBinding</code>, <code>ClusterRoleBinding</code> …. 等 resource 來定義，相互關係可以參考下圖：</p><p><img src="/blog/images/kubernetes/k8s-serviceaccount-permission.png" alt="Kubernetes - Service Account, Role, RoleBinding, ClusterRole, ClusterRoleBinding"></p><p>以 <a href="https://github.com/influxdata/telegraf/blob/release-1.10/plugins/inputs/kube_inventory/README.md" title="telegraf Kube Inventory Plugin">telegraf Kube Inventory Plugin</a> 的說明為例，需要以下的權限：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">influx:cluster:viewer</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">rbac.authorization.k8s.io/aggregate-view-telegraf:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumes&quot;</span>,<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>,<span class="string">&quot;list&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">influx:telegraf</span></span><br><span class="line"><span class="attr">aggregationRule:</span></span><br><span class="line">  <span class="attr">clusterRoleSelectors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">rbac.authorization.k8s.io/aggregate-view-telegraf:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">rbac.authorization.k8s.io/aggregate-to-view:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span> [] <span class="comment"># Rules are automatically filled in by the controller manager.</span></span><br></pre></td></tr></table></figure><p>上面的設定是透過 <code>aggregationRule</code> 的方式，讓最上面所定義的 <code>influx:cluster:viewer</code> 可以被 reuse；有了權限的設定後，我們要把它指定給某個 service account：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">telegraf</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">influx:telegraf:viewer</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">influx:telegraf</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">telegraf</span></span><br></pre></td></tr></table></figure><p>此時 service account <code>telegraf</code> 中已經有合法 &amp; 可用的 token 可以用來向 API server 取得我們所需要的資訊。</p><h1 id="如何使用-Service-Account-中的-Token"><a href="#如何使用-Service-Account-中的-Token" class="headerlink" title="如何使用 Service Account 中的 Token?"></a>如何使用 Service Account 中的 Token?</h1><p>目前知道有一個名稱為 <code>telegraf</code> 的 service account，裏面有個可用的 token，但是要怎麼使用這個 token 呢?</p><p>答案是要在 pod definition 中加上 <code>serviceAccountName</code> 的設定，例如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;telegraf&quot;</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">&quot;telegraf&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">&quot;telegraf&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="string">...</span></span><br></pre></td></tr></table></figure><p>如此一來就是指定透過上面這個 DaemonSet 產生的 pod，會使用所指定的 service account，但隨之而來的問題是：</p><blockquote><p>指定了 service account 後，token 在哪裡?</p></blockquote><p>在 k8s 中，當使用者在 pod 中指定要使用特定的 service account，k8s 會自動將 service account 掛載到該 pod 中，而 service account 中的 token 就會存在於該 pod 的 <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code> 這個位置，因此程式中只要指定使用上面這個位置的 token 內容，就可以跟 API server 溝通，並取得所需要的資料了!</p><p>所以要開發運行在 k8s 的程式且有與 API server 溝通的需求，建議將 <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code> 這個位置設定為 bearer token 的預設路徑，如此一來，使用者只要有設定好正確的 service account 並將其指定到 pod definition 中，程式理當來說就可以與 API server 溝通了!</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><p><a href="https://godleon.github.io/blog/Kubernetes/k8s-API-Authorization/">了解 Kubernetes 中的授權機制 | 小信豬的原始部落</a></p></li><li><p><a href="http://orchome.com/1308">Kubernetes权限管理之RBAC - OrcHome</a></p></li><li><p><a href="https://github.com/influxdata/telegraf/blob/release-1.10/plugins/inputs/kube_inventory/README.md" title="telegraf Kube Inventory Plugin">telegraf Kube Inventory Plugin</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubernetes Security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Proxmox VE] 修復 cluster 發生的 no quorum 錯誤</title>
      <link href="/blog/Proxmox/Proxmox-Fix-No-Quorum-issue/"/>
      <url>/blog/Proxmox/Proxmox-Fix-No-Quorum-issue/</url>
      
        <content type="html"><![CDATA[<p>今天同事突然跟我說內部 Redmine 服務掛點了，想說它是執行在 OpenShift 上，且這個 OpenShift 是由散佈在 Proxmox cluster 中的多個 VM 所組成，要掛掉實在不容易，結果登入 Proxmox web console，發現整個 cluster 完全都異常，每個 node 都出現錯誤，且對 VM 的操作都會出現以下錯誤訊息：</p><blockquote><p>TASK ERROR: cluster not ready – no quorum?</p></blockquote><p>接著到每一台 node 上執行 <code>pvecm status</code>，會出現以下兩種情況：</p><ol><li><p><code>Cannot initialize CMAP service</code></p></li><li><p><code>Quorum: X Activity blocked</code> (其中 X 為某個數字)</p></li></ol><p>很明顯 cluster status 在同步上有問題，上網找了一下資料，找到了解決方式，出現第一種錯誤的 node，執行以下指令後可以修復：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl restart pve-cluster.service</span><br><span class="line"></span><br><span class="line">$ systemctl restart pvedaemon.service</span><br><span class="line"></span><br><span class="line">$ systemctl restart pveproxy.service</span><br><span class="line"></span><br><span class="line">$ systemctl restart corosync.service </span><br></pre></td></tr></table></figure><p>而第二種錯誤的 node，則需要執行以下指令來修復：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ pvecm expected 1</span><br><span class="line"></span><br><span class="line">$ systemctl restart pve-cluster.service</span><br><span class="line"></span><br><span class="line">$ systemctl restart pvedaemon.service</span><br><span class="line"></span><br><span class="line">$ systemctl restart pveproxy.service</span><br><span class="line"></span><br><span class="line">$ systemctl restart corosync.service </span><br></pre></td></tr></table></figure><p>執行以上指令讓 corosync 服務重新啟動，並抓取原本的 cluster 設定，就會恢復正常了。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://blog.pulipuli.info/2014/08/proxmox-ve-fix-proxmox-ve-cluster-not.html">修復Proxmox VE：集叢未啟動 / Fix Proxmox VE: Cluster Not Ready - 布丁布丁吃什麼？</a></p></li><li><p><a href="https://www.hostloc.com/thread-394364-1-1.html">大佬们 问个proxmox集群的问题-美国VPS综合讨论-全球主机交流论坛 - Powered by Discuz!</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Proxmox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Proxmox </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Prometheus] Basic Concept memo</title>
      <link href="/blog/Prometheus/Prometheus-Basic-Concepts/"/>
      <url>/blog/Prometheus/Prometheus-Basic-Concepts/</url>
      
        <content type="html"><![CDATA[<p>這篇文章單純是研究 <a href="https://prometheus.io/">Prometheus</a> 時隨手記下的筆記….</p><h1 id="關鍵名詞"><a href="#關鍵名詞" class="headerlink" title="關鍵名詞"></a>關鍵名詞</h1><h2 id="Sample"><a href="#Sample" class="headerlink" title="Sample"></a>Sample</h2><p>sample 表示實際的時間序列資料，包含以下內容：</p><ul><li><p>一個精度為 float64 的值</p></li><li><p>一個以 ms 為最小單位的 timestamp</p></li></ul><h2 id="Instance"><a href="#Instance" class="headerlink" title="Instance"></a>Instance</h2><p>泛指 Prometheus 擷取監控資料的 HTTP(s) endpoint，一般會是一個運行中的 process。</p><h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><p>一群 instance 的集合，稱為一個 Job，一般會是用來收集相同目的的資料，例如：因為 scalability &amp; reliability 而讓同一個 process 跑在多台機器上，然後同時監控多台機器中的該 process。</p><h1 id="Data-Model"><a href="#Data-Model" class="headerlink" title="Data Model"></a>Data Model</h1><p>Prometheus 中的 multiple dimension 是透過 <code>metric</code> + <code>label</code> 所呈現出來的，同樣的 metric 資料，但透過不同的 label 搭配可以呈現出不同的維度的資料。</p><h2 id="如何表示資料"><a href="#如何表示資料" class="headerlink" title="如何表示資料"></a>如何表示資料</h2><p>資料的表示需要 <code>metric</code> &amp; <code>label</code> 兩種資訊，範例如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125;</span><br></pre></td></tr></table></figure><p>以下是個實際範例：</p><blockquote><p>api_http_requests_total{method=”POST”, handler=”/messages”}</p></blockquote><p>以上的資料表示法可以在 Prometheus console 的查詢畫面中使用。</p><h1 id="Jobs-amp-Instances"><a href="#Jobs-amp-Instances" class="headerlink" title="Jobs &amp; Instances"></a>Jobs &amp; Instances</h1><p>Prometheus 除了收集 instance metrics 外，還會有其他自動產生並帶上相對應 label 的資料，例如：</p><ul><li><p><code>up&#123;job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;&#125;</code>：<code>1</code> 表示 instance 目前很健康，可以正常取得 metric 資料；若 <code>0</code> 則反之</p></li><li><p><code>scrape_duration_seconds&#123;job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;&#125;</code>：擷取 metric 資料的時間間隔</p></li><li><p><code>scrape_samples_post_metric_relabeling&#123;job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;&#125;</code>：有多少個 sample</p></li><li><p><code>scrape_samples_scraped&#123;job=&quot;&lt;job-name&gt;&quot;, instance=&quot;&lt;instance-id&gt;&quot;&#125;</code></p></li></ul><h1 id="Service-Discovery"><a href="#Service-Discovery" class="headerlink" title="Service Discovery"></a>Service Discovery</h1><p>在一般 Prometheus 的 <code>scrape_configs</code> 設定中，會看到 <code>static_configs</code> 的設定，用來指定 metric endpoint，使用者可以根據需要設定 endpoint list。</p><p>但若是 metrics endpoint 時常變動時怎麼辦? 例如在 OpenStack or Kubernetes 的環境中，VM or Pod 可能都是來來去去，IP address 都是會一直變動的，此時 <code>static_configs</code> 根本無法正確指定要擷取資料的 instance。</p><p>為了解決這個問題，就要透過 <strong>service discovery</strong> 的方式來動態取得 instance list，而 Prometheus 支援很多種 service discovery 的機制，例如：</p><ul><li><p><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#file_sd_config">file_sd_config</a></p></li><li><p><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config">kubernetes_sd_config</a></p></li><li><p><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#openstack_sd_config">openstack_sd_config</a></p></li></ul><p>除了上面的幾個之外，Prometheus 還支援了很多種其他的服務，也包含了多個 public cloud，詳細的清單可以參考<a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration">官網</a>。</p><p>以下蒐集了一些幾個 service discovery 的範例供參考：</p><h2 id="consul-sd-configs"><a href="#consul-sd-configs" class="headerlink" title="consul_sd_configs"></a>consul_sd_configs</h2><ul><li><p><a href="http://ylzheng.com/2018/01/17/prometheus-sd-and-relabel/">Prometheus中的服务发现和relabel | I’m Yunlong</a></p></li><li><p><a href="https://blog.52itstyle.vip/archives/2084/">Consul+Prometheus系统监控之注册发现 - 柒’s Blog</a></p></li></ul><h2 id="kubernetes-sd-config"><a href="#kubernetes-sd-config" class="headerlink" title="kubernetes_sd_config"></a>kubernetes_sd_config</h2><ul><li><p><a href="http://ylzheng.com/2017/07/04/prometheus-kubernates/">Prometheus在Kubernetes下的监控实践 | I’m Yunlong</a></p></li><li><p><a href="https://yunlzheng.gitbook.io/prometheus-book/part-iii-prometheus-shi-zhan/readmd/use-prometheus-monitor-kubernetes">监控Kubernetes集群 - prometheus-book</a></p></li></ul><h1 id="在-Kubernetes-上佈署-Prometheus-的流程"><a href="#在-Kubernetes-上佈署-Prometheus-的流程" class="headerlink" title="在 Kubernetes 上佈署 Prometheus 的流程"></a>在 Kubernetes 上佈署 Prometheus 的流程</h1><p>若是 Prometheus 佈署在 k8s cluster 之外，從外面與 API server 溝通，就必須要準備好正確的憑證資訊。</p><p>但如果將 Prometheus 放到 k8s cluster 內部，就可以使用 k8s 提供的一些方法來取得正確的憑證資訊，流程如下：</p><ol><li><p>建立存放 Prometheus 服務所需要的 namespace (optional)</p></li><li><p>建立 Prometheus 所使用的 ServiceAccount (每個 ServiceAccount 會帶有一個 token)</p></li><li><p>設定 RBAC，並賦予 ServiceAccount 足夠的存取權限</p></li><li><p>佈署 Prometheus 指定使用上一個步驟所設定好的 ServiceAccount</p></li><li><p>在 Prometheus 設定檔中，進行以下設定：</p><ul><li><code>scrape_configs.tls_config.ca_file</code>: <code>/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</code></li><li><code>scrape_configs.bearer_token_file</code>: <code>/var/run/secrets/kubernetes.io/serviceaccount/token</code></li></ul></li></ol><p>如此一來，Prometheus pod 就可以跟 k8s API server 通訊了!</p><h1 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h1><p>Prometheus 中提供了兩種 rule type，分別是：</p><ul><li><p><a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/">Recording Rule</a>：用來協助預先處理 or 計算某些 metric 的值，透過 recording rule，將較為複雜的需求先計算完成，後續就可以直接使用，會比起每次都重新查詢 &amp; 計算來的快的多且省資源</p></li><li><p><a href="https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/">Alerting Rule</a>：讓使用者可以透過 rometheus expression language expressions 自訂需要發送 alarm 的條件</p></li></ul><p>rule 在使用上有幾點需要注意：</p><ul><li><p>rule 可以獨立成一個個單獨的檔案並 include，不一定要全部設定在主設定檔 <code>prometheus.yml</code> 上。</p></li><li><p>透過送給 prometheus process <code>SIGHUP</code> 的訊號，可以在執行期間載入 rule</p></li></ul><h2 id="Recording-Rule-範例"><a href="#Recording-Rule-範例" class="headerlink" title="Recording Rule 範例"></a>Recording Rule 範例</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">groups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line">    <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">record:</span> <span class="string">job:http_inprogress_requests:sum</span></span><br><span class="line">      <span class="attr">expr:</span> <span class="string">sum(http_inprogress_requests)</span> <span class="string">by</span> <span class="string">(job)</span></span><br></pre></td></tr></table></figure><h2 id="Alerting-Rule-範例"><a href="#Alerting-Rule-範例" class="headerlink" title="Alerting Rule 範例"></a>Alerting Rule 範例</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">groups:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">alert:</span> <span class="string">HighErrorRate</span></span><br><span class="line">    <span class="attr">expr:</span> <span class="string">job:request_latency_seconds:mean5m&#123;job=&quot;myjob&quot;&#125;</span> <span class="string">&gt;</span> <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># alert 成立後，進入 pending 狀態，持續超過 10 mins 則進入 firing </span></span><br><span class="line">    <span class="attr">for:</span> <span class="string">10m</span></span><br><span class="line">    <span class="comment"># 為 alert 標記 tag</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">severity:</span> <span class="string">page</span></span><br><span class="line">    <span class="comment"># 可儲存較為複雜的 metadata 資訊</span></span><br><span class="line">    <span class="attr">annotations:</span></span><br><span class="line">      <span class="attr">summary:</span> <span class="string">High</span> <span class="string">request</span> <span class="string">latency</span></span><br></pre></td></tr></table></figure><p>與 <a href="https://prometheus.io/docs/visualization/consoles/">Console Templates</a> 搭配使用：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">groups:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">example</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Alert for any instance that is unreachable for &gt;5 minutes.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">alert:</span> <span class="string">InstanceDown</span></span><br><span class="line">    <span class="attr">expr:</span> <span class="string">up</span> <span class="string">==</span> <span class="number">0</span></span><br><span class="line">    <span class="attr">for:</span> <span class="string">5m</span></span><br><span class="line">    <span class="attr">labels:</span></span><br><span class="line">      <span class="attr">severity:</span> <span class="string">page</span></span><br><span class="line">    <span class="attr">annotations:</span></span><br><span class="line">      <span class="attr">summary:</span> <span class="string">&quot;Instance <span class="template-variable">&#123;&#123; $labels.instance &#125;&#125;</span> down&quot;</span></span><br><span class="line">      <span class="attr">description:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; $labels.instance &#125;&#125;</span> of job <span class="template-variable">&#123;&#123; $labels.job &#125;&#125;</span> has been down for more than 5 minutes.&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Alert for any instance that has a median request latency &gt;1s.</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">alert:</span> <span class="string">APIHighRequestLatency</span></span><br><span class="line">    <span class="attr">expr:</span> <span class="string">api_http_request_latencies_second&#123;quantile=&quot;0.5&quot;&#125;</span> <span class="string">&gt;</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">for:</span> <span class="string">10m</span></span><br><span class="line">    <span class="attr">annotations:</span></span><br><span class="line">      <span class="attr">summary:</span> <span class="string">&quot;High request latency on <span class="template-variable">&#123;&#123; $labels.instance &#125;&#125;</span>&quot;</span></span><br><span class="line">      <span class="attr">description:</span> <span class="string">&quot;<span class="template-variable">&#123;&#123; $labels.instance &#125;&#125;</span> has a median request latency above 1s (current value: <span class="template-variable">&#123;&#123; $value &#125;&#125;</span>s)&quot;</span></span><br></pre></td></tr></table></figure><h1 id="儲存系統"><a href="#儲存系統" class="headerlink" title="儲存系統"></a>儲存系統</h1><p>可參考以下文章：</p><ul><li><p><a href="https://k2r2bai.com/2018/06/27/devops/prometheus-storage/">Prometheus 儲存系統解析 - KaiRen’s Blog</a></p></li><li><p><a href="http://ylzheng.com/2018/03/06/promethus-local-storage/">Prometheus高可用(1)：理解本地存储 | I’m Yunlong</a></p></li><li><p><a href="http://ylzheng.com/2018/03/07/promethues-remote-storage/">Prometheus高可用(2)：理解远端存储 | I’m Yunlong</a></p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://k2r2bai.com/2018/06/10/devops/prometheus-intro/">Prometheus 介紹與基礎入門 - KaiRen’s Blog</a></p></li><li><p><a href="https://k2r2bai.com/2018/06/27/devops/prometheus-storage/">Prometheus 儲存系統解析 - KaiRen’s Blog</a></p></li><li><p><a href="https://k2r2bai.com/2018/06/29/devops/prometheus-federation/">了解 Prometheus Federation 功能 - KaiRen’s Blog</a></p></li><li><p><a href="https://k2r2bai.com/2018/07/01/devops/prometheus-ha/">Prometheus 高可靠實現方式 - KaiRen’s Blog</a></p></li><li><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/prometheus">Kubernetes - Prometheus Add-on</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Prometheus </tag>
            
            <tag> Telemetry </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Debug &amp; Troubleshooting 簡介</title>
      <link href="/blog/Kubernetes/k8s-debug-and-troubleshooting-Overview/"/>
      <url>/blog/Kubernetes/k8s-debug-and-troubleshooting-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="Application-Introspection-and-Debugging"><a href="#Application-Introspection-and-Debugging" class="headerlink" title="Application Introspection and Debugging"></a>Application Introspection and Debugging</h1><p><code>kubectl get event</code> 是屬於 namespace level，若要檢視其他 namespace 的 event，就要額外加上 <code>--namespace</code> 參數</p><h1 id="Auditing"><a href="#Auditing" class="headerlink" title="Auditing"></a>Auditing</h1><p>等到之後了解 Elasticsearch + Fluentd + Kibana 之後再來研究這個部份</p><h1 id="Core-metrics-pipeline"><a href="#Core-metrics-pipeline" class="headerlink" title="Core metrics pipeline"></a>Core metrics pipeline</h1><ul><li><p>k8s 從 v1.8 以後提供了 metrics API，可以提供系統使用資源的資訊，但由於 k8s 本身不儲存這些資料，因此若要查詢一段時間內的資源使用率，就必須使用外部服務來達成</p></li><li><p>要使用 metrics API，就必須先在 k8s cluster 中安裝 <a href="https://github.com/kubernetes-incubator/metrics-server">metrics server</a> 才行</p></li><li><p>每個 node 上佈署的 kubelet 會提供 summary API，而 metrics server 就是透過 summary API 蒐集相關資訊</p></li></ul><h1 id="Debug-Init-Containers"><a href="#Debug-Init-Containers" class="headerlink" title="Debug Init Containers"></a>Debug Init Containers</h1><p>在 Pod 中可以定義 init container，甚至可以定義多個 init container 協助提供正式服務的 container 做一些前置的設定工作，而 init container 工作的方式如下：</p><ul><li><p>init container 總是會執行到結束為止</p></li><li><p>若定義多個 init container，會依序一個一個執行，且必須等到前一個 init container 執行成功後，才會輪到下一個</p></li><li><p>如果任何一個 init container 工作執行失敗，k8s 就會重新啟動 pod 並再度啟動所有的 init container 繼續執行工作，除非 restartPolicy 設定為 <code>Never</code></p></li></ul><p>若是 pod 啟動時因為 init container 發生錯誤，就會出現類似以下訊息：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NAME         READY     STATUS     RESTARTS   AGE</span><br><span class="line">&lt;pod-name&gt;   0/1       Init:1/2   0          7s</span><br></pre></td></tr></table></figure><p>可以透過 <code>kubectl describe pod &lt;pod-name&gt;</code> 檢視更細節的 log 資訊：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Init Containers:</span><br><span class="line">  &lt;init-container-1&gt;:</span><br><span class="line">    Container ID:    ...</span><br><span class="line">    ...</span><br><span class="line">    State:           Terminated</span><br><span class="line">      Reason:        Completed</span><br><span class="line">      Exit Code:     0</span><br><span class="line">      Started:       ...</span><br><span class="line">      Finished:      ...</span><br><span class="line">    Ready:           True</span><br><span class="line">    Restart Count:   0</span><br><span class="line">    ...</span><br><span class="line">  &lt;init-container-2&gt;:</span><br><span class="line">    Container ID:    ...</span><br><span class="line">    ...</span><br><span class="line">    State:           Waiting</span><br><span class="line">      Reason:        CrashLoopBackOff</span><br><span class="line">    Last State:      Terminated</span><br><span class="line">      Reason:        Error</span><br><span class="line">      Exit Code:     1</span><br><span class="line">      Started:       ...</span><br><span class="line">      Finished:      ...</span><br><span class="line">    Ready:           False</span><br><span class="line">    Restart Count:   3</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>若是 pod status 的開頭為 <code>Init:</code>，就表示目前的狀態(or 問題)與 init container 有關，以下是與 init container 的 pod status 範例列表：</p><ul><li><p><code>Init:N/M</code>：共有 M 的 init container，目前已經完成 N 個</p></li><li><p><code>Init:Error</code>：有一個 init container 無法正確完成指定工作</p></li><li><p><code>Init:CrashLoopBackOff</code>：有一個 init container 一直無法正確完成指定工作</p></li><li><p><code>Pending</code>：尚未啟動任何 init container</p></li><li><p><code>PodInitializing</code> or <code>Running</code>：init container 相關工作都已經正確執行完璧</p></li></ul><h1 id="Debug-Pods-and-ReplicationControllers"><a href="#Debug-Pods-and-ReplicationControllers" class="headerlink" title="Debug Pods and ReplicationControllers"></a>Debug Pods and ReplicationControllers</h1><h2 id="Pod-無法正常啟動"><a href="#Pod-無法正常啟動" class="headerlink" title="Pod 無法正常啟動"></a>Pod 無法正常啟動</h2><p>pod 無法正常啟動，原因大概可以歸為幾類，若是持續處於 <code>pending</code> 狀態，可能的原因是：</p><ul><li><p>資源不足 (可增加更多 node or 移除不必要的 pod)</p></li><li><p>使用了 <code>hostPort</code>，但沒注意到 hostPort 的使用數量是有限的，儘量改用 <code>service</code> 提供外部存取</p></li></ul><p>若是 pod 持續處於 <code>waiting</code> 狀態，可能的原因是：</p><ul><li>無法正確的 pull container image</li></ul><p>若是 pod 會 crash or 一直不是處於很健康的狀態，就可以透過 <code>kubectl logs</code> 來檢視 log 資訊，或是使用 <code>kubectl exec</code> 在 container 中執行特定指令來進行測試。</p><p>因此這個部份會用到的指令大概如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 pod 相關資訊</span></span><br><span class="line">$ kubectl describe pods <span class="variable">$&#123;POD_NAME&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 node 的系統資源是否足夠</span></span><br><span class="line">$ kubectl get nodes -o yaml | grep <span class="string">&#x27;\sname\|cpu\|memory&#x27;</span></span><br><span class="line">$ kubectl get nodes -o json | jq <span class="string">&#x27;.items[] | &#123;name: .metadata.name, cap: .status.capacity&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 pod 中的特定 container log</span></span><br><span class="line">$ kubectl logs <span class="variable">$&#123;POD_NAME&#125;</span> <span class="variable">$&#123;CONTAINER_NAME&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視上一次的 failure log</span></span><br><span class="line">$ kubectl logs --previous <span class="variable">$&#123;POD_NAME&#125;</span> <span class="variable">$&#123;CONTAINER_NAME&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 pod 中的特定 container 中執行命令</span></span><br><span class="line"><span class="comment"># 若 pod 中只有一個 container，則 &quot;-c $&#123;CONTAINER_NAME&#125;&quot; 可以省略</span></span><br><span class="line"><span class="comment"># 但 container 必須已經啟動才可以</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> <span class="variable">$&#123;POD_NAME&#125;</span> -c <span class="variable">$&#123;CONTAINER_NAME&#125;</span> -- <span class="variable">$&#123;CMD&#125;</span> <span class="variable">$&#123;ARG1&#125;</span> <span class="variable">$&#123;ARG2&#125;</span> ... <span class="variable">$&#123;ARGN&#125;</span></span><br></pre></td></tr></table></figure><h2 id="Pod-啟動了，但行為不如預期"><a href="#Pod-啟動了，但行為不如預期" class="headerlink" title="Pod 啟動了，但行為不如預期"></a>Pod 啟動了，但行為不如預期</h2><p>在 k8s 中建立 application 時，有時候會忽略使用者的輸入錯誤，例如將 <code>command</code> 打成 <code>commnd</code>，這樣會造成 pod 被建立了，但指定的 command 並沒有正確的被執行，導致於 pod 行為不如預期，為了避免這樣的情況，在建立 pod 時可加上 <code>--validate</code> 參數，確保 YAML 語法的 100% 正確性，以下是個簡單範例：</p><blockquote><p>kubectl create –validate -f mypod.yaml</p></blockquote><p>再來可以確認在 API server 中儲存的 pod definition 是否與當初建立的時候一致，可以透過類似 <code>kubectl get pods/mypod -o yaml &gt; mypod-on-apiserver.yaml</code> 的指令輸出 API server 的 pod definition，然後仔細比對原始的 pod definition，有時候兩者的些許不同就是造成 pod 行為不如預期的主要原因。</p><h1 id="Debug-Services"><a href="#Debug-Services" class="headerlink" title="Debug Services"></a>Debug Services</h1><p>由於從 k8s 外面並無法直接存取 pod，因此當 service 出問題時，要利用一些特別的手段來檢查，以下會說明一些常見的方式。</p><h2 id="確認-service-存在"><a href="#確認-service-存在" class="headerlink" title="確認 service 存在"></a>確認 service 存在</h2><p>首先最基本的，就是透過 <code>kubectl get svc svc_name</code> 來判斷 service 是否存在，是否有對應到 pod 對外開放的 port。</p><h2 id="在-pod-中執行命令"><a href="#在-pod-中執行命令" class="headerlink" title="在 pod 中執行命令"></a>在 pod 中執行命令</h2><p>接著要進入到 k8s 內部進行檢查，但首先必須要有一個已經成功啟動的 pod，才可以透過這個 pod 進入到 k8s 內部進行網路相關的檢查，透過以下指令可以在 pod 中執行命令：</p><blockquote><p>kubectl exec <POD-NAME> -c <CONTAINER-NAME> – <COMMAND></p></blockquote><p>但若是沒有現成的 pod 可用怎辦? 那可以自行啟動一個 nettools pod 來使用，裏面已經帶有一些常用網路工具在內：</p><blockquote><p>kubectl run -it –rm –restart=Never nettools –image=godleon/nettools sh</p></blockquote><h2 id="檢測-service-是否正常運作"><a href="#檢測-service-是否正常運作" class="headerlink" title="檢測 service 是否正常運作"></a>檢測 service 是否正常運作</h2><p>再來是檢測從 k8s 內部可否存取到 service &amp; DNS 是否運作正常：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 從 nettools pod 中檢測是否可以存取 service DNS</span></span><br><span class="line">u@nettools_pod$ wget -O- hostnames</span><br><span class="line">wget: bad address <span class="string">&#x27;hostnames&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 從下面的輸出可以看出 service DNS 目前無法正確解析</span></span><br><span class="line">u@nettools_pod$ nslookup hostnames</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">** server cannot find hostnames: NXDOMAIN</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若是有對應的 service 就會出現以下訊息</span></span><br><span class="line"><span class="comment"># 然後一般也都會可以 ping 的通</span></span><br><span class="line">u@nettools_pod$ nslookup kubernetes</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.233.0.1</span><br><span class="line"></span><br><span class="line">u@nettools_pod$ nslookup kubernetes.default</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.233.0.1</span><br><span class="line"></span><br><span class="line">u@nettools_pod$ nslookup coredns</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">** server cannot find coredns: NXDOMAIN</span><br><span class="line"></span><br><span class="line">u@nettools_pod$ nslookup coredns.kube-system</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    coredns.kube-system.svc.cluster.local</span><br><span class="line">Address: 10.233.0.3</span><br><span class="line"></span><br><span class="line">u@nettools_pod$ ping coredns.kube-system</span><br><span class="line">PING coredns.kube-system.svc.cluster.local (10.233.0.3) 56(84) bytes of data.</span><br><span class="line">64 bytes from coredns.kube-system.svc.cluster.local (10.233.0.3): icmp_seq=1 ttl=64 time=0.041 ms</span><br><span class="line">... (略)</span><br></pre></td></tr></table></figure><p>比較需要注意的是 service name 的部份需要額外加上 namespace name 才可以，例如上方的範例 <code>coredns</code>，屬於 <code>kube-system</code> namespace，要查找的時候就要用 <code>coredns.kube-system</code> 來查找；但若是上面的 busybox pod 是位於 kube-system namespace 中，就可以不需要加上 namespace name，但一切還是以 <code>/etc/resolv.conf</code> 的設定為準。</p><h2 id="以-IP-based-的方式檢測-service"><a href="#以-IP-based-的方式檢測-service" class="headerlink" title="以 IP-based 的方式檢測 service"></a>以 IP-based 的方式檢測 service</h2><p>假設 DNS 解析並非問題所在，可以考慮往 IP based 的方式檢測 service 是否正常，例如要測試 HTTP 服務，可以透過 <code>curl SERVICE_IP:port</code> 的方式測試是否有回應。</p><h2 id="Service-背後有-Endpoints-嗎"><a href="#Service-背後有-Endpoints-嗎" class="headerlink" title="Service 背後有 Endpoints 嗎?"></a>Service 背後有 Endpoints 嗎?</h2><p>service 要可以正常運作，必須仰賴後方的 endpoint，可透過 <code>kubectl -n NAMESPACE_NAME get endpoints SERVICE_NAME</code> 來找到 service endpoint，再以上一個步驟 IP-based 的方式檢測 endpoint 服務是否正常。</p><h2 id="kube-proxy-有正常運作嗎"><a href="#kube-proxy-有正常運作嗎" class="headerlink" title="kube-proxy 有正常運作嗎?"></a>kube-proxy 有正常運作嗎?</h2><p>當 service 的部份檢查都沒有問題了，從 cluster 內部測試都是正常，但從外面連進來就是不正常，此時的問題可能就是發生在 kube-proxy 上。</p><p>由於每個 node 都會有安裝 <code>kube-proxy</code> 的服務，因此可以透過 <code>ps aux | grep &#39;kube-proxy&#39;</code> 指令檢查 kube-proxy 服務是否有存在。</p><p>當 kube-proxy 服務有存在，可以進一步透過 <a href="https://www.itread01.com/content/1519304536.html">journalctl</a> 來查詢是否有 kube-proxy 的相關錯誤，若是有出現類似無法連結到 master 的錯誤，那就可能需要再檢查一下 node configuration。</p><h3 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h3><blockquote><p>以下說明是當 kube-proxy 以 iptables 模式安裝時</p></blockquote><p>如何確認 kube-proxy 有正確寫入 iptables rules?</p><p>由於每個 namespace 都會帶有一個名稱為 <code>kubernetes</code> 的 service，我們可以透過以下指令檢查是否有對應的 iptables rules 存在：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ iptables-save | grep <span class="string">&#x27;default/kubernetes&#x27;</span></span><br><span class="line">-A KUBE-SERVICES ! -s 10.233.64.0/18 -d 10.233.0.1/32 -p tcp -m comment --comment <span class="string">&quot;default/kubernetes:https cluster IP&quot;</span> -m tcp --dport 443 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 10.233.0.1/32 -p tcp -m comment --comment <span class="string">&quot;default/kubernetes:https cluster IP&quot;</span> -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y</span><br></pre></td></tr></table></figure><p>若有出現相關資訊，表示 kube-proxy 有正確寫入 iptables rules。</p><h3 id="IPVS"><a href="#IPVS" class="headerlink" title="IPVS"></a>IPVS</h3><blockquote><p>等下回有安裝 IPVS 模式時再來補這一塊!</p></blockquote><h1 id="Troubleshoot-Clusters"><a href="#Troubleshoot-Clusters" class="headerlink" title="Troubleshoot Clusters"></a>Troubleshoot Clusters</h1><p>這部份要說明的是 cluster level 的 debug，當 k8s cluster 發生問題，運行不如預期或故障時，該怎麼 debug 呢? 以下分成幾個部份來說明。</p><h2 id="檢視-log"><a href="#檢視-log" class="headerlink" title="檢視 log"></a>檢視 log</h2><p>透過 <code>journalctl</code> 指令搭配以下關鍵字，就可以查到各個 component 所對應的 log：</p><ul><li><p>kube-apiserver (@master-node)</p></li><li><p>kube-scheduler (@master-node)</p></li><li><p>kube-controller (@master-node)</p></li><li><p>kube-controller-manager (@master-node)</p></li><li><p>kubelet (@worker-node)</p></li><li><p>kube-proxy (@worker-node)</p></li></ul><h2 id="一般故障狀況"><a href="#一般故障狀況" class="headerlink" title="一般故障狀況"></a>一般故障狀況</h2><p>當某些故障狀況發生時，會對 k8s 中的特定功能造成影響，例如：</p><ul><li><p>API server 關機 or 掛掉時</p><ul><li>對 pod, service, replication controller 等等都無法 start, stop &amp; upgrade</li><li>已經存在的 pod 會繼續工作，除非它們依賴 k8s API</li></ul></li><li><p>當 API server 的 storage 掛掉時</p><ul><li>API server 會無法正常啟動</li><li>其他 node 上的 kubelet 再也無法與 API server 通訊，但會持續保持現存的 pod 的運作 &amp; service proxy 工作</li><li>重新啟動 API server 之前，手動恢復 API server 的狀態是必要的</li></ul></li><li><p>提供 supporting server(node controller, replication controller manager, scheduler, etc) 的 VM 關機 or 掛掉時</p><ul><li>暫時無法與 API server 通訊</li><li>無所謂，因為 persistent state 不在這些 service 上，只要後續有恢復即可</li></ul></li><li><p>worker node 關機</p><ul><li>在該 node 上的 pod 會停止運作，一段時間後會被 reschedule 到其他 node 上</li></ul></li><li><p>kubelet 軟體錯誤</p><ul><li>無法在 node 中啟動新的 pod</li><li>可能會誤刪 pod</li><li>該 kubelet 所在的 node 會被標記為 unhealthy</li><li>replication controller 可能會在其他 node 啟動新的 pod</li></ul></li></ul><p>當然我們也可以做一些預防措施，避免上述情況的發生，例如：</p><ul><li><p>利用 IaaS 平台上，VM 故障自動重新啟動的功能</p></li><li><p>利用 IaaS 平台上所提供的相對可靠的 storage for API server &amp; etcd service</p></li><li><p>將 k8s cluster 佈署成具有 HA 的特性</p></li><li><p>定期對 API server 的 storage 作 snapshot</p></li><li><p>不要直接使用 pod，改用 replication controller + service</p></li><li><p>application 要設計成可以容忍非預期的 restart</p></li><li><p>佈署成多個獨立的 cluster</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/">Application Introspection and Debugging - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/">Core metrics pipeline - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/">Debug Init Containers - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/">Debug Pods and ReplicationControllers - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">Logging Architecture - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/">Troubleshoot Applications - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/">Troubleshoot Clusters - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/">Troubleshooting - Kubernetes</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Troubleshooting </tag>
            
            <tag> CKA Troubleshooting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Persistent Volume (Claim) Overview</title>
      <link href="/blog/Kubernetes/k8s-PersistentVolume-Overview/"/>
      <url>/blog/Kubernetes/k8s-PersistentVolume-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>管理 storage 一直都是一個不簡單的課題，為了讓 developer 可以更專注在開發上，k8s 中提出了兩個概念(resource object)，分別是 <code>Persistent Volume(PV)</code> &amp; <code>Persistent Volume Claim(PVC)</code>，透過這兩個概念將<code>連結 storage</code> 的過程抽象化，讓使用者不需要了解 storage 在底層是如何運作的，只要了解如何使用即可，大幅降低使用者操作 storage 的困難度。</p><p><code>Persistent Volume(PV)</code> 由 storage 管理者負責產生，在 k8s 中就是一種可用的 storage resource，同樣也是一種 volume plugin，但有自己獨立的 lifecycle，且包含的就是實際與 storage 連結的實作細節。</p><blockquote><p>目前 k8s 支援的 PV type 可以參考<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes">此網址</a></p></blockquote><p><code>Persistent Volume Claim(PVC)</code> 則是來自使用者的 storage request，就跟 pod 一樣都是要消耗特定的資源，PVC 消耗 PV 資源(pod 消耗 node 資源)，PVC 指定特定 size or access mode 的 PV(pod 可以指定 CPU, memory … etc)。</p><p>由於 PVC 可以允許使用者自行指定所需要 PV 的相關屬性，因此除了 size, access mode 外，可能還會有 performance 的需求。而為了滿足不同的使用目的，cluster administrator 就必須要準備好不同的 PV 來處理不同 PVC 的需求；若是 PVC 數量不多且需求單純，手動產生 PV 可能還可以接受，但若是 PVC 的數量眾多且需求多變，那可能就需要 <code>[StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/)</code> 的協助了!</p><h1 id="PV-amp-PVC-Interaction-Lifecycle"><a href="#PV-amp-PVC-Interaction-Lifecycle" class="headerlink" title="PV &amp; PVC Interaction Lifecycle"></a>PV &amp; PVC Interaction Lifecycle</h1><p>有了上面對 PV &amp; PVC 的基礎了解後，這個部份要說明從 PV 的產生到 PV &amp; PVC 相互連結，一直到最後結束的這一段 lifecycle，會有以下的事件發生：</p><ul><li><p>Provisioning</p></li><li><p>Binding</p></li><li><p>Using</p></li><li><p>Storage Object in Use Protection</p></li><li><p>Reclaiming</p></li><li><p>Expanding Persistent Volumes Claims</p></li></ul><p>以下就仔細說明每個事件的細節。</p><h2 id="Provisioning"><a href="#Provisioning" class="headerlink" title="Provisioning"></a>Provisioning</h2><p>基本上，產生 PV 的方法有 <code>static</code> &amp; <code>dynamic</code> 兩種，而這兩種分別說明如下：</p><h3 id="Static"><a href="#Static" class="headerlink" title="Static"></a>Static</h3><p>static 很單純，就是 cluster administrator 預先建立幾個 PV，接著使用者在後續新增 PVC 的時候，可以相互 match 並開始使用 storage</p><h3 id="Dynamic"><a href="#Dynamic" class="headerlink" title="Dynamic"></a>Dynamic</h3><p>dynamic 不一樣的地方在於，<strong>是當 k8s 中現存的 PV 沒有符合使用者的 PVC 需求時，會嘗試自動產生符合需求條件的 PV</strong>，而這樣的自動化行為，則必須仰賴 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/" title="Kubernetes Storage Class">StorageClass</a> 來達成。</p><p>但要自動化產生 PV 來使用，必須要完成以下兩個條件：</p><ul><li><p>cluster administrator 必須預先設定好 StorageClass</p></li><li><p>在 PVC 設定中的 <code>spec.storageClassName</code> 必須設定已經存在的 StorageClass</p></li><li><p>若是要設定 default StorageClass，就必須要啟用 <code>DefaultStorageClass</code> admission controller (需要在 API server 的參數 <code>--enable-admission-plugins</code> 上加入 <code>DefaultStorageClass</code>)</p></li></ul><h2 id="Binding"><a href="#Binding" class="headerlink" title="Binding"></a>Binding</h2><p>一般在 PVC 被建立之後，k8s 會自動在 cluster 中尋找是否有合適的 PV 可用(例如：容量, access mode … etc)，若是尋找到就會將兩者連結(bind)起來；反之若是一直找不到合適的 PV，就會一直處在 <code>Unbound</code> 的狀態。</p><p>但如果是透過動態的方式所產生的 PV，就一定會與該 PVC 連結起來，產生出來的 PV 不一定會完全符合 PVC 需求(<strong>可能會超過，但不會低於需求</strong>)，而且只會是個一對一的結果。</p><h2 id="Using"><a href="#Using" class="headerlink" title="Using"></a>Using</h2><p>在上個步驟中，cluster 會協助使用者的 PVC 需求找到相對應的 PV，然後 Pod 則會將 PVC 當作是 volume 一樣使用 &amp; 掛載到特定目錄上。</p><ul><li><p>關於 access mode 的部份，pod 會根據 PVC 中的定義來進行存取行為</p></li><li><p>當 PVC 已經與 PV 連結完成，pod 就可以根據需求一直使用，沒有使用時間的限制</p></li></ul><h2 id="Reclaiming"><a href="#Reclaiming" class="headerlink" title="Reclaiming"></a>Reclaiming</h2><p>當使用者已經不需要 volume 時，就可以將 PVC 從 cluster 刪除，此時與之連結的 PV 就會被釋放，並且可以提供其他 PVC 進行 reclaim，而 PV 中有個 reclaim policy，是用來告訴 cluster 當 PV 被釋放後，要如何處理已經存在的資料，目前支援的選項有 <code>Retained</code>, <code>Recycled</code> &amp; <code>Deleted</code> 三種。</p><h3 id="Retained"><a href="#Retained" class="headerlink" title="Retained"></a>Retained</h3><p>若是 reclaim policy 設定為 Retain，當 PVC 刪除後(此時 PV 狀態為 <code>Released</code>)，cluster administrator 可以進行手動的 reclaim，步驟如下：</p><ol><li><p>刪除 PV (對應的實際資料並不會消失)</p></li><li><p>清除實際資料 (optional, 如果有需求可做)</p></li><li><p>若是使用已經存在的資料，那就再建立一個 PV，並指到原本的儲存空間</p></li></ol><p>完成以上步驟後，就會有一個帶有原本資料且可以使用的 PV 了。</p><h3 id="Deleted"><a href="#Deleted" class="headerlink" title="Deleted"></a>Deleted</h3><p>若是 volume plugin 支援刪除功能(例如：AWS EBS, GCE PD, Azure Disk, or Cinder volume)，使用帶有 reclaim policy <code>Delete</code> 的 PVC，當這個 PVC 被刪除後，相對應的 PV &amp; 實際的 storage 空間也都會一併被刪除。</p><p>若是使用 StorageClass，透過 StorageClass 所產生出來的 PV 都會繼承來自 StorageClass 的設定(若沒有此設定，則預設為 <code>Delete</code>)，因此 cluster administrator 建立 StorageClass 時必須要根據使用者的實際期望來設定。</p><blockquote><p>若是 PV 被建立後，reclaim policy 不是所預期的設定，還是可以透過 edit or patch 的方式修改</p></blockquote><h3 id="Recycled"><a href="#Recycled" class="headerlink" title="Recycled"></a>Recycled</h3><p>reclaim policy <code>Recycled</code> 未來將會被廢棄，若有同樣的需求，建議改用 dynamic provisioning 來取代。</p><h2 id="Expanding-Persistent-Volumes-Claims"><a href="#Expanding-Persistent-Volumes-Claims" class="headerlink" title="Expanding Persistent Volumes Claims"></a>Expanding Persistent Volumes Claims</h2><p>當 volume 容量不足時，擴充就變成了另一個需要面臨的問題，目前在 k8s 中，PVC 的擴充已經是預設支援的功能，而支援 PVC 擴充的 volume type 多屬於 public cloud，而 private cloud 可用的選項大概有：</p><ul><li><p>OpenStack Cinder</p></li><li><p>Ceph RBD</p></li><li><p>GlusterFS</p></li><li><p>Portworx</p></li></ul><p>但這功能也不是完全沒有限制，除了要上述的 volume type 之外，<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/" title="Kubernetes Storage Class">Storage Class</a> 中也要預先有 <code>allowVolumeExpansion: true</code> 的定義。</p><p>而要擴充的方法也很簡單，透過 <code>kubectl edit pvc/xxxx</code> 的方式，設定個較大的 size value 後存檔即可，而這個行為不會產生新的 PV，而是對已經連結的 PV 進行擴充。</p><h3 id="當-Volume-中包含檔案系統"><a href="#當-Volume-中包含檔案系統" class="headerlink" title="當 Volume 中包含檔案系統"></a>當 Volume 中包含檔案系統</h3><p>若 PV 的源頭屬於 block device，因此就會包含一個使用者所需要的檔案系統，可能是 Ext4 or XFS，但在這情況下要進行容量擴充就會有限制：</p><ul><li><p>目前僅有 XFS, EXt3, Ext4 的檔案系統可以進行擴充</p></li><li><p>Pod 必須要以 ReadWrite mode 掛載</p></li><li><p>要擴充前必須將 pod 先移除，等待 volume 擴充完畢再重新建立 &amp; 掛載</p></li></ul><blockquote><p>當 PVC status 變成 <code>FileSystemResizePending</code> 後就可以重新建立 pod 了</p></blockquote><h3 id="擴充使用中的-PVC"><a href="#擴充使用中的-PVC" class="headerlink" title="擴充使用中的 PVC"></a>擴充使用中的 PVC</h3><p>這功能目前還屬於 alpha 階段，使用上則會有以下幾點需要注意：</p><ul><li><p>需要開啟 <code>ExpandInUsePersistentVolumes</code> feature gate</p></li><li><p>由於是擴充”<strong>使用中</strong>“的 PVC，因此 PVC 必須要被某個 pod 掛載才行</p></li></ul><h1 id="Persistent-Volumes"><a href="#Persistent-Volumes" class="headerlink" title="Persistent Volumes"></a>Persistent Volumes</h1><p>要了解 PV 的特性，可以從簡單的定義檔開始：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv0003</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">slow</span></span><br><span class="line">  <span class="attr">mountOptions:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">hard</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">nfsvers=4.1</span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/tmp</span></span><br><span class="line">    <span class="attr">server:</span> <span class="number">172.17</span><span class="number">.0</span><span class="number">.2</span></span><br></pre></td></tr></table></figure><p>上面是一個簡單使用 NFS 的 PV 定義檔，但卻可以從裏面得到很多資訊。</p><h2 id="Capacity"><a href="#Capacity" class="headerlink" title="Capacity"></a>Capacity</h2><p>PV 都會有一個 <code>.spec.capacity</code> 的屬性，用來告訴使用者這個 PV 有多少容量可以使用，可用 Ei, Pi, Ti, Gi, Mi, Ki … 等來標記。</p><blockquote><p>目前只有 capacity 可作為 resource request 的依據，未來可能還會增加 IOPS, throughput …. 等等</p></blockquote><h2 id="Volume-Mode"><a href="#Volume-Mode" class="headerlink" title="Volume Mode"></a>Volume Mode</h2><p>在 v1.9 之前，所有的 volume plugin 都會自動在 PV 上建立 file system，但目前(v1.13)已經可以透過設定 <code>.spec.volumeMode: Block</code> 的方式來將 PV 作為 raw block device 使用。</p><blockquote><p><code>Filesystem</code> 是預設值，而 <code>Block</code> 的功能目前(v1.13) 依然屬於 beta</p></blockquote><h2 id="Access-Modes"><a href="#Access-Modes" class="headerlink" title="Access Modes"></a>Access Modes</h2><p>PV 可以透過 <code>ReadWriteOnce</code>(一個 node 可 read-write，可縮寫為 <code>RWO</code>), <code>ReadOnlyMany</code>(一個 node 可 write，多個 node 可 read-only，可縮寫為 <code>ROX</code>), <code>ReadWriteMany</code>(多個 node 可 read-write，可縮寫為 <code>RWX</code>) 三種存取模式提供掛載，但存取控制機制本身並非由 k8s 支援，而是由 PV resource provider 支援（例如上面的例子為 NFS，就可以支援 <code>ReadWriteMany</code>，但 Ceph RBD 則不行）。</p><p>此外，雖然 NFS 可以提供 <code>ReadWriteMany</code> access mode，但在 NFS share 本身的設定就有可能只是 RO，因此在 PV 上還是會有 Access Mode 的設定，主要的用意就是要用來告訴使用者目前這個 PV 所支援的 access mode 為何。</p><blockquote><p>需要注意的是，每個 PV 掛載時只能使用一種 access mode，而每種 volume 支援的 access mode 可以參考<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">官網列表</a></p></blockquote><h2 id="Class"><a href="#Class" class="headerlink" title="Class"></a>Class</h2><p>PV 可以擁有 class 的屬性，可透過 <code>spec.storageClassName</code> 來設定；若 PV 設定了 class，那就只能讓帶有相同 class request 的 PVC 連結；反之亦然。</p><h2 id="Reclaim-Policy"><a href="#Reclaim-Policy" class="headerlink" title="Reclaim Policy"></a>Reclaim Policy</h2><p>透過 <code>spec.persistentVolumeReclaimPolicy</code> 來設定，上面已經有提到，有 <code>Retain</code>, <code>Recycle</code>, <code>Delete</code> 三種可以設定。</p><h2 id="Mount-Options"><a href="#Mount-Options" class="headerlink" title="Mount Options"></a>Mount Options</h2><p>k8s cluster 管理者可以在建立 PV 時，指定額外的 mount option，並在 volume 實際被掛載時套用；但需要注意的是，這功能並不是所有的 volume type 都支援，而且每個 volume type 可用的 mount option 也不盡相同，若是在不支援 mount option 的 volume type 中使用 mount option，就會無法掛載成功。</p><p>若要知道哪些 volume type 支援 mount option，可以參考<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#mount-options">官網說明列表</a>。</p><h2 id="Node-Affinity"><a href="#Node-Affinity" class="headerlink" title="Node Affinity"></a>Node Affinity</h2><p>在 PV 中可以設定 Node Affinity，目的在於限定特定的 node 才可以用來存取 volume，而使用帶有 Node Affinity 設定的 PV 的 pod，就僅會被分派到那些符合條件的 node 上面去。</p><h2 id="Phase"><a href="#Phase" class="headerlink" title="Phase"></a>Phase</h2><p>這個部份其實就類似 PV 的生命周期了，共包含4個階段：</p><ul><li><p><code>Available</code>: 可被 PVC 使用的狀態</p></li><li><p><code>Bound</code>: 已經連結到特定的 PVC</p></li><li><p><code>Released</code>: PVC 已經被刪除，但 cluster 尚未進行 reclaim 的工作</p></li><li><p><code>Failed</code>: reclaim 失敗後會進入的狀態</p></li></ul><h1 id="Persistent-Volume-Claim"><a href="#Persistent-Volume-Claim" class="headerlink" title="Persistent Volume Claim"></a>Persistent Volume Claim</h1><p>每一個 PVC 都包含了 spec 相關資訊，k8s 就是利用這些資訊協助尋找合適的 PV 並執行掛載工作，以下是一個 PVC 的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myclaim</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">8Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">slow</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">release:</span> <span class="string">&quot;stable&quot;</span></span><br><span class="line">    <span class="attr">matchExpressions:</span></span><br><span class="line">      <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">environment</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">dev</span>]&#125;</span><br></pre></td></tr></table></figure><h2 id="Access-Modes-1"><a href="#Access-Modes-1" class="headerlink" title="Access Modes"></a>Access Modes</h2><p>透過 <code>spec.accessModes</code> 設定，Access Mode 就跟 PV 的 Access Mode 相同，共有 <code>ReadWriteOnce</code>(<code>RWO</code>), <code>ReadOnlyMany</code>(<code>ROX</code>), <code>ReadWriteMany</code>(<code>RWX</code>) 三種。</p><h2 id="Volume-Modes"><a href="#Volume-Modes" class="headerlink" title="Volume Modes"></a>Volume Modes</h2><p>透過 <code>spec.volumeMode</code> 設定，跟 PV 相同，volume mode 可以設定為 file system(<code>Filesystem</code>) 或是 raw block device(<code>Block</code>)。</p><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><p>透過 <code>spec.resources</code> 設定，目前關於 storage 中可以設定的僅有 size 一項，以後可能還會支援 IOPS 或其他選項。</p><h2 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h2><p>透過 <code>spec.selector</code> 設定，就是透過 k8s 中的 label 機制，選擇合適的 PV 來繫結，可使用 <code>matchLabels</code> 做精準的配對，也可以透過 <code>matchExpressions</code> 作範圍性的配對。</p><h2 id="Class-1"><a href="#Class-1" class="headerlink" title="Class"></a>Class</h2><p>透過 <code>spec.storageClassName</code> 設定，可用來指定已經存在的 storage class，因此若是設定了 storageClassName，就只有由指定的 storage class 所產生出來的 PV，才可以被此 PVC 繫結。</p><p>而 <code>spec.storageClassName</code> 若設定為空白，k8s 則會透過 label or resource request 來協助找到合適的 PV；但若是 k8s 管理者是將 <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass">DefaultStorageClass</a> admission plugin 開啟，則規則就會變得比較複雜些：</p><ul><li><p>若管理者有設定 <strong>DefaultStorageClass admission plugin</strong>，PVC 就只能與透過 DefaultStorageClass 產生的 PV 做繫結</p></li><li><p>在既有的 storage class 中設定 annotation <code>storageclass.kubernetes.io/is-default-class: true</code> 的效果也等同於開啟 DefaultStorageClass admission plugin</p></li><li><p>若是有多個 storage class 都有設定 annotation <code>storageclass.kubernetes.io/is-default-class: true</code>，則 admission control 會禁止建立新的 PVC</p></li><li><p>若沒開啟 <strong>DefaultStorageClass admission plugin</strong> 也沒設定 annotation <code>storageclass.kubernetes.io/is-default-class: true</code>，則沒有設定 <code>spec.storageClassName</code>(設定空白字串也是相同意思) 的 PVC 就只能與沒有 class 資訊的 PV 繫結</p></li><li><p>若 PVC 同時設定了 selector &amp; storageClassName，就只有同時符合兩個條件的 PV 可以與之繫結</p></li></ul><h1 id="Pod-如何使用-PV"><a href="#Pod-如何使用-PV" class="headerlink" title="Pod 如何使用 PV ?"></a>Pod 如何使用 PV ?</h1><p>有了上面 PV &amp; PVC 的概念後，那 pod 要如何使用 PV 呢? 答案是<strong>必須透過 PVC，並將 PVC 當作 volume 使用</strong>，以下是一個簡單的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mypod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">myfrontend</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="comment"># 指定掛載的目錄</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">&quot;/var/www/html&quot;</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">mypd</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mypd</span></span><br><span class="line">      <span class="comment"># 掛載 PVC &quot;myclaim&quot;</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">myclaim</span></span><br></pre></td></tr></table></figure><p>因此，正確的繫結關係應該是 <code>Pod &lt;--&gt; PVC &lt;--&gt; PV</code>，所以已經成功與 PV 繫結的 PVC 是必要的。</p><p>此外，還有幾點觀念必須知道的：</p><ul><li><p>PVC 屬於 namespace level resource，因此 Pod 只能與同一個 namespace 中的 PVC 進行繫結 </p></li><li><p>PV 屬於 cluster level resource，但若要支援 <code>ROX</code>(ReadOnlyMany) or <code>RWX</code>(ReadWriteMany)，就只限於同一個 namespace 中的 PVC 才可以</p></li></ul><h1 id="使用-raw-Block-Volume"><a href="#使用-raw-Block-Volume" class="headerlink" title="使用 raw Block Volume"></a>使用 raw Block Volume</h1><p>從 v1.13 開始以 beta 的形式支援 raw block volume，目前支援的 storage 還不是很多，詳細的清單可以查詢<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support">官網列表</a>。</p><p>以下是一個使用 fiber channel block volume 的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PV 定義</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">block-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="comment"># 指定 volume mode 為 &quot;Block&quot;</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Block</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="comment"># 以下是 fiber channel 連線相關資訊</span></span><br><span class="line">  <span class="attr">fc:</span></span><br><span class="line">    <span class="attr">targetWWNs:</span> [<span class="string">&quot;50060e801049cfd1&quot;</span>]</span><br><span class="line">    <span class="attr">lun:</span> <span class="number">0</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># PVC 定義</span></span><br><span class="line"><span class="comment"># 系統會自動將此 PVC 與上面的 PV(name=&#x27;block-pv&#x27;) 進行繫結</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">block-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Block</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Pod 定義</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-with-block-volume</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">fc-container</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">fedora:26</span></span><br><span class="line">      <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>]</span><br><span class="line">      <span class="attr">args:</span> [ <span class="string">&quot;tail -f /dev/null&quot;</span> ]</span><br><span class="line">      <span class="comment"># 將 block volume 掛載為 /dev/xvda</span></span><br><span class="line">      <span class="comment"># 因為是 block volume，因此要改成指定 devicePath</span></span><br><span class="line">      <span class="attr">volumeDevices:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">          <span class="attr">devicePath:</span> <span class="string">/dev/xvda</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">block-pvc</span></span><br></pre></td></tr></table></figure><h1 id="如何撰寫可攜性高的-Storage-Configuration"><a href="#如何撰寫可攜性高的-Storage-Configuration" class="headerlink" title="如何撰寫可攜性高的 Storage Configuration ?"></a>如何撰寫可攜性高的 Storage Configuration ?</h1><p>搞清楚上面 Pod, PVC, PV 相關概念後，若是真有弄清楚 k8s storage 的設計概念，就會大概有以下的概念：</p><ul><li><p>並非每個 k8s cluster 都會有相同的 storage 設定</p></li><li><p>承上，這也是 storage class 會被設計出來的原因</p></li></ul><p>因此要撰寫可攜性高的設定也就不是什麼太困難的事情，只要大概遵守以下幾個原則即可：</p><ul><li><p>不要使用 volume，改用 PVC</p></li><li><p>不要包含 PV 的設定，因為在其他的 k8s cluster 很有可能會無法建立相同的 PV</p></li><li><p>給使用者指定 storage class (k8s 使用者應該知道有什麼 storage class 可用)，讓 PVC 使用 storage class</p></li><li><p>若使用者沒指定 storage class，那就應該會有 default storage class 的相關設定</p></li><li><p>若 PVC 一直處於無法與任何 PV 繫結的狀況，那就需要詢問 k8s cluster 管理者</p></li></ul><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>前一篇文章提到了 volume 的概念，在 k8s 中也同時支援了相當多的 cloud volume；而這裡指的 cloud volume 像是 <code>awsElasticBlockStore</code>, <code>gcePersistentDisk</code>…等，使用 clouod volume 的缺點在於使用者必須知道這些 volume 的設定細節，例如：volume ID, file system type…等資訊，在某些時候還是挺麻煩的，因此才會有 <code>persistent volume claim(PVC)</code> 這樣的 resource object 來抽象 &amp; 隱藏這些設定的細節。</p><p><img src="/blog/images/kubernetes/k8s_volume-pvc.png" alt="Multiple Containers in a Pod"></p><p>從上圖可以看出，在使用者使用 PVC 來取得 volume 的架構下，storage 管理者可以先準備好所謂的 <code>persistent volume pool</code>，而這個 volume pool 可以包含各式各樣的 volume(可來自 local or clouod)，而這些 volume 都會再被抽象一層變成 <code>persistent volume</code>，並賦予相關的屬性(例如：<strong>accessModes</strong>, <strong>volume size</strong>, <strong>tag</strong>)。</p><p>有了多個 PV 後，使用者就可以定義 PVC，並指定所需要的 PV 屬性為何，k8s 就會自動將配對成功的 PV 與 PVC 繫結在一起，並掛載到使用者所需要的 mount point 上，整個過程使用者完全不需要知道到底 volume 是從哪裡來，底層 storage 的細節也都完全不需要知道。</p><p>而這些抽象的設計，最終的目的就是在於要<strong>讓使用者可以專注在開發上，而非 infra 的設定上</strong>。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass">DefaultStorageClass Admission Controller</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Storage </tag>
            
            <tag> CKA Storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Volume Overview</title>
      <link href="/blog/Kubernetes/k8s-Volume-Overview/"/>
      <url>/blog/Kubernetes/k8s-Volume-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>使用 container 的開發者都知道，在 container 中的檔案不是永久存在的，隨著 container 的重啟，檔案就會隨之消失，當然這情況在 k8s pod 中也是相同；此外，在 k8s pod 中的多個 container 之間時常也會有檔案共享的需求，而為了解決這些問題，k8s 中提出了 <code>Volume</code> 這個抽象概念。</p><p>因此回到系統本質來看，<code>Volume</code> 的出現就是為了解決以下的問題：</p><ul><li><p>檔案在 container 中無法永續存活</p></li><li><p>pod 內多個 container 之間有檔案共享的需求</p></li></ul><p>回到 container 來看，其實 Docker 也有 <a href="https://docs.docker.com/storage/volumes/">Volume</a> 的概念，相較於 k8s Volume 比較沒那麼著重在管理層面。在一開始 Docker Volume 僅支援 local directory，到最近才慢慢開始有 <a href="https://docs.docker.com/engine/extend/legacy_plugins/#types-of-plugins">Volume plugin</a> 來支援多種不同的 storage。</p><blockquote><p>其實 volume plugin 的功能也不算是很完整 (例如：在 Docker v1.17 時，使用 volume plugin 時還無法傳入任何參數)</p></blockquote><p>反之 k8s 對於 volume 的管理就相對嚴謹：</p><ul><li><p>volume lifetime 與 pod lifetime 相同 (會一直存活到 pod 中的 container 全部消失為止)</p></li><li><p>只要 pod 存在，資料就不會因為 container restart 而消失</p></li><li><p>基本上 volume 會以 directory 的型式存在於 pod 中，pod 中的每一個 container 都可以存取 (至於 directory 如何產生、資料實際存在哪裡則是由 volume type 決定，k8s 不會負責管理這個)</p></li><li><p>在 pod 中要使用 volume 就必須使用 <code>.spec.volumes</code> &amp; <code>.spec.containers.volumeMounts</code> 兩個欄位進行設定</p></li><li><p>一個 volume 僅能給一個 pod 使用</p></li></ul><h1 id="Kubernetes-支援那些-Volume-Type"><a href="#Kubernetes-支援那些-Volume-Type" class="headerlink" title="Kubernetes 支援那些 Volume Type ?"></a>Kubernetes 支援那些 Volume Type ?</h1><p>k8s 現有支援的 volume 相當的多，有些來自 public cloud(例如：awsElasticBlockStore, azureDisk, gcePersistentDisk … 等等)，也有些是屬於 on-premise 的(例如：glusterfs, nfs, iscsi … 等等)，詳細列表可以參考<a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes">官網連結</a>。</p><p>以下就比較常會在 on-premise 環境碰到的 volume type 作個簡單說明：</p><h2 id="emptyDir"><a href="#emptyDir" class="headerlink" title="emptyDir"></a>emptyDir</h2><p>emptyDir 在使用上有以下幾點特性：</p><ul><li><p>emptyDir 會在 pod 被分配到特定的 node 後產生，一開始永遠是一個空的目錄</p></li><li><p>emptyDir 的 life cycle 是跟著 pod + node 一起走，只要該 pod 持續的在原本的 node 上運行，emptyDir 就會保留著；相反的，只要兩個條件有任一消失，emptyDir 的資料也會跟著消失</p></li><li><p>若是有資料存進去 emptyDir，node 會把資料放在 RAM(設定 <code>emptyDir.medium</code> 為 <code>Memory</code>，emptyDir 會以 tmpfs 的方式提供) or local SSD/HD 上</p></li><li><p>存放在 SSD or HD，取決於 <code>/var/lib/kubelet</code> 目錄位於哪種硬碟上 (也就是說 emptyDir 會存放在 <code>/var/lib/kubelet</code> 目錄中)</p></li><li><p>每個在 pod 中的 container，可以將 emptyDir 掛載在不同的路徑</p></li><li><p>目前無法限制使用量</p></li></ul><p>在官網中有提到一些 emptyDir 的 use case：</p><ul><li><p>scratch space, such as for a disk-based merge sort</p></li><li><p>checkpointing a long computation for recovery from crashes</p></li><li><p>holding files that a content-manager Container fetches while a webserver Container serves the data</p></li></ul><p>由於這幾項 use case 我並沒有接觸過，因此就不太多著墨了。</p><h2 id="hostPath"><a href="#hostPath" class="headerlink" title="hostPath"></a>hostPath</h2><p>若是希望 volume 新增後裏面就存放一些 node 上已經存在的資料，可以使用 hostPath。</p><p>通常這不是一般 pod 會用的 volume type，只有在一些情況下例外，例如：</p><ul><li><p>container 需要監控 host Docker 的內部運作細節，會需要掛載 host <code>/var/lib/docker</code> 資料夾</p></li><li><p>container 中執行 cAdvisor，需要掛在 host <code>/sys</code> 目錄</p></li><li><p>init container，用來產生另一個 container 需要的 host path</p></li></ul><p>此外，hostPath 還細分成好幾種不同的類型。分別介紹如下：</p><table><thead><tr><th>類型</th><th>運作行為</th></tr></thead><tbody><tr><td><code>(空字串)</code></td><td>掛載 host path 到 pod 之前，不會做任何檢查</td></tr><tr><td><code>DirectoryOrCreate</code></td><td>若 host path 指定的目錄不存在，則會新建一個目錄，並設定為權限 0755，目錄的 ownership &amp; group 則是 kubelet</td></tr><tr><td><code>Directory</code></td><td>host path 指定的目錄必須存在</td></tr><tr><td><code>FileOrCreate</code></td><td>若 host path 指定的檔案不存在，則會新建一個檔案，並設定為權限 0755，目錄的 ownership &amp; group 則是 kubelet</td></tr><tr><td><code>File</code></td><td>host path 指定的檔案必須存在</td></tr><tr><td><code>Socket</code></td><td>host path 指定的 UNIX socket 必須存在</td></tr><tr><td><code>CharDevice</code></td><td>host path 指定的 character device 必須存在</td></tr><tr><td><code>BlockDevice</code></td><td>host path 指定的 block device 必須存在</td></tr></tbody></table><blockquote><p>hostPath volume 的內容僅能被 root 修改，因此若有修改需求的話，就必須把 container 以 privileged mode 執行，或是修改 hostPath volume 的權限</p></blockquote><p>以下是個 hostPath 使用範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-pd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">k8s.gcr.io/test-webserver</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test-container</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/test-pd</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">test-volume</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-volume</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="comment"># directory location on host</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/data</span></span><br><span class="line">      <span class="comment"># this field is optional</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Directory</span></span><br></pre></td></tr></table></figure><blockquote><p>目前 hostPath 無法限制使用量，因此在使用時可能必須注意磁碟空間的問題</p></blockquote><h2 id="local"><a href="#local" class="headerlink" title="local"></a>local</h2><p><code>local</code> volume type 在 v1.10 時變成 beta 版本，主要用來表示在 host 上掛載的 local storage device，例如：disk, partition, directory … 等等。</p><p>相較於 hostPath，<code>local</code> volume type 有以下幾點的不同：</p><ul><li><p>只能以 PersistentVolume 的方式被使用，尚不支援 dynamic provisioning</p></li><li><p>由於 <code>local</code> 可以額外設定 <strong>nodeAffinity</strong>，因此 k8s scheduler 會協助挑選合適的 node 來使用</p></li></ul><p><code>local</code> volume type 看似比 <code>hostPath</code> 更有彈性些，但使用上還是有些地方必須注意，例如：</p><ul><li><p>local volume 的可用性依然是跟著 node 走，因為不一定每個 node 都有相同的 disk or partition 配置</p></li><li><p>如果 node 的狀態轉變成 unhealthy，同時也會造成 local volume 無法使用(因為資料就是存在於該 unhealthy node 上)，進而造成掛載 local volume 的 pod 也無法存取資料</p></li><li><p>承上，若 application 只能使用 local volume 的話，那就要考量到資料無法正常存取時的容錯設計</p></li></ul><p>以下是個設定 local volume 的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">100Gi</span></span><br><span class="line">  <span class="comment"># 若要額外設定 volumeMode 這個欄位</span></span><br><span class="line">  <span class="comment"># 就必須要開啟 feature gate &quot;BlockVolume&quot;</span></span><br><span class="line">  <span class="comment"># 除了 Filesystem 之外，也可以設定為 &quot;Block&quot;(預設為 &quot;Filesystem&quot;)</span></span><br><span class="line">  <span class="comment"># 來指定使用 raw block device</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">local-storage</span></span><br><span class="line">  <span class="attr">local:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/mnt/disks/ssd1</span></span><br><span class="line">  <span class="comment"># nodeAffinity 為必要欄位</span></span><br><span class="line">  <span class="comment"># 目的是為了讓 k8s scheduler 可以協助找到合適的 node</span></span><br><span class="line">  <span class="attr">nodeAffinity:</span></span><br><span class="line">    <span class="attr">required:</span></span><br><span class="line">      <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">example-node</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-storage</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/no-provisioner</span></span><br><span class="line"><span class="attr">volumeBindingMode:</span> <span class="string">WaitForFirstConsumer</span></span><br></pre></td></tr></table></figure><p>從上面的範例可以看出，local volume 是由 Storageclass 所產生，透過 <code>volumeBindingMode: WaitForFirstConsumer</code> 的設定，讓 PV 會在 PVC 需求實際發生(可能透過 node selector, pod affinity, pod anti-affinity … 等設定)時才會產生。</p><h2 id="nfs"><a href="#nfs" class="headerlink" title="nfs"></a>nfs</h2><p>NFS volume type 就是利用外部的 NFS share(這部份要自己準備好，k8s 不負責這個) 來存放資料，有以下幾種特性：</p><ul><li><p>跟 emptyDir 不同，NFS volume 中的資料不會因為 pod 被刪除而一併被刪除</p></li><li><p>可以預先把資料存放在 NFS volume 後再掛載</p></li><li><p>NFS volume 可以同時被多個 pod 掛載 &amp; 寫入，因此可以利用此特性來同時分享資料給多個 pod</p></li></ul><h2 id="secret"><a href="#secret" class="headerlink" title="secret"></a>secret</h2><p>secret 顧名思義就是存放機密性的資料，而 secret volume 有以下幾個特點：</p><ul><li><p>要使用 secret volume 必須先定義 secret resource object 後，再掛載為 secret volume</p></li><li><p>secret volume 會存在於 <code>/tmpfs</code> 中，也就是 RAM 中，不會存在於硬碟上</p></li><li><p>若透過 <a href="https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath">subPath</a> 掛載 secret volume，若是後續 secret volume 有更新，就不會反應到該 subPath 上</p></li></ul><p>而 secret 的詳細使用方式可以參考<a href="https://kubernetes.io/docs/concepts/configuration/secret/">此篇文章</a>。</p><h2 id="configMap"><a href="#configMap" class="headerlink" title="configMap"></a>configMap</h2><p>k8s 透過 configMap，提供使用者一個可以將任何設定相關資料放到 pod 中的方式，透過 <code>configMap</code> 關鍵字就可以將 configMap resource object 以 volume 的型式掛載到 pod 中使用。</p><p>以下是一個 <strong>configMap -&gt; File</strong> 簡單的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">special-config</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">special.level:</span> <span class="string">very</span></span><br><span class="line">  <span class="attr">special.type:</span> <span class="string">charm</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">dapi-test-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-container</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">k8s.gcr.io/busybox</span></span><br><span class="line">      <span class="comment"># 在 /etc/config 目錄中就會看到兩個檔案</span></span><br><span class="line">      <span class="comment"># /etc/config/special.level =&gt; very</span></span><br><span class="line">      <span class="comment"># /etc/config/special.type =&gt; charm</span></span><br><span class="line">      <span class="attr">command:</span> [ <span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;ls /etc/config/&quot;</span> ]</span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">/etc/config</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-volume</span></span><br><span class="line">      <span class="attr">configMap:</span></span><br><span class="line">        <span class="comment"># 指定要掛載的 configMap 名稱</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">special-config</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure><p>然而 configMap 掛載到 pod 之後不一定只能以 file 的型式存在，也可以是以環境變數(Environment Variable)的型式存在，詳細的操作可以參考<a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">官網文件</a>。</p><h1 id="使用-subPath"><a href="#使用-subPath" class="headerlink" title="使用 subPath"></a>使用 subPath</h1><p>有時候我們要掛載的目錄中可能包含了多個子目錄，而這些子目錄恰巧又分別被多個不同的 container 使用，此時就可以透過 <code>subPath</code> 的方式來簡化 volume 的設定，以下有個簡單的 LAMP 範例，在這個範例的 volume 中有兩個目錄，分別是 <code>mysql</code> &amp; <code>html</code>，然後透過 subPath 的方式分別掛載到 pod 中的不同 container 上：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-lamp-site</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">    <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">mysql</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MYSQL_ROOT_PASSWORD</span></span><br><span class="line">        <span class="attr">value:</span> <span class="string">&quot;rootpasswd&quot;</span></span><br><span class="line">      <span class="comment"># 掛載 volume 中的 &quot;mysql&quot; subPath</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/lib/mysql</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">        <span class="attr">subPath:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">php</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">php:7.0-apache</span></span><br><span class="line">      <span class="comment"># 掛載 volume 中的 &quot;html&quot; subPath</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/var/www/html</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">        <span class="attr">subPath:</span> <span class="string">html</span></span><br><span class="line">    <span class="comment"># 包含多個目錄(mysql &amp; html)的 volume 設定</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">site-data</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">my-lamp-site-data</span></span><br></pre></td></tr></table></figure><h2 id="搭配-Expanded-Environment-Variables"><a href="#搭配-Expanded-Environment-Variables" class="headerlink" title="搭配 Expanded Environment Variables"></a>搭配 Expanded Environment Variables</h2><p>透過 Expanded Environment Variables　的搭配，可以在 YAML 定義檔中，使用與 pod 相關的識別資訊，可以讓 subPath 在使用上更加的彈性，<br>以下是一個簡單的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">container1</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">POD_NAME</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> [ <span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;while [ true ]; do echo &#x27;Hello&#x27;; sleep 10; done | tee -a /logs/hello.txt&quot;</span> ]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir1</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/logs</span></span><br><span class="line">      <span class="attr">subPath:</span> <span class="string">$(POD_NAME)</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">workdir1</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/log/pods</span></span><br></pre></td></tr></table></figure><p>透過 <code>$(POD_NAME)</code> 讓不同的 pod 的 log 可以存放在不同的目錄中，並以 pod name 作為區別，藉此可以提升 debug 的困難度。</p><h1 id="Out-of-Tree-Volume-Plugins"><a href="#Out-of-Tree-Volume-Plugins" class="headerlink" title="Out-of-Tree Volume Plugins"></a>Out-of-Tree Volume Plugins</h1><p><a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes">k8s 支援了相當多的 volume type</a>，市面上常看到的 storage 基本上都包含了，而使用這些 storage 所需要的程式碼都被整合到 k8s 的 code repository 中，這表示都是經過驗證，使用者不需要安裝任何額外的 plguin 就可以使用的，但程式碼要進入到 k8s code repository 也代表的要進行嚴格的審查。</p><p>但若是其他的 storage 廠商也希望可以跟 k8s 整合時該怎麼做呢? 答案就是透過 <code>Container Storage Interface (CSI)</code> &amp; <code>Flexvolume</code> 這兩種 volume plugin。</p><p>透過 CSI &amp; Flexvolume，storage 廠商可以自行開發 volume plugin 而不需要經過 k8s 社群的審查(不代表不嚴謹)，可以獨立的開發而不受影響，並佈署在 k8s 上成為一個擴充套件來使用，而關於 out-of-tree volume plugin 如何使用，可以參考<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/flexvolume.md">官網文件</a>。</p><h2 id="FlexVolume"><a href="#FlexVolume" class="headerlink" title="FlexVolume"></a>FlexVolume</h2><p>從 v1.2 版開始，k8s 提供了 FlexVolume 的方式讓 storage 廠商可以自行開發可以與 k8s 整合的 volume plugin(driver)。k8s 會以 exec-based 的方式與 volume driver 溝通，因此相對應的 FlexVolume driver binary 就要預先放到每個 node 預定定義好的目錄下。</p><p>而 FlexVolume 在使用上也會有些限制與問題存在，例如：</p><ul><li><p>FlexVolume 為了要安裝 driver 相關檔案到每個 node 上，需要 root 存取權限</p></li><li><p>FlexVolume driver 的相依性套件也要預先準備在每個 node 上，%這代表著也要 root 權限來安裝</p></li></ul><p>因此現在 k8s 社群建議使用 CSI，而非 FlexVolume。</p><h2 id="Container-Storage-Interface-CSI"><a href="#Container-Storage-Interface-CSI" class="headerlink" title="Container Storage Interface (CSI)"></a>Container Storage Interface (CSI)</h2><p>顧名思義，<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md">CSI</a> 就是為了 container orchestration systems(例如：k8s，但不僅限於 k8s) 定義了一系列的標準 interface，讓 container 可以透過這些 interface 與 storage 互動。</p><p>而 CSI 在 k8s 的支援上，從 v1.9 為 alpha，v1.10 為 beta，最後到 v1.13 正式 GA。</p><p>關於 CSI 的使用細節，由於目前還沒使用過，所以這個部份就先跳過，但還是有些額外的資訊需要注意：</p><ul><li><p>k8s v1.13 後開始要廢除 CSI spec 0.2 &amp; 0.3 版本的支援</p></li><li><p>CSI driver 不一定會與所有的 k8s 版本相容，使用前要注意一下</p></li><li><p>從 k8s v1.11 開始，CSI 開始支援 <strong>raw block volume</strong>，目前為 <code>alpha</code>，需要額外開啟 <code>BlockVolume</code> &amp; <code>CSIBlockVolume</code> 兩個 feature gate</p></li><li><p>無法在 pod 定義中直接使用 CSI volume，必須透過 <code>PersistentVolumeClaim</code> 才可以使用</p></li><li><p>若要自行開發 CSI plugin，可以參考 <a href="https://kubernetes-csi.github.io/docs/">Kubernetes CSI Developer Reference</a></p></li></ul><h1 id="Mount-Propagation"><a href="#Mount-Propagation" class="headerlink" title="Mount Propagation"></a>Mount Propagation</h1><p>Mount Propagation 允許可以將掛載在某個 container 中的 volume 分享給在同一個 pod 中的 container，甚至是該 pod 所在的 node 的一種特殊功能。</p><p>要使用 Mount Propagation 功能，必須在 <code>Container.volume.mounts</code> 中加上 <code>propagation</code> 參數，而可用的選項有以下幾種：</p><ul><li><p><code>None</code>：已經掛載到 container 的 volume，即使從 host 端在其子目錄上加上其他 mount，container 也不會收到；在 container 中建立的 mount 也不會讓 host 看到(<strong>default mode</strong>，等同於 Linux 中的 <code>private</code> mount propagation)</p></li><li><p><code>HostToContainer</code>：與上面較為不同，container 可以收到 host 後續在子目錄加上的 mount；但在 container 中建立的 mount 不會讓 host 看到(等同於 Linux 中的 <code>rslave</code> mount propagation)</p></li><li><p><code>Bidirectional</code>：除了有 <code>HostToContainer</code> 的行為之外，在 container 中掛載的其他內容也會在 host 上出現 (等同於 Linux 中的 <code>rshared</code> mount propagation)</p></li></ul><p><code>Bidirectional</code> 模式在使用上其實蠻危險的，因此僅限於 Priviledged Container 中使用，此外在 container 中建立的任何 volume 都必須在 container 結束時一併移除。</p><p>最後，若真的要使用到 Mount Propagation 的功能，必須在 Docker 中加上 <code>MountFlags=shared</code> 設定，並重新啟動 Docker service。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>在 k8s 中，透過將 volume 抽離出 container 成為一個獨立的 resource object，透過此方式 <strong>volume state</strong> 可以被保存，即使分別由不同的 container 存取，而存在 volume 裡面的資料也就跟著 volume 本身的 life cycle 一起走。</p><p>藉由以上方式，讓開發者與 storage 管理者可以分開管理各自負責的領域，最終的目的就是要讓開發者可以專注在開發上，而非 infrastucture 本身。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes - Kubernetes</a></p></li><li><p><a href="https://www.youtube.com/watch?v=VB7vI9OT-WQ">(39) Kubernetes Volumes 1: emptydir, NFS, YAML, volumes, and intro to Persistent Volume Claims - YouTube</a></p></li><li><p><a href="https://www.youtube.com/watch?v=OulmwTYTauI">(39) Kubernetes Volumes 2: Understanding Persistent Volume (PV) and Persistent Volume Claim (PVC) - YouTube</a></p></li><li><p><a href="https://www.youtube.com/watch?v=X6Vkz-ny574">(39) Kubernetes Volumes 3: How things connect - YouTube</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#local">Storage Classes - Kubernetes</a></p></li><li><p><a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">kubernetes-sigs/sig-storage-local-static-provisioner: Static provisioner of local volumes</a></p></li><li><p><a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md">community/volume-plugin-faq.md at master · kubernetes/community</a></p></li><li><p><a href="https://segmentfault.com/a/1190000016567617">談談k8s1.12新特性–Mount propagation(掛載命名空間的傳播) - k8s小店 - SegmentFault 思否</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Storage </tag>
            
            <tag> CKA Storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Network Policy Overview</title>
      <link href="/blog/Kubernetes/k8s-Network-Policy-Overview/"/>
      <url>/blog/Kubernetes/k8s-Network-Policy-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="什麼是-Network-Policy"><a href="#什麼是-Network-Policy" class="headerlink" title="什麼是 Network Policy?"></a>什麼是 Network Policy?</h1><p>network policy 是 k8s 在 <code>v1.7</code> 版後正式支援的功能，在 k8s 中是一組用來定義 pod 之間(或是與 endpoint) 之間是否能夠互相通訊的規範，並以 <code>NetworkPolicy</code> resource object 的型式存在，同樣是透過 Label 的方式來選擇 pod &amp; 定義規則。</p><p>但由於 network policy 並非 k8s 核心功能，而是由 network plugin 所實現，因此若要使用 network policy 就必須要使用支援此功能的 CNI plugin 才行。(例如：<a href="https://www.projectcalico.org/" title="Kubernetes CNI plugin - Calico">Calico</a>)</p><blockquote><p>若是使用不支援 network policy 的 CNI plugin 卻設定 network policy，也不是不行，只是不會有任何效果而已</p></blockquote><p>另外，在一般的情況下，k8s 中所有的 pod 都是可以互相溝通的，跟帶有什麼 label or 位於那個 namespace 並沒有什麼關係，即使加上了 network policy，增加了透過 Label Selector 來篩選流量的效果，但只要不是設定被阻擋的來源，即使是不同 namespace 的 pod 也還是可以來存取。</p><h1 id="如何設定-Network-Policy"><a href="#如何設定-Network-Policy" class="headerlink" title="如何設定 Network Policy?"></a>如何設定 Network Policy?</h1><p>以下是一個標準的設定範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-network-policy</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 指定套用 network policy 的 pod</span></span><br><span class="line">  <span class="comment"># 若沒指定 podSelector，就表示將 network policy 套用到 namespace 中的所有 pod</span></span><br><span class="line">  <span class="attr">podSelector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">role:</span> <span class="string">db</span></span><br><span class="line">  <span class="comment"># 設定 network policy 包含的 policy type 有那些</span></span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br><span class="line">  <span class="comment"># ingress 用來設定從外面進來的流量的白名單</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span></span><br><span class="line">    <span class="comment"># 指定 IP range</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">172.17</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">        <span class="comment"># 例外設定</span></span><br><span class="line">        <span class="attr">except:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">172.17</span><span class="number">.1</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">    <span class="comment"># 帶有特定 label 的 namespace 中所有的 pod</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">project:</span> <span class="string">myproject</span></span><br><span class="line">    <span class="comment"># 同一個 namespace 中帶有特定 label 的 pod</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">podSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">role:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">  <span class="comment"># egress 用來設定 pod 對外連線的白名單</span></span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">10.0</span><span class="number">.0</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">5978</span></span><br></pre></td></tr></table></figure><p>透過上面的設定，會達成以下的效果：</p><ul><li><p>在 namespace <code>default</code> 中帶有 <code>role=db</code> label 的 pod 的進(<strong>ingress</strong>)出(<strong>egress</strong>)流量會被管制</p></li><li><p>允許下列的白名單存取帶有 <code>role=db</code> label 的 pod 的 <code>TCP:6379</code></p><ul><li><p>帶有 <code>role=frontend</code> label 的 pod (無論任何 namespace 皆可)</p></li><li><p>namespace <code>default</code> 中帶有 <code>project=myproject</code> label 的 pod (跟著 network policy 的 namespace)</p></li><li><p>IP 落在 172.17.0.0/16(但不包含 172.17.1.0/24) 的來源端</p></li></ul></li><li><p>允許帶有 <code>role=db</code> label 的 pod 對外連線到 IP 範圍為 <code>10.0.0.0/24</code> 的 <code>TCP:5978</code></p></li></ul><blockquote><p>比較需要注意的是，如果沒有設定 <code>namespaceSelector</code>，那 network policy 的效果就只會對所在 namespace 中的 pod 有效</p></blockquote><h1 id="細部解析-from-ingress-amp-to-exgress-的行為"><a href="#細部解析-from-ingress-amp-to-exgress-的行為" class="headerlink" title="細部解析 from(ingress) &amp; to(exgress) 的行為"></a>細部解析 from(ingress) &amp; to(exgress) 的行為</h1><p>從上面的範例可以看出，network policy 進出流量的規則可以分成兩類：</p><ul><li><p><strong>ingress</strong> 搭配 <code>from</code>，負責管理從外部進來的流量</p></li><li><p><strong>exgress</strong> 搭配 <code>to</code>，負責管理從內部出去的流量</p></li></ul><p>而上述進出兩類的規則中，共有 3 種方式可以來定義所要進行管理的目標，分別是 <code>podSelector</code>, <code>namespaceSelector</code>, &amp; <code>ipBlock</code></p><h2 id="podSelector"><a href="#podSelector" class="headerlink" title="podSelector"></a>podSelector</h2><p>這是用來管理與 network policy 同一個 namespace 中的 pod 時會用到的設定方式，以上述的例子為例，該 NetworkPolicy 是位於 <code>default</code> namespace 中，因此單用 podSelector 的話就只會將規則套用在位於 <code>default</code> namespace 中的 pod 上。 </p><h2 id="namespaceSelector"><a href="#namespaceSelector" class="headerlink" title="namespaceSelector"></a>namespaceSelector</h2><p>顧名思義，這是是透過 Label 來篩選 namespace 的，而 namespace 的確也是可以設定 label，只是很少看到此類範例，以下是個簡單示範：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新增一個 namespace 名稱為 new-ns</span></span><br><span class="line">$ kubectl create namespace new-ns</span><br><span class="line">namespace/new-ns created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前 namespace 詳細資訊(此時無 label 資訊)</span></span><br><span class="line">$ kubectl describe ns/new-ns</span><br><span class="line">Name:         new-ns</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Active</span><br><span class="line">...(略)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在新增的 namespace 上增加 label</span></span><br><span class="line">$ kubectl label ns/new-ns project=myproject</span><br><span class="line">namespace/new-ns labeled</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新檢視 namespace 資訊，可以看見 label 資訊已經上去了</span></span><br><span class="line">$</span><br><span class="line">$ kubectl describe ns/new-ns</span><br><span class="line">Name:         new-ns</span><br><span class="line">Labels:       project=myproject</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Active</span><br><span class="line">...(略)</span><br></pre></td></tr></table></figure><p>而設定 <code>namespaceSelector</code> 的方式會讓被篩選中的 namespace 中所有的 pod 都套用 network policy。</p><h2 id="podSelector-namespaceSelector"><a href="#podSelector-namespaceSelector" class="headerlink" title="podSelector + namespaceSelector"></a>podSelector + namespaceSelector</h2><p>這個就很清楚了，效果就是兩個加起來，以下是個設定範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">user:</span> <span class="string">alice</span></span><br><span class="line">      <span class="attr">podSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">role:</span> <span class="string">client</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure><p>首先會找到帶有 <code>user=alice</code> label 的 namespace，並把 network policy 套用在該 namespace 中帶有 <code>role=client</code> 的 pod 上。</p><h2 id="ipBlock"><a href="#ipBlock" class="headerlink" title="ipBlock"></a>ipBlock</h2><p>這就是很直覺的單純以 IP range 來進行流量的管理，但一般這個設定是用來過濾從外面進來的網路流量，而不是內部的，因為畢竟內部的 pod 所取得的 IP 並不是固定的，也無法預測。</p><h1 id="設定-Default-Policy"><a href="#設定-Default-Policy" class="headerlink" title="設定 Default Policy"></a>設定 Default Policy</h1><p>有時候，網路規則的套用是針對整個 namespace 時，希望可以設定一個對所有 pod 都有效的 default network policy，此時該怎麼做呢?</p><p>而在設定 default policy 之前，只要了解幾項大原則，其實就不難推敲出如何設定：</p><ul><li><p>若要選擇所有的 pod，則將 <strong>podSelector</strong> 設定為 <code>&#123;&#125;</code></p></li><li><p>Network Policy 的設定，若是設定 <code>policyType</code>，就表示要全部拒絕該種連線(ingress/egress)的流量</p></li><li><p>若要打開上面的限制，則要在 <code>spec.ingress</code> or <code>spec.egress</code> 中設定</p></li></ul><p>有了上面 3 個大原則點的設定說明之後，可以衍生出 4 種預設規則，如下：</p><ul><li><p>若要拒絕從外面進來的網路流量，要在 <strong>spec.policyTypes</strong> 中包含 <code>Ingress</code>，並且不要設定 <code>ingress</code></p></li><li><p>若要拒絕從內部出去的網路流量，要在 <strong>spec.policyTypes</strong> 中包含 <code>Egress</code>，並且不要設定 <code>egress</code></p></li><li><p>若要接受從外面進來的網路流量，要在 <strong>spec.policyTypes</strong> 中包含 <code>Ingress</code>，並將 <code>spec.ingress</code> 設定為 <code>&#123;&#125;</code></p></li><li><p>若要接受從內部出去的網路流向，要在 <strong>spec.policyTypes</strong> 中包含 <code>Egress</code>，並將 <code>spec.egress</code> 設定為 <code>&#123;&#125;</code></p></li></ul><p>以下有幾個實際範例可以參考：</p><h2 id="拒絕所有從外面進來的網路流量"><a href="#拒絕所有從外面進來的網路流量" class="headerlink" title="拒絕所有從外面進來的網路流量"></a>拒絕所有從外面進來的網路流量</h2><p>要為 namespace 中的每個 pod 設定預設的 network policy，拒絕所有從外面進來的網路流量，基本上要完成兩件事情：</p><ul><li><p>選擇所有的 pod</p></li><li><p>設定拒絕所有從外面進來的網路流量的 network policy</p></li></ul><p>因此按照上面的設定規則，可透過以下設定來完成：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-deny</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 選擇所有的 pod</span></span><br><span class="line">  <span class="attr">podSelector:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br><span class="line">  <span class="comment"># 拒絕所有從外面進來的網路流量(不設定 ingress)</span></span><br></pre></td></tr></table></figure><h2 id="允許所有從外面進來的流量"><a href="#允許所有從外面進來的流量" class="headerlink" title="允許所有從外面進來的流量"></a>允許所有從外面進來的流量</h2><p>其實 k8s 預設的行為就是這樣了! 不過還是說明一下，要達成<strong>允許所有從外面進來的流量</strong>的目標，要完成兩件事情：</p><ol><li><p>選擇所有的 pod</p></li><li><p>設定允許所有從外面進來的網路流量的 network policy</p></li></ol><blockquote><p>雖然這原本就是 k8s 的預設行為，但一旦 allow-all 的 policy 被設定後，還是會有專屬的 iptable rules 來管理這些 pod 的流量</p></blockquote><p>因此按照上面的設定規則，可透過以下設定來完成：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  <span class="attr">name:</span> <span class="string">allow-all</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 選擇所有的 pod</span></span><br><span class="line">  <span class="attr">podSelector:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br><span class="line">  <span class="comment"># 允許所有從外面進來的網路流量</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> &#123;&#125;</span><br></pre></td></tr></table></figure><h2 id="拒絕所有從內部出去的流量"><a href="#拒絕所有從內部出去的流量" class="headerlink" title="拒絕所有從內部出去的流量"></a>拒絕所有從內部出去的流量</h2><p>要為 namespace 中的每個 pod 設定預設的 network policy，拒絕所有從內部出去的網路流量，基本上要完成兩件事情：</p><ul><li><p>選擇所有的 pod</p></li><li><p>設定拒絕所有從內部出去的網路流量的 network policy</p></li></ul><p>因此按照上面的設定規則，可透過以下設定來完成：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-deny</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br></pre></td></tr></table></figure><h2 id="允許所有從內部出去的流量"><a href="#允許所有從內部出去的流量" class="headerlink" title="允許所有從內部出去的流量"></a>允許所有從內部出去的流量</h2><p>這同樣也是 k8s 的預設行為! 要達成<strong>允許所有從內部出去的流量</strong>的目標，要完成兩件事情：</p><ol><li><p>選擇所有的 pod</p></li><li><p>設定允許所有從內部出去的網路流量的 network policy</p></li></ol><blockquote><p>雖然這原本就是 k8s 的預設行為，但一旦 allow-all 的 policy 被設定後，還是會有專屬的 iptable rules 來管理這些 pod 的流量</p></blockquote><p>因此按照上面的設定規則，可透過以下設定來完成：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">allow-all</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> &#123;&#125;</span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br></pre></td></tr></table></figure><h1 id="實作測試"><a href="#實作測試" class="headerlink" title="實作測試"></a>實作測試</h1><p>首先說明，以下的實驗環境是使用 [Kubespray][<a href="https://github.com/kubernetes-sigs/kubespray]">https://github.com/kubernetes-sigs/kubespray]</a> 安裝，並使用 <a href="https://www.projectcalico.org/" title="Kubernetes CNI plugin - Calico">calico</a> 3.1.3 作為 k8s CNI plugin。</p><blockquote><p>透過 kubespray 安裝 k8s + calico，會自動在每一個 node 上安裝 [calicoctl][<a href="https://docs.projectcalico.org/v3.3/reference/calicoctl/commands/]">https://docs.projectcalico.org/v3.3/reference/calicoctl/commands/]</a> CLI tool 來協助管理者可以用來檢查 calici 目前的服務相關資訊</p></blockquote><p>接著，有了以上對 network policy 的了解之後，要來做個實驗來驗證一下 network policy 是否真的可以有效的限制 pod 的進出流量，以下有兩個實驗</p><h2 id="限制-pod-對外的網路流量"><a href="#限制-pod-對外的網路流量" class="headerlink" title="限制 pod 對外的網路流量"></a>限制 pod 對外的網路流量</h2><p>在這個實驗中，準備要限制帶有 <code>network-policy-test1</code> label 的 pod 的對外流量，禁止其對外存取。</p><p>首先，利用以下語法，先在 namespace <code>new-ns</code> 中產生一個新的 pod：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deny-egress-test</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">&quot;deny-egress-test&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">&quot;new-ns&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nettools</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">travelping/nettools</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;sleep 3600&#x27;</span>]</span><br></pre></td></tr></table></figure><p>在預設情況下，對外是正常的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n new-ns <span class="built_in">exec</span> deny-egress-test -- ping -c 3 8.8.8.8</span><br><span class="line">PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=1 ttl=57 time=2.29 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=2 ttl=57 time=2.37 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=3 ttl=57 time=2.28 ms</span><br><span class="line"></span><br><span class="line">--- 8.8.8.8 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2002ms</span><br><span class="line">rtt min/avg/max/mdev = 2.280/2.318/2.379/0.043 ms</span><br></pre></td></tr></table></figure><p>接著套用以下的 network policy：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deny-egress-test</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">&quot;new-ns&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">&quot;deny-egress-test&quot;</span></span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Egress</span></span><br></pre></td></tr></table></figure><p>套用完規則後，再度測試對外連線：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此時對外連線已經被擋住了</span></span><br><span class="line">$ c</span><br><span class="line">PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span><br><span class="line"></span><br><span class="line">--- 8.8.8.8 ping statistics ---</span><br><span class="line">3 packets transmitted, 0 received, 100% packet loss, time 2014ms</span><br><span class="line"></span><br><span class="line"><span class="built_in">command</span> terminated with <span class="built_in">exit</span> code 1</span><br></pre></td></tr></table></figure><p>那在 Calico 中的 policy 又是如何呈現的呢? 這個部份必須透過 <code>calicoctl</code> 來檢查：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢查指定的 namespace 中有哪些現存的 policy</span></span><br><span class="line">$ calicoctl get policy --namespace=new-ns</span><br><span class="line">NAMESPACE   NAME                           </span><br><span class="line">new-ns      knp.default.deny-egress-test</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 policy 的內容內容</span></span><br><span class="line">$ calicoctl get policy knp.default.deny-egress-test -o wide --namespace=new-ns</span><br><span class="line">NAMESPACE   NAME                           ORDER   SELECTOR                                                               </span><br><span class="line">new-ns      knp.default.deny-egress-test   1000    projectcalico.org/orchestrator == <span class="string">&#x27;k8s&#x27;</span> &amp;&amp; app == <span class="string">&#x27;deny-egress-test&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視更詳細的 policy 內容</span></span><br><span class="line">$ calicoctl get policy knp.default.deny-egress-test -o yaml --namespace=new-ns</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2018-12-25T20:57:06Z</span><br><span class="line">  name: knp.default.deny-egress-test</span><br><span class="line">  namespace: new-ns</span><br><span class="line">  resourceVersion: <span class="string">&quot;12038597&quot;</span></span><br><span class="line">  uid: a38b8b9a-0887-11e9-a683-52fddc671d88</span><br><span class="line">spec:</span><br><span class="line">  order: 1000</span><br><span class="line">  selector: projectcalico.org/orchestrator == <span class="string">&#x27;k8s&#x27;</span> &amp;&amp; app == <span class="string">&#x27;deny-egress-test&#x27;</span></span><br><span class="line">  types:</span><br><span class="line">  - Egress</span><br></pre></td></tr></table></figure><h2 id="限制對於-pod-的存取"><a href="#限制對於-pod-的存取" class="headerlink" title="限制對於 pod 的存取"></a>限制對於 pod 的存取</h2><p>在這個實驗中，要完成以下的測試內容：</p><ol><li><p>建立一個 nginx pod，作為被存取的 pod</p></li><li><p>建立一個 pod(name=<code>nginx-can-access</code>, label=<code>priv=can-access</code>) 作為存取測試之用</p></li><li><p>建立另一個 pod(name=<code>nginx-cannot-access</code>) 作為存取測試之用</p></li></ol><p>首先使用下面的 YAML 設定建立用來測試的 pod：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-network-policy-test</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">&quot;nginx-network-policy-test&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">&quot;new-ns&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:1.14-alpine</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-can-access</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">priv:</span> <span class="string">&quot;can-access&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">&quot;new-ns&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nettools</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">travelping/nettools</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;sleep 3600&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-cannot-access</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">&quot;new-ns&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nettools</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">travelping/nettools</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;sleep 3600&#x27;</span>]</span><br></pre></td></tr></table></figure><p>完成 pod 的設定後，首先確認一下 nginx pod 是否可以正常被存取：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得 nginx pod IP 為 10.233.103.218</span></span><br><span class="line">$ kubectl -n new-ns get pod -o wide</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE    IP               NODE              NOMINATED NODE</span><br><span class="line">deny-egress-test            1/1     Running   24         24h    10.233.76.30     leon-k8s-node05   &lt;none&gt;</span><br><span class="line">nginx-can-access            1/1     Running   0          96s    10.233.103.219   leon-k8s-node04   &lt;none&gt;</span><br><span class="line">nginx-cannot-access         1/1     Running   0          96s    10.233.76.31     leon-k8s-node05   &lt;none&gt;</span><br><span class="line">nginx-network-policy-test   1/1     Running   0          6m7s   10.233.103.218   leon-k8s-node04   &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 尚未套用 network policy 前，可以正常存取</span></span><br><span class="line">$ kubectl -n new-ns <span class="built_in">exec</span> nginx-can-access -- /bin/sh -c <span class="string">&quot;ping -c 2 10.233.103.218&quot;</span></span><br><span class="line">PING 10.233.103.218 (10.233.103.218) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.233.103.218: icmp_seq=1 ttl=63 time=0.133 ms</span><br><span class="line">64 bytes from 10.233.103.218: icmp_seq=2 ttl=63 time=0.035 ms</span><br><span class="line"></span><br><span class="line">--- 10.233.103.218 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.035/0.084/0.133/0.049 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 尚未套用 network policy 前，可以正常存取</span></span><br><span class="line">root@leon-k8s-node00:/srv/k8s/network_policy<span class="comment"># kubectl -n new-ns exec nginx-cannot-access -- /bin/sh -c &quot;ping -c 2 10.233.103.218&quot;</span></span><br><span class="line">PING 10.233.103.218 (10.233.103.218) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.233.103.218: icmp_seq=1 ttl=62 time=0.645 ms</span><br><span class="line">64 bytes from 10.233.103.218: icmp_seq=2 ttl=62 time=0.294 ms</span><br><span class="line"></span><br><span class="line">--- 10.233.103.218 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.294/0.469/0.645/0.176 ms</span><br></pre></td></tr></table></figure><p>接著我們要限制 <code>nginx-cannot-access</code> 不能存取 nginx pod，所以套用以下設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deny-nginx-access</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">new-ns</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">&quot;nginx-network-policy-test&quot;</span></span><br><span class="line">  <span class="attr">policyTypes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">Ingress</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">podSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">priv:</span> <span class="string">&quot;can-access&quot;</span></span><br></pre></td></tr></table></figure><p>套用完上述規則後，再度測試存取 nginx pod：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 白名單中的 nginx-can-access 依然可以存取</span></span><br><span class="line">$ kubectl -n new-ns <span class="built_in">exec</span> nginx-can-access -- /bin/sh -c <span class="string">&quot;ping -c 2 10.233.103.218&quot;</span></span><br><span class="line">PING 10.233.103.218 (10.233.103.218) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.233.103.218: icmp_seq=1 ttl=63 time=0.139 ms</span><br><span class="line">64 bytes from 10.233.103.218: icmp_seq=2 ttl=63 time=0.099 ms</span><br><span class="line"></span><br><span class="line">--- 10.233.103.218 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1001ms</span><br><span class="line">rtt min/avg/max/mdev = 0.099/0.119/0.139/0.020 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 但不在白名單中的 nginx-cannot-access 已經無法存取</span></span><br><span class="line">$ kubectl -n new-ns <span class="built_in">exec</span> nginx-cannot-access -- /bin/sh -c <span class="string">&quot;ping -c 2 10.233.103.218&quot;</span></span><br><span class="line">PING 10.233.103.218 (10.233.103.218) 56(84) bytes of data.</span><br><span class="line"></span><br><span class="line">--- 10.233.103.218 ping statistics ---</span><br><span class="line">2 packets transmitted, 0 received, 100% packet loss, time 999ms</span><br><span class="line"></span><br><span class="line"><span class="built_in">command</span> terminated with <span class="built_in">exit</span> code 1</span><br></pre></td></tr></table></figure><p>最後檢視 calico policy：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ calicoctl get policy -o wide --namespace=new-ns</span><br><span class="line">WARNING: Your kernel does not support swap <span class="built_in">limit</span> capabilities or the cgroup is not mounted. Memory limited without swap.</span><br><span class="line">NAMESPACE   NAME                            ORDER   SELECTOR                                                                        </span><br><span class="line">new-ns      knp.default.deny-egress-test    1000    projectcalico.org/orchestrator == <span class="string">&#x27;k8s&#x27;</span> &amp;&amp; app == <span class="string">&#x27;deny-egress-test&#x27;</span>            </span><br><span class="line">new-ns      knp.default.deny-nginx-access   1000    projectcalico.org/orchestrator == <span class="string">&#x27;k8s&#x27;</span> &amp;&amp; app == <span class="string">&#x27;nginx-network-policy-test&#x27;</span>   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ calicoctl get policy knp.default.deny-nginx-access -o yaml --namespace=new-ns</span><br><span class="line">WARNING: Your kernel does not support swap <span class="built_in">limit</span> capabilities or the cgroup is not mounted. Memory limited without swap.</span><br><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: 2018-12-26T21:11:59Z</span><br><span class="line">  name: knp.default.deny-nginx-access</span><br><span class="line">  namespace: new-ns</span><br><span class="line">  resourceVersion: <span class="string">&quot;12203982&quot;</span></span><br><span class="line">  uid: e1ffb3df-0952-11e9-a683-52fddc671d88</span><br><span class="line">spec:</span><br><span class="line">  ingress:</span><br><span class="line">  - action: Allow</span><br><span class="line">    destination: &#123;&#125;</span><br><span class="line">    <span class="built_in">source</span>:</span><br><span class="line">      selector: projectcalico.org/orchestrator == <span class="string">&#x27;k8s&#x27;</span> &amp;&amp; priv == <span class="string">&#x27;can-access&#x27;</span></span><br><span class="line">  order: 1000</span><br><span class="line">  selector: projectcalico.org/orchestrator == <span class="string">&#x27;k8s&#x27;</span> &amp;&amp; app == <span class="string">&#x27;nginx-network-policy-test&#x27;</span></span><br><span class="line">  types:</span><br><span class="line">  - Ingress</span><br></pre></td></tr></table></figure><p>此外若想要了解更多 network policy + Calico 的示範，可以參考 Calico 官網的資訊：</p><ul><li><p><a href="https://docs.projectcalico.org/v3.4/getting-started/kubernetes/tutorials/simple-policy">Calico Tutorial - Simple Policy Demo</a></p></li><li><p><a href="https://docs.projectcalico.org/v3.4/getting-started/kubernetes/tutorials/stars-policy/">Calico Tutorial - Stars Policy Demo</a></p></li><li><p><a href="https://docs.projectcalico.org/v3.4/getting-started/kubernetes/tutorials/advanced-policy">Calico Tutorial - Controlling ingress and egress traffic with network policy</a></p></li><li><p><a href="https://docs.projectcalico.org/v3.4/getting-started/kubernetes/tutorials/app-layer-policy/">Calico Tutorial - Application layer policy tutorial</a></p></li></ul><h1 id="SCTP-的支援"><a href="#SCTP-的支援" class="headerlink" title="SCTP 的支援"></a>SCTP 的支援</h1><p>在 v1.12 開始，network policy 已經以 beta 的型式開始支援 SCTP，因此若希望可以在 network policy 中來管理 SCTP 的流量，只要完成以下的設定即可：</p><ol><li><p>開啟 <code>SCTPSupport</code> feature gate</p></li><li><p>使用支援 SCTP 的 CNI plugin (例如：<a href="https://www.projectcalico.org/" title="Kubernetes CNI plugin - Calico">Calico</a>)</p></li></ol><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Network Policies - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/">Declare Network Policy - Kubernetes</a></p></li><li><p><a href="https://docs.projectcalico.org/v3.4/getting-started/kubernetes/tutorials/simple-policy">Calico Tutorial - Simple Policy Demo</a></p></li><li><p><a href="https://blog.opskumu.com/calico.html">Kumu’s Blog - Calico 笔记备忘</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubernetes Security </tag>
            
            <tag> CKA </tag>
            
            <tag> CKA Security </tag>
            
            <tag> Kubernetes Networking </tag>
            
            <tag> CKA Networking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Package Manager - Helm 簡介</title>
      <link href="/blog/Kubernetes/k8s-Helm-Introduction/"/>
      <url>/blog/Kubernetes/k8s-Helm-Introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="什麼是-Kubernetes-Helm"><a href="#什麼是-Kubernetes-Helm" class="headerlink" title="什麼是 Kubernetes Helm?"></a>什麼是 Kubernetes Helm?</h1><p>首先來看一下<a href="https://github.com/kubernetes/helm" title="Kubernetes Helm">官網</a>的解釋：</p><blockquote><p>Helm is a tool for managing Kubernetes charts. Charts are packages of pre-configured Kubernetes resources.</p></blockquote><p>Helm 就有點像是 Ubuntu 中的 APT 系統，也類似 Ansible 中的 Galaxy；簡單來說就是可以將一個 or 多個應用包裝成一個服務，並透過 chart 的形式發佈，讓大家可以方便在 k8s 上安裝特定的服務。</p><p>詳細的說明 &amp; 架構可以參考此篇文章 —-&gt; <a href="http://www.inwinstack.com/zh/2017/06/15/the-kubernetes-package-manager-helm/">The Kubernetes Package Manager : Helm - inwinSTACK</a></p><p>以下也有兩張圖可以說明一下 Helm 系統架構：</p><p><img src="/blog/images/kubernetes/helm-infra-1.jpg" alt="Kubernetes Persistent Volume Provisioning"></p><p><img src="/blog/images/kubernetes/helm-infra-2.png" alt="Kubernetes Persistent Volume Provisioning"></p><p>從上面的文章中，要弄清楚 <code>Helm client</code> &amp; <code>Tiller server</code> 的關係：</p><ul><li><p>Tiller server 是用來與 API server 溝通，使用 chart 在 k8s cluster 上建立服務</p></li><li><p>Helm client 則是用來操作 Tiller server 用</p></li></ul><p>此外，佈署 Helm 時可以將其佈署在 cluster level(整個 k8s cluster 皆可用)，也可以佈署在 namespace level(只會在該 namespace 可用)，這部份可以根據自己的需求去決定。</p><h1 id="Helm-要解決什麼問題"><a href="#Helm-要解決什麼問題" class="headerlink" title="Helm 要解決什麼問題?"></a>Helm 要解決什麼問題?</h1><p>一個工具被開發出來，總是被賦予要解決問題的任務，而 Helm 到底可以協助我們解決什麼樣的問題呢?</p><p>先來討論目前的趨勢：</p><ul><li><p>線上的服務開始往 micro service 的方向走</p></li><li><p>k8s 似乎也變成整個 container 大一統的平台</p></li></ul><p>因此使用 k8s 作為承載 micro service 的平台的確也是很正常的事情，但 k8s 為了解決各式各樣的問題，提供了大量的 resource object(pod, deployment, service … etc)，因此如何利用這些 resource object 來組成一個完整的系統，整個過程是很複雜的，其中包含了很多不同面向的問題需要處理，例如：</p><ul><li><p>如何管理、編輯 &amp; 更新大量的 k8s resource object 設定檔?</p></li><li><p>如何快速佈署一個含有很多設定檔的 k8s application?</p></li><li><p>如何分享 &amp; 重複利用 k8s resource object 設定檔?</p></li><li><p>如何透過 parameter &amp; template 的方式，使用相同的設定檔來支援不同環境上的佈署?</p></li><li><p>如何管理 application 的 deployment, rollback, diff &amp; history?</p></li><li><p>Application 發佈後如何進行驗證?</p></li></ul><p>在 micro service 的過程中，林林總總的問題也跑了出來，而 Helm 就是為了解決上述問題而被開發出來的工具，它使用了以下幾個方式來解決上述的問題：</p><ol><li><p>將 k8s resource object 打包到一個 chart 中 (chart 是多個 k8s resource object configuration 的集合)</p></li><li><p>將 chart 保存在 chart repository 中，透過 repository 的方式來儲存 &amp; 分享 chart</p></li><li><p>Helm 則利用 chart 來進行 deployment, upgrade, rollback, version control …. 等工作</p></li></ol><p>透過以上方式，大幅簡化在 k8s 上佈署 application 的複雜度。</p><h1 id="安裝-Helm-client"><a href="#安裝-Helm-client" class="headerlink" title="安裝 Helm client"></a>安裝 Helm client</h1><p>在 k8s cluster 安裝 tiller server 之前，我們要先來準備 Helm client，使用以下的指令安裝最新版的 Helm：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://storage.googleapis.com/kubernetes-helm/helm-$(curl -s https://api.github.com/repos/helm/helm/releases/latest | jq --raw-output <span class="string">&#x27;.tag_name&#x27;</span>)-linux-amd64.tar.gz -O helm-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">$ tar -zxvf helm-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line">$ sudo mv linux-amd64/helm /usr/<span class="built_in">local</span>/bin/helm</span><br></pre></td></tr></table></figure><h1 id="Helm-三大重要概念"><a href="#Helm-三大重要概念" class="headerlink" title="Helm 三大重要概念"></a>Helm 三大重要概念</h1><p>學習 Helm 有 3 個重要概念必須先了解，分別是：</p><ol><li><p>Chart</p></li><li><p>Repository</p></li><li><p>Release</p></li></ol><h2 id="Chart"><a href="#Chart" class="headerlink" title="Chart"></a>Chart</h2><p>Chart 其實就是一堆 k8s resource object definition 的集合，目的就是要在 k8s 上佈署多個不同的 resource object；而為了要重複利用，這些 resource object definition 必須要支援 template, parameter … 等相關功能。</p><p>這概念上就類似 APT dpkg, YUM rpm …. 等套件系統。</p><h2 id="Repository"><a href="#Repository" class="headerlink" title="Repository"></a>Repository</h2><p>Repository 就是存放 Chart 的地方，由 Helm client 管理。</p><h2 id="Release"><a href="#Release" class="headerlink" title="Release"></a>Release</h2><p>Release 則是由 chart 產生的一個個 instance，一個 chart 可以被佈署多次，每一個佈署都是每個獨立的 instance，在 Helm 中則稱為 <code>Release</code>，每個 release name 都不同且獨立運作的。</p><h1 id="將-Helm-佈署至-Cluster-Level"><a href="#將-Helm-佈署至-Cluster-Level" class="headerlink" title="將 Helm 佈署至 Cluster Level"></a>將 Helm 佈署至 Cluster Level</h1><h2 id="設定-current-context"><a href="#設定-current-context" class="headerlink" title="設定 current context"></a>設定 current context</h2><p>這裡要先確認 Helm 要安裝在哪個 k8s cluster 中(如果有多個的話)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定 default context</span></span><br><span class="line">$ kubectl config use-context admin-cluster.local</span><br><span class="line">Switched to context <span class="string">&quot;admin-cluster.local&quot;</span>.</span><br></pre></td></tr></table></figure><h2 id="為-Tiller-server-開啟權限"><a href="#為-Tiller-server-開啟權限" class="headerlink" title="為 Tiller server 開啟權限"></a>為 Tiller server 開啟權限</h2><p>從上面的說明可看出，其實 tiller 就是一個用來與 k8s API server 溝通的 service，因此我們要賦予 tiller 所需要的權限。</p><p>權限設定的流程如下：</p><ol><li><p>在 <code>kube-system</code> namespace 中建立 Service Account <code>tiller</code></p></li><li><p>建立 ClusterRoleBinding，將 Service Account <code>tiller</code> 與 Cluster Role <code>cluster-admin</code> 作繫結</p></li></ol><p>首先準備以下檔案來進行設定，名稱為 <code>rbac-config.yaml</code>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 Service Account &quot;tiller&quot; 給 Helm service 與 API server 認證之用</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 ClusterRoleBinding，將 Role &quot;cluser-admin&quot; 權限賦予 Service Account &quot;tiller&quot; </span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tiller</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><p>接著將上面的設定套用到 k8s 中:</p><blockquote><p>kubectl apply -f rbac-config.yaml</p></blockquote><p>最後進行 Helm 的初始化：</p><blockquote><p>helm init –service-account tiller</p></blockquote><p>如此一來 Helm client 就會開始透過 kubectl 與 k8s cluster 溝通並建立所需要的相關資源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment --all-namespaces | grep tiller</span><br><span class="line">kube-system   tiller-deploy             1         1         1            0           11s</span><br></pre></td></tr></table></figure><h1 id="將-Helm-佈署至-Namespace-Level"><a href="#將-Helm-佈署至-Namespace-Level" class="headerlink" title="將 Helm 佈署至 Namespace Level"></a>將 Helm 佈署至 Namespace Level</h1><p>這個部份的步驟其實跟 cluster level 差不多，大概就是：</p><ol><li><p>在特定的 Namespace(假設是 <code>helm-lab</code>) 中建立一個 Service Account(假設為 <code>tiller</code>)</p></li><li><p>建立一個只有 <code>helm-lab</code> namespace 中擁有所有權限的 Role(假設為 <code>tiller-manager</code>)</p></li><li><p>建立一個 RoleBinding(假設為 <code>tiller-binding</code>)，將 <code>tiller-manager</code> 與 <code>tiller</code> 進行繫結</p></li></ol><p>如此一來相關的權限設定大致上就完成了。</p><p>最後執行以下命令完成初始化：</p><blockquote><p>helm init –service-account tiller</p></blockquote><p>詳細的設定可以參考<a href="https://docs.helm.sh/using_helm/#role-based-access-control">官網的說明</a>。</p><h1 id="開始使用-Kubernetes-Helm"><a href="#開始使用-Kubernetes-Helm" class="headerlink" title="開始使用 Kubernetes Helm"></a>開始使用 Kubernetes Helm</h1><h2 id="新增-namespace"><a href="#新增-namespace" class="headerlink" title="新增 namespace"></a>新增 namespace</h2><p>這邊先建立一個全新的 namespace(<code>helm-lab</code>) 來進行 Helm 的測試，並設定 default namespace：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 namespace</span></span><br><span class="line">$ kubectl create namespace helm-lab</span><br><span class="line">namespace <span class="string">&quot;helm-lab&quot;</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 context 所使用的 namespace</span></span><br><span class="line">$ kubectl config set-context <span class="string">&quot;admin-cluster.local&quot;</span> --namespace=helm-lab</span><br><span class="line">Context <span class="string">&quot;admin-cluster.local&quot;</span> modified.</span><br></pre></td></tr></table></figure><h2 id="更新-Helm-Repo"><a href="#更新-Helm-Repo" class="headerlink" title="更新 Helm Repo"></a>更新 Helm Repo</h2><p>接著更新 Helm repository，Helm client 會取得所有 stable 的 charts：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得最新的 chart 資訊</span></span><br><span class="line">$ helm repo update</span><br><span class="line">Hang tight <span class="keyword">while</span> we grab the latest from your chart repositories...</span><br><span class="line">...Skip <span class="built_in">local</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">&quot;stable&quot;</span> chart repository</span><br><span class="line">Update Complete. ⎈ Happy Helming!⎈ </span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜尋 helm charts (會出現很多 charts info)</span></span><br><span class="line">$ helm search -l</span><br><span class="line">.... (略)</span><br></pre></td></tr></table></figure><h2 id="安裝第一個-Helm-Chart"><a href="#安裝第一個-Helm-Chart" class="headerlink" title="安裝第一個 Helm Chart"></a>安裝第一個 Helm Chart</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜尋 mysql 相關的 Helm chart</span></span><br><span class="line">$ helm search mysql</span><br><span class="line">NAME                                CHART VERSION    APP VERSION    DESCRIPTION                                                 </span><br><span class="line">stable/mysql                        0.11.0           5.7.14         Fast, reliable, scalable, and easy to use open-source rel...</span><br><span class="line">.... (略)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安裝 mysql helm chart</span></span><br><span class="line">$ helm install stable/mysql</span><br><span class="line">NAME:   wayfaring-butterfly</span><br><span class="line">LAST DEPLOYED: Sat Dec 22 07:34:54 2018</span><br><span class="line">NAMESPACE: helm-lab</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Secret</span><br><span class="line">NAME                       TYPE    DATA  AGE</span><br><span class="line">wayfaring-butterfly-mysql  Opaque  2     0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME                            DATA  AGE</span><br><span class="line">wayfaring-butterfly-mysql-test  1     0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/PersistentVolumeClaim</span><br><span class="line">NAME                       STATUS   VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">wayfaring-butterfly-mysql  Pending  0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                       TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)   AGE</span><br><span class="line">wayfaring-butterfly-mysql  ClusterIP  10.233.22.63  &lt;none&gt;       3306/TCP  0s</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME                       DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">wayfaring-butterfly-mysql  1        1        1           0          0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                       READY  STATUS   RESTARTS  AGE</span><br><span class="line">wayfaring-butterfly-mysql-6bbdd45dc-jdsn6  0/1    Pending  0         0s</span><br><span class="line">.... (略)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出已經安裝的 Helm chart</span></span><br><span class="line">$ helm ls</span><br><span class="line">NAME                   REVISION    UPDATED                     STATUS      CHART           APP VERSION    NAMESPACE</span><br><span class="line">wayfaring-butterfly    1           Sat Dec 22 07:34:54 2018    DEPLOYED    mysql-0.11.0    5.7.14         helm-lab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前的 resource object</span></span><br><span class="line">$ kubectl get all</span><br><span class="line">NAME                                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/wayfaring-butterfly-mysql-6bbdd45dc-jdsn6   0/1     Pending   0          5m51s</span><br><span class="line"></span><br><span class="line">NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE</span><br><span class="line">service/wayfaring-butterfly-mysql   ClusterIP   10.233.22.63   &lt;none&gt;        3306/TCP   5m51s</span><br><span class="line"></span><br><span class="line">NAME                                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/wayfaring-butterfly-mysql   1         1         1            0           5m51s</span><br><span class="line"></span><br><span class="line">NAME                                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/wayfaring-butterfly-mysql-6bbdd45dc   1         1         0       5m51s</span><br></pre></td></tr></table></figure><p>helm chart 的每一個佈署，都被稱為一個 <code>release</code>，因此可以用同樣的 helm chart 安裝多個 release，從上面可以看到，在沒有指定 release name 的情況下，helm client 會自動幫你產生一個，上面的範例則是 <code>wayfaring-butterfly</code></p><p>若要移除 release，可以使用 helm delete 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm delete wayfaring-butterfly --purge</span><br><span class="line">release <span class="string">&quot;wayfaring-butterfly&quot;</span> deleted</span><br></pre></td></tr></table></figure><p>雖然 release 被刪除，但是還是可以回溯的，因為 Helm 會幫你保留歷史紀錄：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 已經被刪除的 release 還是可以查到歷史紀錄</span></span><br><span class="line">$ helm status wayfaring-butterfly</span><br><span class="line">LAST DEPLOYED: Sat Dec 22 07:34:54 2018</span><br><span class="line">NAMESPACE: helm-lab</span><br><span class="line">STATUS: DELETED</span><br><span class="line">..... (略)</span><br></pre></td></tr></table></figure><p>若想要回溯已經刪除的 release，可以透過 <code>helm rollback</code> 來完成。</p><h1 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h1><p>Helm 提供了 k8s 使用者一個方便佈署服務的方式，雖然 micro service 將 service 管理變得複雜了，但透過 Helm 的協助，可以讓管理者有條理地管理各個不同的服務 &amp; 版本，再也不用跟一堆 YAML configuration 奮鬥，大幅減少人為的錯誤，並提升工作效率。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://medium.com/@evenchange4/%E4%BA%94%E5%88%86%E9%90%98-kubernetes-%E6%9C%89%E6%84%9F-e51f093cb10b">五分鐘 Kubernetes 有感 – Michael Hsu – Medium</a></p></li><li><p><a href="https://docs.helm.sh/using_helm/#quickstart">Helm Docs | Quickstart</a></p></li><li><p><a href="https://whmzsu.github.io/helm-doc-zh-cn/">Helm User Guide</a></p></li><li><p><a href="https://hk.saowen.com/a/2be587a04c4ec2ae2d5542397ad68a94bd102e3aa94e2e58f313f70a3d87f8c9">簡化Kubernetes應用部署工具-Helm - 掃文資訊</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Helm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安裝 Nexus Repository Manager 作為 Docker Hub Mirror &amp; Docker Image Proxy</title>
      <link href="/blog/Nexus_Repository/docker-configure-proxy-with-nexus/"/>
      <url>/blog/Nexus_Repository/docker-configure-proxy-with-nexus/</url>
      
        <content type="html"><![CDATA[<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p>原本公司內部對於 docker image 的需求很簡單，就是 docker hub mirror 而已，一開始用 <a href="https://goharbor.io/">VMware Harbor</a> 就可以滿足需求，但後來需求逐漸變多了，大概條列如下：</p><ul><li><p>docker hub mirror</p></li><li><p>private docker registry</p></li><li><p>docker registry proxy of certain public registry(e.g., <code>quay.io</code>, <code>gcr.io</code>)</p></li></ul><p>後來找到 <a href="https://www.sonatype.com/nexus-repository-oss">Sonatype Nexus Repository</a> 用來解決上面的需求，而且這個不僅可以作為 docker image 的 proxy，還可以作為 npm, rpm, deb … 等軟體套件的 proxy，因此就花了點時間研究一下如何安裝使用。</p><h1 id="安裝需求"><a href="#安裝需求" class="headerlink" title="安裝需求"></a>安裝需求</h1><p>基本上安裝需求很簡單，只有兩樣：</p><ul><li><p>Java Runtime version 8</p></li><li><p>很大的硬碟空間 (為了儲存 proxy 下來的 docker image)</p></li></ul><h1 id="安裝流程"><a href="#安裝流程" class="headerlink" title="安裝流程"></a>安裝流程</h1><h2 id="下載-Nexus-Repository-OSS"><a href="#下載-Nexus-Repository-OSS" class="headerlink" title="下載 Nexus Repository OSS"></a>下載 Nexus Repository OSS</h2><p>首先到<a href="https://www.sonatype.com/nexus-repository-oss">官網</a>下載 Nexus3 Repository OSS，並解壓縮到較大硬碟空間的分割上，以下是這次安裝的版本：</p><ul><li><p>官網最新版本：<code>nexus-3.14.0-04-unix.tar.gz</code></p></li><li><p>解壓縮路徑：<code>/data/nexus3</code></p></li></ul><p>解壓縮後會有兩個目錄，分別是 <code>nexus-3.14.0-04</code> &amp; <code>sonatype-work</code></p><h2 id="設定-HTTPS"><a href="#設定-HTTPS" class="headerlink" title="設定 HTTPS"></a>設定 HTTPS</h2><h3 id="準備-certificates"><a href="#準備-certificates" class="headerlink" title="準備 certificates"></a>準備 certificates</h3><p>這個部份就略過了，需要的人可以去 <a href="https://www.sslforfree.com/">SSL For Free</a> 申請免費且合法的憑證，比較需要注意的是 SSL For Free 僅會提供 CER 格式的 certificate &amp; private key，因此後面的設定中會需要使用其他工具轉成其他格式。</p><blockquote><p>假設申請的 certificate 為 wildcard certificate for <code>*.example.com</code>，檔名分別為 </p></blockquote><h3 id="設定目標"><a href="#設定目標" class="headerlink" title="設定目標"></a>設定目標</h3><p>在以下步驟中，我們預設要將 nexus repository 的 DNS 設定為 <code>nexus.example.com</code>，因此先建立一個可被 Nexus Repository Manager 用的 certificate，必須是 JKS(Java keystore) 的格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將 certificate &amp; private key 轉成 PKCS12 格式</span></span><br><span class="line">$ openssl pkcs12 -<span class="built_in">export</span> -name nexus.example.com -inkey path_to_private.key -<span class="keyword">in</span> path_to_wildcards.example.com.crt -out complete_key.p12</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 PKCS12 憑證匯入新建的 JKS 憑證</span></span><br><span class="line"><span class="comment"># 並設定 jks(Java keystore) 的密碼為 password</span></span><br><span class="line">$ keytool -importkeystore -deststorepass password -destkeypass password -destkeystore keystore.jks -srckeystore complete_key.p12 -srcstoretype PKCS12 -srcstorepass password -<span class="built_in">alias</span> nexus.example.com</span><br></pre></td></tr></table></figure><h3 id="設定-HTTPS-1"><a href="#設定-HTTPS-1" class="headerlink" title="設定 HTTPS"></a>設定 HTTPS</h3><p>這裡有兩個目錄變數需要先定義：</p><ul><li><p><code>$install-dir</code>：**/data/nexus3/nexus-3.14.0-04**</p></li><li><p><code>$data-fir</code>：**/data/nexus3/sonatype-work/nexus3**</p></li></ul><p>設定流程如下：</p><ol><li><p>將上述產生的 <code>keystore.jks</code> 複製到 <code>$install-dir/etc/ssl/</code> 中，完整路徑應該是 <strong>$install-dir/etc/ssl/keystore.jks</strong></p></li><li><p>編輯 <code>$data-dir/etc/nexus.properties</code>，新增設定 <code>application-port-ssl=443</code> (port number 可以自訂)</p></li><li><p>編輯 <code>$data-dir/etc/nexus.properties</code>，移除 <code>nexus-args</code> 的註解，並加上 <code>$&#123;jetty.etc&#125;/jetty-https.xml</code> (使用逗號隔開設定)</p></li><li><p>編輯 <code>$install-dir/etc/jetty/jetty-https.xml</code>，檢視 <code>sslContextFactory</code> section，確保檔名 &amp; 密碼跟上面設定的 certificate 是相同的 (若檔名是 <strong>keystore.jks</strong> &amp; 密碼是 <strong>password</strong> 就可以忽略此步驟)</p></li></ol><h1 id="啟動-Nexus-Respository-Manager"><a href="#啟動-Nexus-Respository-Manager" class="headerlink" title="啟動 Nexus Respository Manager"></a>啟動 Nexus Respository Manager</h1><p>到此為止，Nexus 還是無法直接以 HTTPS 的形式啟動，因為還需要設定 Base Url，首先要以 HTTP 啟動 Nexus：</p><blockquote><p>/data/nexus3/nexus-3.14.0-04/bin/nexus start</p></blockquote><p>接著以 <code>admin</code> 的身份登入(預設密碼為 <code>admin123</code>)，進入 <code>System -&gt; Capabilities</code>，新增 <strong>Base Url</strong>，並設定為 <code>https://nexus.example.com</code> 並儲存。</p><p>最後重新啟動 Nexus Repository Manager：</p><blockquote><p>/data/nexus3/nexus-3.14.0-04/bin/nexus restart</p></blockquote><p>接著應該就可以用 <a href="https://nexus.example.com/">https://nexus.example.com</a> 登入了!</p><h1 id="設定-Docker-Image-Mirror-amp-Proxy"><a href="#設定-Docker-Image-Mirror-amp-Proxy" class="headerlink" title="設定 Docker Image Mirror &amp; Proxy"></a>設定 Docker Image Mirror &amp; Proxy</h1><p>這個部份就不細說了，因為下面的參考文章都寫的很清楚，基本上就是設定以下內容：</p><ul><li><p>如果要設定 private docker registry，那就設定 docker(hosted) repository</p></li><li><p>如果要設定 docker hub mirror，那就設定 docker(proxy) repository (設定 <code>https://registry-1.docker.io</code>)</p></li><li><p>如果要設定 docker proxy，那就選 docker(proxy) repository(gcr.io 設定 <code>https://gcr.io</code>；quay.io 設定 <code>https://quay.io</code>)</p></li><li><p>如果要設定一個統一的 docker proxy 入口，則設定 docker(group) repository，並把需要的 docker mirror or proxy 加入</p></li></ul><p>比較需要注意的是，需要到 <code>Security -&gt; Realms</code> 中將 <code>Docker Bearer Token Realm</code> 設定為 Active，如此一來才可以使用匿名的方式存取 docker proxy。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://help.sonatype.com/repomanager3/security/configuring-ssl#ConfiguringSSL-InboundSSL-ConfiguringtoServeContentviaHTTPS">Nexus3 Repository Manager 3 &gt; Security &gt; Configuring SSL</a></p></li><li><p><a href="https://mtijhof.wordpress.com/2018/07/23/using-nexus-oss-as-a-proxy-cache-for-docker-images/">Using Nexus OSS as a proxy/cache for Docker images – Tech by Maarten</a></p></li><li><p><a href="https://segmentfault.com/a/1190000015629878">使用nexus3.x配置docker镜像仓库及仓库代理 - 个人文章 - SegmentFault 思否</a></p></li><li><p><a href="http://zhangyuyu.github.io/2018/01/09/Nexus-%E6%9E%84%E5%BB%BA%E5%B9%B6%E4%B8%8A%E4%BC%A0docker-image%E8%87%B3Sonatype-Nexus/">Nexus - 构建并上传docker image至Sonatype Nexus | 若见喻笺</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Nexus Repository </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
            <tag> Nexus Repository </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Connecting Applications with Services</title>
      <link href="/blog/Kubernetes/k8s-Connecting-Apps-with-Services/"/>
      <url>/blog/Kubernetes/k8s-Connecting-Apps-with-Services/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes-如何將-container-相互連接"><a href="#Kubernetes-如何將-container-相互連接" class="headerlink" title="Kubernetes 如何將 container 相互連接?"></a>Kubernetes 如何將 container 相互連接?</h1><p>假設在多台機器的環境中，每一台機器都安裝了 Docker，要讓這些起在這些機器上的 container 相互連結，若僅依賴 Docker 本身所提供的網路功能的話，就只能透過 <code>HOST_IP:PORT</code> 的方式來提供 container 相互溝通的能力；這樣的方式在同一個 Host 上所使用的 port 是不能重複的，這種方法在大型 or 多人的的環境上是不可行的，要與每個使用者溝通協調可以使用的 port 幾乎是不可能的事情。</p><p>因此在 k8s 中提出了 pod 的概念，並且透過額外建立 overlay network 的方式讓每個 pod 擁有自己的 IP(cluster private IP)，且可以相互溝通，當然也沒有 port conflict 的問題。</p><p>k8s 透過了 pod &amp; overlay network 解決了 container 在不同 Host 之間的溝通問題，但還有另外一個問題，就是 pod IP 並不會固定，可能因為 container 重啟的關係而導致 IP 變更，此時其他 Application 就有可能無法透過原本的 IP 連線到該 pod，此時怎辦?</p><p>在 k8s 就額外了提供 <code>Service</code> resource object，透過 <code>DNS + Pod Load Balancer</code> 的概念，來解決這個問題。</p><h1 id="開放-Pod-在-Cluster-內部存取"><a href="#開放-Pod-在-Cluster-內部存取" class="headerlink" title="開放 Pod 在 Cluster 內部存取"></a>開放 Pod 在 Cluster 內部存取</h1><p>首先透過<a href="https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/service/networking/run-my-nginx.yaml">以下的 YAML 定義</a>建立一個搭配兩個(replica=2) nginx Pod 的 Deployment：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>套用了以上設定後，可以檢視一下目前的 Pod 清單：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE   IP               NODE              NOMINATED NODE</span><br><span class="line">my-nginx-756f645cd7-dwzrp   1/1     Running   0          15s   10.233.76.12     leon-k8s-node05   &lt;none&gt;</span><br><span class="line">my-nginx-756f645cd7-zp8gq   1/1     Running   0          15s   10.233.103.203   leon-k8s-node04   &lt;none&gt;</span><br></pre></td></tr></table></figure><p>可以看到每個 pod 都已經取得 IP，然後在任何一個 node 執行 curl 應該都可以得到類似以下結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://10.233.76.12</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">....(略)</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><h1 id="建立-Service"><a href="#建立-Service" class="headerlink" title="建立 Service"></a>建立 Service</h1><p>透過上一個部份，有了兩個在 cluster 內部都可以存取到的 pod，正常情況下，透過 pod IP 存取都是沒什麼問題的，但是若是 pod 因為某些原因重啟了(例如：node 掛掉)而造成 IP 變更時，那原本的 IP 就存取不到了，那怎麼辦?</p><p>於是 k8s 就提供了 Service 來解決這個問題，透過將 pod IP 這一層抽象化，讓直接對外提供服務的不是 pod 本身而已 service，且搭配內部的 DNS service，讓 cluster 內部的其他 application 可以簡單的透過 domain name 來存取 service，而 service 會將 network traffic 平均的分流到 pod member 中。</p><p>此外，service 有些特性也需要注意一下：</p><ul><li><p>每個 service 會被分配一個獨一無二的 IP (cluster IP)</p></li><li><p>這個 IP 的生命周期是跟著 service 的</p></li><li><p>只要 service 活著，service IP 就不會變更</p></li></ul><p>那如何建立一個 service 呢? 搭配上面的 pod，可以套用<a href="https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/service/networking/nginx-svc.yaml">以下設定</a>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-nginx</span></span><br></pre></td></tr></table></figure><p>也可以直接透過 expose 指令來處理：(expose 指令會協助新增 service)</p><blockquote><p>kubectl expose deployment/my-nginx</p></blockquote><p>接著來檢視一下建立 service 後的狀況：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc/my-nginx</span><br><span class="line">NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">my-nginx   ClusterIP   10.233.12.73   &lt;none&gt;        80/TCP    19s</span><br><span class="line"></span><br><span class="line">$ kubectl get svc/my-nginx</span><br><span class="line">NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">my-nginx   ClusterIP   10.233.12.73   &lt;none&gt;        80/TCP    19s</span><br><span class="line">root@leon-k8s-node00:/srv/k8s/connect_app_with_svc<span class="comment"># kubectl describe svc/my-nginx</span></span><br><span class="line">Name:              my-nginx</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            run=my-nginx</span><br><span class="line">Annotations:       ... (略)</span><br><span class="line">Selector:          run=my-nginx</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.233.12.73</span><br><span class="line">Port:              &lt;<span class="built_in">unset</span>&gt;  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         10.233.103.203:80,10.233.76.12:80</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure><p>此時若是 <strong>curl http://[CLUSTER_IP]:[PORT]</strong> 也會得到跟上一個部份中一樣的結果</p><h1 id="存取-Service"><a href="#存取-Service" class="headerlink" title="存取 Service"></a>存取 Service</h1><p>在 k8s cluster 中要存取 service 有兩種主要方式，分別是：</p><ul><li><p><strong>Environment Variable</strong></p></li><li><p><strong>DNS</strong></p></li></ul><p>其中 <code>Environment Variable</code> 已經是內建的，而 DNS 的部份則是必須要安裝 addon 才會有，以下來看看兩種不同的方法是如何存取 service。</p><h2 id="Environment-Variable-環境變數"><a href="#Environment-Variable-環境變數" class="headerlink" title="Environment Variable(環境變數)"></a>Environment Variable(環境變數)</h2><p>當一個 pod 被建立時，kubelet 就會自動的增加一些與 service 相關的環境變數進到 pod 中，但這樣的作法其實造成了一些問題，為什麼這樣說呢? 以下做個簡單的實驗：</p><p>首先檢視一下在目前 pod 中的環境變數資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">exec</span> my-nginx-756f645cd7-dwzrp -- printenv | grep SERVICE</span><br><span class="line">KUBERNETES_SERVICE_HOST=10.233.0.1</span><br><span class="line">KUBERNETES_SERVICE_PORT=443</span><br><span class="line">KUBERNETES_SERVICE_PORT_HTTPS=443</span><br></pre></td></tr></table></figure><p>大家應該會發現，上面這個 IP(<code>10.233.0.1</code>) 並不是屬於這個 pod 的 service 的 IP，但如果真的去 curl 這個 IP:Port 會得到以下結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ curl https://10.233.0.1</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;kind&quot;</span>: <span class="string">&quot;Status&quot;</span>,</span><br><span class="line">  <span class="string">&quot;apiVersion&quot;</span>: <span class="string">&quot;v1&quot;</span>,</span><br><span class="line">  <span class="string">&quot;metadata&quot;</span>: &#123;</span><br><span class="line">    </span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;status&quot;</span>: <span class="string">&quot;Failure&quot;</span>,</span><br><span class="line">  <span class="string">&quot;message&quot;</span>: <span class="string">&quot;forbidden: User \&quot;system:anonymous\&quot; cannot get path \&quot;/\&quot;&quot;</span>,</span><br><span class="line">  <span class="string">&quot;reason&quot;</span>: <span class="string">&quot;Forbidden&quot;</span>,</span><br><span class="line">  <span class="string">&quot;details&quot;</span>: &#123;</span><br><span class="line">    </span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;code&quot;</span>: 403</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>會造成這樣的原因是因為這些 pod 建立的時間比 service 還要早，因此 kubelet 在填入環境變數的資訊時，並沒有 service 的訊息可以填入，因此才改填入 <code>--service-cluster-ip-range</code> 設定中的第一個 IP 來替代。</p><p>那要怎麼樣讓環境變數可以變成正確的值呢? 很簡單，只要把 pod 砍了再重新產生即可。</p><p>由於這些 pod 是由 Deployment 管理，因此可以透過將 Deployment 中的 replica 設定變為 0，再改回 2，這樣就可以讓 pod 重新產生，取得正確的 service 環境變數設定：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將 replica 改為 0，藉此砍掉所有的 pod</span></span><br><span class="line">$ kubectl scale deployment/my-nginx --replicas=0</span><br><span class="line">deployment.extensions/my-nginx scaled</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認 pod 都已經被移除</span></span><br><span class="line">$ kubectl get deployment/my-nginx</span><br><span class="line">NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/my-nginx   0         0         0            0           24h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新將 replica 改為 2</span></span><br><span class="line">$ kubectl scale deployment/my-nginx --replicas=2</span><br><span class="line">deployment.extensions/my-nginx scaled</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 pod status &amp; 新生成的 pod name</span></span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE              NOMINATED NODE</span><br><span class="line">my-nginx-756f645cd7-cpvw6   1/1     Running   0          5m37s   10.233.76.13     leon-k8s-node05   &lt;none&gt;</span><br><span class="line">my-nginx-756f645cd7-vvmll   1/1     Running   0          5m37s   10.233.103.204   leon-k8s-node04   &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 pod 環境變數</span></span><br><span class="line"><span class="comment"># 此時的 service 相關的環境變數就正確了</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> my-nginx-756f645cd7-cpvw6 -- printenv | grep SERVICE</span><br><span class="line">MY_NGINX_SERVICE_HOST=10.233.12.73</span><br><span class="line">MY_NGINX_SERVICE_PORT=80</span><br><span class="line">KUBERNETES_SERVICE_HOST=10.233.0.1</span><br><span class="line">KUBERNETES_SERVICE_PORT=443</span><br><span class="line">KUBERNETES_SERVICE_PORT_HTTPS=443</span><br></pre></td></tr></table></figure><p>從上面的實驗可以看出，在 service 建立之後才產生的 pod，就會取得正確的環境變數了。</p><h2 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h2><p>DNS 則是 addon，不會預設安裝進 k8s 中，但目前普遍 k8s 安裝相關的專案都會協助安裝 DNS service(kube-dns or CoreDNS)，我們可以透過以下指令檢視：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get svc</span><br><span class="line">NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">coredns                ClusterIP   10.233.0.3     &lt;none&gt;        53/UDP,53/TCP,9153/TCP   35d</span><br><span class="line">kubernetes-dashboard   ClusterIP   10.233.22.60   &lt;none&gt;        443/TCP                  35d</span><br></pre></td></tr></table></figure><p>從上面可以看出來，我的 k8s cluster 中的 DNS service 是 CoreDNS，並非 kube-dns。</p><p>接著以下要同時檢視之前建立的 my-nginx service 是否可以正確的在 cluster 內部解析：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.233.0.1     &lt;none&gt;        443/TCP   35d</span><br><span class="line">my-nginx     ClusterIP   10.233.12.73   &lt;none&gt;        80/TCP    24h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增一個 busybox pod 並直接進入其 terminal</span></span><br><span class="line">$ kubectl run curl --image=radial/busyboxplus:curl -i --tty</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 pod DNS server 設定</span></span><br><span class="line"><span class="comment"># 可以看出被加入了 </span></span><br><span class="line">[ root@curl-5cc7b478b6-29bjq:/ ]$ cat /etc/resolv.conf </span><br><span class="line">nameserver 10.233.0.3</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local qct.io</span><br><span class="line">options ndots:5</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 nslookup 查詢之前新增的 my-nginx service</span></span><br><span class="line"><span class="comment"># 解析出來的結果跟上面使用 kubectl 檢視的結果是相同的</span></span><br><span class="line">[ root@curl-5cc7b478b6-29bjq:/ ]$ nslookup my-nginx</span><br><span class="line">Server:    10.233.0.3</span><br><span class="line">Address 1: 10.233.0.3 coredns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-nginx</span><br><span class="line">Address 1: 10.233.12.73 my-nginx.default.svc.cluster.local</span><br></pre></td></tr></table></figure><h1 id="如何更安全的存取-Service"><a href="#如何更安全的存取-Service" class="headerlink" title="如何更安全的存取 Service"></a>如何更安全的存取 Service</h1><p>以下示範如何設定一個名稱為 <code>my-secure-nginx</code> 的 HTTPS service。</p><h2 id="產生-TLS-certificate-amp-建立-Secret"><a href="#產生-TLS-certificate-amp-建立-Secret" class="headerlink" title="產生 TLS certificate &amp;  建立 Secret"></a>產生 TLS certificate &amp;  建立 Secret</h2><p>首先產生 TLS certificate，並取得 Base64 編碼結果：(記得 TLS certificate 的 domain 要設定為 <code>my-secure-nginx</code>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 產生 TLS certificate</span></span><br><span class="line">$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/nginx.key -out /tmp/nginx.crt -subj <span class="string">&quot;/CN=my-secure-nginx/O=my-secure-nginx&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 Base64 編碼結果</span></span><br><span class="line">$ cat /tmp/nginx.crt | base64</span><br><span class="line">LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS ... (略)</span><br><span class="line">$ cat /tmp/nginx.key | base64 </span><br><span class="line">LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS ... (略)</span><br></pre></td></tr></table></figure><p>套用以下 YAML 設定，建立一個 Secret resource object：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;v1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;Secret&quot;</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;my-secure-nginx-secret&quot;</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">nginx.crt:</span> <span class="string">&quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS ... (略)&quot;</span></span><br><span class="line">  <span class="attr">nginx.key:</span> <span class="string">&quot;LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS ... (略)&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 確認 secret 是否有正確的被建立</span></span><br><span class="line">$ kubectl get secret/my-secure-nginx-secret</span><br><span class="line">NAME                     TYPE     DATA   AGE</span><br><span class="line">my-secure-nginx-secret   Opaque   2      30s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 secret 詳細內容</span></span><br><span class="line">$ kubectl get secret/my-secure-nginx-secret -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  nginx.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS ... (略)</span><br><span class="line">  nginx.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS ... (略)</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  ... (略)</span><br><span class="line">  creationTimestamp: 2018-11-18T19:54:17Z</span><br><span class="line">  name: my-secure-nginx-secret</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: <span class="string">&quot;6164328&quot;</span></span><br><span class="line">  selfLink: /api/v1/namespaces/default/secrets/my-secure-nginx-secret</span><br><span class="line">  uid: bb87fa8e-eb6b-11e8-890b-66712a0dd587</span><br><span class="line"><span class="built_in">type</span>: Opaque</span><br></pre></td></tr></table></figure><h2 id="新增-nginx-SSL-相關設定"><a href="#新增-nginx-SSL-相關設定" class="headerlink" title="新增 nginx SSL 相關設定"></a>新增 nginx SSL 相關設定</h2><p>在 k8s 中，一般我們會以 ConfigMap 的方式來作為處理設定檔的方式，因此套用以下的 YAML 設定來新增 nginx 設定檔：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-secure-nginx-config</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">default.conf:</span> <span class="string">|</span></span><br><span class="line">    <span class="string">server</span> &#123;</span><br><span class="line">            <span class="string">listen</span> <span class="number">80</span> <span class="string">default_server;</span></span><br><span class="line">            <span class="string">listen</span> [<span class="string">::</span>]<span class="string">:80</span> <span class="string">default_server</span> <span class="string">ipv6only=on;</span></span><br><span class="line"></span><br><span class="line">            <span class="string">listen</span> <span class="number">443</span> <span class="string">ssl;</span></span><br><span class="line"></span><br><span class="line">            <span class="string">root</span> <span class="string">/usr/share/nginx/html;</span></span><br><span class="line">            <span class="string">index</span> <span class="string">index.html;</span></span><br><span class="line"></span><br><span class="line">            <span class="string">server_name</span> <span class="string">localhost;</span></span><br><span class="line">            <span class="string">ssl_certificate</span> <span class="string">/etc/nginx/ssl/nginx.crt;</span></span><br><span class="line">            <span class="string">ssl_certificate_key</span> <span class="string">/etc/nginx/ssl/nginx.key;</span></span><br><span class="line"></span><br><span class="line">            <span class="string">location</span> <span class="string">/</span> &#123;</span><br><span class="line">                    <span class="string">try_files</span> <span class="string">$uri</span> <span class="string">$uri/</span> <span class="string">=404;</span></span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 確認 ConfigMap 已經設定完成</span></span><br><span class="line">kubectl get configmap/my-secure-nginx-config</span><br><span class="line">NAME                     DATA   AGE</span><br><span class="line">my-secure-nginx-config   1      11s</span><br></pre></td></tr></table></figure><h2 id="新增-Deployment-amp-Service"><a href="#新增-Deployment-amp-Service" class="headerlink" title="新增 Deployment &amp; Service"></a>新增 Deployment &amp; Service</h2><p>最後要新增 Deployment &amp; Service 之前，要先確認以下幾件事情：</p><ol><li><p>secret 已經新增，名稱為 <code>my-secure-nginx-secret</code>，裡面的檔案為 <code>nginx.crt</code> &amp; <code>nginx.key</code></p></li><li><p>ConfigMap 已經新增，名稱為 <code>my-secure-nginx-config</code>，作為 nginx 的設定，檔名為 <code>default.conf</code></p></li><li><p>nginx default 設定檔位置為 <code>/etc/nginx/conf.d/default.conf</code> (檔名與 ConfigMap 中的 data 設定相同)</p></li><li><p>nginx 中預設存放 certificate 的路徑為 <code>/etc/nginx/ssl</code>，檔名為 <code>nginx.crt</code> &amp; <code>nginx.key</code> (與 secret 中的 data 設定相同)</p></li></ol><p>確認了以上幾個要項之後，就可以套用以下設定來建立 Deployment &amp; Service：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-secure-nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-secure-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">my-secure-nginx</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-secure-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">my-secure-nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">my-secure-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">        <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">my-secure-nginx-secret</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">configmap-volume</span></span><br><span class="line">        <span class="attr">configMap:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">my-secure-nginx-config</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginxhttps</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ymqytw/nginxhttps:1.5</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;/home/auto-reload-nginx.sh&quot;</span>]</span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">443</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/index.html</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line">          <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/ssl</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/conf.d</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">configmap-volume</span></span><br></pre></td></tr></table></figure><h2 id="從外部驗證-HTTPS-服務"><a href="#從外部驗證-HTTPS-服務" class="headerlink" title="從外部驗證 HTTPS 服務"></a>從外部驗證 HTTPS 服務</h2><p>套用以上設定後，檢視一下目前系統狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 僅列出與 my-secure-nginx 相關的物件</span></span><br><span class="line">$ kubectl get all</span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/my-secure-nginx-66646484d-hwrkr   1/1     Running   0          10s</span><br><span class="line"></span><br><span class="line">NAME                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">service/my-secure-nginx   NodePort    10.233.43.113   &lt;none&gt;        80:30779/TCP,443:31770/TCP   10s</span><br><span class="line"></span><br><span class="line">NAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/my-secure-nginx   1         1         1            1           10s</span><br><span class="line"></span><br><span class="line">NAME                                        DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/my-secure-nginx-66646484d   1         1         1       10s</span><br></pre></td></tr></table></figure><p>由於 service 設定為 NodePort 的關係，以下測試可以從外面來進行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 測試 HTTP (without TLS certificate)</span></span><br><span class="line">$ curl http://10.107.13.10:30779</span><br><span class="line">... (略)</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試 HTTPS (with TLS certificate)</span></span><br><span class="line"><span class="comment"># 因為這是 self-signed certificate，因此必須加上 -k(--insecure) 參數</span></span><br><span class="line">$ curl -k https://10.107.13.10:31770</span><br><span class="line">... (略)</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br></pre></td></tr></table></figure><h2 id="從內部驗證-HTTPS-服務"><a href="#從內部驗證-HTTPS-服務" class="headerlink" title="從內部驗證 HTTPS 服務"></a>從內部驗證 HTTPS 服務</h2><p>若從內部驗證，我們就可以使用已經存在的 TLS certificate 來進行驗證，首先套用以下設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">curl-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">curlpod</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">curlpod</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">secret-volume</span></span><br><span class="line">        <span class="attr">secret:</span></span><br><span class="line">          <span class="attr">secretName:</span> <span class="string">my-secure-nginx-secret</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">curlpod</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">sh</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">while</span> <span class="literal">true</span><span class="string">;</span> <span class="string">do</span> <span class="string">sleep</span> <span class="number">1</span><span class="string">;</span> <span class="string">done</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">radial/busyboxplus:curl</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/nginx/ssl</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">secret-volume</span></span><br></pre></td></tr></table></figure><p>接著取得 pod 的名稱並進行驗證：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得 pod 名稱</span></span><br><span class="line">$ kubectl get pods -l app=curlpod</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">curl-deployment-78959f7dcc-4csbk   1/1     Running   0          13s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 pod 中的 curl 命令搭配 CA 憑證來驗證 (此時就不需要 -k 參數了)</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> curl-deployment-78959f7dcc-4csbk -- curl https://my-secure-nginx --cacert /etc/nginx/ssl/nginx.crt</span><br><span class="line">... (略)</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">... (略)</span><br></pre></td></tr></table></figure><p>看到 <strong>Welcome to nginx!</strong> 且沒有憑證相關問題的警告就沒錯啦!</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#exposing-the-service">Connecting Applications with Services - Kubernetes</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Networking </tag>
            
            <tag> CKA Networking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] DNS for Service &amp; Pod</title>
      <link href="/blog/Kubernetes/k8s-DNS-for-Service-and-Pod/"/>
      <url>/blog/Kubernetes/k8s-DNS-for-Service-and-Pod/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>DNS service 在 k8s cluster 內部負責 service discovery 的重責大任←讓使用者可以跳脫 IP based 的思維，以 domain name 的方式來進行 application 的架構設計。</p><p>而 k8s DNS service 其實也是由一個 DNS pod + service 所組成，並且告訴 kubelet 將每個 container 的 DNS resolution 都指向這個 DNS service，藉由此方式，每個 pod 在存取其他 service 時，都會向一開始就安裝好的 DNS pod 進行 domain name 的詢問。</p><h1 id="Domain-Name-解析規則"><a href="#Domain-Name-解析規則" class="headerlink" title="Domain Name 解析規則"></a>Domain Name 解析規則</h1><p>要了解 k8s Domain Name 的解析規則，必須清楚知道以下兩件事情：</p><ol><li><p>k8s 中有 namespace 的概念，由於不同的 namespace 中可以有同樣名稱的 service or pod，因此 DNS 解析的部份就需要考慮 namespace</p></li><li><p>k8s cluster domain name，若是未設定，預設就會是 <code>cluster.local</code></p></li></ol><p>有了以上兩個概念之後，接著可以繼續往下。</p><p>假設目前 k8s 有以下幾個 resource object：</p><ul><li><p>兩個 namespace，分別是 <code>ns1</code> &amp; <code>ns2</code></p></li><li><p>在 ns1 中，有個 service 名稱為 <code>svc1</code>，與其相關連的 pod 為 <code>pod1</code></p></li><li><p>在 ns2 中，有個 service 名稱為 <code>svc2</code>，與其相關連的 pod 為 <code>pod2</code></p></li></ul><p>假設目前有個 pod 位於 ns1 中：</p><ul><li><p>可透過 domain name <code>svc1</code> or <code>svc1.ns1</code> or <code>svc1.ns1.svc.cluster.local</code> 存取 svc1</p></li><li><p>可透過 domain name <code>svc2.ns2</code> or <code>svc2.ns2.svc.cluster.local</code> 存取 svc2 (但無法使用 <code>svc1</code>，因為在不同的 namespace 中)</p></li></ul><p>反之亦然。</p><h1 id="DNS-for-Service"><a href="#DNS-for-Service" class="headerlink" title="DNS for Service"></a>DNS for Service</h1><h2 id="準備環境"><a href="#準備環境" class="headerlink" title="準備環境"></a>準備環境</h2><p>為了進行後面關於 DNS 的實驗，我們透過以下的 YAML 建立 service：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">k8s-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">run:</span> <span class="string">k8s-nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">k8s-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">k8s-nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">svc-cluster</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">k8s-nginx</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">  </span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">svc-headless</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">k8s-nginx</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br></pre></td></tr></table></figure><p>套用以上設定後，可以看到類似以下資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視套用上面設定後的結果</span></span><br><span class="line">$ kubectl get all</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/k8s-nginx-6fb585ddf-4kf46   1/1     Running   0          95s</span><br><span class="line">pod/k8s-nginx-6fb585ddf-6fbkq   1/1     Running   0          95s</span><br><span class="line">pod/k8s-nginx-6fb585ddf-lhdzp   1/1     Running   0          95s</span><br><span class="line"></span><br><span class="line">NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/kubernetes     ClusterIP   10.233.0.1     &lt;none&gt;        443/TCP   32d</span><br><span class="line">service/svc-cluster    ClusterIP   10.233.49.77   &lt;none&gt;        80/TCP    95s</span><br><span class="line">service/svc-headless   ClusterIP   None           &lt;none&gt;        80/TCP    95s</span><br><span class="line"></span><br><span class="line">NAME                        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/k8s-nginx   3         3         3            3           95s</span><br><span class="line"></span><br><span class="line">NAME                                  DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps/k8s-nginx-6fb585ddf   3         3         3       95s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 endpoint 相關資訊</span></span><br><span class="line">$ kubectl get endpoints</span><br><span class="line">NAME           ENDPOINTS                                               AGE</span><br><span class="line">.... (略)</span><br><span class="line">svc-cluster    10.233.103.201:80,10.233.76.10:80,10.233.76.11:80       108s</span><br><span class="line">svc-headless   10.233.103.201:80,10.233.76.10:80,10.233.76.11:80       108s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 k8s DNS service IP</span></span><br><span class="line">$ kubectl -n kube-system get svc</span><br><span class="line">NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">coredns                ClusterIP   10.233.0.3     &lt;none&gt;        53/UDP,53/TCP,9153/TCP   32d</span><br></pre></td></tr></table></figure><h2 id="A-records"><a href="#A-records" class="headerlink" title="A records"></a>A records</h2><p>在以上的範例中(namespace = <code>default</code>)，k8s 會為 service 自動建立 A record 如下：</p><ul><li>svc-cluster.default.svc.cluster.local</li></ul><blockquote><p>一般的 type=ClusterIP 的 service，因此會有一個匹配的 cluster IP</p></blockquote><ul><li>svc-headless.default.svc.cluster.local</li></ul><blockquote><p><code>clusterIP: None</code>，屬於 headless service，因此不會有 cluster IP，解析出來的結果會是其 endpoints 資訊</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 從 host level 使用 nslookup</span></span><br><span class="line">$ nslookup</span><br><span class="line">&gt; server 10.233.0.3   <span class="comment"># 切換 DNS server 到 k8s DNS svc</span></span><br><span class="line">Default server: 10.233.0.3</span><br><span class="line">Address: 10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 &quot;default&quot; namespace 中的 &quot;svc-cluster&quot; service domain name</span></span><br><span class="line"><span class="comment"># (type = ClusterIP)</span></span><br><span class="line">Name:    svc-cluster.default.svc.cluster.local</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    svc-cluster.default.svc.cluster.local</span><br><span class="line">Address: 10.233.49.77</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 &quot;default&quot; namespace 中的 &quot;svc-headless&quot; service domain name</span></span><br><span class="line"><span class="comment"># (clusterIP: None)</span></span><br><span class="line">&gt; svc-headless.default.svc.cluster.local</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    svc-headless.default.svc.cluster.local</span><br><span class="line">Address: 10.233.103.201</span><br><span class="line">Name:    svc-headless.default.svc.cluster.local</span><br><span class="line">Address: 10.233.76.11</span><br><span class="line">Name:    svc-headless.default.svc.cluster.local</span><br><span class="line">Address: 10.233.76.10</span><br></pre></td></tr></table></figure><h2 id="SRV-records"><a href="#SRV-records" class="headerlink" title="SRV records"></a>SRV records</h2><p>除了 A record 之外，k8s DNS 還會額外建立相對應的 SRV record，並且用以下的命名規則來產生：</p><blockquote><p>[_my-port-name].[_my-port-protocol].[svc_name].[namespace_name].svc.cluster.local</p></blockquote><p>以下進行實際的查詢來檢視一下 DNS SRV record 產生的結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### 繼續上面的 nslookup 測試 ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改查詢 type 為 SRV</span></span><br><span class="line">&gt; <span class="built_in">set</span> <span class="built_in">type</span>=srv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試 [_my-port-name].[_my-port-protocol].[svc_name].[namespace_name].svc.cluster.local</span></span><br><span class="line">&gt; _http._tcp.svc-cluster.default.svc.cluster.local</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">_http._tcp.svc-cluster.default.svc.cluster.local    service = 0 100 80 svc-cluster.default.svc.cluster.local.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試 [_my-port-name].[_my-port-protocol].[headless_svc_name].[namespace_name].svc.cluster.local</span></span><br><span class="line">&gt; _http._tcp.svc-headless.default.svc.cluster.local                                           </span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">_http._tcp.svc-headless.default.svc.cluster.local    service = 0 33 80 10-233-103-201.svc-headless.default.svc.cluster.local.</span><br><span class="line">_http._tcp.svc-headless.default.svc.cluster.local    service = 0 33 80 10-233-76-10.svc-headless.default.svc.cluster.local.</span><br><span class="line">_http._tcp.svc-headless.default.svc.cluster.local    service = 0 33 80 10-233-76-11.svc-headless.default.svc.cluster.local.</span><br></pre></td></tr></table></figure><h1 id="DNS-for-Pod"><a href="#DNS-for-Pod" class="headerlink" title="DNS for Pod"></a>DNS for Pod</h1><h2 id="A-record"><a href="#A-record" class="headerlink" title="A record"></a>A record</h2><p>跟 service 相同，每個 pod 在產生的時候也會以下述的格式分配一個 DNS A record 在 k8s DNS service 中：</p><blockquote><p>[pod-ip-address].[namespace-name].pod.cluster.local</p></blockquote><p>因此假設 pod 的 ip 為 <code>1.2.3.4</code>，namespace 為 <code>default</code>，且 cluster domain 為 <code>cluster.local</code>，那就會有 <code>1-2-3-4.default.pod.cluster.local</code> 這筆 A record 產生。</p><blockquote><p>以上的部份是<a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#a-records-1">官方文件</a>上提到會有的設定，但實際上實驗結果(k8s version = <code>1.12.1</code>)並不是這樣! 並沒有在 k8s DNS service 中看到上述的 A record，因此這個部份還有帶後續釐清</p></blockquote><h2 id="Pod-的-hostname-amp-subdomain"><a href="#Pod-的-hostname-amp-subdomain" class="headerlink" title="Pod 的 hostname &amp; subdomain"></a>Pod 的 hostname &amp; subdomain</h2><p>這裡要分成兩個部份來說：</p><ul><li><p><code>hostname</code>: 在預設情況下，每個 pod 的 <strong>hostname</strong> 都會使用在 pod 定義中 <code>metadata.name</code> 的值</p></li><li><p><code>subdomain</code>：預設情況下不會有這個部份的設定出現</p></li></ul><p>但以上兩個其實都是可以在 pod spec 中定義的，分別是 <code>.spec.hostname</code> &amp; <code>.spec.subdomain</code>。</p><p>因此假設有個 pod spec 有以下設定：</p><ul><li><p><code>.spec.hostname</code>: <strong>foo</strong></p></li><li><p><code>.spec.subdomain</code>: <strong>bar</strong></p></li></ul><p>並新增到 namespace <code>default</code> 中，就會在該 pod 中的 <strong>/etc/hosts</strong> 中多出 <code>foo.bar.default.svc.cluster.local</code> &amp; pod IP 的對應記錄，但是並不會出現在 k8s DNS service 中。</p><blockquote><p>不會出現在 k8s DNS service 中的部份，跟<a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-hostname-and-subdomain-fields">官網文件</a>說明有出入，同樣也是待後續釐清</p></blockquote><h2 id="DNS-Policy-amp-Config"><a href="#DNS-Policy-amp-Config" class="headerlink" title="DNS Policy &amp; Config"></a>DNS Policy &amp; Config</h2><p>此外 pod spec 中還可以根據自身的需求，額外設定 DNS policy &amp; config，但在進行這樣的設定之前，建議先把 Linux <code>/etc/resolv.conf</code> 中的相關設定搞清楚，會比較容易理解這一段。</p><p>關於 Linux <code>/etc/resolv.conf</code> 的設定說明，可以參考下列文章：</p><ul><li><p><a href="http://www.tsnien.idv.tw/Internet_WebBook/chap13/13-8%20DNS%20%E5%AE%A2%E6%88%B6%E7%AB%AF%E8%A8%AD%E5%AE%9A.html">DNS 客戶端設定</a></p></li><li><p><a href="https://www.jianshu.com/p/2c1c081cc521">Linux下域名解析的优化 - 简书</a></p></li><li><p><a href="https://my.oschina.net/guol/blog/114297">Linux 本地dns配置文件详解 - 好脑袋和烂笔头 - 开源中国</a></p></li><li><p><a href="http://blog.51cto.com/linuxtro/282776">DNS域名解析时的顺序问题-linux on the way-51CTO博客</a></p></li></ul><p>有了以上的 DNS 觀念後，進行 pod DNS policy &amp; config 設定時就會清楚每個設定所產生的效果會是如何了。</p><p>而這個部份(pod DNS policy &amp; config)已經有網友有相當詳細的說明，可以參考下列文章：</p><ul><li><a href="https://www.hwchiu.com/kubernetes-dns.html">[Kubernetes] DNS setting in your Pod | Hwchiu Learning Note</a></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">DNS for Services and Pods - Kubernetes</a></p></li><li><p><a href="https://dotblogs.com.tw/jerrymow/2014/01/06/139132">DNS (bind) 設定 - SRV 紀錄 - Office 365 - Lync Server | 黃昏的甘蔗 - 點部落</a></p></li><li><p><a href="https://www.hwchiu.com/kubernetes-dns.html">[Kubernetes] DNS setting in your Pod | Hwchiu Learning Note</a></p></li><li><p><a href="http://www.tsnien.idv.tw/Internet_WebBook/chap13/13-8%20DNS%20%E5%AE%A2%E6%88%B6%E7%AB%AF%E8%A8%AD%E5%AE%9A.html">DNS 客戶端設定</a></p></li><li><p><a href="https://www.jianshu.com/p/2c1c081cc521">Linux下域名解析的优化 - 简书</a></p></li><li><p><a href="https://my.oschina.net/guol/blog/114297">Linux 本地dns配置文件详解 - 好脑袋和烂笔头 - 开源中国</a></p></li><li><p><a href="http://blog.51cto.com/linuxtro/282776">DNS域名解析时的顺序问题-linux on the way-51CTO博客</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Networking </tag>
            
            <tag> CKA Networking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Service Overview</title>
      <link href="/blog/Kubernetes/k8s-Service-Overview/"/>
      <url>/blog/Kubernetes/k8s-Service-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>關於 Service，有兩個重點部份是必須先了解的：</p><ol><li><p>當 workload 變成 micro service(pod) 進到 k8s 後，在存取上會遇到什麼問題?</p></li><li><p>Service 如何解決這些問題?</p></li></ol><h2 id="Pod-存取問題"><a href="#Pod-存取問題" class="headerlink" title="Pod 存取問題"></a>Pod 存取問題</h2><p>pod 的生命周期不是並永久的，一旦掛掉之後就不會在自己恢復，因此若使用者針對 pod 有多個 replica 的需求，k8s 中額外設計了像是 <code>ReplicaSet</code> or <code>Deployment</code> 這一類的 resource object 來確保 pod 掛掉之後可以再重啟。</p><p>但 pod 重啟以後會發生什麼問題呢? 答案就是 **”IP 也會跟著改變”**。</p><blockquote><p>其實佈署 pod 時也無法指定 IP address</p></blockquote><p>由於 pod IP 可能隨時會變動，因此若是依照傳統 application 的設定方式，使用 ip 設定 DB or web 的位址，上到 k8s 之後可能就很快就無法正常運作，因此 pod 的存取就變成了一個大問題。</p><h2 id="Service-如何解決問題"><a href="#Service-如何解決問題" class="headerlink" title="Service 如何解決問題?"></a>Service 如何解決問題?</h2><p>有鑑於上面的問題，k8s 就提出了 <code>Service</code> 這個 resource object，在 pod 的前方提供了一個抽象層，讓外部的服務可以用 <strong>domain name</strong> 的方式存取 pod，而 <strong>domain name &lt;–&gt; Pod</strong> 這一段問題，就由 Service 來處理。</p><p>但利用 domain name 來存取終究還是需要一個 IP address，而每個 Service 都會自帶 VIP，讓 network traffic 有辦法正常送過來，並導到後方真正提供服務的 pod。</p><p>而當 network traffic 進到 service 後，要決定導到哪些 pod 是要怎麼做呢? 答案則是透過 <code>Label Selector</code>。</p><h1 id="定義-Service"><a href="#定義-Service" class="headerlink" title="定義 Service"></a>定義 Service</h1><h2 id="搭配-selector"><a href="#搭配-selector" class="headerlink" title="搭配 selector"></a>搭配 selector</h2><p>由於要先有 Pod 才會有定義 Service 的需求，因此假設 k8s 中已經有一些 Pod 的存在(同時對外開放 TCP port 9376)，並帶有 <code>app=MyApp</code> 的 label，此時就可以定義一個 Service 來作為這些 pod 前方的抽象層，透過 domain name 的方式提供服務：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># type 一共有四種(ClusterIP, NodePort, LoadBalancer, ExternalName)</span></span><br><span class="line">  <span class="comment"># 預設是 ClusterIP</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="comment"># 選擇帶有 &quot;app=MyApp&quot; 的 pod</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">MyApp</span></span><br><span class="line">  <span class="comment"># Service 實際對外服務的設定</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="comment"># 此為 Pod 對外開放的 port number</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>透過以上的定義，會產生出以下的 network topology：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pod  &lt;---&gt;  Endpoint(tcp:9376)  &lt;---&gt; Service(tcp:80, with VIP)</span><br></pre></td></tr></table></figure><p>Service 會為每個符合 label selector 設定的 pod 建立一個 Endpoint 的 resource object 來與之搭配。</p><p>此外，在 Service 設定上還有以下有幾個需要額外注意的重點：</p><ul><li><p>若是 targetPort 不設定，預設會與 <code>spec.ports.port</code> 相同</p></li><li><p>若 protocol 不設定，預設使用 <code>TCP</code></p></li><li><p>從 v1.12 開始支援 <a href="https://zh.wikipedia.org/wiki/%E6%B5%81%E6%8E%A7%E5%88%B6%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE">SCTP</a>(適合用於電信產業 or 提供串流服務時使用)，但預設關閉，需要透過 <code>SCTPSupport</code> feature gate 開啟</p></li></ul><h2 id="不與-selector-搭配"><a href="#不與-selector-搭配" class="headerlink" title="不與 selector 搭配"></a>不與 selector 搭配</h2><p>一般 Service 是作為 存取 pod 用的抽象層，但其實作為其他 backend 的抽象層，而這一類的需求可能來自於下列的情況：</p><ul><li><p>在生產環境中有個 external database，但在測試環境中是使用 internal database</p></li><li><p>想要將 Service 指到位於其他 namespace or cluster 中的 service</p></li><li><p>想要將某些 workload 移到 k8s 上面跑，但有些 backend service 依然還在 k8s 之外</p></li></ul><p>在以上的情況下，就不需要設定 label selector，因為此時的 service 並不是以 pod 作為 backend service，因此可以透過以下的定義來建立：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>由於沒有設定 label selector，因此也就不會產生相對應的 endpoint，此時我們就需要自己建立一個 Endpoint resource object，並指到真正的 backend service：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">1.2</span><span class="number">.3</span><span class="number">.4</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9376</span></span><br></pre></td></tr></table></figure><p>透過以上兩組設定，就可以將到達 my-service 的網路流量導向 1.2.3.4:9376 去。</p><p>另外，在設定 Endpoint 時有幾點需要注意：</p><ol><li><p>不能使用 loopback (127.0.0.0/8), link-local (169.254.0.0/16), or link-local multicast (224.0.0.0/24) … 等幾段 IP</p></li><li><p>也不能使用設定在 k8s 中的 cluster ip 網段</p></li></ol><blockquote><p>以上的範例的 service type 皆為 <code>ClusterIP</code>；另外還有一種稱為 <code>ExternalName</code>，同樣也是沒有 selector 但以 DNS name 為基礎來進行設定</p></blockquote><h2 id="Multi-Port-Services"><a href="#Multi-Port-Services" class="headerlink" title="Multi-Port Services"></a>Multi-Port Services</h2><p>若是 service 想要對外開放多個 port 也沒有問題：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">MyApp</span></span><br><span class="line">  <span class="comment"># 透過指定不同的 name 可以清楚知道每個 port 的目的</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9376</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">https</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9377</span></span><br></pre></td></tr></table></figure><blockquote><p>但相對的後端的 pod 也要有開放多個 port 才有效果</p></blockquote><h1 id="Virtual-IPs-amp-service-proxies"><a href="#Virtual-IPs-amp-service-proxies" class="headerlink" title="Virtual IPs &amp; service proxies"></a>Virtual IPs &amp; service proxies</h1><p>其實 Service 從 v1.0 開始到現在，在功能面上已經進化了不少，透過了解這些功能的發展歷史，為何新功能會被發展出來? 以及新功能的優缺點 … 等等，使用者可以根據需求選擇適合的 solution。</p><h2 id="演進歷史"><a href="#演進歷史" class="headerlink" title="演進歷史"></a>演進歷史</h2><p>每個 k8s 的 node 中都會有一個 <code>kube-proxy</code> 的服務，用來處理到達 Service(不包含 <strong>ExternalName</strong> type) VIP 的網路流量。</p><p>而 Service 的發展歷史大概如下：</p><ol><li><p><code>v1.0</code>：Service 此時作為一個 proxy 且只能進行 layer 4(TCP/UDP over IP) 的處理，而 proxy process 存在於 userspace</p></li><li><p><code>v1.1</code>：新增了 Ingress，透過 Ingress 可以處理 layer 7(HTTP) 的流量，而 Service 的部份同時也有 iptables proxy mode 新增</p></li><li><p><code>v1.2</code>：iptables proxy mode 變成 Service 預設的運作模式</p></li><li><p><code>v1.8</code>：新增 ipvs proxy mode (v1.9 之後變成 beta 功能)</p></li></ol><p>以下說明三種不同的 proxy mode(userspace, iptables, ipvs) 是如何實現，以及每一種的優缺點。</p><h2 id="userspace-proxy-mode"><a href="#userspace-proxy-mode" class="headerlink" title="userspace proxy mode"></a>userspace proxy mode</h2><p><img src="/blog/images/kubernetes/k8s_service-userspace-proxy-mode.png" alt="Kubernetes Service userspace proxy mode"></p><p>kube-proxy 會為每一個 service 隨機開啟一個 port 在本地端上，任何通往該 port 的網路流量都會經由 kube-proxy 導向後方的 pod(也就是 Endpoint)；後來 service 增加了 clusterIP + port 的方式，可以透過 IP:port 的方式存取 service，但所有的網路流量依然會透過 kube-proxy 來導向後方的 pod。</p><p>然而這樣的問題在於，由於 kube-proxy 本身是屬於 userspace process，因此實際上網路封包的處理會變成：</p><blockquote><p>service –&gt; kube-proxy(userspace) –&gt; Linux kernel(kernelspace) –&gt; pod(userspace)</p></blockquote><p>一個網路封包必須從 userspace 進入到 kernelspace，再回到 userspace，整體的效能會損耗不少，因此後來才會有其他 proxy mode 的出現。</p><h2 id="iptables-proxy-mode"><a href="#iptables-proxy-mode" class="headerlink" title="iptables proxy mode"></a>iptables proxy mode</h2><p><img src="/blog/images/kubernetes/k8s_service-iptables-proxy-mode.png" alt="Kubernetes Service userspace proxy mode"></p><p>在 iptables proxy mode 中，完全利用了 kernel netfilter 來實現 service 的功能，而每個 service 依然是由 <code>ClusterIP</code> + <code>Port</code> + <code>Endpoint(Pod)</code> 的組合，而這些組合會變成一條一條的 Linux iptables 規則並設定到 k8s cluster 的每一個 node 上</p><p>由於所有網路流量都透過 netfilter 處理，因此也就不會有封包在 userspace &amp; kernelspace 中來來回回的狀況發生，也因此效能上比起 userspace proxy mode 會比較好。</p><p>當然這樣的作法也產生另外一個問題，就是 <strong>Pod 會重啟，那要如何正確的維護 iptables rules?</strong></p><p>而這問題的解決方式並不難，只要讓 k8s 知道 pod 是否還可以正常服務即可，我們可以設定 k8s 中的 [Pod Readiness Probes] 來達到此目的。</p><p>最後，其實 iptables proxy mode 並沒有解決 performace 的問題，因為當 service &amp; endpoint 的組合的數量龐大時，那就會產生成千上萬的 iptables rules 在 node 上，每個網路封包進來都要比對所有規則，對於資源的虛耗可想而知。</p><h2 id="ipvs-proxy-mode"><a href="#ipvs-proxy-mode" class="headerlink" title="ipvs proxy mode"></a>ipvs proxy mode</h2><p><img src="/blog/images/kubernetes/k8s_service-ipvs-proxy-mode.png" alt="Kubernetes Service userspace proxy mode"></p><p>在 ipvs proxy mode 中，kube-proxy 一樣會監控 Service &amp; Endpoint 的變化，並呼叫 <code>netlink</code> 來同步產生對應的 ipvs rules。</p><p>跟 iptables 很類似，ipvs 的運作也是基於 netfilter hook function，但在 kernel space 中是搭配 hash table 來運作，因此在效率上比起 iptables proxy mode 快上許多，也不會因為規則變多而讓效能急遽下降。</p><p>此外，ipvs 在負載平衡的演算法部份提供了更多選項，例如：<code>rr</code>(round-robin), <code>lc</code>(least connection), <code>dh</code>(destination hashing), <code>sh</code>(source hashing), <code>sed</code>(shortest expected delay) 以及 <code>nq</code>(never queue) … 等等。</p><p>在此模式下，任何到達 ServiceVIP:port 的網路封包都會直接被導向後端的 pod，跟 iptables proxy mode 很像，但就是由 virtual server 的角色來處理掉了。</p><blockquote><p>要安裝 ipvs proxy mode 必須安裝 IPVS kernel module 才行，否則會被強制改回 iptables proxy mode</p></blockquote><h1 id="Service-Discovery"><a href="#Service-Discovery" class="headerlink" title="Service Discovery"></a>Service Discovery</h1><p>Service 身為多個 pod 的服務入口，必須要有簡單的方式可以找到這些 Service 才對，在 k8s 中提供了兩種模式來進行 service discovery，分別是<code>環境變數</code> &amp; <code>DNS</code>。</p><h2 id="環境變數"><a href="#環境變數" class="headerlink" title="環境變數"></a>環境變數</h2><p>在此模式下，k8s 會在 pod 分派到 worker node 上時，塞進一些與這個 pod 相關的 service 環境變數資訊，並以 <code>&#123;SVCNAME&#125;_SERVICE_HOST</code> &amp; <code>&#123;SVCNAME&#125;_SERVICE_PORT</code> 兩種格式的環境變數存在 pod 中。</p><p>假設指定 service <code>redis-master</code> 並開放 <code>TCP port 6379</code>，且 Service cluster IP address 為 <code>10.0.0.11</code>，在 pod 中就會出現以下環境變數：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">REDIS_MASTER_SERVICE_HOST</span>=<span class="number">10.0</span>.<span class="number">0.11</span></span><br><span class="line"><span class="attr">REDIS_MASTER_SERVICE_PORT</span>=<span class="number">6379</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT</span>=tcp://<span class="number">10.0</span>.<span class="number">0.11</span>:<span class="number">6379</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT_6379_TCP</span>=tcp://<span class="number">10.0</span>.<span class="number">0.11</span>:<span class="number">6379</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT_6379_TCP_PROTO</span>=tcp</span><br><span class="line"><span class="attr">REDIS_MASTER_PORT_6379_TCP_PORT</span>=<span class="number">6379</span></span><br><span class="line"><span class="attr">REDIS_MASTER_PORT_6379_TCP_ADDR</span>=<span class="number">10.0</span>.<span class="number">0.11</span></span><br></pre></td></tr></table></figure><p>雖然這個模式不需要 DNS resolution，但會有另外兩個問題：</p><ol><li><p>環境變數的管理上會較為複雜</p></li><li><p>Service 必須要比 Pod 更早出現才行(必須考慮佈署時的順序)，否則 pod 不會有相對應的環境變數存在(因為這些環境變數是在 pod 生成時放入的)</p></li></ol><p>因此其實不太推荐使用這個模式來佈署 Service。</p><h2 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h2><p>在此模式下，會在 k8s cluster 中安裝一個額外的 DNS server(<a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/kube-dns">KubeDNS</a> or <a href="https://coredns.io/">CoreDNS</a>) 來處理 DNS resolution 的問題，那 Service 的命名方式呢?</p><p>假設 service 所在的 namespace 為 <code>my-ns</code>，service name 為 <code>my-service</code>，cluster domain name 為 <code>cluster.local</code>(安裝時指定)，那 service 就可以用以下的 DNS 來存取：</p><ul><li><p><code>my-service</code> (必須在同一個 namespace 中)</p></li><li><p><code>my-service.my-ns</code></p></li><li><p><code>my-service.my-ns.cluster.local</code></p></li></ul><p>而 port number 的部份同樣也可以查詢，以上面的例子來說，設定了一個 port name 為 <code>http</code> &amp; protocol <code>TCP</code>，就可以透過 DNS SRV 查詢 <code>_http._tcp.my-service.my-ns</code> 來取得所設定的 port number。</p><blockquote><p>若是要使用 <code>ExternalName</code> 的功能，就只能使用 DNS 模式。</p></blockquote><h1 id="Headless-Service"><a href="#Headless-Service" class="headerlink" title="Headless Service"></a>Headless Service</h1><p>介紹 Headless Service 之前，先回顧一下一般的 service 是如何運作的….</p><p>一般的 service 都會帶有一個 ClusterIP，而 client 存取 service 時，k8s 中的 DNS server 會協助將 service domain name 解析為對應的 ClusterIP，而網路流量到達 ClusterIP 後，就會由 kube-proxy 所維護的 iptables rules 來將其分配到後端的 pod 中</p><p>詳細的運作流程可以看此篇文章(<a href="https://www.hwchiu.com/kubernetes-service-ii.html">[Kubernetes] How to Implement Kubernetes Service - ClusterIP | Hwchiu Learning Note</a>)的介紹，說明的很詳細。</p><p>那 Headless 又是怎麼回事? 顧名思義就是一個沒有定義 ClusterIP 的 Service (將 <code>.spec.clusterIP</code> 設定為 <code>None</code>)，以下是一個簡單的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure><p>沒有定義 ClusterIP 的 Service 會有什麼特性 or 效果呢?</p><ul><li><p>沒有 ClusterIP，因此存取 service 時，k8s DNS 就沒有任何 ClusterIP 的資訊可以回應給 client</p></li><li><p>若有搭配 Label Selectors，k8s 就會建立相對應的 endpoint，而存取 service 時，k8s DNS 就會直接回應 endpoint list 的資訊(<strong>A record</strong>)，因此 <strong>client 可以使用 service domain name 直接存取到 pod</strong></p></li></ul><blockquote><p>這代表原本 service 透過 Linud iptables 所提供的負載平衡的功能就沒有了，但 client 可以根據 endpoint 的資訊來對 pod 進行直接的存取</p></blockquote><ul><li>若是沒有搭配 Label Selector，就沒有 ClusterIP 也沒有對應的 Endpoint，但應該沒有人會做這麼做，而是改以以 ExternalName type 的方式來進行(後面會提到)</li></ul><p>最後，需要知道的是，在一般情況下 Headless Service 應該沒什麼機會會使用到，但若是要定義 <code>Statefulful</code> resource object，就必須要搭配 Headless Service 才行。</p><h1 id="Service-Types"><a href="#Service-Types" class="headerlink" title="Service Types"></a>Service Types</h1><p>上面所介紹的部份，其實都是屬於 ClusterIP type 的 service，但其實 service type 一共有四種，分別是：</p><ul><li><p><code>ClusterIP</code>：提供 cluster 內部存取</p></li><li><p><code>NodePort</code>：供外部存取</p></li><li><p><code>LoadBalancer</code>：供外部存取 (但只限於支援此 type 的 public cloud)</p></li><li><p><code>ExternalName</code>：單純回應給 client 外部的 DNS CName record (並非由 k8s cluster 內部的 pod 服務)</p></li></ul><h2 id="ClusterIP"><a href="#ClusterIP" class="headerlink" title="ClusterIP"></a>ClusterIP</h2><p>這個部份上面就有介紹過了，詳細的運作行為可以參考以下文章說明：</p><ul><li><a href="https://www.hwchiu.com/kubernetes-service-ii.html">[Kubernetes] How to Implement Kubernetes Service - ClusterIP | Hwchiu Learning Note</a></li></ul><blockquote><p>只是必須注意的是，將 type 設定為 ClusterIP 的 service，只能在 k8s cluster 內部存取；若是要提供給外部存取，則是要使用下面介紹的 NodePort &amp; LoadBalancer 兩種 service type</p></blockquote><h2 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h2><p>在私有內部的環境中，要讓 k8s cluster 外部存取佈署好的 pod，透過 NodePort type service 是其中一個方式，以下是一個設定 NodePort type service 的簡單範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nodeport-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 將 type 設定為 NodePort</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="comment"># 選擇帶有 &quot;app=MyApp&quot; 的 pod</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">MyApp</span></span><br><span class="line">  <span class="comment"># Service 實際對外服務的設定</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9376</span></span><br><span class="line">    <span class="comment"># 指定 NodePort number(.spec.ports[*].nodePort)</span></span><br><span class="line">    <span class="comment"># 也可不指定(30000~32767)</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">30036</span></span><br></pre></td></tr></table></figure><p>套用以上設定後，可以看到 cluster 內部就出現了相對應的資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get all</span><br><span class="line">NAME                          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">.... (略)</span><br><span class="line">service/my-nodeport-service   NodePort    10.233.44.13   &lt;none&gt;        80:30036/TCP   7s</span><br></pre></td></tr></table></figure><p>而且 kube-proxy 還會在每個 node 上放上相對應的 iptables rule：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不僅放上規則，還會附帶清楚的註解</span></span><br><span class="line">$ iptables-save | grep my-nodeport-service</span><br><span class="line">-A KUBE-EXTERNAL-SERVICES -p tcp -m comment --comment <span class="string">&quot;default/my-nodeport-service: has no endpoints&quot;</span> -m addrtype --dst-type LOCAL -m tcp --dport 30036 -j REJECT --reject-with icmp-port-unreachable</span><br><span class="line">-A KUBE-SERVICES -d 10.233.44.13/32 -p tcp -m comment --comment <span class="string">&quot;default/my-nodeport-service: has no endpoints&quot;</span> -m tcp --dport 80 -j REJECT --reject-with icmp-port-unreachable</span><br></pre></td></tr></table></figure><p>最後，NodePort service 在使用上有以下幾個重點需要注意：</p><ul><li><p>NodePort 所分配到的 port number 是從 API server 啟動參數中的 <code>--service-node-port-range</code> 來的，預設是 <code>30000 ~ 32767</code></p></li><li><p>若要自訂 NodePort number，則是在 Service 中定義 <code>.spec.ports[*].nodePort</code> (只能使用在上面定義的範圍內的 port number)</p></li><li><p>NodePort 預設的有效範圍是 node 的所有 interface</p></li><li><p>若希望只由特定的 IP 來處理 NodePort 的流量，可透過  API server 啟動參數中的 <code>--nodeport-addresses</code> 來指定(例如：使用 <code>--nodeport-addresses=127.0.0.0/8</code> 表示所有 NodePort 流量都會有 loopback interface 來處理)</p></li><li><p>設定 NodePort 後，可以透過兩種方式存取 Service，分別是 <code>&lt;NodeIP&gt;:spec.ports[*].nodePort</code> &amp; <code>.spec.clusterIP:spec.ports[*].port</code></p></li></ul><p>關於 NodePort 詳細的運作原理，可以參考此篇文章 =&gt; <a href="https://www.hwchiu.com/kubernetes-service-iii.html">[Kubernetes] How to Implement Kubernetes Service - NodePort | Hwchiu Learning Note</a></p><h2 id="Load-Balancer"><a href="#Load-Balancer" class="headerlink" title="Load Balancer"></a>Load Balancer</h2><p>Load Balancer type 是另一個讓外部可以直接存取 cluster 內部 service 的方式。</p><p>但這個 Service Type 需要與外部的 load balancer 服務搭配，因此目前僅有在 public cloud or OpenStack 上才有支援，但由於我目前都不是在這環境上架設 k8s，因此這個部份就先略過。</p><p>有需求的人可以參考<a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer">官網文件說明</a>。</p><h2 id="ExternalName"><a href="#ExternalName" class="headerlink" title="ExternalName"></a>ExternalName</h2><p>ExternalName 的設定其實就是把 Service 導向指定的 DNS name，而不是 service 中 label selector 所設定的 pod，以下是一個簡單範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">prod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ExternalName</span></span><br><span class="line">  <span class="attr">externalName:</span> <span class="string">my.database.example.com</span></span><br></pre></td></tr></table></figure><p>當 cluster 內部查找 <code>my-service.prod.svc</code> 的時候，k8s DNS service 就只會回應 <code>my.database.example.com</code> 這個 CNAME recrd。</p><p>但回應 CNAME record 跟其他 type 有何差別? 其實就是當存取 ExternalName type 的 service 時，網路流量的導向是發生在 DNS level 而不是透過 proxying or forwarding 達成的。</p><blockquote><p>注意! 若要使用 ExternalName，k8s DNS service 的部份僅能安裝 <code>kube-dns</code>，且版本需要是 <code>1.7</code> 以上</p></blockquote><h2 id="External-IP"><a href="#External-IP" class="headerlink" title="External IP"></a>External IP</h2><p>這不算是一個 service type，但可以讓使用者指定 <code>service 要黏在哪個 IP 上</code>，以下是個簡單範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-service</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">MyApp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9376</span></span><br><span class="line">  <span class="comment"># 此 service 只會將進入 80.11.12.10:80 的 網路流量</span></span><br><span class="line">  <span class="comment"># 導向後端的 endpoints </span></span><br><span class="line">  <span class="attr">externalIPs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">80.11</span><span class="number">.12</span><span class="number">.10</span></span><br></pre></td></tr></table></figure><p>透過以上設定，service 只會將進入 <code>80.11.12.10:80</code> 的 網路流量導向使用 Label Selector 選出來的 endpoint(Pod) 中。</p><p>而需要注意的是，External IP 的部份並不屬於 k8s 個管理範圍內，使用者若要使用就必須自己負責管理好這個部份。</p><h1 id="Future-Plan"><a href="#Future-Plan" class="headerlink" title="Future Plan"></a>Future Plan</h1><p>k8s 是一個快速持續進化的平台，因此 Service 的部份也是有相關計劃準備要繼續改進，而改進的方向大概會是：</p><ul><li><p>proxy policy 在管理上可以更細緻(例如：master-elected or sharded)，而不是只有簡單的 round-robin load balancing</p></li><li><p>Service 未來會有真正的 load balancer，而不像目前 VIP 是指負責轉發封包</p></li><li><p>可能增加對 L7(HTTP) 的支援</p></li><li><p>可能支援更多種 ingress mode</p></li></ul><h1 id="一些-Service-背後的運作細節"><a href="#一些-Service-背後的運作細節" class="headerlink" title="一些 Service 背後的運作細節"></a>一些 Service 背後的運作細節</h1><h2 id="Service-VIP-如何避免衝突"><a href="#Service-VIP-如何避免衝突" class="headerlink" title="Service VIP 如何避免衝突?"></a>Service VIP 如何避免衝突?</h2><p>為了讓使用者可以自訂 service port number，卻又不能發生衝突，因此這意味著要保證每個 service 都會被分配到唯一的 IP。</p><p>但這件事情是怎麼辦到的? 透過一個 global allocation map 來完成! (這個 map 存在於 etcd 上)</p><p>且 k8s cluster 會遵循以下規則運作：</p><ul><li><p>在 service 建立(or 移除)的時候，k8s cluster 內部的 allocator 會更新這個 global allocation map，將 IP &amp; 其他 service 相關資訊進行新增(or 刪除)</p></li><li><p>global allocation map object 在 service 建立(or 移除) 的時候一定要存在，否則系統會回報操作失敗的訊息</p></li><li><p>k8s cluster 內部會有另外一個 background controller 來負責確保 global allocation map object 可以正確的被生成</p></li><li><p>background controller 同時還會協助確保是否管理者的操作違反規則 &amp; 清理釋放目前沒有 service 佔用的 IP</p></li></ul><h2 id="IPs-amp-VIPs"><a href="#IPs-amp-VIPs" class="headerlink" title="IPs &amp; VIPs"></a>IPs &amp; VIPs</h2><p>service VIP 不像是 pod IP 是個真實的 IP，而是透過 Linux iptables rules 去定義當網路流量進到 VIP 時應該如何導向正確的 endpoints(也就是 pod IP)，而這個處理過程如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">networking traffic ---&gt; Service_VIP:port ---&gt; Pod:targetPort</span><br></pre></td></tr></table></figure><p>若是直接對 service VIP 進行存取，但不是存取原本在 service 中定義好的 port，就完全不會得到任何回應。</p><p>而當 service 建立 or 刪除時，相對應的 DNS record 也會由 k8s 中的 DNS service 所維護，確保使用者可以透過 domain name 找到 service VIP。</p><h1 id="關於-SCTP-protocol-的支援"><a href="#關於-SCTP-protocol-的支援" class="headerlink" title="關於 SCTP protocol 的支援"></a>關於 SCTP protocol 的支援</h1><p>目前在 v1.12 中還是個 <strong>alpha</strong> 功能，但使用上其實還是很多限制的：</p><ul><li><p>預設不開啟，需透過設定 feature gate 中的 <code>SCTPSupport</code> 來開啟此功能</p></li><li><p>若要做 <a href="http://applezulab.netdpi.net/network/sctp_introduction/sctp-multi-homing">multihomed SCTP association</a>，需要搭配可以為 pod 分配多個 interface &amp; IP 的 CNI plugin 才行</p></li><li><p>NAT 的部份需要搭配相對應的 kernel module 才可以正常運作</p></li><li><p>若搭配 <code>type=LoadBalancer</code>，就需要 cloud provider 也一起支援 SCTP protocol 才行，但目前(2018 年底)都還沒有 public cloud provider 支援這個功能</p></li><li><p>Windows worker node 目前並不支援這功能(2018 年底)</p></li><li><p>userspace kube-proxy 不支援此功能</p></li></ul><p>因此看起來要使用 SCTP protocol，可能就還需要再多等一陣子…..</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/">Service - Kubernetes (中文)</a></p></li><li><p><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">Feature Gates - Kubernetes</a></p></li><li><p><a href="https://blog.xuite.net/champyen/champ/5396649-Stream+Control+Transmission+Protocol+-+SCTP">Stream Control Transmission Protocol - SCTP @ 網路黑貓 :: 隨意窩 Xuite日誌</a></p></li><li><p><a href="https://ieevee.com/tech/2017/01/20/k8s-service.html">谈谈kubernets的service组件的Virtual IP</a></p></li><li><p><a href="https://my.oschina.net/jxcdwangtao/blog/839650">kube-proxy工作原理 - WaltonWang’s Blog - 开源中国</a></p></li><li><p><a href="https://www.jianshu.com/p/8a61de3f8be9">LVS原理介绍 - 简书</a></p></li><li><p><a href="https://ieevee.com/tech/2017/03/22/k8s-headless-service.html">kubernets: Headless Services</a></p></li><li><p><a href="https://www.hwchiu.com/kubernetes-service-ii.html">[Kubernetes] How to Implement Kubernetes Service - ClusterIP | Hwchiu Learning Note</a></p></li><li><p><a href="https://www.hwchiu.com/kubernetes-service-iii.html">[Kubernetes] How to Implement Kubernetes Service - NodePort | Hwchiu Learning Note</a></p></li><li><p><a href="http://dockone.io/article/4884">http://dockone.io/article/4884</a></p></li><li><p><a href="https://akomljen.com/kubernetes-tips-part-1/">Kubernetes Tips - Part 1</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Networking </tag>
            
            <tag> CKA Networking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Taints and Tolerations</title>
      <link href="/blog/Kubernetes/k8s-Taints-and-Tolerations/"/>
      <url>/blog/Kubernetes/k8s-Taints-and-Tolerations/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><code>taint</code> 跟<a href="k8s-Assigning-Pod-to-Nodes">上一篇</a>提到的 node affinity 雖然都是屬於 scheduling 的一部份，但要達成的目的其實完全相反：</p><ul><li><p><code>node affinity</code>：設計如何讓 pod 被分派到某個 worker node</p></li><li><p><code>taint</code>：設計讓 pod 如何<strong>不要</strong>被分派到某個 worker node</p></li></ul><p>而 taint 並非單獨運作，而是與 <code>toleration</code> 共同搭配使用，目的就是要避免讓 pod 被分派到不正確 or 不合適的 worker node 上，運作原理大概如下：</p><blockquote><p>如果有特定的 node 被加上了 taint(汙點)，pod 就不會被分派到上面，除非 pod spec 有設定 toleration(容忍) 來接受這些 taint (必須全部 taint 都接受才行)</p></blockquote><h1 id="設定-Taint-amp-Toleration"><a href="#設定-Taint-amp-Toleration" class="headerlink" title="設定 Taint &amp; Toleration"></a>設定 Taint &amp; Toleration</h1><p>運作規則要分為以下幾個部份說明，分別是：</p><ul><li><p>如何為 node 設定 taint，避免 pod 被分派到上面</p></li><li><p>如何為 node 移除 taint</p></li><li><p>如何在 pod spec 中設定 toleration，讓 pod 可以分派到有 taint 的 node 上</p></li></ul><h2 id="如何設定-node-taint"><a href="#如何設定-node-taint" class="headerlink" title="如何設定 node taint"></a>如何設定 node taint</h2><p>每個 taint 都有以下 3 個屬性：</p><ol><li><p><code>Key</code></p></li><li><p><code>Value</code></p></li><li><p><code>Effect</code>：共有三種，分別是 <strong>NoSchedule</strong>, <strong>PreferNoSchedule</strong> &amp; <strong>NoExecute</strong></p></li></ol><p>而設定 node taint 很簡單，透過 kubectl 執行以下指令將 3 個屬性給入即可即可：</p><blockquote><p>kubectl taint nodes node1 key=value:NoSchedule</p></blockquote><p>以下是一個實際操作範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get node</span><br><span class="line">NAME              STATUS   ROLES    AGE   VERSION</span><br><span class="line">... (略)</span><br><span class="line">leon-k8s-node03   Ready    node     15d   v1.12.1</span><br><span class="line">leon-k8s-node04   Ready    node     15d   v1.12.1</span><br><span class="line">leon-k8s-node05   Ready    node     15d   v1.12.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 node 狀態細節，查看 taint 設定狀態</span></span><br><span class="line"><span class="comment"># 可以看出目前並沒有任何的 taint 設定</span></span><br><span class="line">$ kubectl describe node/leon-k8s-node03</span><br><span class="line">Name:               leon-k8s-node03</span><br><span class="line">Roles:              node</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/hostname=leon-k8s-node03</span><br><span class="line">                    node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">... (略)</span><br><span class="line">Taints:             &lt;none&gt;</span><br><span class="line">Unschedulable:      <span class="literal">false</span></span><br><span class="line">... (略)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 為 node03 加上 taint</span></span><br><span class="line">$ kubectl taint nodes leon-k8s-node03 key=value:NoSchedule</span><br><span class="line">node/leon-k8s-node03 tainted</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新檢視 node 狀態細節，察看 taint 設定狀態</span></span><br><span class="line"><span class="comment"># 目前已經多了一個 taint 的設定</span></span><br><span class="line">$ kubectl describe node/leon-k8s-node03</span><br><span class="line">Name:               leon-k8s-node03</span><br><span class="line">Roles:              node</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/hostname=leon-k8s-node03</span><br><span class="line">                    node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">... (略)</span><br><span class="line">Taints:             key=value:NoSchedule</span><br><span class="line">Unschedulable:      <span class="literal">false</span></span><br><span class="line">... (略)</span><br></pre></td></tr></table></figure><p>當以上步驟完成後，後續新增進來的 pod 就不會被分派到這個 node 上。</p><h2 id="如何移除-taint"><a href="#如何移除-taint" class="headerlink" title="如何移除 taint"></a>如何移除 taint</h2><p>移除的語法很簡單，只要在 taint 的 Key:Effect 後面加上 <code>-</code> 即可：</p><blockquote><p>kubectl taint nodes node1 key:NoSchedule-</p></blockquote><h2 id="設定-pod-toleration"><a href="#設定-pod-toleration" class="headerlink" title="設定 pod toleration"></a>設定 pod toleration</h2><p>pod toleration 是設定在 pod spec 中，我們可以設定以下的 pod toleration 讓 pod 可以接受經過上面指令所產生的 node taint：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 表示可以接受&quot;帶有 key=value &amp; effect=NoSchedule&quot; 的 taint</span></span><br><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;key&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Equal&quot;</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;value&quot;</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure><p>也可以是下面這樣設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 表示可以接受&quot;存在 key(不論 value 為何) &amp; effect=NoSchedule&quot; 的 taint</span></span><br><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;key&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure><p>在 toleration 的設定中，有一些比較特別的案例需要說明一下：</p><ul><li><p>operator 若是沒設定，預設值就是 <code>Equal</code></p></li><li><p>若僅有設定 operator 為 Exists，卻沒有設定 key，那表示會 tolerate(容忍) 所有的 taint</p></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br></pre></td></tr></table></figure><ul><li>僅有設定 key，沒有設定 effect，表示只有帶有相同 key 的 taint 都會被容忍</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;key&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br></pre></td></tr></table></figure><h1 id="Effect-運作規則說明"><a href="#Effect-運作規則說明" class="headerlink" title="Effect 運作規則說明"></a>Effect 運作規則說明</h1><p>由於 key &amp; value 這兩個部份是比較容易理解的，而比較有差異的部份主要落在 <code>Effect</code> 的部份，因此以下就 Effect 進行比較詳細的說明。</p><p>Taint Effect 共有分成三種，分別是：</p><ul><li><p><code>NoSchedule</code></p></li><li><p><code>PreferNoSchedule</code></p></li><li><p><code>NoExecute</code></p></li></ul><p>由於 taint &amp; toleration 是互相抵消的關係，若 node 上有設定 taint，而 pod spec 中又有設定符合的 toleration，就會互相抵消；而 k8s scheduler 在判斷時會以最後剩下的 taint 來進行工作分派的依據。</p><p>而 effect <code>NoExecute</code> 的 taint 被加到 node 的時候也會影響到目前正在該 node 上運作的 pod。</p><h2 id="NoSchedule"><a href="#NoSchedule" class="headerlink" title="NoSchedule"></a>NoSchedule</h2><p>假設最後某個 node 上留下的 taint 的 effect 為 <code>NoSchedule</code>，那 k8s 就不會把該 pod 分派到該 node 上，但不影響正在運作中的 pod。</p><h2 id="PreferNoSchedule"><a href="#PreferNoSchedule" class="headerlink" title="PreferNoSchedule"></a>PreferNoSchedule</h2><p>假設最後某個 node 上留下的 taint 的 effect 為 <code>PreferNoSchedule</code>，那 k8s 就儘量不會把該 pod 分派到該 node 上(最後要是沒辦法的時候還是會破功)，但不影響正在運作中的 pod。</p><h2 id="NoExecute"><a href="#NoExecute" class="headerlink" title="NoExecute"></a>NoExecute</h2><p>假設某個 node 被設定了 effect 為 <code>NoExecute</code> 的 taint，那 k8s 還會把已經存在該 node 上的 pod 趕走，也不會把該 pod 分派到該 node 上。</p><blockquote><p>設定 <code>tolerationSeconds</code> 可以表示在 taint 被增加之後，<strong>帶有相對應 toleration 的 pod 還可以在該 node 上存在多久</strong></p></blockquote><h1 id="實際上如何運用"><a href="#實際上如何運用" class="headerlink" title="實際上如何運用?"></a>實際上如何運用?</h1><p>上面提到了 taint &amp; toleration 的運作規則，也說明了不同的 taint effect 會有哪些不同的影響，那實際上在什麼樣的情況可以用 taint &amp; toleration 機制來處理呢?</p><p>基本上，taint 機制設計的目的，就是<strong>不要讓 pod 被分派到某個 node 上</strong>，而 toleration 則是在上述的前提下可以進行例外的設定，因此就可以有類似以下的 use case 產生：</p><h2 id="專用的-Node"><a href="#專用的-Node" class="headerlink" title="專用的 Node"></a>專用的 Node</h2><p>如果希望某個 node 只給特定群組的使用者使用，可以使用以下的方式增加 taint：</p><blockquote><p>kubectl taint nodes nodename dedicated=groupName:NoSchedule</p></blockquote><p>有了上面的 taint 設定後，一般的 pod 就不會被分派到上面；而該群組的使用者只要在 pod spec 加入對應的 toleration 即可讓 pod 分配到專用的 node 上，而若是覺得每次都要設定 pod spec 很麻煩，或許可以考慮 k8s <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/" title="Kubernetes Admission Controllers">Admission Controller</a> 來讓這件事情變得簡單些。</p><blockquote><p>Admission Contoller 可以自動幫 pod 加上 toleration</p></blockquote><blockquote><p>設定 pod toleration 並不是保證一定會被分派到專屬的 node 上，也有可能還是被分派到到其他沒有 taint 的 node 上；因此若是要讓 pod scheduler 可以更精確的把 pod 放到專屬的 node，可以搭配 <strong>Node Label + nodeSelector</strong> 一起使用</p></blockquote><h2 id="帶有特殊硬體的-Node"><a href="#帶有特殊硬體的-Node" class="headerlink" title="帶有特殊硬體的 Node"></a>帶有特殊硬體的 Node</h2><p>由於特殊硬體(例如：GPU, FPGA … etc)一般來說都不便宜，因此在一個 cluster 中可能會僅有某些 node 安裝特殊硬體，而通常這一類的特殊硬體還是多少都會消耗 CPU &amp; memory 資源，因此在合理的思考下，就不會讓一般不需要用到到特殊硬體被分派到這些 node 上。</p><p>因此，可以在帶有這些特殊硬體的 node 上設定類似以下的 taint：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes nodename special=<span class="literal">true</span>:NoSchedule</span><br><span class="line"></span><br><span class="line">$ kubectl taint nodes nodename special=<span class="literal">true</span>:PreferNoSchedule</span><br><span class="line"></span><br><span class="line">$ kubectl taint nodes nodename gpu=<span class="literal">true</span>:NoSchedule</span><br><span class="line"></span><br><span class="line">$ kubectl taint nodes nodename fpga=<span class="literal">true</span>:NoSchedule</span><br></pre></td></tr></table></figure><p>而 pod toleration or <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/" title="Kubernetes Admission Controllers">Kubernetes Admission Controller</a> 的設定當然也不能或缺；但若是真的使用特殊硬體，也可以考慮使用 <a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources">Extended Resource</a> + <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration">ExtendedResourceToleration admission controller</a> 的機制來處理 &amp; 運用。</p><blockquote><p>透過 <code>Extended Resource</code> + <code>ExtendedResourceToleration admission controller</code> 可以確保 pod 可以被自動的加上被分派到帶有特殊硬體的 node 上所需要的 toleration 設定</p></blockquote><h2 id="當-Node-發生問題時"><a href="#當-Node-發生問題時" class="headerlink" title="當 Node 發生問題時"></a>當 Node 發生問題時</h2><p>當 node 發生問題時(或是任何其他會造成該 node 無法繼續提供服務的情況)，管理者需要考慮驅逐目前在上面運行中的 pod，可以透過加上 taint(Effect=<code>NoExecute</code>) 的方式達成，k8s 就會自動幫忙處理後續的作業。</p><h1 id="Taint-based-Evictions"><a href="#Taint-based-Evictions" class="headerlink" title="Taint based Evictions"></a>Taint based Evictions</h1><p>首先再度說明當 node 被加上 taint(Effct=<code>NoExecute</code>) 後會自動的發生以下行為：</p><ol><li><p>沒有設定對應 toleration 的 pod 會被馬上驅離</p></li><li><p>有設定相對應 toleration 的 pod 則會繼續留在該 node 上</p></li><li><p>有設定對應 toleration 的 pod，也設定 <code>tolerationSeconds</code>，會在該 node 上停留一陣子(根據設定值)後就會被驅離</p></li></ol><p>由於以上這些行為都是自動完成的，因此在 Kubernetes v1.6 之後，就可以開始利用這個機制來執行當特定 node 發生問題時的處理。</p><blockquote><p>由於 k8s 中有 node controller 會持續監控每個 node 的狀態並回報，因此當它發現某些 node 有狀況時，可以透過為這個 node 增加 taint 的方式，將上面正在運作的 pod 驅離到其他 node 上去執行</p></blockquote><p>為了自動化的完成這些事情，k8s 設計了幾個專用的 taint 來描述 node 的相關問題：</p><ul><li><p><code>node.kubernetes.io/not-ready</code>：Node is not ready</p></li><li><p><code>node.kubernetes.io/unreachable</code>：node controller 無法聯繫到該 node</p></li><li><p><code>node.kubernetes.io/out-of-disk</code>：磁碟空間不足</p></li><li><p><code>node.kubernetes.io/memory-pressure</code>：記憶體快要耗盡</p></li><li><p><code>node.kubernetes.io/disk-pressure</code>：磁碟空間快要耗盡</p></li><li><p><code>node.kubernetes.io/network-unavailable</code>：網路有問題</p></li><li><p><code>node.kubernetes.io/unschedulable</code>：無法分派 pod 到該 node</p></li><li><p><code>node.cloudprovider.kubernetes.io/uninitialized</code>：node 尚未初始化完成，還無法使用 (這是給 public cloud provider 用的 taint)</p></li></ul><p>可能會有人有疑問….如果 node 一旦被偵測出有問題，上面的 pod 就會馬上被驅離，但可能只是斷線一下就回復了，馬上驅離的作法妥當嗎?</p><p>因此為了處理這樣的狀況，當 node 發生問題時，除了在該 node 上增加一個 taint 的設定外，還會在該 node 上的每個 pod 加上相對應的 toleration 設定，並設定 tolerationSeconds=300，這表示每個 pod 都還可以留在該 node 上 5 分鐘。</p><blockquote><p>tolerationSeconds 的設定是 k8s 自動給進去的，若要更改預設值 300，可以透過 <a href="https://github.com/kubernetes/kubernetes/tree/master/plugin/pkg/admission/defaulttolerationseconds">DefaultTolerationSeconds admission controller</a></p></blockquote><p>此外，要讓 taint based eviction 運作，必須要開啟 <code>TaintBasedEvictions</code> feature gate (預設是關閉的)</p><blockquote><p>設定這功能需要注意 node controller 中的 <code>--node-eviction-rate</code>(預設 0.1，表示 10 秒驅離一個 pod) 設定，必須很小心的設定這個值，避免大規模驅離 pod 時造成整個 cluster 崩潰的連鎖效應 (特別是當 master node 發生崩潰的時候)</p></blockquote><p>最後，官網文件有提到 DaemonSet 最自動加上以下 toleration(Effect=<code>NoExecute</code>)：</p><ul><li><p><strong>node.alpha.kubernetes.io/unreachable</strong></p></li><li><p><strong>node.kubernetes.io/not-ready</strong></p></li></ul><p>但在 v1.12 的環境下檢視已經存在的 DaemonSet，並沒有看到這樣的設定，這部份有帶後續釐清。</p><h1 id="Taint-Nodes-by-Condition"><a href="#Taint-Nodes-by-Condition" class="headerlink" title="Taint Nodes by Condition"></a>Taint Nodes by Condition</h1><p>如果我們希望當 node 發生特定問題的時候，不要自動被加上 taint，避免 pod 無法被分派到該 node 時，可以怎麼做呢? 答案是透過 <code>TaintNodesByCondition</code> feature。</p><p>在 v1.12 後，<code>TaintNodesByCondition</code> feature 已經變成 beta 功能且預設開啟，管理者可以透過這個功能<strong>選擇性</strong>的忽略某些 node 的狀況，而 <code>TaintNodesByCondition</code> 實際運作狀況是這樣的：</p><ul><li><p>若 node 發生狀況時，k8s 會自動加上 Effect=<code>NoSchedule</code> 的 taint 到該 node 上</p></li><li><p>若 TaintNodesByCondition 有設定忽略特定狀況，就不會加上 taint</p></li></ul><blockquote><p>跟 TaintBasedEviction 不同，一個是增加 Effect=<code>NoExecute</code>(TaintBasedEviction) 的 taint，一個是增加 Effect=<code>NoSchedule</code>(TaintNodesByCondition) 的 taint</p></blockquote><p>最後，官網文件有提到 DaemonSet controller 會為所有的 DaemonSet 自動增加以下 toleration(Effect=<code>NoSchedule</code>)：</p><ul><li><p><strong>node.kubernetes.io/memory-pressure</strong></p></li><li><p><strong>node.kubernetes.io/disk-pressure</strong></p></li><li><p><strong>node.kubernetes.io/out-of-disk</strong> (只有對重要的 pod)</p></li><li><p><strong>node.kubernetes.io/unschedulable</strong> (v1.10 之後)</p></li><li><p><strong>node.kubernetes.io/network-unavailable</strong> (若設定使用 hoet network 時)</p></li></ul><p>但在 v1.12 的環境下檢視已經存在的 DaemonSet，並沒有看到這樣的設定，這部份有帶後續釐清。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Taints and Tolerations - Kubernetes</a></p></li><li><p><a href="https://jimmysong.io/posts/kubernetes-taint-and-toleration/">Kubernetes中的Taint和Toleration（污点和容忍） - 宋净超的博客|Cloud Native|云原生布道师</a></p></li><li><p><a href="https://blog.frognew.com/2018/05/taint-and-toleration.html">Kubernetes Pod调度进阶：Taints(污点)和Tolerations(容忍) — 青蛙小白</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Scheduling </tag>
            
            <tag> CKA Scheduling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Assigning Pods to Nodes</title>
      <link href="/blog/Kubernetes/k8s-Assigning-Pod-to-Nodes/"/>
      <url>/blog/Kubernetes/k8s-Assigning-Pod-to-Nodes/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>k8s 其中一個很大的優點在於很強大的 scheduler，在絕大多數時的情況下，使用者只要提交自己想要佈署的服務，k8s 就會自動的找到合適的 worker node 來運行，完全不用擔心 woker node 之間工作負載不平衡的狀況發生。</p><p>但有些時候，我們可能會希望可以人為介入 pod scheduling 的工作，例如：</p><ul><li><p>希望讓 pod 運作在某些帶有某些特定硬體(例如：SSD)的 worker node 上</p></li><li><p>希望某些 pod 可以被固定放在一起</p></li></ul><p>k8s 也提供了一些機制可以讓我們自己決定 job scheduling 是如何進行的，而<strong>這些機制基本上都是建立在 label select 的基礎上完成的</strong>，以下來逐一介紹。</p><h1 id="nodeSelector"><a href="#nodeSelector" class="headerlink" title="nodeSelector"></a>nodeSelector</h1><p>nodeSelector 是目前 k8s 提供的方式中最簡單的一個，只要在 pod spec 上指定所希望的 key/value pair 作為 nodeSelector，k8s 就會協助找到有相同 label 的 worker node 來接手工作。</p><p>但 nodeSelector 要怎麼用呢? 以下是簡單的流程說明：</p><h2 id="為-Worker-Node-設定-Label"><a href="#為-Worker-Node-設定-Label" class="headerlink" title="為 Worker Node 設定 Label"></a>為 Worker Node 設定 Label</h2><p>上面提到可以指定 pod 到帶有特定 key/value pair 的 worker node 上，那當然 worker node 上一定要有特定的 key/value pair 才行，以下是為 worker node 指定 key/value pair 的方式</p><blockquote><p>kubectl label nodes/<strong>&lt;node-name&gt;</strong> <strong>&lt;label-key&gt;</strong>=<strong>&lt;label-value&gt;</strong></p></blockquote><p>以下是實際示範過程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視目前每個 node 所帶的 label</span></span><br><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME              STATUS   ROLES    AGE   VERSION   LABELS</span><br><span class="line">... (略)</span><br><span class="line">leon-k8s-node03   Ready    node     12d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node03,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">leon-k8s-node04   Ready    node     12d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node04,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">leon-k8s-node05   Ready    node     12d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node05,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增一個 label(disk_type=ssd) 到 node05</span></span><br><span class="line">$ kubectl label node/leon-k8s-node05 disk_type=ssd</span><br><span class="line">node/leon-k8s-node05 labeled</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看出 node05 已經有新的 label 出現了</span></span><br><span class="line">$ kubectl get nodes --show-labels</span><br><span class="line">NAME              STATUS   ROLES    AGE   VERSION   LABELS</span><br><span class="line">... (略)</span><br><span class="line">leon-k8s-node03   Ready    node     12d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node03,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">leon-k8s-node04   Ready    node     12d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node04,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">leon-k8s-node05   Ready    node     12d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk_type=ssd,kubernetes.io/hostname=leon-k8s-node05,node-role.kubernetes.io/node=<span class="literal">true</span></span><br></pre></td></tr></table></figure><blockquote><p>從上面的 label 資訊可以看出，其實 k8s 剛安裝好後，就會為每個 node 自動加上一些預設的 label，這些 label 可能會在未來作為識別 or scheduling 時的一個依據，通常 public cloud provider 上佈署的 k8s，也會根據需求為每個 node 加上預設的 label 資訊</p></blockquote><h2 id="為-Pod-spec-設定-nodeSelector"><a href="#為-Pod-spec-設定-nodeSelector" class="headerlink" title="為 Pod spec 設定 nodeSelector"></a>為 Pod spec 設定 nodeSelector</h2><p>接著以下就是新增一個帶有 nodeSelector 的 pod，範例如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">env:</span> <span class="string">test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="comment"># 設定的地方在 pod.nodeSelector</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">disk_type:</span> <span class="string">ssd</span></span><br></pre></td></tr></table></figure><p>套用以上設定後，檢視一下 pod 的內容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 pod 運作細節</span></span><br><span class="line">$ kubectl describe pod/nginx</span><br><span class="line">Name:               nginx</span><br><span class="line">Namespace:          default</span><br><span class="line">Priority:           0</span><br><span class="line">PriorityClassName:  &lt;none&gt;</span><br><span class="line"><span class="comment"># 這裡可以看出 pod 有確實被分派到 node05 來執行</span></span><br><span class="line">Node:               leon-k8s-node05/10.107.13.15</span><br><span class="line">Start Time:         Wed, 24 Oct 2018 19:38:28 +0000</span><br><span class="line">Labels:             env=<span class="built_in">test</span></span><br><span class="line">Status:             Running</span><br><span class="line">... (略)</span><br><span class="line"><span class="comment"># 從此欄位可以看出我們所指定的 nodeSelector 參數</span></span><br><span class="line">Node-Selectors:  disk_type=ssd</span><br><span class="line">... (略)</span><br></pre></td></tr></table></figure><h1 id="Built-in-Node-Labels"><a href="#Built-in-Node-Labels" class="headerlink" title="Built-in Node Labels"></a>Built-in Node Labels</h1><p>剛安裝好 k8s 後，如果有曾經檢查過 node status 的話，可能會發現其實每個 node 都會有一些內建的 label set，以下是剛安裝好 v1.12.1 後的結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodes --show-labels</span><br><span class="line">NAME              STATUS   ROLES    AGE   VERSION   LABELS</span><br><span class="line">leon-k8s-node00   Ready    master   14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node00,node-role.kubernetes.io/master=<span class="literal">true</span></span><br><span class="line">leon-k8s-node01   Ready    master   14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node01,node-role.kubernetes.io/master=<span class="literal">true</span></span><br><span class="line">leon-k8s-node02   Ready    master   14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node02,node-role.kubernetes.io/master=<span class="literal">true</span></span><br><span class="line">leon-k8s-node03   Ready    node     14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node03,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">leon-k8s-node04   Ready    node     14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node04,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">leon-k8s-node05   Ready    node     14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk_type=ssd,kubernetes.io/hostname=leon-k8s-node05,node-role.kubernetes.io/node=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>可以看出內建的 label set 有以下幾項：</p><ul><li><p><strong>beta.kubernetes.io/arch</strong></p></li><li><p><strong>beta.kubernetes.io/os</strong></p></li><li><p><strong>kubernetes.io/hostname</strong></p></li><li><p><strong>node-role.kubernetes.io/master</strong></p></li><li><p><strong>node-role.kubernetes.io/node</strong></p></li></ul><p>以上是在本地端環境安裝 k8s 後的結果，若是在 public cloud 上，為了管理方便，可能會有用來表示 Zone or AZ 這一類資訊 label 存在。</p><p>而以上這些內建的 node label，也會跟下面提到 affinity 機制在運行的時候會有相關，而這些 node label 則會被作為 <code>topologyKey</code>(下面會看到) 來使用。</p><blockquote><p>為何這些 node label 要稱為 <code>topologyKey</code>? 因為從人類的角度來看，這些 node 的規劃透過以 topology 的型式來呈現，會比較容易理解</p></blockquote><h1 id="Node-Affinity-amp-anti-affinity"><a href="#Node-Affinity-amp-anti-affinity" class="headerlink" title="Node Affinity &amp; anti-affinity"></a>Node Affinity &amp; anti-affinity</h1><p>上述的 nodeSelector 提供了一個簡單的方式讓使用者可以將 pod 分派到帶有特定 label 的 worker node 上，但 nodeSelector 有時候並無法滿足現實世界的複雜需求，因此 <strong>affinity/anti-affinity</strong> 的功能就應運而生了。(目前在 v1.12 中還是屬於 beta feature)</p><p>而 affinity/anti-affinity 主要加強了幾個地方：</p><ol><li><p>可用更彈性的方法來指定多個 label 時的組合，而不再只能用 <strong>AND</strong></p></li><li><p>以往只能設定是否完全符合條件，現在可以用 <code>preference(希望可以有，但沒有也沒關係)</code> 的方式來設定</p></li><li><p>可以指定跟帶有某些 label 的 <code>pod</code> 放在一起(or 不要放在一起)，而不是只能指定 worker node label：這樣有助於讓某些 pod 可以被放在同一個 worker node(或不被放在一起)</p></li></ol><p>從上面幾個加強的地方可以看出，<strong>affinity/anti-affinity</strong> 這個 feature 共分為兩大類，分別是：</p><ul><li><p>node affinity</p></li><li><p>inter-pod affinity/anti-affinity</p></li></ul><p>nodeSelector 目前還是可以正常使用，但最終會被 node affinity 取代，因為 node affinity 可以完全取代 nodeSelector 的功能之外，還可以提供更多的其他功能，就像 RepplicationController &amp; ReplicaSet 的關係是相同的。(ReplicaSet 取代了 RepplicationController)</p><h2 id="Node-affinity-anti-affinity"><a href="#Node-affinity-anti-affinity" class="headerlink" title="Node affinity/anti-affinity"></a>Node affinity/anti-affinity</h2><p>node affinity 在 v1.2 之後所提供的功能，類似 nodeSelector ，有以下兩種類型：</p><ul><li><p><code>requiredDuringSchedulingIgnoredDuringExecution</code></p></li><li><p><code>preferredDuringSchedulingIgnoredDuringExecution</code></p></li></ul><p>上面的設定可以拆開三個部份來看，分別是：</p><ul><li><p><code>requiredDuringScheduling</code>：一定要 node 符合條件，scheduler 才會把 pod 分派到上面去跑</p></li><li><p><code>preferredDuringScheduling</code>：會儘量嘗試找尋合適條件的 node，但不強制</p></li><li><p><code>IgnoredDuringExecution</code>：表示當 pod 已經正在運作中了，即使 node 的 label 在之後遭到變更，也不會影響正在運作中的 pod</p></li></ul><p>從上面的說明就可以知道，兩種組合的搭配所得到的結果會是如何。</p><blockquote><p>之後還會推出 <code>requiredDuringSchedulingRequiredDuringExecution</code> 的功能，而且同樣的，看字面的意思就可以很清楚了解這個設定會有什麼樣的效果</p></blockquote><p>以下是一個 node affinity 的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">with-node-affinity</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">affinity:</span></span><br><span class="line">    <span class="attr">nodeAffinity:</span></span><br><span class="line">      <span class="comment"># 一定要滿足以下條件才可作 pod scheduling</span></span><br><span class="line">      <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="comment"># nodeSelector 的條件定義</span></span><br><span class="line">        <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="comment"># node 一定有帶有 以下任何一種 label 才可以</span></span><br><span class="line">          <span class="comment"># &quot;kubernetes.io/e2e-az-name=e2e-az1&quot;</span></span><br><span class="line">          <span class="comment"># &quot;kubernetes.io/e2e-az-name=e2e-az2&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/e2e-az-name</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">e2e-az1</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">e2e-az2</span></span><br><span class="line">      <span class="comment"># 儘量滿足以下條件即可作 pod scheduling</span></span><br><span class="line">      <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">      <span class="comment"># 這是屬於 prefer 的權重設定(1-100)，符合條件就會得到此權重值</span></span><br><span class="line">      <span class="comment"># pod 會被分配到最後加總數值最高的 node</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">preference:</span></span><br><span class="line">          <span class="attr">matchExpressions:</span></span><br><span class="line">          <span class="comment"># 儘量尋找帶有 label &quot;another-node-label-key=another-node-label-value&quot; 的 node</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">another-node-label-key</span></span><br><span class="line">            <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">            <span class="attr">values:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">another-node-label-value</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">with-node-affinity</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/pause:2.0</span></span><br></pre></td></tr></table></figure><p>除了上面範例中的 <code>In</code> 之外，k8s 還支援了其他的 operator 像是 <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>, <code>Gt</code>, <code>Lt</code> … 等等，可以根據實際需求使用。</p><p>此外，node affinity 在使用上還有一些其他規則的存在，例如：</p><ul><li><p>若同時設定 <code>nodeSelector</code> &amp; <code>nodeAffinity</code>，那 scheduler 就會尋找同時滿足兩個條件要求的 node</p></li><li><p>如果在 <code>nodeAffinity</code> 中設定多個 <code>nodeSelectorTerms</code>，那就只要滿足任何一個 <strong>nodeSelectorTerms</strong> 的要求即可</p></li><li><p>node affinity 目前只有在 pod scheduling 的時候會有用途，當 pod 已經在 node 上運行後，即使 node label 變更了也不會影響正在上面運行中的 pod，除非之後 <code>requiredDuringSchedulingRequiredDuringExecution</code> 的功能有推出</p></li><li><p>在 prefer 的設定中，有個 <code>weight</code> 的權重值(1-100)可以設定，而 scheduler 在決定前，還會加上其他 node priority function 來進行綜合考量，最後 pod 會被分配到數值計算結果最高的 node 上去</p></li></ul><h2 id="Inter-pod-affinity-anti-affinity"><a href="#Inter-pod-affinity-anti-affinity" class="headerlink" title="Inter-pod affinity/anti-affinity"></a>Inter-pod affinity/anti-affinity</h2><p>為什麼需要 pod affinity/anti-affinity?</p><blockquote><p>這功能有時候挺有用的，特別是跟 ReplicaSets, StatefulSets 或是 Deployments 一起搭配的時候；例如：我希望把 workload 分派到特定的 topology 的運行(例如：同一個 node)</p></blockquote><p>pod affinity 的功能是在 v1.4 的時候提出，其實這跟 node affinity 很類似，只是 scheduler 要尋找的是目前有哪些正在運行中的 <strong>pod</strong> 帶有符合條件的 label set，而不是 node。</p><p>規則的設定原則應該是<code>幫忙尋找符合設定條件的 Pod &amp; 所在的 node 在哪裡，並把 Pod 放到該 node 上面運行</code></p><p>而且 pod affinity 跟 node affinity 相同，也是有以下兩種設定類型可以使用：</p><ul><li><p><code>requiredDuringSchedulingIgnoredDuringExecution</code></p></li><li><p><code>preferredDuringSchedulingIgnoredDuringExecution</code></p></li></ul><p>而兩種設定的運作規則當然也跟 node affinity 相同，但通常 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 常與 pod anti-affinity 搭配使用，而通常這兩者的搭配，大多是要達成<strong>儘量將 pod 分散配置</strong>的目的才會這樣使用。</p><p>當實際要設定 pod affinity/anti-affinity，有兩個條件可以用來進行設定：</p><ul><li><p>pod label set：這個部份其實很清楚，跟 node label set 其實是一樣的東西</p></li><li><p>node topology key：這個部份通常就是 k8s 的 build-in node label (表示希望檢查 worker node 是否也符合條件)</p><blockquote><p>topologyKey 的設定用意在於，如果有多個 pod 需要分配，並指定了 topologyKey，那 scheduler 在分配時就<strong>不可以</strong>(注意! 是不可以!)將多個 pod 放到帶有相同 value 的topologyKey(Label) 的 node 上</p></blockquote></li></ul><p>以下用一個實際的例子來說明：</p><p>假設 k8s cluster 中有 3 個 worker node(node03, node04, node05)，且要在裡面安裝一個三份 replica 的 web application，而這個 web application 使用到 redis 作為 cache，為了平均的分散 work load，希望可以將 pod 佈署成：</p><ul><li><p><code>node03</code>: redis + web-app</p></li><li><p><code>node04</code>: redis + web-app</p></li><li><p><code>node05</code>: redis + web-app</p></li></ul><p>也就是每個 worker 都帶有一份 redis + web app，這要如何透過 pod affinity/anti-affinity 來完成呢? 可以參考以下步驟：</p><h3 id="確認-topologyKey-node-Label"><a href="#確認-topologyKey-node-Label" class="headerlink" title="確認 topologyKey (node Label)"></a>確認 topologyKey (node Label)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 從上面可以看出，每個 worker node 中的 &quot;kubernetes.io/hostname&quot; 都帶有不同的值，可以拿來作為 topologyKey</span></span><br><span class="line">$ kubectl get node --show-labels</span><br><span class="line">NAME              STATUS   ROLES    AGE   VERSION   LABELS</span><br><span class="line">... (ignore master nodes)</span><br><span class="line">leon-k8s-node03   Ready    node     14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node03,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">leon-k8s-node04   Ready    node     14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=leon-k8s-node04,node-role.kubernetes.io/node=<span class="literal">true</span></span><br><span class="line">leon-k8s-node05   Ready    node     14d   v1.12.1   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk_type=ssd,kubernetes.io/hostname=leon-k8s-node05,node-role.kubernetes.io/node=<span class="literal">true</span></span><br></pre></td></tr></table></figure><h3 id="佈署-redis，並確保分散在不同的-node"><a href="#佈署-redis，並確保分散在不同的-node" class="headerlink" title="佈署 redis，並確保分散在不同的 node"></a>佈署 redis，並確保分散在不同的 node</h3><p>接著要來佈署 redis 作為 cache service，但由於要將 redis 分散到不同的 node，因此進行了以下設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">redis-cache</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">store</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">store</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="comment"># 確保 pod 不會落在帶有 app in &quot;store&quot; label 的 pod 所在的 worker node 上</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">store</span></span><br><span class="line">            <span class="comment"># pod 在分配時要確保 worker node 帶有名稱為 &quot;kubernetes.io/hostname&quot; 的 label</span></span><br><span class="line">            <span class="comment"># 但不能有相同的 value</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">&quot;kubernetes.io/hostname&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-server</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">redis:3.2-alpine</span></span><br></pre></td></tr></table></figure><p>套用完以上設定後，可以看到 k8s 成功的將 redis cache 分散到不同的 worker node 上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE              NOMINATED NODE</span><br><span class="line">redis-cache-7d87c677b5-wg7rb   1/1     Running   0          11s   10.233.76.3      leon-k8s-node05   &lt;none&gt;</span><br><span class="line">redis-cache-7d87c677b5-wn8s2   1/1     Running   0          11s   10.233.103.193   leon-k8s-node04   &lt;none&gt;</span><br><span class="line">redis-cache-7d87c677b5-zjbwd   1/1     Running   0          11s   10.233.102.130   leon-k8s-node03   &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="佈署-web-app，並跟-redis-放一起"><a href="#佈署-web-app，並跟-redis-放一起" class="headerlink" title="佈署 web app，並跟 redis 放一起"></a>佈署 web app，並跟 redis 放一起</h3><p>為了提升效能，佈署 web application 的時候，每個 web application 就要搭配一個 redis cache，可以透過以下設定完成：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web-server</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">web-store</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">web-store</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="comment"># 跟佈署 redis 相同，不要讓 web app 分派到同樣的 worker node 上</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">web-store</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">&quot;kubernetes.io/hostname&quot;</span></span><br><span class="line">        <span class="comment"># 這是跟 redis cache 可以一對一放在一起並分散到不同 worker node 的關鍵</span></span><br><span class="line">        <span class="attr">podAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="comment"># 在這裡設定 redis cache 的 label set</span></span><br><span class="line">          <span class="comment"># 指定要找到 redis 所在的 worker node 來放 web app pod</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">store</span></span><br><span class="line">            <span class="comment"># 但是要分散到不同的 worker node 上</span></span><br><span class="line">            <span class="comment"># (需要帶有 kubernetes.io/hostname label 的 worker node，但 value 不能相同)</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">&quot;kubernetes.io/hostname&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web-app</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.12-alpine</span></span><br></pre></td></tr></table></figure><p>套用完以上設定後，就可以看到 k8s 系統中已經將 web app 跟 redis 一對一對的分派到不同的 worker node 上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE   IP               NODE              NOMINATED NODE</span><br><span class="line">redis-cache-7d87c677b5-wg7rb   1/1     Running   0          18m   10.233.76.3      leon-k8s-node05   &lt;none&gt;</span><br><span class="line">redis-cache-7d87c677b5-wn8s2   1/1     Running   0          18m   10.233.103.193   leon-k8s-node04   &lt;none&gt;</span><br><span class="line">redis-cache-7d87c677b5-zjbwd   1/1     Running   0          18m   10.233.102.130   leon-k8s-node03   &lt;none&gt;</span><br><span class="line">web-server-68844db94f-72mhj    1/1     Running   0          13s   10.233.76.4      leon-k8s-node05   &lt;none&gt;</span><br><span class="line">web-server-68844db94f-ldp9s    1/1     Running   0          13s   10.233.102.131   leon-k8s-node03   &lt;none&gt;</span><br><span class="line">web-server-68844db94f-m58p9    1/1     Running   0          13s   10.233.103.194   leon-k8s-node04   &lt;none&gt;</span><br></pre></td></tr></table></figure><blockquote><p>因此從上面的例子可以看到，若不要讓 pod 分配到同一個 node 上，只要設定 <code>podAntiAffinity</code> + <code>topologyKey: &quot;kubernetes.io/hostname&quot;</code> 即可 (因為 <code>kubernetes.io/hostname</code> 是 built-in 的 label，並且都會帶上不同的 value)</p></blockquote><h2 id="使用-affinity-的注意事項"><a href="#使用-affinity-的注意事項" class="headerlink" title="使用 affinity 的注意事項"></a>使用 affinity 的注意事項</h2><p>官網的文件中有提到，由於 inter-pod affinity/anti-affinity 需要消耗大量的計算資源來作比對，因此若是 cluster node 數大於 700 的情況下，不建議使用這個功能，會大大降低整個 cluster 的運作速度。</p><p>不過是說一般人要用到超過 700 個 node 的 k8s cluster 似乎也是沒什麼機會倒是真的….</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">Assigning Pods to Nodes - Kubernetes</a></p></li><li><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/nodeaffinity.md">Node affinity and NodeSelector - Kubernetes Design Doc</a></p></li><li><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/podaffinity.md">Inter-pod topological affinity and anti-affinity - Kubernetes Design Doc</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Scheduling </tag>
            
            <tag> CKA Scheduling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] 設定 StorageClass (以 NFS 為例)</title>
      <link href="/blog/Kubernetes/k8s-Config-StorageClass-with-NFS/"/>
      <url>/blog/Kubernetes/k8s-Config-StorageClass-with-NFS/</url>
      
        <content type="html"><![CDATA[<h1 id="2019-05-06-更新"><a href="#2019-05-06-更新" class="headerlink" title="2019-05-06 更新"></a>2019-05-06 更新</h1><p>目前在 NFS storage 的部份，已經變成 <code>NFS Provisioner</code> &amp; <code>NFS-Client Provisioner</code> 兩種了：</p><ul><li><p><a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs">NFS Provisioner</a>：會在 k8s 中啟動一個 NFS server 來使用</p></li><li><p><a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client">NFS-Client Provisioner</a>：這部份跟下面原本介紹的相同，使用者必須先設定好一個外部的 NFS server，然後將這個 plugin 指定過去，它就會在需要時在上面建立目錄並存放資料。</p></li></ul><blockquote><p>若有 NFS external storage 的需求，可以參考上面新的連結來設定，新的方法已經改用 Helm 來管理，佈署安裝的流程變得相當簡單</p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前有介紹過 StorageClass 搭配 GlusterFS + Heketi 作為後端的 storage，但 GlusterFS 可能對很多人來說還是有點複雜，因此想說介紹一個比較容易入門的 storage，學習 k8s 的過程才不會覺得很艱難。</p><p>只要用過 Linux，大概 NFS 幾乎就會是個必學的服務，因此這邊要介紹以 NFS 作為 StorageClass 後端 storage 的設定方式，讓 k8s 可以動態的在 NFS share 上產生所需要 volume 來使用。</p><h1 id="運作原理說明"><a href="#運作原理說明" class="headerlink" title="運作原理說明"></a>運作原理說明</h1><p>基本上要正確設定 StorageClass + NFS，大概要準備以下幾個東西：</p><ol><li><p>一個可用的 NFS share</p></li><li><p><code>NFS provisioner</code>：主要有兩個工作，一個是實際在 NFS share 中建立 volume(其實就是一般的 directory)，另一個則是建立 PV，並與 NFS volume 作繫結</p></li><li><p><code>Service Account</code>：這是用來管控 NFS provisioner 在 k8s 中可以運行的權限</p></li><li><p><code>StorageClass</code>：負責建立 PVC 並呼叫 NFS provisioner 進行設定工作，並讓 PVC 與 PV 繫結</p></li></ol><p>詳細的運作流程可以參考下圖：</p><p><img src="/blog/images/kubernetes/dynamic-volume-provision.jpg" alt="Kubernetes Persistent Volume Provisioning"></p><h1 id="設定過程"><a href="#設定過程" class="headerlink" title="設定過程"></a>設定過程</h1><h2 id="設定-NFS-share"><a href="#設定-NFS-share" class="headerlink" title="設定 NFS share"></a>設定 NFS share</h2><p>這個部份就留給大家自己去作了，網路上有非常多的教學可以查。</p><p>假設這裡使用以下的 NFS share：</p><ul><li><p>IP：<code>10.1.2.3</code></p></li><li><p>Export Path: <code>/var/k8s-nfs-share</code></p></li></ul><h2 id="設定-Service-Account-amp-對應權限"><a href="#設定-Service-Account-amp-對應權限" class="headerlink" title="設定 Service Account &amp; 對應權限"></a>設定 Service Account &amp; 對應權限</h2><p>如果要精準的管理權限，那就必須要自訂一個 service account 並搭配 k8s 中的 RBAC 機制來進行設定。</p><p>在以下的範例中會完成兩件事情：</p><ol><li><p>新增 service account <code>nfs-client-provisioner</code>，作為 NFS provisioner 的權限來源</p></li><li><p>在 k8s 中有賦予 service account 足夠的權限處理跟 StorageClass &amp; PersistentVolumeClaim 相關的工作 (透過 <strong>Role</strong> + <strong>RoleBinding</strong> + <strong>ClusterRole</strong> + <strong>ClusterRoleBinding</strong>)</p></li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-client-provisioner-runner</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumes&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;delete&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;persistentvolumeclaims&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;update&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;storage.k8s.io&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;storageclasses&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;events&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">run-nfs-client-provisioner</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-client-provisioner-runner</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">leader-locking-nfs-client-provisioner</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span> [<span class="string">&quot;endpoints&quot;</span>]</span><br><span class="line">    <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>, <span class="string">&quot;create&quot;</span>, <span class="string">&quot;update&quot;</span>, <span class="string">&quot;patch&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">leader-locking-nfs-client-provisioner</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">leader-locking-nfs-client-provisioner</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure><h2 id="安裝-NFS-provisioner"><a href="#安裝-NFS-provisioner" class="headerlink" title="安裝 NFS provisioner"></a>安裝 NFS provisioner</h2><p>NFS provisioner 負責以下工作：</p><ol><li><p>在 NFS share 中產生 volume(directory)</p></li><li><p>新增 PV</p></li><li><p>告知 PVC 已經完成 PV 的設定，讓 PVC 與 PV 繫結</p></li></ol><p>使用以下的設定檔佈署 NFS provisioner</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">Recreate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/external_storage/nfs-client-provisioner:latest</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-root</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/persistentvolumes</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PROVISIONER_NAME</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">my-nfs-provisioner</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_SERVER</span></span><br><span class="line">              <span class="attr">value:</span> <span class="number">10.1</span><span class="number">.2</span><span class="number">.3</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_PATH</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">/var/k8s-nfs-share</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-root</span></span><br><span class="line">          <span class="attr">nfs:</span></span><br><span class="line">            <span class="attr">server:</span> <span class="number">10.1</span><span class="number">.2</span><span class="number">.3</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/var/k8s-nfs-share</span></span><br></pre></td></tr></table></figure><h2 id="建立-StorageClass"><a href="#建立-StorageClass" class="headerlink" title="建立 StorageClass"></a>建立 StorageClass</h2><p>透過以下設定檔，建立 StorageClass 來使用上面所佈署好的 NFS provisioner：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">my-nfs-storage</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">my-nfs-provisioner</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">archiveOnDelete:</span> <span class="string">&quot;false&quot;</span></span><br></pre></td></tr></table></figure><p>以上的設定都佈署完之後，就可以在系統中看到 StorageClass 出現：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get storageclass</span><br><span class="line">NAME                         PROVISIONER               AGE</span><br><span class="line">... (略)</span><br><span class="line">my-nfs-storage               my-nfs-provisioner        5h10m</span><br></pre></td></tr></table></figure><h1 id="驗證佈署是否成功"><a href="#驗證佈署是否成功" class="headerlink" title="驗證佈署是否成功"></a>驗證佈署是否成功</h1><p>驗證的步驟如下：</p><ol><li><p>建立 PVC，指定上面所設定好的 StorageClass</p></li><li><p>建立 Pod，使用上一個步驟設定好的 PVC</p></li></ol><h2 id="建立-PVC"><a href="#建立-PVC" class="headerlink" title="建立 PVC"></a>建立 PVC</h2><p>使用下面的設定來建立一個測試用的 PVC：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-claim</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">volume.beta.kubernetes.io/storage-class:</span> <span class="string">&quot;my-nfs-storage&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Mi</span></span><br></pre></td></tr></table></figure><p>如果可以從系統中看到 PVC Status 為 <code>Bound</code> 就表示建立成功了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 PVC</span></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">test-claim   Bound    pvc-052b8e95-d8f4-11e8-9dda-54ab3a09ec1d   1Mi        RWX            my-nfs-storage        5s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 PV</span></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                                 STORAGECLASS          REASON   AGE</span><br><span class="line">pvc-052b8e95-d8f4-11e8-9dda-54ab3a09ec1d   1Mi        RWX            Delete           Bound    default/test-claim                                                    managed-nfs-storage            2m46s</span><br></pre></td></tr></table></figure><p>上面的訊息表示 StorageClass 成功完成了以下幾件工作：</p><ol><li><p>在 NFS share 上建立 volume</p></li><li><p>建立 PV，並與上述的 NFS volume 作繫結</p></li><li><p>將 PVC 與 PV 繫結</p></li></ol><h2 id="建立-Pod"><a href="#建立-Pod" class="headerlink" title="建立 Pod"></a>建立 Pod</h2><p>最後就是建立 Pod 並指定掛載 PVC，而以下的設定如果成功在 NFS 上建立檔案，這個 Pod 的狀態就會顯示完成，反之則會顯示失敗：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-pod</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">gcr.io/google_containers/busybox:1.24</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line">    <span class="attr">args:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;-c&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1&quot;</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-pvc</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">&quot;/mnt&quot;</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">&quot;Never&quot;</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-pvc</span></span><br><span class="line">      <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">        <span class="attr">claimName:</span> <span class="string">test-claim</span></span><br></pre></td></tr></table></figure><p>檢視一下 Pod 產生的結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">test-pod                                  0/1     Completed   0          9s</span><br></pre></td></tr></table></figure><p>看到 Pod 的 Status 為 Completed 表示 StorageClass + NFS 成功設定完成囉!</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client">Kubernetes NFS-Client Provisioner</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Storage </tag>
            
            <tag> CKA Storage </tag>
            
            <tag> NFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] 分配 &amp; 管理 container 所使用到的計算資源</title>
      <link href="/blog/Kubernetes/k8s-Scheduling-Manage-Compute-Resource-for-Container/"/>
      <url>/blog/Kubernetes/k8s-Scheduling-Manage-Compute-Resource-for-Container/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>當設定 pod spec 時，若要針對計算資源進行管控，可以加入 CPU &amp; memory 的相關設定，藉此可以讓 scheduler 可以更做出更好的分配決定，讓 pod 可以落在更合適的 worker node 上。</p><h1 id="Kubernetes-支援那些-Compute-Resource-Types"><a href="#Kubernetes-支援那些-Compute-Resource-Types" class="headerlink" title="Kubernetes 支援那些 Compute Resource Types?"></a>Kubernetes 支援那些 Compute Resource Types?</h1><p>在 k8s 中預設支援可用的計算資源為 CPU &amp; memory，其中：</p><ul><li><p>CPU 以 core 為單位</p></li><li><p>Memory 以 byte 為單為</p></li></ul><p>因此在設定 pod spec 時可以指定每一個 container 需要多少計算資源，k8s 就會在硬體資源足夠的前提下，自動分配資源來完成工作。</p><p>而設定 Compute Resource Request/Limitation 的方式則是透過以下 4 的參數：</p><ul><li><p><code>spec.containers[].resources.limits.cpu</code></p></li><li><p><code>spec.containers[].resources.limits.memory</code></p></li><li><p><code>spec.containers[].resources.requests.cpu</code></p></li><li><p><code>spec.containers[].resources.requests.memory</code></p></li></ul><blockquote><p>目前 CPU &amp; Memory 的 resource limit 僅可以套用在 container 上而不是 pod 上</p></blockquote><h1 id="CPU-amp-Memory-的單位"><a href="#CPU-amp-Memory-的單位" class="headerlink" title="CPU &amp; Memory 的單位"></a>CPU &amp; Memory 的單位</h1><h2 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h2><p>CPU 的單位通常是 VM 中的 vCore or vCPU，或是實體機中的一個 thread(開啟 Hyperthread 的前提下)，無論 worker node 跑在 CPU core 數有多大的機器，設定所能取得都是一樣的。</p><p>其中設定的單位是 <code>m</code>，每 <strong>1000m = 1 vCore</strong>，也可以使用分數，因此設定的方式可以是：</p><ul><li><p>1 (相當於 1000m)</p></li><li><p>0.5 (相當於 500m)</p></li><li><p>300m (相當於 0.3)</p></li></ul><blockquote><p>設定 1m 是不被允許的，官方建議最低從 100m 開始</p></blockquote><h2 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h2><p>Memory 設定的單位最低則是從 byte 開始，而使用的單位可以是單一字母的 <code>E, P, T, G, M, K</code>，也可以是雙字母的 <code>Ei, Pi, Ti, Gi, Mi, Ki</code>(比較常見)，以下是幾個設定範例：</p><ul><li><p>104857600 (相當於 100 MB = 100<em>1024</em>1024)</p></li><li><p>100M</p></li><li><p>100Mi</p></li></ul><h2 id="設定範例"><a href="#設定範例" class="headerlink" title="設定範例"></a>設定範例</h2><p>有了上面的概念後，以下是個簡單的設定範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">db</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MYSQL_ROOT_PASSWORD</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;password&quot;</span></span><br><span class="line">    <span class="comment"># 第 1 個 container 的 resource limit 設定</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;250m&quot;</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;128Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">wp</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">wordpress</span></span><br><span class="line">    <span class="comment"># 第 2 個 container 的 resource limit 設定</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;250m&quot;</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;128Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span></span><br></pre></td></tr></table></figure><h1 id="硬體資源實際上如何被分配"><a href="#硬體資源實際上如何被分配" class="headerlink" title="硬體資源實際上如何被分配?"></a>硬體資源實際上如何被分配?</h1><p>以上講的部份，其實都僅是個抽象的概念，目的就是要讓使用者了解 k8s 支援 resource limit，以及 resource limit 如何定義 &amp; 使用。</p><p>而硬體資源實際上如何被分配呢? 這件事情要從兩個角度來看：</p><h2 id="帶有-Resource-Request-的-Pod-如何被指派"><a href="#帶有-Resource-Request-的-Pod-如何被指派" class="headerlink" title="帶有 Resource Request 的 Pod 如何被指派?"></a>帶有 Resource Request 的 Pod 如何被指派?</h2><p>由於 k8s 中的每一個 worker node 都有其 CPU &amp; Memory 上限，因此關於 pod 如何被指派到特定的 worker node 上運行，大致上有幾個原則：</p><ul><li><p>將要被分配的 pod 中所有 container 所要求的資源一定會小於 worker node 目前剩餘資源</p></li><li><p>即使 container 要求的資源很低，若是 worker node 無法滿足，scheduler 也不會把 pod 分配過去運作(確保尖峰時段造成 workload peak 讓 worker node 無法負荷)</p></li></ul><h2 id="帶有-Resource-Limit-的-Pod-是如何運作的"><a href="#帶有-Resource-Limit-的-Pod-是如何運作的" class="headerlink" title="帶有 Resource Limit 的 Pod 是如何運作的?"></a>帶有 Resource Limit 的 Pod 是如何運作的?</h2><p>由於最底層的 container 不是由 k8s 負責的，因此資源限制 &amp; 使用這件事情上就得仰賴各個 container runtime(例如：docker, cri-o … 等等)來處理，當然也是有規則來處理這件事情，以下用 Docker 為例來說明：</p><ul><li><p><code>spec.containers[].resources.requests.cpu</code> 的值會被除以 1024 後，轉換為 core value 後傳給 docker，這就是在 <strong>docker run</strong> 時帶入 <a href="https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources"><code>--cpu-shares</code></a> 參數的效果</p></li><li><p><code>spec.containers[].resources.limits.cpu</code> 的值則會被除以 100，而出來的結果就是 container 可以在<strong>每 100ms cpu time</strong>中使用多少 cpu time</p></li></ul><blockquote><p>k8s 預設使用的 quota period 為 100ms，但最低其實可以到 1ms</p></blockquote><ul><li><code>spec.containers[].resources.limits.memory</code> 則是類似在 <strong>docker run</strong> 中帶入 <a href="https://docs.docker.com/engine/reference/run/#/user-memory-constraints"><code>--momery</code></a> 參數的效果</li></ul><h2 id="如果-resource-使用額度爆了怎辦"><a href="#如果-resource-使用額度爆了怎辦" class="headerlink" title="如果 resource 使用額度爆了怎辦?"></a>如果 resource 使用額度爆了怎辦?</h2><p>若是遇到 resource 使用額度爆掉或是目前資源已經不足的的情況，k8s 會根據以下準則來處理：</p><ul><li><p>若是 container 使用超過所設定的 <strong>memory limit</strong>，可能就會被終止；如果這個 container 是可以被重新啟動的，kubelet 就會將其重新啟動</p></li><li><p>若是 container 使用超過所設定的 <strong>memory request</strong>，當負責運行該 container 的 worker node 若是遇到記憶體不足時，會整個 pod 一起被趕走</p></li><li><p>container 使用超過 CPU 使用額度就不一定會被終止 (但因為 cgroup 限制的關係，其實也超過不了多少)</p></li><li><p>若 container 要求比目前 worker 都還更多的資源，那就會造成無法被分派到任一個 worker node 的狀況</p></li></ul><h1 id="遇到-Resource-相關問題怎麼解"><a href="#遇到-Resource-相關問題怎麼解" class="headerlink" title="遇到 Resource 相關問題怎麼解?"></a>遇到 Resource 相關問題怎麼解?</h1><p>以下針對因為資源不足所可能造成的問題作一些說明:</p><h2 id="Pod-一直無法成功分派"><a href="#Pod-一直無法成功分派" class="headerlink" title="Pod 一直無法成功分派"></a>Pod 一直無法成功分派</h2><p>若是發現 pod 一直無法被分派到任何一個 worker node，然後透過 <code>kubectl describe pod/xxxx</code> 指令又發現有 <strong>PodExceedsFreeCPU</strong> or <strong>PodExceedsFreeCPU</strong> 等關鍵字時，大概就是因為 worker node 資源不足造成的，此時可以透過以下幾種方式解決：</p><ul><li><p>增加資源較為充足的 worker node</p></li><li><p>砍掉不需要的 pod，讓資源釋放出來</p></li><li><p>確認 pod 中的 container 所要求的資源沒有超過目前 worker node 所能提供的</p></li></ul><blockquote><p>透過 <code>kubectl describe node/xxxx</code> 可以檢視 node 詳細資訊(一共有多少資源，有那些 pod 在上面、消耗多少資源 …. etc)，其中 <code>Allocatable</code> 欄位會列出 worker node 可用的 CPU &amp; Memory 資源</p></blockquote><h2 id="Pod-被終止"><a href="#Pod-被終止" class="headerlink" title="Pod 被終止"></a>Pod 被終止</h2><p>若 pod 不斷被重啟，但又不是程式邏輯錯誤所造成時，可能就是因為資源不足所造成的，這時候可以透過指令 <code>kubectl get pod -o json</code>，可以仔細看看是不是有 <code>terminated:map</code> &amp; <code>reason</code> 關鍵字。就可以看出 pod 被終止的真正原因。</p><h1 id="可以限制磁碟使用空間嗎"><a href="#可以限制磁碟使用空間嗎" class="headerlink" title="可以限制磁碟使用空間嗎?"></a>可以限制磁碟使用空間嗎?</h1><p>也許有人會問，若是 container 沒有外掛 external storage，然後又無限制的使用磁碟空間的話，那 worker node 上的磁碟空間不就有可能爆掉? 是否可以透過磁碟使用的限制來避免這個問題的發生呢?</p><p>這個問題的答案是 “<strong>Yes</strong>“，在 v1.8 後，使用者可以開始針對 ephemeral storage 進行磁碟空間的限制，詳述如下：</p><h2 id="什麼是-ephemeral-storage"><a href="#什麼是-ephemeral-storage" class="headerlink" title="什麼是 ephemeral storage?"></a>什麼是 ephemeral storage?</h2><p>這功能在原生的 container runtime 並沒有提供，因此 k8s 就必須自己來處理，因此在 v1.8 版後釋出了一個名稱為 <code>ephemeral-storage</code> 的 resource，用來管理 host 上的短暫儲存空間。</p><blockquote><p>ephemeral-storage 在 v1.10 後被列為正式功能，不需在透過 feature gate(<code>LocalStorageCapacityIsolation</code>) 開啟；但目前在 v1.12 中還是 beta，尚未 GA</p></blockquote><p>在每個 k8s node 上的 kubelet 在運行時，總是會需要儲存一些資料在本地端，因此以下的其中兩個空間就會被拿來利用：</p><ul><li><p>kubelet root directory: <code>/var/lib/kubelet</code></p></li><li><p>log directory: <code>/var/log</code></p></li></ul><p>由於被 kubelet 管理到的部份，例如：pod log, image layer, container writable latyer，甚至是在 pod spec 中定義 <code>emptyDir</code>，都會利用到上面提到的這兩個空間，而 ephemeral-storage 就是針對上述這兩個空間進行管理</p><blockquote><p>若是在安裝時額外將 images later or container writable layer 放到其他獨立的分割區中，ephemeral-storage 就管不著了 (只能管理到主分割區 <code>/</code>)</p></blockquote><h2 id="設定-ephemeral-storage-使用限制"><a href="#設定-ephemeral-storage-使用限制" class="headerlink" title="設定 ephemeral storage 使用限制"></a>設定 ephemeral storage 使用限制</h2><p>ephemeral storage 額度設定可透過以下兩個設定來完成：</p><ul><li><p><code>spec.containers[].resources.limits.ephemeral-storage</code></p></li><li><p><code>spec.containers[].resources.requests.ephemeral-storage</code></p></li></ul><p>設定的單位跟 memory 其實一樣，可以是單一字母的 <code>E, P, T, G, M, K</code>，也可以是雙字母的 <code>Ei, Pi, Ti, Gi, Mi, Ki</code>，以下是一個簡單的設定範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 整個 pod 一共要求 4G 的 ephemeral storage 空間</span></span><br><span class="line">  <span class="comment"># 以及使用 ephemeral storage 的空間限制為 8G</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">db</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MYSQL_ROOT_PASSWORD</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;password&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment"># db 要求 2G 的 ephemeral storage 空間</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">ephemeral-storage:</span> <span class="string">&quot;2Gi&quot;</span></span><br><span class="line">      <span class="comment"># db 使用 ephemeral storage 的空間限制為 4G</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">ephemeral-storage:</span> <span class="string">&quot;4Gi&quot;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">wp</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">wordpress</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment"># wp 要求 2G 的 ephemeral storage 空間</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">ephemeral-storage:</span> <span class="string">&quot;2Gi&quot;</span></span><br><span class="line">      <span class="comment"># wp 使用 ephemeral storage 的空間限制為 4G</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">ephemeral-storage:</span> <span class="string">&quot;4Gi&quot;</span></span><br></pre></td></tr></table></figure><h2 id="其他問題"><a href="#其他問題" class="headerlink" title="其他問題"></a>其他問題</h2><p>其他比較常見的問題可能會有：</p><h3 id="帶有-ephemeral-storage-requests-的-pod-要怎麼被分派"><a href="#帶有-ephemeral-storage-requests-的-pod-要怎麼被分派" class="headerlink" title="帶有 ephemeral-storage requests 的 pod 要怎麼被分派?"></a>帶有 ephemeral-storage requests 的 pod 要怎麼被分派?</h3><p>由於 kubelet 會運行在每一個 node 上，因此會定期回報 node 相關資訊，當然也包含磁碟容量。</p><blockquote><p>每個 node 的 ephemeral storage 可透過 <code>kuebctl describe node/xxxx</code> 指令查詢，並尋找 <strong>Allocatable &gt; ephemeral-storage</strong> 即可知道有多少磁碟空間可用</p></blockquote><p>因此假設在 CPU &amp; memory 都沒問題的情況下，<strong>磁碟空間最大</strong>且<strong>符合 pod 中所有 container 對於磁碟空間要求</strong>的 node 會被選到。</p><h3 id="帶有-ephemeral-storage-limit-的-pod-會如何運作"><a href="#帶有-ephemeral-storage-limit-的-pod-會如何運作" class="headerlink" title="帶有 ephemeral-storage limit 的 pod 會如何運作"></a>帶有 ephemeral-storage limit 的 pod 會如何運作</h3><p>由於帶有 ephemeral-storage limit，因此 k8s 就會持續監控 pod 中的 container 的磁碟使用狀況是否有超過設定限制，pod 就會被趕走。</p><p>而以上是以 container level 的角度來看，但若是以 pod level 來看，只要所有 container 使用空間 &amp; emptyDir volume 使用空間的加總超過使用限制，pod 也會被趕走。</p><h1 id="其它擴充資源-Extended-Resources"><a href="#其它擴充資源-Extended-Resources" class="headerlink" title="其它擴充資源(Extended Resources)"></a>其它擴充資源(Extended Resources)</h1><p>由於資源這東西不會只有 CPU/Memoery/Disk 三種，總是會有其他的資源，例如：GPU。但這一類的資源並不是普遍存在在每一台機器上，因此在 k8s 就不會以內建支援，因此就必須改用擴充的方式進行資源管理。</p><blockquote><p>額外擴充的資源就不會屬於 <code>kubernetes.io</code> domain，而是可以由管理者自己定義</p></blockquote><p>既然 extended resource 需要由管理者自行定義，那要讓 container 使用到 extended resource，最基本就要完成兩件事情：</p><ol><li><p>管理者要告訴 k8s 有哪些 extended resource (透過 advertise 的方式)</p></li><li><p>在 pod spec 中要有對於這些 extended resource 的 request 設定</p></li></ol><p>而 extended resource 可以根據其所影響的範圍，可以分為 node-level &amp; cluster-level：</p><h2 id="Node-level-extended-resources"><a href="#Node-level-extended-resources" class="headerlink" title="Node-level extended resources"></a>Node-level extended resources</h2><p>node level 的資源即是跟 node 相關的資源，而 node-level extended resource 有分為兩種：</p><h3 id="Device-plugin-managed-resources"><a href="#Device-plugin-managed-resources" class="headerlink" title="Device plugin managed resources"></a>Device plugin managed resources</h3><p>由於愈來愈多硬體加速的需求在雲平台上面跑出來，因此在 k8s 就提出了 </p><p>這是因為通常會是特定的硬體，例如 k8s 中有提出 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/" title="Device Plugin">Device Plugin</a> 的概念，讓使用者可以自訂在某些 node 上會有特定的硬體資源(例如：GPU, FPGA, InfiniBand …. 等等)。</p><h3 id="其他資源"><a href="#其他資源" class="headerlink" title="其他資源"></a>其他資源</h3><p>至於其他資源的方式，由於沒有一個明確的 resource object 來處理這個，因此可以透過發一個 HTTP <code>PATCH</code> request 給 API server，為特定 node 上的 <code>status.capacity</code> 資訊中增加特定的資源，完成了之後，透過 <code>kubectl describe node/xxxx</code> 就可以在 <code>status.allocatable</code> 資訊中看到所新增的資源。</p><p>一旦新增了 extended resource，kubelet 就會根據 pod 已經所消耗的數量，定期匯報資訊給 API server，因此 scheduler 在進行 pod 分派的時候就可以執行正確的決定。</p><p>以下用一個簡單的例子來說明如何為特定的 node 新增 extended resource：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME              STATUS   ROLES    AGE   VERSION</span><br><span class="line">leon-k8s-node00   Ready    master   8d    v1.12.1</span><br><span class="line">leon-k8s-node01   Ready    master   8d    v1.12.1</span><br><span class="line">leon-k8s-node02   Ready    master   8d    v1.12.1</span><br><span class="line">leon-k8s-node03   Ready    node     8d    v1.12.1</span><br><span class="line">leon-k8s-node04   Ready    node     8d    v1.12.1</span><br><span class="line">leon-k8s-node05   Ready    node     8d    v1.12.1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 node03 的 status.capacity 資訊</span></span><br><span class="line">$ kubectl describe node/leon-k8s-node03</span><br><span class="line">Name:               leon-k8s-node03</span><br><span class="line">Roles:              node</span><br><span class="line">....(略)</span><br><span class="line">Capacity:</span><br><span class="line"> attachable-volumes-azure-disk:  16</span><br><span class="line"> cpu:                            4</span><br><span class="line"> ephemeral-storage:              64989928Ki</span><br><span class="line"> hugepages-1Gi:                  0</span><br><span class="line"> hugepages-2Mi:                  0</span><br><span class="line"> memory:                         4046000Ki</span><br><span class="line"> pods:                           110</span><br><span class="line">....(略)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 HTTP PATCH，在 node03 中新增一個 example.com/foo 的 extended resource，數量為 5</span></span><br><span class="line"><span class="comment"># 其中 ~1 是 &quot;/&quot; 字元經過 url encoding 之後的結果</span></span><br><span class="line">$ curl --header <span class="string">&quot;Content-Type: application/json-patch+json&quot;</span> \</span><br><span class="line">--request PATCH \</span><br><span class="line">--data <span class="string">&#x27;[&#123;&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/status/capacity/example.com~1foo&quot;, &quot;value&quot;: &quot;5&quot;&#125;]&#x27;</span> \</span><br><span class="line">http://localhost:8080/api/v1/nodes/leon-k8s-node03/status</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再一次檢視 node03 的 status.capacity 資訊</span></span><br><span class="line"><span class="comment"># 可以看到新增的 exmaple.com/foo 已經出現</span></span><br><span class="line">$ kubectl describe node/leon-k8s-node03</span><br><span class="line">Name:               leon-k8s-node03</span><br><span class="line">Roles:              node</span><br><span class="line">....(略)</span><br><span class="line">Capacity:</span><br><span class="line"> attachable-volumes-azure-disk:  16</span><br><span class="line"> cpu:                            4</span><br><span class="line"> ephemeral-storage:              64989928Ki</span><br><span class="line"> example.com/foo:                5</span><br><span class="line"> hugepages-1Gi:                  0</span><br><span class="line"> hugepages-2Mi:                  0</span><br><span class="line"> memory:                         4046000Ki</span><br><span class="line"> pods:                           110</span><br><span class="line">....(略)</span><br></pre></td></tr></table></figure><h2 id="Cluster-level-extended-resource"><a href="#Cluster-level-extended-resource" class="headerlink" title="Cluster-level extended resource"></a>Cluster-level extended resource</h2><p>cluster level 的資源就不會與特定的 node 綁定，因此就由更上層的 scheduler extender 來管理，這個部份等之後有接觸過 &amp; 有更多實用的資訊後再來補….</p><h1 id="未來發展方向"><a href="#未來發展方向" class="headerlink" title="未來發展方向"></a>未來發展方向</h1><p>從上面可以看出，其實 k8s 目前可以進行 resource limit 的能力 &amp; 範圍還是相當有限，但 IT 的世界本來就是一直在進步的，因此有些概念被提了出來，未來可能會往以下幾個方向來改進：</p><ul><li><p>現在的 resource limit 是設定在 container level，未來可能可以設定在 pod level</p></li><li><p>未來希望可以新增更多 resource type 的支援，當然也包含自訂的部份</p></li><li><p>支援 resource overcommitment，藉此來達成某種程度的 QoS</p></li><li><p>以目前來說，一個 CPU 其實在不同的 provider 之間的定義是不同的，未來會考慮將這個問題解決</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">Managing Compute Resources for Containers - Kubernetes</a></p></li><li><p><a href="https://yq.aliyun.com/articles/594066">体验ephemeral-storage特性来对Kubernetes中的应用做存储的限制和隔离-博客-云栖社区-阿里云</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Scheduling </tag>
            
            <tag> CKA Scheduling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Job, CronJob &amp; TTL Controller Overview</title>
      <link href="/blog/Kubernetes/k8s-Job-Overview/"/>
      <url>/blog/Kubernetes/k8s-Job-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Job-amp-CroneJob"><a href="#What-is-Job-amp-CroneJob" class="headerlink" title="What is Job &amp; CroneJob?"></a>What is Job &amp; CroneJob?</h1><p>Kubernetes Job 就像 Linux <code>at</code> 一樣，是個執行<strong>一次性工作</strong>的手段，而 Kubernetes CroneJob 也就跟 Linux <code>CronJob</code> 一樣，是個<strong>定期執行特定工作</strong>的手段。</p><p>但回到 k8s 本身來看，Job &amp; CroneJob 這兩件事情是如何被完成的?</p><h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><p>在 k8s 中，一個 Job 會建立一個 or 多個 pod 來執行工作，並確保指定數量的 pod 有成功的完成工作並終止。</p><p>這代表 k8s 會去追蹤 Job pod 執行的狀態，並確保有成功的執行完成，當指定數量(例如: 5 個中的其中 3 個)的 Job pod 完成工作後，這個 Job 就會被註記為 complete。</p><p>因此換個角度來看，透過 Job 來建立 pod 執行工作的目的，是為了確保工作一定有完成，若是因為特殊原因(例如: node hardware failure, node reboot) 而造成 pod 無法正常完成工作，Job 則會自動啟動新的 pod 來繼續完成工作。</p><p>此外，Job 還有以下特性：</p><ul><li><p>當 Job 被刪除時，所有相關的 pod 也都一併會被刪除</p></li><li><p>Job 可以同時執行多個 pod 來完成多個工作</p></li></ul><h2 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h2><h1 id="試試第一個-Job"><a href="#試試第一個-Job" class="headerlink" title="試試第一個 Job"></a>試試第一個 Job</h1><p>有了以上的概念後，接著要來試試看執行第一個 k8s Job，使用以下定義：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">perl</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;perl&quot;</span>,  <span class="string">&quot;-Mbignum=bpi&quot;</span>, <span class="string">&quot;-wle&quot;</span>, <span class="string">&quot;print bpi(2000)&quot;</span>]</span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">4</span></span><br></pre></td></tr></table></figure><p>以下是操作 Job 的過程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以儲存成本地檔案執行 or 執行以下指令也可</span></span><br><span class="line">$ kubectl apply -f https://k8s.io/examples/controllers/job.yaml</span><br><span class="line">job.batch/pi created</span><br><span class="line"></span><br><span class="line">$ kubectl describe <span class="built_in">jobs</span>/pi</span><br><span class="line">Name:           pi</span><br><span class="line">Namespace:      default</span><br><span class="line">Selector:       controller-uid=efcfc835-c815-11e8-acb2-66712a0dd587</span><br><span class="line">Labels:         controller-uid=efcfc835-c815-11e8-acb2-66712a0dd587</span><br><span class="line">                job-name=pi</span><br><span class="line">...(略)</span><br><span class="line">Parallelism:    1</span><br><span class="line">Completions:    1</span><br><span class="line">Start Time:     Thu, 04 Oct 2018 20:41:57 +0000</span><br><span class="line">Pods Statuses:  1 Running / 0 Succeeded / 0 Failed</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  controller-uid=efcfc835-c815-11e8-acb2-66712a0dd587</span><br><span class="line">           job-name=pi</span><br><span class="line">  Containers:</span><br><span class="line">   pi:</span><br><span class="line">    Image:      perl</span><br><span class="line">    Port:       &lt;none&gt;</span><br><span class="line">    Host Port:  &lt;none&gt;</span><br><span class="line">    Command:</span><br><span class="line">      perl</span><br><span class="line">      -Mbignum=bpi</span><br><span class="line">      -wle</span><br><span class="line">      <span class="built_in">print</span> bpi(2000)</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:       &lt;none&gt;</span><br><span class="line">  Volumes:        &lt;none&gt;</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age   From            Message</span><br><span class="line">  ----    ------            ----  ----            -------</span><br><span class="line">  Normal  SuccessfulCreate  9s    job-controller  Created pod: pi-hnqjx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 pod 完整名稱</span></span><br><span class="line">$ pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath=&#123;.items..metadata.name&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視執行結果</span></span><br><span class="line">$ kubectl logs <span class="variable">$pods</span></span><br><span class="line">3.141592653589793238462643383279502884197169.....</span><br></pre></td></tr></table></figure><h1 id="如何正確的撰寫-Job"><a href="#如何正確的撰寫-Job" class="headerlink" title="如何正確的撰寫 Job"></a>如何正確的撰寫 Job</h1><p>以下用一個範例來做說明：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># apiVersion, kind, metadata 這3個欄位是必須的</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">parallel-jobs-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># .spec.selector 不需要設定</span></span><br><span class="line">  <span class="comment"># selector:</span></span><br><span class="line">  <span class="comment">#   matchLabels:</span></span><br><span class="line">  <span class="comment">#     demo: jobs</span></span><br><span class="line">  <span class="attr">parallelism:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">completions:</span> <span class="number">15</span></span><br><span class="line">  <span class="comment"># .spec.template 是 .spec 中唯一必須的欄位</span></span><br><span class="line">  <span class="comment"># 這部份就跟 pod template 其實是相同的</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">demo:</span> <span class="string">jobs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sleep</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sleep 5; if [ $((RANDOM % 4)) -eq 0 ]; then exit 1; fi&quot;</span>]</span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure><h1 id="關於-Job-的平行處理"><a href="#關於-Job-的平行處理" class="headerlink" title="關於 Job 的平行處理"></a>關於 Job 的平行處理</h1><h2 id="Parallel-Job-Type"><a href="#Parallel-Job-Type" class="headerlink" title="Parallel Job Type"></a>Parallel Job Type</h2><p>由於平行處理的部份稍微複雜點，因此把這個部份額外拉出來說明，Job type 共分為三種：</p><h3 id="非平行處理的工作"><a href="#非平行處理的工作" class="headerlink" title="非平行處理的工作"></a>非平行處理的工作</h3><ul><li><p>一般同時間只會有一個 pod 啟動做事情，除非有 pod 工作執行失敗</p></li><li><p>當 pod 成功終止後，job 狀態便轉為完成</p></li></ul><h3 id="帶有固定完成數量的平行處理工作"><a href="#帶有固定完成數量的平行處理工作" class="headerlink" title="帶有固定完成數量的平行處理工作"></a>帶有固定完成數量的平行處理工作</h3><ul><li><p>同時平行處理的工作數量是由 <code>.spec.parallelism</code> 所設定</p></li><li><p>在 <code>.spec.completions</code> 中有設定完成的數量(必須是大於零的整數值)</p></li><li><p>當 successful pod 的數量到達 <code>.spec.completions</code> 所設定時，這個 job 的狀態就會轉換為完成</p></li></ul><p>以上面的例子作示範，會有類似以下的輸出結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get all</span><br><span class="line">NAME                           READY     STATUS      RESTARTS   AGE</span><br><span class="line">pod/parallel-jobs-demo-5tstd   0/1       Error       0          1m</span><br><span class="line">pod/parallel-jobs-demo-6knkl   0/1       Completed   0          1m</span><br><span class="line">pod/parallel-jobs-demo-6vdgt   0/1       Completed   0          1m</span><br><span class="line">pod/parallel-jobs-demo-6ztmv   0/1       Completed   0          1m</span><br><span class="line">pod/parallel-jobs-demo-969rc   0/1       Completed   0          2m</span><br><span class="line">pod/parallel-jobs-demo-cbwtp   0/1       Error       0          2m</span><br><span class="line">pod/parallel-jobs-demo-cc8tb   0/1       Completed   0          1m</span><br><span class="line">pod/parallel-jobs-demo-dlzg8   0/1       Error       0          2m</span><br><span class="line">pod/parallel-jobs-demo-dvff6   0/1       Error       0          1m</span><br><span class="line">pod/parallel-jobs-demo-gqqwc   0/1       Completed   0          2m</span><br><span class="line">pod/parallel-jobs-demo-js5qg   0/1       Completed   0          1m</span><br><span class="line">pod/parallel-jobs-demo-kwq7j   0/1       Completed   0          2m</span><br><span class="line"><span class="comment"># 當 SUCCESSFUL 數量為 14 時，就只剩這個 pod 在運作</span></span><br><span class="line"><span class="comment"># 原因應該是 k8s 避免 SUCCESSFUL pod 的數量會超過 DESIRED</span></span><br><span class="line"><span class="comment"># 因此原本會有同時 5 個 job 平行處理，最後只會長出一個來執行</span></span><br><span class="line">pod/parallel-jobs-demo-mccn2   0/1       Completed   0          44s</span><br><span class="line">pod/parallel-jobs-demo-mcgv2   0/1       Error       0          1m</span><br><span class="line">pod/parallel-jobs-demo-mp6kf   0/1       Completed   0          2m</span><br><span class="line">pod/parallel-jobs-demo-nmcjk   0/1       Completed   0          2m</span><br><span class="line">pod/parallel-jobs-demo-s8hxl   0/1       Completed   0          1m</span><br><span class="line">pod/parallel-jobs-demo-t7767   0/1       Completed   0          2m</span><br><span class="line">pod/parallel-jobs-demo-w2mfb   0/1       Error       0          2m</span><br><span class="line">pod/parallel-jobs-demo-w6dqv   0/1       Completed   0          2m</span><br><span class="line">pod/parallel-jobs-demo-x725p   0/1       Error       0          1m</span><br><span class="line">pod/parallel-jobs-demo-xvtlt   0/1       Completed   0          1m</span><br><span class="line">pod/pi-hnqjx                   0/1       Completed   0          2d</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/kubernetes   ClusterIP   10.233.0.1   &lt;none&gt;        443/TCP   33d</span><br><span class="line"></span><br><span class="line">NAME                           DESIRED   SUCCESSFUL   AGE</span><br><span class="line">job.batch/parallel-jobs-demo   15        15           2m</span><br><span class="line">job.batch/pi                   1         1            2d</span><br></pre></td></tr></table></figure><p>基本上 job pod 會一直長出來執行工作，一直到 <strong>DESIRED = SUCCESSFUL</strong> 為止。</p><h3 id="與-work-queue-搭配的平行工作"><a href="#與-work-queue-搭配的平行工作" class="headerlink" title="與 work queue 搭配的平行工作"></a>與 work queue 搭配的平行工作</h3><ul><li><p>不需要設定 <code>.spec.completions</code>，此時會預設為 <strong>.spec.parallelism = .spec.completions</strong></p></li><li><p>需要透過外部的服務來協助執行</p></li></ul><blockquote><p>這部份的應用目前不是很清楚對應的實際範例，之後若有對這部份更詳細的了解，再回來補齊</p></blockquote><h2 id="Parallel-Job-Control"><a href="#Parallel-Job-Control" class="headerlink" title="Parallel Job Control"></a>Parallel Job Control</h2><p>基本上，<code>.spec.parallelism</code> 可以設定為任何非負整數值，若是沒有設定的話，則預設為 <code>1</code>，若設定為 <code>0</code>，則表示 Job 暫停，直到被改成大於 0 為止。</p><p>然而，實際上同時執行的工作數量可能會少於 <code>.spec.parallelism</code> 所指定的值，可能的原因如下：</p><ul><li><p>若有設定 <code>.spec.completions</code>，同時執行的 job 數量加上已經成功完成的 job 數量就不會超過這個數字，因此到後面會愈來愈少 job 同時執行</p></li><li><p>若是 work queue jobs，若是有任何的 pod 已經成功結束，就不會再有新的 pod 產生</p></li><li><p>controller 可能沒空回應請求</p></li><li><p>因為某些原因造成無法再建立新的 pod (例如: ResourceQuota 不足 or 權限不足)</p></li></ul><p>因此若有想要使用 pod 來執行 parallel job，需要注意的地方其實是很多的。</p><h1 id="當-Pod-or-Container-無法成功執行"><a href="#當-Pod-or-Container-無法成功執行" class="headerlink" title="當 Pod or Container 無法成功執行"></a>當 Pod or Container 無法成功執行</h1><p>只要是人寫出來的程式，總是會有 bug 的，因此 container 中的程式執行時發生錯誤而造成 exit code 大於 0 的狀況也是時常會發生，因此程式開發者就必須要了解到在 k8s 中有什麼機制可以來處理這樣的狀況。</p><h2 id="Restart-Policy"><a href="#Restart-Policy" class="headerlink" title="Restart Policy"></a>Restart Policy</h2><p>這個部份的設定來自於 <code>.spec.template.spec.restartPolicy</code>，當 <strong>Container</strong> 工作執行失敗時，k8s 會進行處理的方式，而：</p><ul><li><p><code>OnFailure</code>：pod 依然會待在原本的 node 上，但 pod 中的 container 會重新啟動；而 application 的開發者必須要撰寫相關程式碼處理當 fail 後，重新啟動時要如何恢復正常狀態的工作</p></li><li><p><code>Never</code>：此時 container 不會再重新啟動，就維持失敗的狀態</p></li></ul><p>最後 pod 中的 container 若是有全部成功執行，pod 的狀態就會變成 complete，反之則是 failed。</p><p>接著是當 <strong>Pod</strong> 的部份，首先下圖說明 pod life cycle：</p><p><img src="/blog/images/kubernetes/kubernetes-pod-life-cycle-status.jpg" alt="Pod Life Cycle"></p><p>可以看出當 pod 被判定為 failed，k8s 會自動將其重新啟動並回到 Running 的狀態；而因為不明原因重新啟動後，為了要讓 application 能夠恢復到正常運作的狀態，可能需要處理暫存檔、lock、未完成的輸出…等等，則是 application 開發者需要自己寫程式處理。</p><blockquote><p>restart policy 是套用在 container 上而非 pod，因此即使 <code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code>，不是因為 container exit code &gt; 0 造成的失敗還是會讓 pod 重新啟動 (例如: node failed)</p></blockquote><p>因此了解了以上 k8s 的處理邏輯後，就可以得出以下結論：</p><ul><li><p>即使設定 <code>.spec.parallelism = 1</code> + <code>.spec.completions = 1</code> + <code>.spec.template.spec.restartPolicy = &quot;Never&quot;</code>，同樣的程式可能還是會執行兩次(因為 pod failed 造成的重新啟動)，因此程式設計師永遠需要考慮程式執行第二次以上時所需要的額外處理 </p></li><li><p>若 <code>.spec.parallelism = 1</code> &amp; <code>.spec.completions = 1</code> 皆大於 1，此時就會有多個 pod 同時運行的狀況發生，因此 application 開發者也要妥善處理程式同時多個運作時的相互溝通問題</p></li></ul><h2 id="Backoff-Limit"><a href="#Backoff-Limit" class="headerlink" title="Backoff Limit"></a>Backoff Limit</h2><p>當 pod failed 之後的重新啟動，在 k8s 中稱為 backoff，但每一次的重新啟動會稍微延長 delay 的時間，從 10s, 20s, 40s … 等比例上升，一直到達 5 mins 為止，而這個 delay 的數字會在 pod 成功執行 10 mins 後重置。</p><h1 id="Job-Termination"><a href="#Job-Termination" class="headerlink" title="Job Termination"></a>Job Termination</h1><p>要終止 job pod 的方式除了上面所提到的，不斷的 pod failed 造成達到 backoff limit 之外，還可以設定 <code>.spec.activeDeadlineSeconds</code> 來直接設定 job 可存活的時間，而 activeDeadlineSeconds 的優先權大於 backoffLimit</p><p>若是觀察 <code>activeDeadlineSeconds</code> 比 <code>backoffLimit</code> 提早發生，可使用下面的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">parallel-jobs-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">activeDeadlineSeconds:</span> <span class="number">40</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">demo:</span> <span class="string">jobs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sleep</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sleep 3; exit 1&quot;</span>]</span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure><blockquote><p>若要觀察 <code>backoffLimit</code> 比 <code>activeDeadlineSeconds</code> 提早發生，則可將數字調整成 <code>3</code> &amp; <code>300</code></p></blockquote><h1 id="如何清除執行完的-Job"><a href="#如何清除執行完的-Job" class="headerlink" title="如何清除執行完的 Job"></a>如何清除執行完的 Job</h1><p>當 job 完成後，不會再有 pod 被產生，原本已經存在的 pod 也不會被刪除，而保留這些 pod 可以用來檢視有沒有特別的錯誤或是警告、察看相關的 log 或是診斷輸出之類的訊息。</p><p>然而這些資料也許很多不會需要永久保存，如果沒有進行清理的話，整個系統就會被很多不需要的資訊佔據，因此 k8s 提供了幾個方式可以讓使用者作清理的工作：</p><h2 id="手動使用-kubectl"><a href="#手動使用-kubectl" class="headerlink" title="手動使用 kubectl"></a>手動使用 kubectl</h2><p>這個很直覺，以上面的例子來說，就是執行類似下面兩個指令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 name，刪除特定的 job </span></span><br><span class="line">$ kubectl delete <span class="built_in">jobs</span>/pi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過原本產生 job 的檔案來刪除 job</span></span><br><span class="line">$ kubectl delete -f ./job.yaml</span><br></pre></td></tr></table></figure><h2 id="自動清理-Job"><a href="#自動清理-Job" class="headerlink" title="自動清理 Job"></a>自動清理 Job</h2><p>如果可以自動完成，誰想要手動呢? 而自動的方式可以有兩種，其中的第一種是利用 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a> 來管理 Job，並利用 CronJob 的特性定期的清理不需要的 Job 資訊。</p><p>另一種則是在 <code>v1.12</code> 提出來的 <strong>TTL(Time to Live)</strong> 的機制，在 v1.12 中新增了一個 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/">TTL controller</a> 來實現 TTL 的功能，透過設定 <code>.spec.ttlSecondsAfterFinished</code> 欄位來設定 TTL，以下是一個設定範例檔：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pi-with-ttl</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 此 Job 將會在成功之後的 30 秒自動移除</span></span><br><span class="line">  <span class="comment"># 若設定為 0，則 Job 執行完畢之後就會被馬上清除</span></span><br><span class="line">  <span class="comment"># 若沒有設定，Job 就永遠不會被清除</span></span><br><span class="line">  <span class="attr">ttlSecondsAfterFinished:</span> <span class="number">30</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pi</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">perl</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;perl&quot;</span>,  <span class="string">&quot;-Mbignum=bpi&quot;</span>, <span class="string">&quot;-wle&quot;</span>, <span class="string">&quot;print bpi(2000)&quot;</span>]</span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure><p>而 TTL controller 在移除 Job 會連同所以相關的 resource object(例如: pod) 一同移除，但不會全然暴力移除，若是有設定 finalizer 的話，會先執行完後才會移除相關的 object。</p><p>關於 <code>.spec.ttlSecondsAfterFinished</code> 的設定以及對應的運作的規則如下：</p><ul><li><p>假設設定 30，當 Job 執行完後的 30 秒，會被 TTL controller 自動清除</p></li><li><p>若設定 0，當 Job 執行完畢後會被馬上清除</p></li><li><p>若沒有設定，Job 則永遠不會被清除</p></li></ul><blockquote><p>注意! 這功能只有 v1.12.0 以後可用，而且需要啟用 feature gate <code>TTLAfterFinished</code></p></blockquote><h1 id="如何在-Job-指定-pod-selector"><a href="#如何在-Job-指定-pod-selector" class="headerlink" title="如何在 Job 指定 pod selector?"></a>如何在 Job 指定 pod selector?</h1><p>一般來說，建立 job 時是不需要設定 <code>.spec.selector</code>，即使設定了 API server 也會回報錯誤給你，讓你知道這件事情是不被允許的；而這樣的作法是為了確保 Job 產生出來的 pod 中的 <code>.spec.selector</code> 不會互相重複而造成問題，因此 Job pod selector 統一由 k8s 自動來分配。</p><p>但如果在某些時候真的需要覆寫這些自動產生出來的 pod selector 資訊時，要怎麼做? 這問題的答案當然還是要回頭設定 <code>.spec.selector</code>，但方法有些不同。</p><p>首先要說明要採取這樣的行動會有以下幾點需要注意：</p><ul><li><p>必須確保設定的 <code>.spec.selector</code> 是獨一無二的</p></li><li><p>若跟其他 job pod selector 有重複，可能會被刪除，或是讓其他 job 在計算完成數量時發生錯誤(畢竟 k8s 都是靠 selector 在完成這些事情的)</p></li><li><p>若 pod selector 跟其他 controller(例如: replication controller) 所管理的 pod 相同時，可能會有無法預期的事情發生</p></li></ul><blockquote><p>k8s 本身並不會阻止你設定有問題的 pod selector，因此這部份需要自己確認好</p></blockquote><p>接著要說明如何自訂 <code>.spec.selector</code> (嚴格來說並不全然算自訂)。</p><p>首先假設有個名稱為 <code>old</code> 的 job 已經正在運行，已經有部份的 pod 被啟動並執行中，有些 pod 還沒開始啟動，此時若想要保留原本正在執行的 pod，但又希望後面產生的 pod 可以套用新的 pod template 時該怎麼做?</p><p>直接使用 <code>kubectl edit job/old</code> 來完成嗎? 答案是<strong>不行</strong>的，k8s 不允許更改 job。</p><p>正確的流程應該是這樣：</p><h2 id="確認-job-old-所自動產生的-pod-selector"><a href="#確認-job-old-所自動產生的-pod-selector" class="headerlink" title="確認 job/old 所自動產生的 pod selector"></a>確認 job/old 所自動產生的 pod selector</h2><p>可以透過 <code>kubectl describe job/old</code> 命令來檢視，假設出現下面的訊息：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">old</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">job-uid:</span> <span class="string">a8f3d00d-c6d2-11e5-9f87-42010af00002</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure><p>取得上面自動產生的 pod selector(<code>job-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002</code>) 之後，就可以繼續往下做</p><h2 id="刪除-job-old"><a href="#刪除-job-old" class="headerlink" title="刪除 job/old"></a>刪除 job/old</h2><p>接著可以使用以下指令移除 job，但又保留目前正常執行中的 pod: </p><blockquote><p>kubectl delete jobs/old –cascade=false</p></blockquote><h2 id="重新建立-Job"><a href="#重新建立-Job" class="headerlink" title="重新建立 Job"></a>重新建立 Job</h2><p>接著就是修改 job YAML 定義檔，並加入上面取得的 pod selector，並搭配 <code>manualSelector: true</code>(<code>.spec.manualSelector</code>) 的關鍵設定，讓 k8s 同意我們在 job 中自訂 <code>.spec.selector</code>，因此 job YAML 大概會是下面這個樣子：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">new</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">manualSelector:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">job-uid:</span> <span class="string">a8f3d00d-c6d2-11e5-9f87-42010af00002</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure><p>接下來 job/new 就可以完全接手原本的 pod 並使用新的 pod template 產生新的 pod 繼續往下執行。</p><h1 id="有其他方式執行-Job-嗎"><a href="#有其他方式執行-Job-嗎" class="headerlink" title="有其他方式執行 Job 嗎?"></a>有其他方式執行 Job 嗎?</h1><p>有人可能會想知道，在 k8s 內是否有其他機制可以來執行所謂”一次性的工作”，當然是有的，只是是不是真的合適的問題而已。</p><h2 id="Bare-Pod"><a href="#Bare-Pod" class="headerlink" title="Bare Pod"></a>Bare Pod</h2><p>bare pod 的問題就是掛掉不會自己啟動，而 job pod 會自動幫你重新啟動 pod 並取代原本掛掉的 pod，因此若要確保工作一定會執行完成，還是建議使用 job。</p><h2 id="Replication-Controller"><a href="#Replication-Controller" class="headerlink" title="Replication Controller"></a>Replication Controller</h2><p>基本上 job 是 replication controller 的互補。</p><p>因為 replication controller 的目的是確保 pod 不會被終止，可以維持使用者所期待的狀態；而 job 正好相反，它會在 job pod 都執行完畢後終止 pod，甚至進行清理。</p><p>也因為這樣，在 job 的定義中一定要設定 <code>.spec.template.restartPolicy</code>  為 <code>Never</code> or <code>OnFailure</code>，因為若是不設定的話，預設會是 <code>Always</code>，但這樣就違反了 job 的設計原則了。</p><h2 id="Single-Job-starts-Controller-Pod"><a href="#Single-Job-starts-Controller-Pod" class="headerlink" title="Single Job starts Controller Pod"></a>Single Job starts Controller Pod</h2><p>這是最麻煩且完全自幹的方式，在 job pod 中放一個控制用的 container，而這些控制邏輯完全由使用者自己控制。</p><p>這樣的方式擁有最大的自主權跟彈性，但這樣就無法有效的與 k8s 提供的機制進行整合。</p><h1 id="CronJob-1"><a href="#CronJob-1" class="headerlink" title="CronJob"></a>CronJob</h1><p>CronJob 其實就是定時執行的 job，但在使用上有一些特性是需要了解的：</p><ul><li><p>一般 CronJob 會在指定的時間執行一次指定的 Job</p></li><li><p>但也有可能兩個 job 被同時產生出來並執行，這種事情不太常發生，但必須還是要考慮到，因此 job 的執行工作要能夠確保是 <code>idempotent</code>(就像是設計 Ansible playbook 一樣，無論執行多少次工作，都會得到相同的結果)</p></li><li><p>CronJob controller 會檢查從上次產生 job 之後到現在，有多少次 job 被遺漏執行，有漏掉就會補上，但如果這個數目超過 100，CronJob 就不會執行工作CronJob 並回報錯誤</p></li><li><p>若有設定 <code>startingDeadlineSeconds</code>(假設是 200)，那 CronJob 檢查有多少次 job 被遺漏執行時，就只會檢查最近 200 秒所漏掉的 job，而不是上述的狀況</p></li><li><p>若 CronJob 無法在指定時間產生 job pod 來執行工作，就會被判定為 <code>missed</code> (例如: <code>concurrencyPolicy</code> 設定為 <code>Forbid</code>，但要產生新 job pod 時還有上一個 job pod 正在執行)</p></li></ul><p>最後補上關於 startingDeadlineSeconds 的說明，若是設定 CronJob 在 <strong>08:30:00</strong> 執行 &amp; 設定 <code>startingDeadlineSeconds</code> 為 600，但又因為某個原因讓 CronJob controller 在 <strong>08:29:00</strong> ~ <strong>08:42:00</strong> 這一段時間無法運作；在這樣的情況下，會超過 dealine 的設定，因此這個 CronJob 就不會被啟動。</p><p>以上述的情況來說，延長 <code>startingDeadlineSeconds</code> 或許是個比較好的處理方式，畢竟晚一點執行總比完全不執行來的好。</p><h1 id="關於-TTL-Controller"><a href="#關於-TTL-Controller" class="headerlink" title="關於 TTL Controller"></a>關於 TTL Controller</h1><p>k8s 可以讓使用者執行很多一次性的工作，但執行完成後，即使過程中的 log 或是 resource object 本身都已經不再需要了，它們都還是會一直留在 k8s 上，於是就會看到很多狀態為 Complete(or Failed) 的 resource object，久而久之為了環境不要過度雜亂，可能一段時間就要清理一次，不論是手動 or 寫 script 完成，但其實這是一件挺麻煩的事情。</p><p>而在 v1.12 之後，k8s 新增了 TTL controller 這個新功能，目的就是<strong>透過限制已經執行完成的 resource object 的生命周期</strong>，藉此來達成自動清理的目的；這個設計其實是針對多種不同的 resource object，但在 v1.12 中，僅支援 Job 而已，未來可能或擴充支援到 Pod or Custom Resource。</p><blockquote><p>TTL 在 v1.12 版本中仍是 alpha 而已，且必須透過啟用 <code>TTLAfterFinished</code> feature gate 才可使用</p></blockquote><h2 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用?"></a>如何使用?</h2><p>使用 TTL controller 的方式就是透過在 Job 上設定 <code>.spec.ttlSecondsAfterFinished</code>，而設定的方式有以下幾種：</p><ul><li><p>設定在 Job 上完建立前</p></li><li><p>透過 kubectl edit 的方式(or apply -f)，設定在 Job 建立後</p></li><li><p>透過 <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">Admission Webhook</a> 的機制，為未來新增的 Job 建立預設的規則，因此當 Job 建立時，TTL 的設定就可以自動被帶入</p></li><li><p>透過 <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks">Admission Webhook</a> 的機制，為已經完成的 Job，根據 resource status or label 等資訊來動態加入 TTL 的設定</p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Jobs - Run to Completion - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/">TTL Controller for Finished Resources - Kubernetes</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Proxmox VE] 安裝 Ceph Luminous</title>
      <link href="/blog/Proxmox/proxmox-install-ceph/"/>
      <url>/blog/Proxmox/proxmox-install-ceph/</url>
      
        <content type="html"><![CDATA[<p>本篇文章介紹在 Proxmox 5.2.1 上安裝 Ceph Luminous 的正確流程</p><p>今天新裝了一台 PVE 5.2.1，準備要把這台也加入原本的 Ceph cluster，於是先下了以下指令：</p><blockquote><p>pveceph install –version luminous</p></blockquote><p>結果出現了下面的訊息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">W: (pve-apt-hook) !! WARNING !!</span><br><span class="line">W: (pve-apt-hook) You are attempting to remove the meta-package &#39;proxmox-ve&#39;!</span><br><span class="line">W: (pve-apt-hook)</span><br><span class="line">W: (pve-apt-hook) If you really you want to permanently remove &#39;proxmox-ve&#39; from your system, run the following command</span><br><span class="line">W: (pve-apt-hook) touch &#39;&#x2F;please-remove-proxmox-ve&#39;</span><br><span class="line">W: (pve-apt-hook) and repeat your apt-get&#x2F;apt invocation.</span><br><span class="line">W: (pve-apt-hook)</span><br><span class="line">W: (pve-apt-hook) If you are unsure why &#39;proxmox-ve&#39; would be removed, please verify</span><br><span class="line">W: (pve-apt-hook) - your APT repository settings</span><br><span class="line">W: (pve-apt-hook) - that you are using &#39;apt-get dist-upgrade&#39; or &#39;apt full-upgrade&#39; to upgrade your system</span><br><span class="line">E: Sub-process &#x2F;usr&#x2F;share&#x2F;proxmox-ve&#x2F;pve-apt-hook returned an error code (1)</span><br><span class="line">E: Failure running script &#x2F;usr&#x2F;share&#x2F;proxmox-ve&#x2F;pve-apt-hook</span><br></pre></td></tr></table></figure><p>Google 了一下發現要先加入 <code>pve-no-subscription</code> 這個 repository 才可以(不要傻傻跟著上面的訊息移除了 <code>proxmox-ve</code>，整個 pve 都會不見)，因此設定 Ceph cluster 的正確步驟應該是如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入 pve-no-subscription repository</span></span><br><span class="line">$ add-apt-repository <span class="string">&quot;deb http://download.proxmox.com/debian/pve <span class="subst">$(lsb_release -cs)</span> pve-no-subscription&quot;</span></span><br><span class="line"></span><br><span class="line">$ apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定安裝 Ceph Luminous</span></span><br><span class="line">$ pveceph install --version luminous</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 Ceph cluster 所使用的網段</span></span><br><span class="line">$ pveceph init --network 10.103.2.0/24</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 ceph monitor daemon</span></span><br><span class="line">$ pveceph createmon</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入 OSD</span></span><br><span class="line">$ pveceph createosd /dev/sdb</span><br><span class="line">$ pveceph createosd /dev/sdc</span><br><span class="line">....(根據實際的環境加入所需要的 Disk)</span><br></pre></td></tr></table></figure><p>步驟很簡單，PVE 已經幫大家完成很多事情了，以上幾個指令就可以把 Ceph Luminous 安裝完成囉!</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://forum.proxmox.com/threads/ceph-install-problem.47147/">Ceph install problem | Proxmox Support Forum</a></p></li><li><p><a href="https://pve.proxmox.com/pve-docs/chapter-sysadmin.html#sysadmin_package_repositories">Proxmox - Host System Administration</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Proxmox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Proxmox </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] DaemonSet Overview</title>
      <link href="/blog/Kubernetes/k8s-DaemonSet-Overview/"/>
      <url>/blog/Kubernetes/k8s-DaemonSet-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-DaemonSet"><a href="#What-is-DaemonSet" class="headerlink" title="What is DaemonSet ?"></a>What is DaemonSet ?</h1><p>DaemonSet 是確保在 Kubernetes 中的每一個 node 上都會有一個指定的 pod 來運行特定的工作，當有新的 node 加入到 Kubernetes cluster 後，系統會自動在那個 node 上長出相同的 DaemonSet pod，當有 node 從 Kubernetes cluster 移除後，該 node 上的 DaemonSet pod 就會自動被清除掉。</p><blockquote><p>也可以讓 DaemonSet 佈署在特定幾個 node 上，透過 scheduling 的機制</p></blockquote><p>了解以上 DaemonSet 的特色後，則會有一些比較標常見的應用例如：</p><ul><li><p>在每個 node 上運行 storage daemon，例如：ceph, glusterd</p></li><li><p>在每個 node 上運行收集 log 用的 daemon，例如：fluentd, logstash</p></li><li><p>在每個 ndoe 上運行監控用的 daemon，例如：<a href="https://github.com/prometheus/node_exporter">Prometheus Node Exporter</a>, <a href="https://collectd.org/">collectd</a>, <a href="https://www.datadoghq.com/">Datadog</a> …. 等等</p></li></ul><p>以上是一個 DaemonSet 作為單一用途的 daemon 時的範例，當然也有可能會有需要同時多個 DaemonSet 來達成某種複雜用途時的場景，這時候就可以還會包含到使用不同的 flag，或是針對不同的硬體型態有不同的 cpu &amp; memory 的資源需求。</p><h1 id="撰寫第一個-DaemonSet"><a href="#撰寫第一個-DaemonSet" class="headerlink" title="撰寫第一個 DaemonSet"></a>撰寫第一個 DaemonSet</h1><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">fluentd-elasticsearch</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">fluentd-logging</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># .spec.selector 一旦定義後就無法再變更了</span></span><br><span class="line">  <span class="comment"># 必須與 .spec.template.metadata.labels 的定義相同</span></span><br><span class="line">  <span class="comment"># 這裡可以使用 matchLabels 或是 matchExpressions(用來處理較為複雜的 label 組合)</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">fluentd-elasticsearch</span></span><br><span class="line">  <span class="comment"># 此為必要欄位，與 pod template 相同</span></span><br><span class="line">  <span class="comment"># 用來定義 DaemonSet 的內容應該要長什麼樣子</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">fluentd-elasticsearch</span></span><br><span class="line">    <span class="comment"># 由於是 DaemonSet 的關係，因此 .spec.template.spec.restartPolicy 永遠是 &quot;Always&quot;</span></span><br><span class="line">    <span class="comment"># 預設值為 &quot;Always&quot;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">fluentd-elasticsearch</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">k8s.gcr.io/fluentd-elasticsearch:1.20</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">limits:</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">200Mi</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">200Mi</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/log</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line">          <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/log</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">        <span class="attr">hostPath:</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">/var/lib/docker/containers</span></span><br></pre></td></tr></table></figure><p>有幾個地方在定義 DaemonSet 的時候需要比較注意的：</p><ul><li><p><code>.spec.selector</code> 一旦定義後就不建議再修改，若是修改可能會造成某些 pod 變成孤兒(因為 DaemonSet controller 無法管理到正確的 pod)</p></li><li><p><code>.spec.selector</code> 的定義必須與 <code>.spec.template.metadata.labels</code> 相同，否則會被 API server 拒絕套用</p></li><li><p>不可以再建立(或是透過其他 controller 建立，例如：<strong>Deployment</strong>)帶有與 DaemonSet 相同 label 組合的 pod，否則會被 DaemonSet 認為是自己所產生的 </p></li></ul><h1 id="誰負責-DaemonSet-中的-Pod-Scheduling"><a href="#誰負責-DaemonSet-中的-Pod-Scheduling" class="headerlink" title="誰負責 DaemonSet 中的 Pod Scheduling?"></a>誰負責 DaemonSet 中的 Pod Scheduling?</h1><p>這個部份在 v1.12 之後已經有所改變。</p><h2 id="DaemonSet-controller-v1-12-之前"><a href="#DaemonSet-controller-v1-12-之前" class="headerlink" title="DaemonSet controller (v1.12 之前)"></a>DaemonSet controller (v1.12 之前)</h2><p>一般來說決定 pod 要在哪個 node 上運行是 k8s scheduler 的工作，但其實 DaemonSet controller 跟 k8s schedule 之間的運作有時就是會有矛盾之處，因為：</p><ul><li><p>若是使用者為特定的 node 設定 <a href="https://kubernetes.io/docs/concepts/architecture/nodes/#manual-node-administration">unschedulable</a>，這通常就會跟 DaemonSet controller 的規則衝突</p></li><li><p>DaemonSet controller 可以在 k8s scheduler 還沒啟動前就佈署 pod，有時對於 k8s cluster bootstrap 是很有用途的</p></li></ul><h2 id="default-scheduler-v1-12-之後"><a href="#default-scheduler-v1-12-之後" class="headerlink" title="default scheduler (v1.12 之後)"></a>default scheduler (v1.12 之後)</h2><p>而為了解決上述的矛盾，在 v1.12 之後，DaemonSet pod 預設是回到由 k8s scheduler 統一來處理分派的工作，藉此避免以下狀況的發生：</p><ul><li><p>不一致的 Pod 生成行為：一般的 pod 在等待被分派前，被建立後會先進入 <code>Pending</code> 狀態，但 DaemonSet Pod 不會先進入 Pending 狀態，這樣的過程可能會讓使用者混淆</p></li><li><p>即使 <a href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/">Pod preemption</a> 被啟用(這是由 k8s scheduler 所負責處理的一個功能)，也不會被 DaemonSet controller 考慮進來</p></li></ul><p>而讓 k8s scheduler 取代 DaemonSet controller 的方法很簡單，只要移除在 DaemonSet 中的 <code>spec.nodeName</code> 的部份，並加入類似以下的 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity">Node Affinity</a> 設定即可：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">nodeAffinity:</span></span><br><span class="line">  <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">    <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">matchFields:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">metadata.name</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">        <span class="attr">values:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">target-host-name</span></span><br></pre></td></tr></table></figure><p>若是原有的 DaemonSet pod 已經有 node affinity 的設定，上面新增的設定則會會覆蓋掉原本舊有的部份。</p><p>此外，針對 DaemonSet 設定的變更的部份，若是變動的部份是 <code>spec.template</code>，DaemonSet controller 並不會理會，因此變更就會無效；但 k8s scheduler 則還是會協助此變更的進行。</p><h2 id="關於-Taints-amp-Tolerations"><a href="#關於-Taints-amp-Tolerations" class="headerlink" title="關於 Taints &amp; Tolerations"></a>關於 Taints &amp; Tolerations</h2><p>然而 scheduling 這件事情由 DaemonSet controller 回到了 k8s scheduler 身上，因此 <code>Taint</code> &amp; <code>Toleration</code> 這兩個機制就會對 DaemonSet 就顯得相對重要了，因為 <code>Noschedule</code> 的設定會影響 DaemonSet pod 的分派。</p><blockquote><p>關於 <code>Taint</code> &amp; <code>Toleration</code> 機制的說明，可以參考<a href="http://cloudnil.com/2017/06/08/Schedule-taints-tolerations/">此文章(深入kubernetes調度之Taints和Tolerations — VF的部落格)</a></p></blockquote><p>而在原本由 DaemonSet controller 負責 scheduling 的情況下，若是 node 被設定為 <strong>unschedulable</strong>，也是同樣會被忽略的，但回到 k8s scheduler 之後，就必須額外增加 <code>Toleration</code> 的設定來達到相同的效果，以下是會被自動加入到 DaemonSet pod 中的 toleration 清單：</p><table><thead><tr><th>Toleration Key</th><th align="center">Effect</th><th align="center">Version</th><th>Description</th></tr></thead><tbody><tr><td><code>node.kubernetes.io/not-ready</code></td><td align="center"><code>NoExecute</code></td><td align="center"><code>1.8+</code></td><td>若 <code>TaintBasedEvictions</code> 啟用時，pod 不會因為 node 發生問題而被驅離</td></tr><tr><td><code>node.kubernetes.io/unreachable</code></td><td align="center"><code>NoExecute</code></td><td align="center"><code>1.8+</code></td><td>若 <code>TaintBasedEvictions</code> 啟用時，pod 不會因為 node 發生問題而被驅離</td></tr><tr><td><code>node.kubernetes.io/disk-pressure</code></td><td align="center"><code>NoSchedule</code></td><td align="center"><code>1.8+</code></td><td></td></tr><tr><td><code>node.kubernetes.io/memory-pressure</code></td><td align="center"><code>NoSchedule</code></td><td align="center"><code>1.8+</code></td><td></td></tr><tr><td><code>node.kubernetes.io/unschedulable</code></td><td align="center"><code>NoSchedule</code></td><td align="center"><code>1.12+</code></td><td>node 上的 unschedulable 屬性對 DaemonSet pod 會失效</td></tr><tr><td><code>node.kubernetes.io/network-unavailable</code></td><td align="center"><code>NoSchedule</code></td><td align="center"><code>1.12+</code></td><td>node 上的 network-unavailable 屬性對 DaemonSet pod(使用 host network) 會失效</td></tr></tbody></table><h1 id="如何與-DaemonSet-Pod-溝通"><a href="#如何與-DaemonSet-Pod-溝通" class="headerlink" title="如何與 DaemonSet Pod 溝通?"></a>如何與 DaemonSet Pod 溝通?</h1><p>其實這跟一般的 pod 的運作方式沒什麼太大不同，不外乎就以下幾種：</p><h2 id="Push"><a href="#Push" class="headerlink" title="Push"></a>Push</h2><p>在這種模式中，沒有 client 存在的必要性，因為是 pod 本身被設定為自動對外回報 or 更新資訊。</p><h2 id="NodePort"><a href="#NodePort" class="headerlink" title="NodePort"></a>NodePort</h2><p>這就是一般的 host IP + port 的概念，外部的 client 可以透過 IP + port 的型式直接存取 pod</p><h2 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h2><p>可以跟 StatefulSet 一樣，建立一個 <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">Headless Service</a> 搭配 pod selector，讓特定的 domain name 在 k8s cluster 內部可以直接被解析為 pod IP</p><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>這就是一般的 service，透過 service VIP 與後面的 pod 溝通，但缺點就是無法指定存取特定的 pod</p><h1 id="更新-DaemonSet"><a href="#更新-DaemonSet" class="headerlink" title="更新 DaemonSet"></a>更新 DaemonSet</h1><p>關於更新</p><ul><li><p>若是 node label 變更了，DaemonSet 可能會在該 node 新增對應的 pod 或是移除不符合 label set 的 pod</p></li><li><p>可以直接修改 DaemonSet pod，但並不是 pod 的每一個欄位都可以修改</p></li><li><p>承上，即使 pod 被修改，下次有新的 node 加入的時候，DaemonSet 依然會用原本的 template 來產生 pod (因為 template 並沒有變動)</p></li><li><p>若是想刪除 DaemonSet 卻想保留 DaemonSet pod，可以使用 <code>kubectl</code> + <code>--cascade=false</code> 來完成</p></li><li><p>承上，若是接著有另一個帶有不同 template 卻相同 label selector 的 DaemonSet 被建立時，原有現存的 pod 不會被修改，但也不會產生新的 pod，此時就<strong>必須要手動將原有的 pod 刪除，讓新的 pod 可以自動被產生</strong></p></li><li><p>v1.6 之後對 DaemonSet 進行 <a href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/">rolling update</a></p></li></ul><h1 id="DeamonSet-的替代方案"><a href="#DeamonSet-的替代方案" class="headerlink" title="DeamonSet 的替代方案?"></a>DeamonSet 的替代方案?</h1><p>由於 DeamonSet 的用途就是在每個(or 特定的) node 上運行一個 pod，確保每個 node 都可以執行特定的工作，例如：monitoring。但這樣子的工作只有 DeamonSet 可以完成嗎? 答案當然不是。</p><p>但為什麼要使用 DeamonSet? 帶來的優點是什麼? 以下列出幾個替代方案來比較：</p><h2 id="Init-Script"><a href="#Init-Script" class="headerlink" title="Init Script"></a>Init Script</h2><p>這是直接在 node 上啟動一個 daemon process(並非 pod)，例如：<code>init</code>, <code>upstartd</code>, <code>systemd</code> … 等等，但使用 DeamonSet 可以帶來其他好處：</p><ul><li><p>可以跟其他的 application 一樣，具備有監控 &amp; 管理 daemon log 的能力</p></li><li><p>像其他 application 一樣使用相同的配置方式 &amp; 語言 (例如: pod template, kubectl)</p></li><li><p>透過在 DeamonSet pod 上設定 resource limit，將 DeamonSet pod 儘量不佔用到 application pod 的資源 </p></li></ul><h2 id="Bare-Pod"><a href="#Bare-Pod" class="headerlink" title="Bare Pod"></a>Bare Pod</h2><p>這是直接在 node 上建立一個 pod，跟 DaemonSet 是相同的。</p><p>但 DeamonSet 是自動完成這件事情，而且在發生像是 node failure 或是進行維護狀態(例如: kernel update) 的事件時，DeamonSet 也會自動的刪除 &amp; 對應的 pod。</p><h2 id="Static-Pod"><a href="#Static-Pod" class="headerlink" title="Static Pod"></a>Static Pod</h2><p>在 k8s 中提供了一個特殊的機制，稱為 <a href="https://kubernetes.io/docs/concepts/cluster-administration/static-pod/">static pod</a>，可以透過在 node 中的某個目錄寫入一個檔案後，由 kubelet 來產生對應的 pod。</p><p>但 static pod 的問題在於不受 API server 或是任何的 client 所管控，因此通常只適用在 cluster bootstrap 的時候用。</p><blockquote><p>在更後面的版本中，static pod 這個機制可能會被移除</p></blockquote><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><p>Deployment 則是跟 DeamonSet 很相似，同樣都是建立 pod，並且持續的監控這個 pod 確保持續存在。</p><p>但 Deployment 比較適合用在 Stateless 的服務(例如 web server)，而這一類的服務的重點在於確保 replica 的數量會一直維持在使用者所提供的 desired status，並非<strong>每一個 node 都要有一個特定的 pod</strong>上。</p><p>因此若是目的是要達成在每個 node 上都運行特定的 pod，DeamonSet 會是比 Deployment 更好的選擇。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet - Kubernetes</a></p></li><li><p><a href="https://jimmysong.io/kubernetes-handbook/concepts/daemonset.html">DaemonSet · Kubernetes Handbook - Kubernetes中文指南/云原生应用架构实践手册 by Jimmy Song(宋净超)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] StatefulSet Overview</title>
      <link href="/blog/Kubernetes/k8s-StatefulSets-Overview/"/>
      <url>/blog/Kubernetes/k8s-StatefulSets-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-StatefulSet"><a href="#What-is-StatefulSet" class="headerlink" title="What is StatefulSet?"></a>What is StatefulSet?</h1><p>StatefulSet 在 v1.9 版後正式支援，是在 Kubernetes 中用來建構 stateful application 的 resource(API) object。</p><p>在一般的觀念裡，container 相當合適作為 stateless application 之用(例如：api service)，但由於 stateful application 的需求眾多(例如：<a href="https://github.com/kubernetes/contrib/tree/master/statefulsets">官網範例中的 ZooKeeper &amp; Kafka 應用</a>)，因此 Kubernetes 就額外增加了一些管理維運的機制，讓 pod 也開始適合承載 stateful application。</p><p>基本上 StatefulSet 中在 pod 的管理上都是與 Deployment 相同，基於相同的 container spec 來進行；而其中的差別在於 <strong>StatefulSet controller 會為每一個 pod 產生一個固定的識別資訊，不會因為 pod reschedule 後有變動</strong>。 </p><h2 id="什麼時候需要使用-StatefulSet"><a href="#什麼時候需要使用-StatefulSet" class="headerlink" title="什麼時候需要使用 StatefulSet?"></a>什麼時候需要使用 StatefulSet?</h2><p>而到底如何研判某某 application 是否需要使用 StatefulSet 來佈署呢? 如果有符合以下條件，就需要使用 StatefulSet 來進行佈署：</p><ul><li><p>需要穩定 &amp; 唯一的網路識別 (pod reschedule 後的 pod name &amp; hostname 都不會變動)</p></li><li><p>需要穩定的 persistent storage (pod reschedule 後還是能存取到相同的資料，基本上用 PVC 就可以解決)</p></li><li><p>佈署 &amp; scale out 的時後，每個 pod 的產生都是有其順序且逐一慢慢完成的</p></li><li><p>進行更新操作時，也是與上面的需求相同</p></li></ul><h2 id="有什麼限制"><a href="#有什麼限制" class="headerlink" title="有什麼限制?"></a>有什麼限制?</h2><ul><li><p>v1.5 之前的版本不支援 StatefulSet，v1.5 ~ v1.9 之間是 beta(需額外開啟此功能)，v1.9 之後才有正式支援</p></li><li><p>storage 的部份一定要綁定 PVC，並綁定到特定的 StorageClass or 預先配置好的 PersistentVolume，確保 pod 被刪除後資料依然存在</p></li><li><p>需要額外定義一個 <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" title="Headless Service">Headless Service</a> 與 StatefulSet 搭配，確保 pod 有固定的 network identity</p></li></ul><blockquote><p>所謂的 network identity，即是可以透過 domain name 直接可以取得 pod IP；實現的方法則是佈署一個 ClusterIP=None 的 Service，讓 cluster 內部存取 service 時，可以直接連到 pod 而不是 service VIP。</p></blockquote><blockquote><p>Headless Service 詳細的設定方式可以參考<a href="https://www.do1618.com/archives/1055/k8s%E4%B9%8Bheadless-service/">此文章</a>。</p></blockquote><h2 id="以-MongoDB-作為示範"><a href="#以-MongoDB-作為示範" class="headerlink" title="以 MongoDB 作為示範"></a>以 MongoDB 作為示範</h2><p><img src="/blog/images/kubernetes/k8s-statefulset-mongodb.png" alt="ReplicaSet"></p><h1 id="如何撰寫一個-StatefulSet"><a href="#如何撰寫一個-StatefulSet" class="headerlink" title="如何撰寫一個 StatefulSet?"></a>如何撰寫一個 StatefulSet?</h1><p>要撰寫一個 StatefulSet，有幾個重要的部份必須涵蓋：</p><ol><li><p>Application &amp; Persistent Volume Claim</p></li><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" title="Headless Service">Headless Service</a></p></li><li><p><code>.spec.selector</code> 所定義的內容(<strong>matchLabels</strong>)必須與 <code>.spec.template.metadata.labels</code> 相同</p></li></ol><p>其他的部份則是跟 Deployment 幾乎是相同的。</p><h2 id="Application-amp-Persistent-Volume-Claim"><a href="#Application-amp-Persistent-Volume-Claim" class="headerlink" title="Application &amp; Persistent Volume Claim"></a>Application &amp; Persistent Volume Claim</h2><p>首先是關於 Application &amp; Persistent Volume Claim 的定義：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># v1.9 版本之前必須使用 &quot;apps/v1beta2&quot;</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="comment"># 必須與 &quot;.spec.template.metadata.labels&quot; 相同</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">serviceName:</span> <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="comment"># 必須與 &quot;.spec.selector.matchLabels&quot; 相同</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">10</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">k8s.gcr.io/nginx-slim:0.8</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">        <span class="comment"># 指定將 pvc 掛載到特定的目錄上</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">  <span class="comment"># 使用 persistent volume 來確保資料不會因為 pod reschedule 而消失</span></span><br><span class="line">  <span class="comment"># 以下是使用 volumeClaimTemplates + StorageClass 來完成</span></span><br><span class="line">  <span class="attr">volumeClaimTemplates:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">accessModes:</span> [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">      <span class="comment"># 在本實驗環境中已經架設好一個 GlusterFS cluster + Heketi，並設定好 StorageClass</span></span><br><span class="line">      <span class="attr">storageClassName:</span> <span class="string">&quot;my-gfs-storageclass&quot;</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure><blockquote><p>關於 GlusterFS 的設定，可參考文章 <a href="https://godleon.github.io/blog/Kubernetes/k8s-use-GlusterFS-with-standalone-Heketi/">[Kubernetes] 快速安裝 GlusterFS + Heketi 並與 StorageClass 搭配使用</a> 進行安裝設定</p></blockquote><p>套用上面的設定後，接著檢視一下在 k8s 中到底生成了哪些 resource object：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得在目前 namespace 中所有的 resource object</span></span><br><span class="line">$ kubectl get all -o wide</span><br><span class="line">NAME        READY     STATUS    RESTARTS   AGE       IP              NODE              NOMINATED NODE</span><br><span class="line">pod/web-0   1/1       Running   0          13h       10.233.64.159   leon-k8s-node03   &lt;none&gt;</span><br><span class="line">pod/web-1   1/1       Running   0          13h       10.233.65.148   leon-k8s-node04   &lt;none&gt;</span><br><span class="line">pod/web-2   1/1       Running   0          13h       10.233.66.155   leon-k8s-node05   &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME                                  TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE       SELECTOR</span><br><span class="line">service/glusterfs-dynamic-www-web-0   ClusterIP   10.233.60.3   &lt;none&gt;        1/TCP     13h       &lt;none&gt;</span><br><span class="line">service/glusterfs-dynamic-www-web-1   ClusterIP   10.233.17.8   &lt;none&gt;        1/TCP     13h       &lt;none&gt;</span><br><span class="line">service/glusterfs-dynamic-www-web-2   ClusterIP   10.233.16.6   &lt;none&gt;        1/TCP     13h       &lt;none&gt;</span><br><span class="line">service/kubernetes                    ClusterIP   10.233.0.1    &lt;none&gt;        443/TCP   22d       &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME                   DESIRED   CURRENT   AGE       CONTAINERS   IMAGES</span><br><span class="line">statefulset.apps/web   3         3         13h       nginx        k8s.gcr.io/nginx-slim:0.8</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得自動生成的 persistent volume claim 資訊</span></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">www-web-0   Bound     pvc-fdda23de-c155-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   13h</span><br><span class="line">www-web-1   Bound     pvc-0b8f49d6-c156-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   13h</span><br><span class="line">www-web-2   Bound     pvc-1d15ce12-c156-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   13h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得自動生成的 persistent volume 資訊(透過 StorageClass 產生)</span></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM               STORAGECLASS          REASON    AGE</span><br><span class="line">pvc-0b8f49d6-c156-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-1   my-gfs-storageclass             13h</span><br><span class="line">pvc-1d15ce12-c156-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-2   my-gfs-storageclass             13h</span><br><span class="line">pvc-fdda23de-c155-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-0   my-gfs-storageclass             13h</span><br></pre></td></tr></table></figure><p>從上面可看出，Pod &amp; Service 這兩個部份的 resource object name，都會帶有一個有順序性的 index，而這個 index 不會因為 pod reschedule 而改變，而且還會將原本的 PVC 掛載回來。(<strong>透過 Service -&gt; PVC 的路徑</strong>)</p><h2 id="Headless-Service"><a href="#Headless-Service" class="headerlink" title="Headless Service"></a>Headless Service</h2><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" title="Headless Service">Headless Service</a> 則是要定義一個 <code>ClusterIP: None</code> 的 service，目的就是讓 cluster 內部的 pod 可以 <strong>透過 DNS 找到 StatefulSet pod IP</strong>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure><p>將以上設定套用到 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得 StatefulSet 的 pod &amp; service 資訊</span></span><br><span class="line">$ kubectl get all -o wide</span><br><span class="line">NAME        READY     STATUS    RESTARTS   AGE       IP              NODE              NOMINATED NODE</span><br><span class="line">pod/web-0   1/1       Running   0          13h       10.233.64.159   leon-k8s-node03   &lt;none&gt;</span><br><span class="line">pod/web-1   1/1       Running   0          13h       10.233.65.148   leon-k8s-node04   &lt;none&gt;</span><br><span class="line">pod/web-2   1/1       Running   0          13h       10.233.66.155   leon-k8s-node05   &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME                                  TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE       SELECTOR</span><br><span class="line">...(略)</span><br><span class="line">service/nginx                         ClusterIP   None          &lt;none&gt;        80/TCP    20m       app=nginx</span><br><span class="line">...(略)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 k8s DNS 的 ip address</span></span><br><span class="line">$ kubectl get svc -n kube-system | grep dns</span><br><span class="line">coredns                ClusterIP   10.233.0.3      &lt;none&gt;        53/UDP,53/TCP   22d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 k8s DNS server 對於 headless service 回應的 dns 資訊</span></span><br><span class="line"><span class="comment"># 從外面檢視需要輸入完整的 domain name(目前在 default)，若是在 pod 中就不需要</span></span><br><span class="line">$ nslookup</span><br><span class="line">&gt; server 10.233.0.3</span><br><span class="line">Default server: 10.233.0.3</span><br><span class="line">Address: 10.233.0.3<span class="comment">#53</span></span><br><span class="line">&gt; nginx.default.svc.cluster.local</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.64.159</span><br><span class="line">Name:    nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.65.148</span><br><span class="line">Name:    nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.66.155</span><br></pre></td></tr></table></figure><p>從上面的訊息可以看出上面所定義 nginx service(<strong>nginx.default.svc.cluster.local</strong>) 所對應到的 IP 都是 pod 本身的 IP。</p><h1 id="如何識別-StatefulSet-產生的-Pod"><a href="#如何識別-StatefulSet-產生的-Pod" class="headerlink" title="如何識別 StatefulSet 產生的 Pod?"></a>如何識別 StatefulSet 產生的 Pod?</h1><p>完成了上面的操作測試後，接著回頭來了解到底在 k8s 中是如何識別 StatefulSet 中的 pod &amp; service。</p><p>每一個 StatefulSet Pod 都有一個獨一無二的識別資訊，但這件事情在 k8s 中是如何被達成的? 其實是分別由以下三種資訊所組成：</p><ul><li><p>表示順序的索引值 (Ordinal Index)</p></li><li><p>穩定的網路識別資訊 (Stable Network ID)</p></li><li><p>穩定的儲存空間 (Stable Storage)</p></li></ul><h2 id="Ordinal-Index"><a href="#Ordinal-Index" class="headerlink" title="Ordinal Index"></a>Ordinal Index</h2><p>若一個 statefulset 包含了 N 個 replica，那每一個 pod 都會被分配到一個獨一無二的索引，從 <code>0</code> ~ <code>N-1</code>，即使 pod reschedule 也不會改變。</p><h2 id="Stable-Network-ID"><a href="#Stable-Network-ID" class="headerlink" title="Stable Network ID"></a>Stable Network ID</h2><p>每個在 statefulset 中的 pod 都會有自己獨一無二的 hostname，命名的規則為 <code>$(statefulset name)-$(ordinal index)</code>，因此在上面的例子中，3 個 pod 的 hostname 就會分別為 <strong>web-0</strong>、<strong>web-1</strong>、<strong>web-2</strong>。</p><p>此外，statefulset 還會透過 <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" title="Headless Service">Headless Service</a> 來維持 pod domain name 是固定指到 pod IP，並使用以下的標準格式存取 domain name：(以下稱為 <strong>governing service domain</strong>)</p><blockquote><p><strong>$(service name).$(namespace).svc.cluster.local</strong></p></blockquote><p>其中 <code>cluster.local</code> 是當初安裝 k8s 所設定的 cluster domain，若安裝時有修改的話，上面的 domain name 也必須跟著調整。</p><p>因此存取每一個 pod 的完整 domain name 如下：</p><blockquote><p><strong>$(podname).$(governing service domain)</strong></p></blockquote><p>以下是一些範例說明：(假設 cluster domain 為 <code>cluster.local</code>)</p><table><thead><tr><th>Service (ns/name)</th><th>StatefulSet (ns/name)</th><th>StatefulSet Domain</th><th>Pod Hostname</th><th>Pod DNS</th></tr></thead><tbody><tr><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td></tr><tr><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td></tr></tbody></table><p>以下透過範例來證實：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得 k8s DNS 的 ip address</span></span><br><span class="line">$ kubectl get svc -n kube-system | grep dns</span><br><span class="line">coredns                ClusterIP   10.233.0.3      &lt;none&gt;        53/UDP,53/TCP   22d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試上述的 dns entry 可否回應正確的 ip address</span></span><br><span class="line">$ nslookup</span><br><span class="line">&gt; server 10.233.0.3</span><br><span class="line">Default server: 10.233.0.3</span><br><span class="line">Address: 10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># $(service name).$(namespace).svc.cluster.local</span></span><br><span class="line">&gt; nginx.default.svc.cluster.local</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.64.159</span><br><span class="line">Name:    nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.65.148</span><br><span class="line">Name:    nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.66.155</span><br><span class="line"></span><br><span class="line"><span class="comment"># $(podname).$(governing service domain)</span></span><br><span class="line"><span class="comment"># $(podname).$(service name).$(namespace).svc.cluster.local</span></span><br><span class="line">&gt; www-0.nginx.default.svc.cluster.local</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    web-0.nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.64.159</span><br><span class="line"></span><br><span class="line"><span class="comment"># $(podname).$(governing service domain)</span></span><br><span class="line"><span class="comment"># $(podname).$(service name).$(namespace).svc.cluster.local</span></span><br><span class="line">&gt; web-1.nginx.default.svc.cluster.local</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    web-1.nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.65.148</span><br><span class="line"></span><br><span class="line"><span class="comment"># $(podname).$(governing service domain)</span></span><br><span class="line"><span class="comment"># $(podname).$(service name).$(namespace).svc.cluster.local</span></span><br><span class="line">&gt; web-2.nginx.default.svc.cluster.local</span><br><span class="line">Server:        10.233.0.3</span><br><span class="line">Address:    10.233.0.3<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:    web-2.nginx.default.svc.cluster.local</span><br><span class="line">Address: 10.233.66.155</span><br></pre></td></tr></table></figure><h2 id="Stable-Storage"><a href="#Stable-Storage" class="headerlink" title="Stable Storage"></a>Stable Storage</h2><p>若是 statefulset 中的 replicas 設定為大於 1，為了確保每個 pod 在產生時都會有各自對應的 persistent storage 可用，在 Storage 的部份就要以 <code>volumeClaimTemplates</code> + <code>StorageClass</code> 來設定，使用以下範例說明：(來自於上方完整範例)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 宣告 volumeClaimTemplates</span></span><br><span class="line"><span class="attr">volumeClaimTemplates:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">  <span class="attr">spec:</span></span><br><span class="line">    <span class="attr">accessModes:</span> [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">    <span class="comment"># 指定搭配的 StorageClass</span></span><br><span class="line">    <span class="attr">storageClassName:</span> <span class="string">&quot;my-gfs-storageclass&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="comment"># 容量需求為 1GB</span></span><br><span class="line">        <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure><p>透過以上的設定，statefulset 中<strong>每個 pod</strong> 副本產生時，k8s 會自動執行以下工作：</p><ol><li><p>StorageClass “<strong>my-gfs-storageclass</strong>“ 會負責對特定的 storage 要求 1GB 的空間</p></li><li><p>StorageClass “<strong>my-gfs-storageclass</strong>“ 動態產生 persistent volume(PV)，並與上面的空間綁定</p></li><li><p>透過 volumeClaimTemplates 為每個 pod 產生一個 persistent volume claim(PVC)，並與步驟 2 的 PV 綁定</p></li><li><p>pod 使用 PVC 並掛載到指定的目錄上</p></li></ol><blockquote><p>透過 StorageClass，可以根據 resource 的需求，產生 persistent volume 並與特定的 storage 綁定 </p></blockquote><p>PV 不會因為 pod 被刪除 or reschedule 而消失(只能手動刪除)，如此才能達成 statefulset 對 storage 的要求。</p><p>接著來檢視一下實際的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 PVC</span></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">www-web-0   Bound     pvc-fdda23de-c155-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   13h</span><br><span class="line">www-web-1   Bound     pvc-0b8f49d6-c156-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   13h</span><br><span class="line">www-web-2   Bound     pvc-1d15ce12-c156-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   13h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 PV</span></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM               STORAGECLASS          REASON    AGE</span><br><span class="line">pvc-0b8f49d6-c156-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-1   my-gfs-storageclass             13h</span><br><span class="line">pvc-1d15ce12-c156-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-2   my-gfs-storageclass             13h</span><br><span class="line">pvc-fdda23de-c155-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-0   my-gfs-storageclass             13h</span><br></pre></td></tr></table></figure><h2 id="Pod-Name-Label"><a href="#Pod-Name-Label" class="headerlink" title="Pod Name Label"></a>Pod Name Label</h2><p>statefulset 產生每個 pod 時，都會自動幫 pod 加上名稱為 <code>statefulset.kubernetes.io/pod-name</code> 的 label，而 label value 就是上面提到的 pod name。</p><p>看看實際操作產生出來的結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod/web-0</span><br><span class="line">Name:               web-0</span><br><span class="line">Namespace:          default</span><br><span class="line">...(略)</span><br><span class="line">Labels:             app=nginx</span><br><span class="line">                    controller-revision-hash=web-6596ffb49b</span><br><span class="line">                    statefulset.kubernetes.io/pod-name=web-0</span><br><span class="line">...(略)</span><br></pre></td></tr></table></figure><p>透過這個特別的 label，就可以讓使用者根據需求，額外定義 service 並附加到指定的 pod 上(透過 label selector)。</p><h1 id="Deployment-amp-Scaling-流程說明"><a href="#Deployment-amp-Scaling-流程說明" class="headerlink" title="Deployment &amp; Scaling 流程說明"></a>Deployment &amp; Scaling 流程說明</h1><h2 id="佈署-amp-Scale-out"><a href="#佈署-amp-Scale-out" class="headerlink" title="佈署 &amp; Scale out"></a>佈署 &amp; Scale out</h2><p>當佈署一個 replica 數量為 N(大於 1) 的 StatefulSet 時，不會像 Deployment 一樣，pod 會同步的產生，而是會有順序的逐一產生，整個流程發生的過程如下：</p><ul><li><p>佈署的順序會是 **{0 –&gt; N-1}**，以上面的例子來說則是 web-0 &gt;&gt; web-1 &gt;&gt; web-2 (可想而知，web-2 不會在 web-0 &amp; web-1 都 Ready 之前就開始產生)</p></li><li><p>當要對 pod 進行 scale 時，predecessor 的狀態必須是 Running &amp; Ready</p><blockquote><p>例如當一個 pod(web-0) 要 scale out 成 2 個 pod(web-0 + web-1) 時，web-0 的狀態一定要是 Running &amp; Ready 才會開始</p></blockquote></li></ul><p>當 StatefulSet 進行 scale out 時，整個過程也會遵守上面的規則。</p><h2 id="刪除-amp-Scale-in"><a href="#刪除-amp-Scale-in" class="headerlink" title="刪除 &amp; Scale in"></a>刪除 &amp; Scale in</h2><p>但若是要刪除 pod，或是進行 scale in 的時候，整個流程發生的過程如下：</p><ul><li><p>以反向 <strong>{N-1 –&gt; 0}</strong> 的順序逐一刪除，以上面的例子來說則是 web-2 &gt;&gt; web-1 &gt;&gt; web-0 (可想而知，web-1 不會在 web-2 完成刪除之前就被刪除)</p></li><li><p>當要終止一個 pod 時，所有的 successor 都必須完成 shutdown 才行</p><blockquote><p>例如要終止三份 replica(web-0 + web-1 + web-2) 中的 web-1 時，web-2 必須要完全終止才行</p></blockquote></li></ul><p>此外，若是把 <code>pod.Spec.TerminationGracePeriodSeconds</code> 設定為 <strong>0</strong>，pod 的刪除就會強制進行，而不會一個一個慢慢來。(不建議這麼做)</p><h2 id="Pod-Management-Policy"><a href="#Pod-Management-Policy" class="headerlink" title="Pod Management Policy"></a>Pod Management Policy</h2><p>上述所說的是 StatefulSet 的預設行為，但在 v1.7 版之後，就可以透過修改 <code>.spec.podManagementPolicy</code> 欄位來改變佈署 &amp; scale 的行為：</p><ul><li><p><strong>OrderedReady</strong>：此為預設值</p></li><li><p><strong>Parallel</strong>：整體的行為會跟 Deployment 相同，同時產生(or 刪除) pod，不考慮順序問題</p></li></ul><h1 id="更新-update-要如何進行"><a href="#更新-update-要如何進行" class="headerlink" title="更新(update)要如何進行?"></a>更新(update)要如何進行?</h1><p>預設情況下，k8s 自有一套進行 update 的準則(預設為 rolling update)，而 update 的過程會把 pod 中的 container, label, resource request/limit, annotation … 等資訊進行變更；而在 v1.7 版之後，就可以透過 <code>.spec.updateStrategy</code> 的設定，停止上面那些自動化的行為。</p><p>目前 <code>.spec.updateStrategy</code> 支援兩種設定：</p><h2 id="On-Delete"><a href="#On-Delete" class="headerlink" title="On Delete"></a>On Delete</h2><p>這其實是 v1.6 版之前的預設行為，只是在 v1.7 版後實作成 OnDelete；當設定 <code>.spec.updateStrategy.type: &quot;OnDelete&quot;</code> 時，對於 sprc template 的變更不會有任何反應，除非使用者手動刪除 pod，讓 replication controller 重新產生 pod，才會套用新的 spec template 設定。</p><h2 id="Rolling-Update"><a href="#Rolling-Update" class="headerlink" title="Rolling Update"></a>Rolling Update</h2><p>這就屬於預設行為(<code>.spec.updateStrategy.type: &quot;RollingUpdate&quot;</code>)，當 spec template 發生變更時，舊的 pod 會逐一的刪除並逐一產生新的 pod。(依然是會有順序性的)</p><h3 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h3><p>若是在 StatefulSet 中有 5 個 replica，但只想要更新其中兩個怎麼辦? 在 k8s 中就提供了 <strong>partition</strong> 的機制來完成這件事情，透過設定 <code>.spec.updateStrategy.rollingUpdate.partition</code> 為一個特定的整數(int)值，當 spec template 變更時，index 大於(or 等於)此整數值的 pod 就會被更新，而小於此整數值的 pod 就不會被更新。</p><p>以上面的例子為例，假設有 5 個 replica，分別為：</p><ul><li>web-0</li><li>web-1</li><li>web-2</li><li>web-3</li><li>web-4</li></ul><p>而 <code>.spec.updateStrategy.rollingUpdate.partition</code> 設定為 <strong>3</strong>，當 spec template 變更時，就只會有以下的 pod 會變更新：</p><ul><li>web-3</li><li>web-4</li></ul><p>因此，若是有<strong>分階段更新/發佈</strong> or <strong>canary deployment</strong> 的需求時，就可以透過 partition 的功能來完成。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/cn/docs/tutorials/stateful-application/basic-stateful-set/">StatefulSet基本使用 - Kubernetes</a></p></li><li><p><a href="https://jimmysong.io/kubernetes-handbook/concepts/statefulset.html">StatefulSet · Kubernetes Handbook - Kubernetes中文指南/云原生应用架构实践手册 by Jimmy Song(宋净超)</a></p></li><li><p><a href="http://docs.kubernetes.org.cn/443.html">Kubernetes StatefulSets _ Kubernetes(K8S)中文文档_Kubernetes中文社区</a></p></li><li><p><a href="https://www.do1618.com/archives/1055/k8s%E4%B9%8Bheadless-service/">k8s之Headless Service - 程序印象</a></p></li><li><p><a href="https://netapp.io/2017/04/07/deploy-containerized-mongodb-kubernetes-netapp/">How to Deploy Containerized MongoDB on Kubernetes, with NetApp - thePub</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] 快速安裝 GlusterFS + Heketi 並與 StorageClass 搭配使用</title>
      <link href="/blog/Kubernetes/k8s-use-GlusterFS-with-standalone-Heketi/"/>
      <url>/blog/Kubernetes/k8s-use-GlusterFS-with-standalone-Heketi/</url>
      
        <content type="html"><![CDATA[<h1 id="緣由"><a href="#緣由" class="headerlink" title="緣由"></a>緣由</h1><p>由於最近在花些時間研究 k8s，但一直研究到 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSet</a> 時，發現對於 StatefulSet 來說，<strong>persistet volume 是必備的</strong>，而且若是有設定 replica 的部份，就演變成 <strong>動態產生 persistent volume 是必備</strong> 的。</p><p>動態產生 PV 這個需求的確是存在的，試想如果有很多不同的 workload or job 都在 k8s 上運行，但同時都需要保存資料時，每次都請 storage manager 開好 PV 是很麻煩的，因此把這個部份的自動化的需求就產生了；而動態產生 <strong>PV</strong>(persistent volume) 這件工作，在 k8s 是由 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Class</a> 這個機制來完成；於是仔細看了一下 provisioner list，決定使用 GlusterFS 來作為 persistent storage 的測試對象。</p><blockquote><p>選擇 GlusterFS 的原因是因為同時支援 ReadWriteOnce/ReadOnlyMany/ReadWriteMany 三種 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes">Access Mode</a>，同時也支援 Quota &amp; Snapshot 等功能</p></blockquote><h1 id="What-is-GlusterFS"><a href="#What-is-GlusterFS" class="headerlink" title="What is GlusterFS?"></a>What is GlusterFS?</h1><p>這個部份就不多著墨，詳細的內容可以參考以下網站：</p><ul><li><a href="http://www.l-penguin.idv.tw/book/Gluster-Storage_GitBook/gluster_intra/gluster_arch.html">Gluster 架構 | Gluster Storage System</a></li></ul><p>上面有提到關於一些 GlsuterFS 重要的概念，例如：<strong>Peer</strong>, <strong>Brick</strong>, <strong>Volume</strong> …. 等等。</p><p>若對 GlusterFS 的運作細節跟優化很有興趣，可以參考<a href="https://docs.gluster.org/en/latest/">官方的文件說明</a>。</p><h1 id="What-is-Heketi"><a href="#What-is-Heketi" class="headerlink" title="What is Heketi?"></a>What is Heketi?</h1><p>首先看看以下官網的說明：</p><blockquote><p>Heketi provides a RESTful management interface which can be used to manage the life cycle of GlusterFS volumes. With Heketi, cloud services like OpenStack Manila, Kubernetes, and OpenShift can dynamically provision GlusterFS volumes with any of the supported durability types. Heketi will automatically determine the location for bricks across the cluster, making sure to place bricks and its replicas across different failure domains. Heketi also supports any number of GlusterFS clusters, allowing cloud services to provide network file storage without being limited to a single GlusterFS cluster.</p></blockquote><p>首先關於 GlusterFS，必須知道以下幾件事情：</p><ul><li><p>GlusterFS 並沒有提供管理用的 REST API service</p></li><li><p>GlusterFS 並沒有提供 Failure Domain 的設計</p></li></ul><blockquote><p>所謂一個 failure domain，就是指一群接上同樣的 switch &amp; power suppoly 的 node</p></blockquote><p>而以上兩個在 GlusterFS 中沒有的設計，Heketi 則是在更上層的抽象設計中，把這兩個功能補足了! 因此細部檢視 Heketi 的功能，就包含了：</p><ul><li><p>提供 REST API service 作為管理 GlusterFS 之用</p></li><li><p>可以讓 OpenStack Manila, Kubernetes, OpenShift 等平台動態產生 volume 並掛載使用</p></li><li><p>可同時管理多個 GlusterFS cluster，可讓使用者把資料放到不同的 GlusterFS cluster 中</p></li><li><p>可根據需求，自動化的在分配 brick 時，分散到不同的 failure domain，提供資料可用性</p></li></ul><blockquote><p>brick 分散到不同的 failure domain = 將資料分散在接上不同 switch &amp; power supply 的 node 上</p></blockquote><h1 id="環境需求"><a href="#環境需求" class="headerlink" title="環境需求"></a>環境需求</h1><h2 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h2><p>假設已經有一個正常運行的 Kubernetes</p><h2 id="GlusterFS"><a href="#GlusterFS" class="headerlink" title="GlusterFS"></a>GlusterFS</h2><p>GlusterFS 的環境安裝過程中，需要 2 個 node 來進行安裝，規格如下：</p><ul><li><p>OS: <code>Ubuntu 18.04 (Bionic)</code></p></li><li><p>CPU &amp; RAM: 由於只是測試環境….因此這邊只給 4 vcore + 4GB RAM</p></li><li><p>Disk: <code>3 (1 for OS, 2 for Data)</code> (<strong>/dev/vda</strong>, <strong>/dev/vdb</strong>, <strong>/dev/vdc</strong>)</p></li></ul><blockquote><p>以上的 node 建議需要設定成搭配 SSH pribate key 進行 passwordless 登入</p></blockquote><h2 id="Heketi"><a href="#Heketi" class="headerlink" title="Heketi"></a>Heketi</h2><p>以下安裝過程中，會將 Heketi 安裝在 GlusterFS cluster 的第一個 node 上，單獨以 docker container 的方式運行</p><h2 id="安裝用機器-Bootstrapper"><a href="#安裝用機器-Bootstrapper" class="headerlink" title="安裝用機器(Bootstrapper)"></a>安裝用機器(Bootstrapper)</h2><p>這台機器可以是實體機 or VM，使用的環境必須為 <code>Ubuntu 18.04</code> or <code>Ubuntu 16.04</code></p><h1 id="安裝事前準備"><a href="#安裝事前準備" class="headerlink" title="安裝事前準備"></a>安裝事前準備</h1><h2 id="取得安裝程式"><a href="#取得安裝程式" class="headerlink" title="取得安裝程式"></a>取得安裝程式</h2><p>首先登入到 bootstrapper，輸入以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update &amp;&amp; sudo apt-get -y install git</span><br><span class="line">$ <span class="built_in">cd</span> /tmp</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/godleon/k8s-lab.git</span><br><span class="line">$ <span class="built_in">cd</span> /tmp/k8s-lab/GlusterFS-with-standalone-Heketi/ansible</span><br></pre></td></tr></table></figure><h2 id="修改安裝參數檔"><a href="#修改安裝參數檔" class="headerlink" title="修改安裝參數檔"></a>修改安裝參數檔</h2><p>以下的安裝參數檔請按照自己的環境進行修改：</p><h3 id="pwd-inventory"><a href="#pwd-inventory" class="headerlink" title="$(pwd)/inventory"></a>$(pwd)/inventory</h3><p>此檔案包含了用來安裝 GlusterFS 的兩個 node 的名稱 &amp; IP 資訊</p><h3 id="pwd-group-vars-all"><a href="#pwd-group-vars-all" class="headerlink" title="$(pwd)/group_vars/all"></a>$(pwd)/group_vars/all</h3><p>這裡包含三種資訊：</p><ol><li><p>每個 node 的 <strong>SSH 連線資訊</strong>(因為安裝程式是使用 Ansible 所開發，因此正確的 SSH 連線資訊是必備的)</p></li><li><p>GlusterFS 的<strong>版本</strong> (預設為 <code>4.1</code>)</p></li><li><p>每個 node 上用來儲存資料的 <strong>device name</strong> (也可以是 <strong>partition name</strong>)</p></li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Login information of GlusterFS nodes</span></span><br><span class="line"><span class="attr">ansible_python_interpreter:</span> <span class="string">/usr/bin/python3</span></span><br><span class="line"><span class="attr">ansible_user:</span> <span class="string">&quot;ubuntu&quot;</span></span><br><span class="line"><span class="attr">ansible_become_user:</span> <span class="string">root</span></span><br><span class="line"><span class="attr">ansible_ssh_private_key_file:</span> <span class="string">/ansible/ssh-privkey/id_rsa</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># GlusterFS version</span></span><br><span class="line"><span class="attr">glusterfs_version:</span> <span class="string">&quot;4.1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># disk list for storing data (not OS disk)</span></span><br><span class="line"><span class="attr">storage_devs:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">&quot;/dev/vdb&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">&quot;/dev/vdc&quot;</span></span><br></pre></td></tr></table></figure><h2 id="準備-SSH-Private-Key"><a href="#準備-SSH-Private-Key" class="headerlink" title="準備 SSH Private Key"></a>準備 SSH Private Key</h2><p>將用來登入 GlusterFS node 用的 ssh private key 放到 <code>$(pwd)/ssh-privkey</code> 目錄中，並命名為 <strong>id_rsa</strong>。</p><h1 id="安裝程式做了那些事情"><a href="#安裝程式做了那些事情" class="headerlink" title="安裝程式做了那些事情?"></a>安裝程式做了那些事情?</h1><p>在開始安裝之前，先說明一下整個安裝程式到底完成了那些事情：</p><ol><li><p>在 2 個乾淨的 node 上安裝 GlusterFS，並設定 peer(這是一個 GlusterFS cluster node 之間相互認識的一個過程)</p></li><li><p>在第一個 node 上安裝 docker</p></li><li><p>透過 docker 啟動 Heketi service container</p></li><li><p>匯入 cluster topology 資訊到 Heketi service</p><blockquote><p>topology 資訊包含 hostname, IP, 用來儲存 data 的 device(or partition name), failure domain … 等資訊</p></blockquote></li><li><p>取得新建立的 cluster ID 並顯示</p></li></ol><p>有了最後一個步驟取得的 cluster ID 後，才可以用來設定 k8s StorageClass for GlusterFS。</p><h1 id="開始安裝"><a href="#開始安裝" class="headerlink" title="開始安裝"></a>開始安裝</h1><p>執行以下命令開始安裝：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /tmp/k8s-lab/GlusterFS-with-standalone-Heketi</span><br><span class="line">$ ./start.sh</span><br></pre></td></tr></table></figure><p>接著安裝程式會開始執行，大概需要 3~5 分鐘完成所有的安裝流程，而安裝完成後會出現類似以下訊息：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">....(略)</span><br><span class="line">TASK [Display topology import result] ******************************************</span><br><span class="line">Wednesday 26 September 2018  05:49:15 +0000 (0:00:10.832)       0:02:01.148 *** </span><br><span class="line">skipping: [gfs02]</span><br><span class="line">ok: [gfs01] =&gt; &#123;</span><br><span class="line">    &quot;import_result.stdout_lines&quot;: [</span><br><span class="line">        &quot;Creating cluster ... ID: 6190e1bfdb33d611af35361d7f72625b&quot;, </span><br><span class="line">        &quot;\tAllowing file volumes on cluster.&quot;, </span><br><span class="line">        &quot;\tAllowing block volumes on cluster.&quot;, </span><br><span class="line">        &quot;\tCreating node 10.107.13.101 ... ID: 18db1bc118707ed70e26213b484068c2&quot;, </span><br><span class="line">        &quot;\t\tAdding device /dev/vdb ... OK&quot;, </span><br><span class="line">        &quot;\t\tAdding device /dev/vdc ... OK&quot;, </span><br><span class="line">        &quot;\tCreating node 10.107.13.102 ... ID: 900110afd1dff7c89c8fd7ef427f4337&quot;, </span><br><span class="line">        &quot;\t\tAdding device /dev/vdb ... OK&quot;, </span><br><span class="line">        &quot;\t\tAdding device /dev/vdc ... OK&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">....(略)</span><br><span class="line">TASK [Display Cluster ID] ******************************************************</span><br><span class="line">Wednesday 26 September 2018  05:49:16 +0000 (0:00:00.116)       0:02:01.970 *** </span><br><span class="line">skipping: [gfs02]</span><br><span class="line">ok: [gfs01] =&gt; &#123;</span><br><span class="line">    &quot;msg&quot;: &quot;Cluster ID = 6190e1bfdb33d611af35361d7f72625b&quot;</span><br><span class="line">&#125;</span><br><span class="line">....(略)</span><br></pre></td></tr></table></figure><p>從上面可以看到 topology 資訊匯入到 Heketi 所產生的回應訊息 &amp; Cluster ID，其中 <strong>Cluster ID</strong>(上面 Cluster ID = <code>6190e1bfdb33d611af35361d7f72625b</code>) 會在下一個步驟中使用到。</p><h1 id="Kubernetes-Cluster-設定"><a href="#Kubernetes-Cluster-設定" class="headerlink" title="Kubernetes Cluster 設定"></a>Kubernetes Cluster 設定</h1><p>由於要掛載 GlusterFS volume，因此在<strong>每個 worker node</strong> 上都必須要準備好 GlusterFS client 才行，由於實驗的 k8s 環境是安裝在 Ubuntu 上，因此可以透過以下指令安裝 GlusterFS client：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo add-apt-repository ppa:gluster/glusterfs-4.1 -y</span><br><span class="line">$ sudo apt-get -y install glusterfs-client</span><br></pre></td></tr></table></figure><p>若是 worker node 是 CentOS，可用下面指令安裝 GlusterFS client：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum -y install centos-release-gluster41</span><br><span class="line">$ yum install -y glusterfs-fuse</span><br></pre></td></tr></table></figure><p>這個部份若是漏掉了，會出現 <strong>mount: unknown filesystem type ‘glusterfs’</strong> 錯誤，在 pod 生成的時候會出現類似下面的訊息：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">the following error information was pulled from the glusterfs log to help diagnose this issue: could not open log file for pod web-0</span><br><span class="line">  Warning  FailedMount  1m  kubelet, leon-k8s-node03  MountVolume.SetUp failed for volume &quot;pvc-6f3abcc5-bfec-11e8-885a-627e9087949f&quot; : mount failed: mount failed: exit status 32</span><br><span class="line">Mounting command: systemd-run</span><br><span class="line">Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/6f3b9642-bfec-11e8-885a-627e9087949f/volumes/kubernetes.io~glusterfs/pvc-6f3abcc5-bfec-11e8-885a-627e9087949f --scope -- mount -t glusterfs -o auto_unmount,log-level=ERROR,log-file=/var/lib/kubelet/plugins/kubernetes.io/glusterfs/pvc-6f3abcc5-bfec-11e8-885a-627e9087949f/web-0-glusterfs.log,backup-volfile-servers=10.107.13.101:10.107.13.102:10.107.13.103 10.107.13.101:vol_e3f5e3f4faf604ab10014c9d1a274624 /var/lib/kubelet/pods/6f3b9642-bfec-11e8-885a-627e9087949f/volumes/kubernetes.io~glusterfs/pvc-6f3abcc5-bfec-11e8-885a-627e9087949f</span><br><span class="line">Output: Running scope as unit run-ra3dac606d12c48fc9cdf4667bf5a9963.scope.</span><br><span class="line">mount: unknown filesystem type &#x27;glusterfs&#x27;</span><br></pre></td></tr></table></figure><h1 id="測試-Kubernetes-與-Heketi-的連結"><a href="#測試-Kubernetes-與-Heketi-的連結" class="headerlink" title="測試 Kubernetes 與 Heketi 的連結"></a>測試 Kubernetes 與 Heketi 的連結</h1><p>在上面有提到 Heketi service 將會裝在第一個 node 上(<strong>gfs01</strong>, IP=<strong>10.107.13.101</strong>)，以 docker container 的方式提供服務；而且為了安裝方便，有些參數已經寫死在參數檔中(例如：admin password)，因此這邊可以彙整出以下資訊：</p><ul><li><p>GlusterFS cluster ID: <code>6190e1bfdb33d611af35361d7f72625b</code></p></li><li><p>Heketi service URL: <code>http://10.107.13.101:8080</code></p></li><li><p>admin password: <code>admin_key</code></p></li></ul><p>有了以上的資訊後，就可以往下進行與 k8s 的整合測試。</p><h2 id="建立-Storage-Class"><a href="#建立-Storage-Class" class="headerlink" title="建立 Storage Class"></a>建立 Storage Class</h2><p>在安裝程式中，預設 admin 的密碼為 <code>admin_key</code>，在 k8s 中可以透過 <a href="https://kubernetes.io/docs/concepts/configuration/secret/">secret</a> 的方式帶入：</p><ul><li>檔名：<code>gfs-storageclass.yml</code></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">heketi-secret</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="comment"># echo -n &quot;admin_key&quot; | base64</span></span><br><span class="line">  <span class="attr">key:</span> <span class="string">YWRtaW5fa2V5</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">kubernetes.io/glusterfs</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;my-gfs-storageclass&quot;</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="comment"># 設定為預設的 storage class</span></span><br><span class="line">    <span class="attr">storageclass.kubernetes.io/is-default-class:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/glusterfs</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">resturl:</span> <span class="string">&quot;http://10.107.13.101:8080&quot;</span></span><br><span class="line">  <span class="attr">clusterid:</span> <span class="string">&quot;6190e1bfdb33d611af35361d7f72625b&quot;</span></span><br><span class="line">  <span class="attr">restauthenabled:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">restuser:</span> <span class="string">&quot;admin&quot;</span></span><br><span class="line">  <span class="attr">secretNamespace:</span> <span class="string">&quot;default&quot;</span></span><br><span class="line">  <span class="attr">secretName:</span> <span class="string">&quot;heketi-secret&quot;</span></span><br><span class="line">  <span class="attr">gidMin:</span> <span class="string">&quot;40000&quot;</span></span><br><span class="line">  <span class="attr">gidMax:</span> <span class="string">&quot;50000&quot;</span></span><br><span class="line">  <span class="attr">volumetype:</span> <span class="string">&quot;replicate:2&quot;</span></span><br><span class="line"><span class="attr">allowVolumeExpansion:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f gfs-storageclass.yml</span><br><span class="line">secret/heketi-secret created</span><br><span class="line">storageclass.storage.k8s.io/my-gfs-storageclass created</span><br></pre></td></tr></table></figure><h2 id="建立-StatefulSet"><a href="#建立-StatefulSet" class="headerlink" title="建立 StatefulSet"></a>建立 StatefulSet</h2><p>這邊使用 StatefulSet 搭配 VolumeClaimTemplate 對 <code>my-gfs-storageclass</code> 進行 dynamic volume provisioning：</p><ul><li>檔名: <code>statefulset.yml</code></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">serviceName:</span> <span class="string">&quot;nginx&quot;</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">10</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">k8s.gcr.io/nginx-slim:0.8</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">  <span class="attr">volumeClaimTemplates:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">accessModes:</span> [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">      <span class="attr">storageClassName:</span> <span class="string">&quot;my-gfs-storageclass&quot;</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f nginx.yml </span><br><span class="line">statefulset.apps/web created</span><br></pre></td></tr></table></figure><p>完成之後，可以看到 PV &amp; PVC 自動被建立，並 bind 到每個 pod 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得 pv 資訊</span></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM               STORAGECLASS          REASON    AGE</span><br><span class="line">pvc-0b8f49d6-c156-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-1   my-gfs-storageclass             1m</span><br><span class="line">pvc-1d15ce12-c156-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-2   my-gfs-storageclass             44s</span><br><span class="line">pvc-fdda23de-c155-11e8-885a-627e9087949f   1Gi        RWO            Delete           Bound     default/www-web-0   my-gfs-storageclass             1m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 pvc 資訊</span></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">www-web-0   Bound     pvc-fdda23de-c155-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   2m</span><br><span class="line">www-web-1   Bound     pvc-0b8f49d6-c156-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   1m</span><br><span class="line">www-web-2   Bound     pvc-1d15ce12-c156-11e8-885a-627e9087949f   1Gi        RWO            my-gfs-storageclass   1m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 剛剛佈署的 StatefulSet 也都完成佈署了</span></span><br><span class="line">$ kubectl get all</span><br><span class="line">NAME        READY     STATUS    RESTARTS   AGE</span><br><span class="line">pod/web-0   1/1       Running   0          2m</span><br><span class="line">pod/web-1   1/1       Running   0          2m</span><br><span class="line">pod/web-2   1/1       Running   0          1m</span><br><span class="line"></span><br><span class="line">NAME                                  TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/glusterfs-dynamic-www-web-0   ClusterIP   10.233.60.3   &lt;none&gt;        1/TCP     2m</span><br><span class="line">service/glusterfs-dynamic-www-web-1   ClusterIP   10.233.17.8   &lt;none&gt;        1/TCP     1m</span><br><span class="line">service/glusterfs-dynamic-www-web-2   ClusterIP   10.233.16.6   &lt;none&gt;        1/TCP     1m</span><br><span class="line">service/kubernetes                    ClusterIP   10.233.0.1    &lt;none&gt;        443/TCP   21d</span><br><span class="line"></span><br><span class="line">NAME                   DESIRED   CURRENT   AGE</span><br><span class="line">statefulset.apps/web   3         3         2m</span><br></pre></td></tr></table></figure><p>看到以上訊息，就表示 <strong>Kubernetes &lt;–&gt; Heketi &lt;–&gt; GlusterFS</strong> 三個不同服務的串接已經正確的設定完成。</p><h1 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h1><p>若是單獨使用 GlusterFS volume 的情況下，完整的流程應該分成兩大部份：</p><ul><li><p>安裝 GlusterFS</p><ul><li>建立 peer information</li><li>check peer status &amp; pool list</li></ul></li><li><p>設定 volume</p><ul><li>新增 volume (設定 replica &amp; 指定使用哪個 node 的 brick)</li><li>掛載 volume 並使用</li></ul></li></ul><p>而 Heketi 把第二個部份的工作都自動化了，因此在設定 Heketi 時，只要匯入所謂的 topology 資訊(每一次匯入則是新增一個 cluster)，告訴 Heketi 以下資訊：</p><ul><li><p>這個 cluster 有哪些 node</p></li><li><p>每個 node 可用來儲存 data 的 device(or partition) name</p></li><li><p>failure domain (這個尚不清楚可以達成什麼效果，但應該跟 data redundacy policy 有關係)</p></li></ul><p>接著，產生 brick &amp; volume 這種煩瑣的工作，就由 Heketi 來負責自動完成。</p><p>然而 Heketi 除了以上功能外，還可以同時管理多個 GlusterFS cluster，因此可以讓使用者在資料配置上不受限於單一 cluster，在規劃 &amp; 運用上也更有彈性；不但可以減輕 storage manager 的負擔，同時也藉由與 Kubernetes 的搭配，讓實際應用程式的開發者可以更專注在應用上，而非複雜的 infra 設定上。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://www.l-penguin.idv.tw/book/Gluster-Storage_GitBook/">Gluster Storage System</a></p></li><li><p><a href="https://github.com/heketi/heketi">heketi/heketi: RESTful based volume management framework for GlusterFS</a></p></li><li><p><a href="https://github.com/heketi/heketi/blob/master/docs/admin/readme.md">Heketi Documentation - Administration</a></p></li><li><p><a href="https://github.com/heketi/heketi/blob/master/docs/admin/topology.md">Prepare Heketi Topology</a></p></li><li><p><a href="https://www.cnblogs.com/breezey/p/8849466.html">独立部署GlusterFS+Heketi实现Kubernetes共享存储 - breezey - 博客园</a></p></li><li><p><a href="http://jamyy.us.to/blog/2014/03/6220.html">GlusterFS 操作備忘 « Jamyy’s Weblog</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Storage </tag>
            
            <tag> CKA Storage </tag>
            
            <tag> GlusterFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Deployment Overview</title>
      <link href="/blog/Kubernetes/k8s-Deployment-Overview/"/>
      <url>/blog/Kubernetes/k8s-Deployment-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Deployment"><a href="#What-is-Deployment" class="headerlink" title="What is Deployment ?"></a>What is Deployment ?</h1><p><img src="/blog/images/kubernetes/k8s-deployment.png" alt="Deployment"></p><p>Deployment 為 pod &amp; replicaset 提供了一個宣告式的設定 &amp; 更新方式，透過定義 desired status，Deployment controller 會在所謂的 <strong>controlled rate</strong> 下達到使用者所期望的狀態，這些機制是由 k8s 自動化完成，因此官方建議應該透過 Deployment 來佈署 pod &amp; replicaset。</p><h1 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h1><p>了解什麼是 Deployment 後，接下來的問題是…….什麼情況下可以使用 Deployment 來解決問題?</p><p>官方文件中列出了不少 use case，來回答此問題的答案：</p><ul><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">Create a Deployment to rollout a ReplicaSet</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">Declare the new state of the Pods</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment">Rollback to an earlier Deployment revision</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment">Scale up the Deployment to facilitate more load</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pausing-and-resuming-a-deployment">Pause the Deployment</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#deployment-status">Use the status of the Deployment</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#clean-up-policy">Clean up older ReplicaSets</a></p></li></ul><p>以下針對上面每個使用案例使用一個範例來說明。</p><h1 id="建立-Deployment"><a href="#建立-Deployment" class="headerlink" title="建立 Deployment"></a>建立 Deployment</h1><p>以下是搭配 ReplicaSet 來產生 3 個 nginx pod：(假設檔名為 <code>nginx-deployment.yaml</code>)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="comment"># deployment metadata</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># deployment name</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 同時建立 3 個 nginx 的 pod </span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># replicaset 的效果套用在帶有 app=nginx 的 pod 上</span></span><br><span class="line">  <span class="comment"># 必須要與下面的 pod label 有相符合</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="comment"># .spec.template 其實就是 pod 的定義</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="comment"># pod metadata</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="comment"># 設定給 pod 的 label 資訊</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="comment"># 可看出這個 pod 只運行了一個 nginx container</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>使用以下指令佈署 deployment：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 透過 &quot;--record&quot; 參數來保留後續的 revision history</span></span><br><span class="line">$ kubectl apply -f nginx-deployment.yaml --record</span><br></pre></td></tr></table></figure><p>接著透過 kubectl 來查詢剛剛佈署的 deployment 相關資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NAME： 列出了在目前 namespace 中的 deployment 清單</span></span><br><span class="line"><span class="comment"># DESIRED： 使用者所宣告的 desired status</span></span><br><span class="line"><span class="comment"># CURRENT： 表示目前有多少個 pod 副本在運行</span></span><br><span class="line"><span class="comment"># UP-TO-DATE： 表示目前有多個個 pod 副本已經達到 desired status</span></span><br><span class="line"><span class="comment"># AVAILABLE： 表示目前有多個 pod 副本已經可以開始提供服務</span></span><br><span class="line"><span class="comment"># AGE： 顯示目前 pod 運行的時間</span></span><br><span class="line">$ kubectl get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         3         3            0           15s</span><br></pre></td></tr></table></figure><p>如果要即時監控 deployment 佈署的狀況，可以使用以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout status deployment/nginx-deployment</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 0 of 3 updated replicas are available...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 1 of 3 updated replicas are available...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 2 of 3 updated replicas are available...</span><br><span class="line">deployment <span class="string">&quot;nginx-deployment&quot;</span> successfully rolled out</span><br></pre></td></tr></table></figure><p>完成之後，AVAILABLE 的數字就會與 DESIRED 相同了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         3         3            3           32s</span><br></pre></td></tr></table></figure><p>接著繼續往下看關於 ReplicaSet 的細節：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ReplicaSet 的名稱會是 [DEPLOYMENT-NAME]-[POD-TEMPLATE-HASH-VALUE]</span></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-67594d6bf6   3         3         3         9m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 ReplicaSet 的細節</span></span><br><span class="line">$ kubectl describe rs/nginx-deployment-67594d6bf6</span><br><span class="line">Name:           nginx-deployment-67594d6bf6</span><br><span class="line">Namespace:      default</span><br><span class="line"><span class="comment"># deployment controller 主動為 replicaset 新增了一個 &quot;pod-template-hash&quot; label</span></span><br><span class="line">Selector:       app=nginx,pod-template-hash=2315082692</span><br><span class="line">Labels:         app=nginx</span><br><span class="line">                pod-template-hash=2315082692</span><br><span class="line">Annotations:    deployment.kubernetes.io/desired-replicas=3</span><br><span class="line">                deployment.kubernetes.io/max-replicas=4</span><br><span class="line">                deployment.kubernetes.io/revision=1</span><br><span class="line">....(略)</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  app=nginx</span><br><span class="line">           pod-template-hash=2315082692</span><br><span class="line">....(略)</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age   From                   Message</span><br><span class="line">  ----    ------            ----  ----                   -------</span><br><span class="line">  Normal  SuccessfulCreate  18m   replicaset-controller  Created pod: nginx-deployment-67594d6bf6-22nrx</span><br><span class="line">  Normal  SuccessfulCreate  18m   replicaset-controller  Created pod: nginx-deployment-67594d6bf6-rkztl</span><br><span class="line">  Normal  SuccessfulCreate  18m   replicaset-controller  Created pod: nginx-deployment-67594d6bf6-ccx87</span><br></pre></td></tr></table></figure><p>透過 Deployment 建立的 ReplicaSet，名稱都會是 **[DEPLOYMENT-NAME]-[POD-TEMPLATE-HASH-VALUE]**，因此透過檢視 ReplicaSet name，其實很容易就可以知道這是由那一個 Deployment 所建立出來的。</p><p>接著再往下看 Pod 的細節：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># deployment controller 同時也幫 pod 增加了一個 &quot;pod-template-hash&quot; label，hash value 也是相同的</span></span><br><span class="line">$ kubectl get pod --show-labels</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE       LABELS</span><br><span class="line">nginx-deployment-67594d6bf6-22nrx   1/1       Running   0          12s       app=nginx,pod-template-hash=2315082692</span><br><span class="line">nginx-deployment-67594d6bf6-ccx87   1/1       Running   0          12s       app=nginx,pod-template-hash=2315082692</span><br><span class="line">nginx-deployment-67594d6bf6-rkztl   1/1       Running   0          12s       app=nginx,pod-template-hash=2315082692</span><br></pre></td></tr></table></figure><p>跟 ReplicaSet 相同，Pod 的命名則會是 <strong>[DEPLOYMENT-NAME]-[POD-TEMPLATE-HASH-VALUE]-[UNIQUE-ID]**，而且會發現被多加了一個名稱為 **pod-template-hash</strong> 的 label</p><h2 id="Pod-Template-Hash-Label"><a href="#Pod-Template-Hash-Label" class="headerlink" title="Pod-Template-Hash Label"></a>Pod-Template-Hash Label</h2><p>關於 <strong>pod-template-hash</strong> label，有幾點：</p><ul><li><p>是由 deployment controller 加到每個 replicaset 的，主要是透過將 template 拿來計算 hash value 的方式來確保每個 replicaset 中的 template 定義不會有重複，</p></li><li><p>pod-template-hash label 同時會被 deployment controller 加到 replicaset &amp; pod 中，利用此 hash value 就可以確認那些 deploymen/replicaset/pod 是屬於同一組</p></li><li><p>replicaset 名稱的 hash value 跟 pod-template-hash 不一樣，但應該是不同的 hash 方法所導致(因為透過相同的 YAML 檔案重新建立 deployment 也還是會得到相同的結果)</p></li></ul><h2 id="如果在另外一個-namespace-中建立相同的-Deployment"><a href="#如果在另外一個-namespace-中建立相同的-Deployment" class="headerlink" title="如果在另外一個 namespace 中建立相同的 Deployment ?"></a>如果在另外一個 namespace 中建立相同的 Deployment ?</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立一個新的 namespace，名稱為 deployment-test</span></span><br><span class="line">$ kubectl create namespace deployment-test</span><br><span class="line">namespace/deployment-test created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將同樣的 deployment 宣告佈署到 deployment-test namespace 中</span></span><br><span class="line">$ kubectl -n deployment-test apply -f nginx-deployment.yaml </span><br><span class="line">deployment.apps/nginx-deployment created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新的 deployment 佈署完成</span></span><br><span class="line">$ kubectl -n deployment-test get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         3         3            3           16s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 replicaset 的狀態</span></span><br><span class="line">$ kubectl -n deployment-test get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-67594d6bf6   3         3         3         21s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 pod 的狀態(包含 label 資訊)</span></span><br><span class="line">$ kubectl -n deployment-test get pod --show-labels</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE       LABELS</span><br><span class="line">nginx-deployment-67594d6bf6-l2jg4   1/1       Running   0          29s       app=nginx,pod-template-hash=2315082692</span><br><span class="line">nginx-deployment-67594d6bf6-phhkg   1/1       Running   0          29s       app=nginx,pod-template-hash=2315082692</span><br><span class="line">nginx-deployment-67594d6bf6-vqptg   1/1       Running   0          29s       app=nginx,pod-template-hash=2315082692</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成實驗，移除 namespace</span></span><br><span class="line">$ kubectl delete ns/deployment-test</span><br></pre></td></tr></table></figure><p>從上面不難看出，即使在新的 namespace，同樣的 deployment 宣告依然會拿到同樣 value 的 pod-template-hash，原因就是因為 <code>.spec.template</code> 並沒有改變，因此透過 hash 產生出來的 value 當然也不會改變。</p><h1 id="更新-Deployment"><a href="#更新-Deployment" class="headerlink" title="更新 Deployment"></a>更新 Deployment</h1><p>剛使用者需要更新 deployment 內容時，就會考慮到這個問題，但更新的發生可能來自以下兩種變更：</p><ol><li><p>修改 replica 的數量 (<code>.spec.replicas</code>)</p></li><li><p>修改 template 的內容 (<code>.spec.template</code>)</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將 container image 的版本從上面的 nginx:1.7.9 改成 nginx:1.9.1</span></span><br><span class="line">$ kubectl <span class="built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.9.1 --record</span><br><span class="line">deployment.extensions/nginx-deployment image updated</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 rollout status</span></span><br><span class="line">$ kubectl rollout status deployment/nginx-deployment</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 1 old replicas are pending termination...</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 1 old replicas are pending termination...</span><br><span class="line">deployment <span class="string">&quot;nginx-deployment&quot;</span> successfully rolled out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新的變更已經成功套用</span></span><br><span class="line">$ kubectl get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         3         3            3           1d</span><br></pre></td></tr></table></figure><blockquote><p>也可以透過 <code>kubectl edit deployment/nginx-deployment</code> 指令直接對 YAML 檔案進行修改。</p></blockquote><p>透過修改 replica 數量不會讓 <strong>pod-template-hash</strong> 有變動，但是修改 template 的內容就會造成 pod-template-hash 的變動了，由於上方已經變更了 container image version，因此來檢查一下目前 replicaset &amp; pod 的狀態資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 replicaset 的狀態</span></span><br><span class="line"><span class="comment"># 可以發現 pod-template-hash 已經從 67594d6bf6 變為 6fdbb596db</span></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-67594d6bf6   0         0         0         1d</span><br><span class="line">nginx-deployment-6fdbb596db   3         3         3         9m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 replicaset 的細節</span></span><br><span class="line"><span class="comment"># pod-template-hash label 的值已經從 2315082692 變為 2986615286</span></span><br><span class="line">$ kubectl describe rs/nginx-deployment-6fdbb596db</span><br><span class="line">Name:           nginx-deployment-6fdbb596db</span><br><span class="line">Namespace:      default</span><br><span class="line">Selector:       app=nginx,pod-template-hash=2986615286</span><br><span class="line">Labels:         app=nginx</span><br><span class="line">                pod-template-hash=2986615286</span><br><span class="line">Annotations:    deployment.kubernetes.io/desired-replicas=3</span><br><span class="line">                deployment.kubernetes.io/max-replicas=4</span><br><span class="line">                deployment.kubernetes.io/revision=2</span><br><span class="line">Controlled By:  Deployment/nginx-deployment</span><br><span class="line">Replicas:       3 current / 3 desired</span><br><span class="line">Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  app=nginx</span><br><span class="line">           pod-template-hash=2986615286</span><br><span class="line">  Containers:</span><br><span class="line">   nginx:</span><br><span class="line">    Image:        nginx:1.9.1</span><br><span class="line">    Port:         80/TCP</span><br><span class="line">    Host Port:    0/TCP</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:       &lt;none&gt;</span><br><span class="line">  Volumes:        &lt;none&gt;</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age   From                   Message</span><br><span class="line">  ----    ------            ----  ----                   -------</span><br><span class="line">  Normal  SuccessfulCreate  9m    replicaset-controller  Created pod: nginx-deployment-6fdbb596db-67cwx</span><br><span class="line">  Normal  SuccessfulCreate  8m    replicaset-controller  Created pod: nginx-deployment-6fdbb596db-r9qmv</span><br><span class="line">  Normal  SuccessfulCreate  8m    replicaset-controller  Created pod: nginx-deployment-6fdbb596db-27dmh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 pod 的狀態</span></span><br><span class="line"><span class="comment"># pod 的名稱也會跟著上面一起改變</span></span><br><span class="line">$ kubectl get pod --show-labels</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE       LABELS</span><br><span class="line">nginx-deployment-6fdbb596db-27dmh   1/1       Running   0          8m        app=nginx,pod-template-hash=2986615286</span><br><span class="line">nginx-deployment-6fdbb596db-67cwx   1/1       Running   0          9m        app=nginx,pod-template-hash=2986615286</span><br><span class="line">nginx-deployment-6fdbb596db-r9qmv   1/1       Running   0          9m        app=nginx,pod-template-hash=2986615286</span><br></pre></td></tr></table></figure><p>從上面的結果可以看出，當 Deployment 中的 <code>.spec.template</code> 改變後，replicaset &amp; pod 中的 <strong>pod-template-hash</strong> 也會一併跟著改變。</p><p>此外，關於 update deployment 這件事情，k8s 在運作機制上會遵守以下原則：</p><ul><li><p>至少會確保有 25% 以上的期望 pod 數量是維持可服務狀態的</p></li><li><p>最多不會讓超過 25% 的 pod 數量無法使用 (在 3 replicas 的條件下，create 會先執行，然後才是 delete)</p></li></ul><p>檢視一下 deployment 的細節就可以看出詳細的運作流程：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe deployment/nginx-deployment</span><br><span class="line">Name:                   nginx-deployment</span><br><span class="line">.... (略)</span><br><span class="line">RollingUpdateStrategy:  25% max unavailable, 25% max surge</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  app=nginx</span><br><span class="line">  Containers:</span><br><span class="line">   nginx:</span><br><span class="line">    Image:        nginx:1.9.1</span><br><span class="line">    Port:         80/TCP</span><br><span class="line">.... (略)</span><br><span class="line">OldReplicaSets:  &lt;none&gt;</span><br><span class="line">NewReplicaSet:   nginx-deployment-6fdbb596db (3/3 replicas created)</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason             Age   From                   Message</span><br><span class="line">  ----    ------             ----  ----                   -------</span><br><span class="line">  <span class="comment"># available pod = 4(new 1, old 3)</span></span><br><span class="line">  Normal  ScalingReplicaSet  20m   deployment-controller  Scaled up replica <span class="built_in">set</span> nginx-deployment-6fdbb596db to 1</span><br><span class="line">  <span class="comment"># available pod = 3(new 1, old 2)</span></span><br><span class="line">  Normal  ScalingReplicaSet  19m   deployment-controller  Scaled down replica <span class="built_in">set</span> nginx-deployment-67594d6bf6 to 2</span><br><span class="line">  <span class="comment"># available pod = 4(new 2, old 2)</span></span><br><span class="line">  Normal  ScalingReplicaSet  19m   deployment-controller  Scaled up replica <span class="built_in">set</span> nginx-deployment-6fdbb596db to 2</span><br><span class="line">  <span class="comment"># available pod = 3(new 2, old 1)</span></span><br><span class="line">  Normal  ScalingReplicaSet  19m   deployment-controller  Scaled down replica <span class="built_in">set</span> nginx-deployment-67594d6bf6 to 1</span><br><span class="line">  <span class="comment"># available pod = 3(new 3, old 1)</span></span><br><span class="line">  Normal  ScalingReplicaSet  19m   deployment-controller  Scaled up replica <span class="built_in">set</span> nginx-deployment-6fdbb596db to 3</span><br><span class="line">  <span class="comment"># available pod = 3(new 3, old 0)</span></span><br><span class="line">  Normal  ScalingReplicaSet  19m   deployment-controller  Scaled down replica <span class="built_in">set</span> nginx-deployment-67594d6bf6 to 0</span><br></pre></td></tr></table></figure><p>以上狀況是在原本的 deployment 已經完成佈署的狀態下，變更 template 後會發生的行為，那如下發生以下狀況：</p><blockquote><p>佈署 5 個副本 + nginx:1.7.9 的 deployment，但當只有 3 個 pod 佈署完成時，馬上將 template 中的 container image version 修改為 nginx:1.9.1</p></blockquote><p>以上面的例子來說，k8s 就不會遵守原本的更新規則，而是會直接砍掉原本的 3 個 pod，然後開始建立新版本的 5 個 pod。</p><p>最後，有個需要注意的是 Label Selector 的部份，在 API version <strong>apps/v1</strong> 之後，<code>.spec.selector</code> 被設定之後就無法再修改了!</p><h1 id="Deployment-版本回溯"><a href="#Deployment-版本回溯" class="headerlink" title="Deployment 版本回溯"></a>Deployment 版本回溯</h1><p>在預設情況下，系統中會包含 Deployment rollout 的歷史資訊(也就是 <code>.spec.template</code> 有變動時)，因此使用者可以在 在 revision history limit 的範圍內，回復到任何一個時間點的狀態。</p><blockquote><p>由於只有當 <code>.spec.template</code> 變更時才會有 revision 記錄，因此改變 replica 數量就不會產生新的 revision 記錄</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將 container image 從 nginx:1.9.1 更新為 nginx:1.91</span></span><br><span class="line">$ kubectl <span class="built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.91</span><br><span class="line">deployment.extensions/nginx-deployment image updated</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 rollout status</span></span><br><span class="line"><span class="comment"># 因為 container image version error，所以無法正確完成工作，因此更新會卡住，按下 Ctrl+C 跳離</span></span><br><span class="line">$ kubectl rollout status deployments nginx-deployment</span><br><span class="line">Waiting <span class="keyword">for</span> deployment <span class="string">&quot;nginx-deployment&quot;</span> rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class="line">^C</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 replicaset 狀態</span></span><br><span class="line"><span class="comment"># 新建的 replicaset 一直在無法完成工作的狀態</span></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-58c7645486   1         1         0         4m</span><br><span class="line">nginx-deployment-67594d6bf6   0         0         0         2d</span><br><span class="line">nginx-deployment-6fdbb596db   3         3         3         23h</span><br><span class="line"></span><br><span class="line"><span class="comment"># pod 則是明確的顯示遇到 image pull 的問題，時間一久就會開始進入 back off 的狀態</span></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                READY     STATUS             RESTARTS   AGE</span><br><span class="line">nginx-deployment-58c7645486-xrgn8   0/1       ImagePullBackOff   0          4m</span><br><span class="line">nginx-deployment-6fdbb596db-27dmh   1/1       Running            0          23h</span><br><span class="line">nginx-deployment-6fdbb596db-67cwx   1/1       Running            0          23h</span><br><span class="line">nginx-deployment-6fdbb596db-r9qmv   1/1       Running            0          23h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 deployment 的細節</span></span><br><span class="line">$ kubectl describe deployment/nginx-deployment</span><br><span class="line">Name:                   nginx-deployment</span><br><span class="line">Namespace:              default</span><br><span class="line">CreationTimestamp:      Mon, 10 Sep 2018 20:13:20 +0000</span><br><span class="line">Labels:                 app=nginx</span><br><span class="line"><span class="comment"># 從 annotation 可以看出目前 deployment status 是由什麼指令造成的</span></span><br><span class="line">Annotations:            deployment.kubernetes.io/revision=3</span><br><span class="line">                        kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">&quot;apiVersion&quot;</span>:<span class="string">&quot;apps/v1&quot;</span>,<span class="string">&quot;kind&quot;</span>:<span class="string">&quot;Deployment&quot;</span>,<span class="string">&quot;metadata&quot;</span>:&#123;<span class="string">&quot;annotations&quot;</span>:&#123;<span class="string">&quot;kubernetes.io/change-cause&quot;</span>:<span class="string">&quot;kubectl apply --filename=nginx-deployment.yaml --r...</span></span><br><span class="line"><span class="string">                        kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.91 --record=true</span></span><br><span class="line"><span class="string">Selector:               app=nginx</span></span><br><span class="line"><span class="string">Replicas:               3 desired | 1 updated | 4 total | 3 available | 1 unavailable</span></span><br><span class="line"><span class="string">....(略)</span></span><br><span class="line"><span class="string">OldReplicaSets:  nginx-deployment-6fdbb596db (3/3 replicas created)</span></span><br><span class="line"><span class="string">NewReplicaSet:   nginx-deployment-58c7645486 (1/1 replicas created)</span></span><br><span class="line"><span class="string">Events:</span></span><br><span class="line"><span class="string">  Type    Reason             Age   From                   Message</span></span><br><span class="line"><span class="string">  ----    ------             ----  ----                   -------</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  1m    deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 3</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  1m    deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 1</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  1m    deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 2</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  1m    deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 2</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  59s   deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 1</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  59s   deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 3</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  48s   deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 0</span></span><br><span class="line"><span class="string">  Normal  ScalingReplicaSet  16s   deployment-controller  Scaled up replica set nginx-deployment-58c7645486 to 1</span></span><br></pre></td></tr></table></figure><p>接著來看一下 rollout history：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 透過 rollout history 指令可以看出曾經下過什麼指令</span></span><br><span class="line">$ kubectl rollout <span class="built_in">history</span> deployment/nginx-deployment</span><br><span class="line">deployments <span class="string">&quot;nginx-deployment&quot;</span></span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         kubectl apply --filename=nginx-deployment.yaml --record=<span class="literal">true</span></span><br><span class="line">2         kubectl <span class="built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.9.1 --record=<span class="literal">true</span></span><br><span class="line">3         kubectl <span class="built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.91 --record=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 rollout hsitory 的細節</span></span><br><span class="line">$ kubectl rollout <span class="built_in">history</span> deployment/nginx-deployment --revision=2</span><br><span class="line">deployments <span class="string">&quot;nginx-deployment&quot;</span> with revision <span class="comment">#2</span></span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:    app=nginx</span><br><span class="line">    pod-template-hash=2986615286</span><br><span class="line">  Annotations:    kubernetes.io/change-cause=kubectl <span class="built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.9.1 --record=<span class="literal">true</span></span><br><span class="line">  Containers:</span><br><span class="line">   nginx:</span><br><span class="line">    Image:    nginx:1.9.1</span><br><span class="line">....(略)</span><br></pre></td></tr></table></figure><blockquote><p>請注意! 在目前的版本 1.11.2 中，如果指令沒有加上 <code>--record</code> 參數，就不會在這邊看到指令的細節(其實這些指令是存放在 deployment 的 annotation 中)</p></blockquote><p>這時候我們知道 revision 2 這個版本是正常的，因此我們可以透過以下指令將 deployment 進行 rollback：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指令效果相同於 &quot;kubectl rollout undo deployment/nginx-deployment --to-revision=2&quot;</span></span><br><span class="line">$ kubectl rollout undo deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新檢視 deployment status =&gt; 正常</span></span><br><span class="line">$ kubectl get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         3         3            3           19m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新檢視 replicaset status =&gt; 正常</span></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-58c7645486   0         0         0         17m</span><br><span class="line">nginx-deployment-67594d6bf6   0         0         0         19m</span><br><span class="line">nginx-deployment-6fdbb596db   3         3         3         18m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新檢視 pod status =&gt; 正常</span></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-6fdbb596db-7zqm6   1/1       Running   0          18m</span><br><span class="line">nginx-deployment-6fdbb596db-kcl8k   1/1       Running   0          18m</span><br><span class="line">nginx-deployment-6fdbb596db-rljdk   1/1       Running   0          18m</span><br></pre></td></tr></table></figure><h1 id="Scale-out-in-Deployment"><a href="#Scale-out-in-Deployment" class="headerlink" title="Scale out/in Deployment"></a>Scale out/in Deployment</h1><h2 id="Scale-out"><a href="#Scale-out" class="headerlink" title="Scale out"></a>Scale out</h2><p>在 k8s 中設定 scale out 相當簡單，以下語法就可以完成：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定目前的 pod repplica 為 10</span></span><br><span class="line">$ kubectl scale deployment/nginx-deployment --replicas=10</span><br><span class="line">deployment.extensions/nginx-deployment scaled</span><br><span class="line"></span><br><span class="line"><span class="comment"># 10 個 pod 順利產生</span></span><br><span class="line">$ kubectl get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   10        10        10           10          23h</span><br></pre></td></tr></table></figure><h2 id="設定-Horizontal-Pod-Autoscaling"><a href="#設定-Horizontal-Pod-Autoscaling" class="headerlink" title="設定 Horizontal Pod Autoscaling"></a>設定 Horizontal Pod Autoscaling</h2><p>如果希望 pod replica 可以根據某個 resource 指標進行自動伸縮，則可以使用 <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">Horizontal Pod Autoscaling</a> 的功能：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl autoscale deployment nginx-deployment --min&#x3D;10 --max&#x3D;20 --cpu-percent&#x3D;80</span><br><span class="line">horizontalpodautoscaler.autoscaling&#x2F;nginx-deployment autoscaled</span><br><span class="line"></span><br><span class="line">$ kubectl get hpa</span><br><span class="line">NAME               REFERENCE                     TARGETS         MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">nginx-deployment   Deployment&#x2F;nginx-deployment   &lt;unknown&gt;&#x2F;80%   10        20        0          5s</span><br></pre></td></tr></table></figure><h2 id="Proportional-Scaling"><a href="#Proportional-Scaling" class="headerlink" title="Proportional Scaling"></a>Proportional Scaling</h2><p>當變更 deployment template 時，其實 k8s 是允許多個版本同時存在的(會嚴格遵守 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-surge">maxSurge</a> &amp; [maxUnavailable] 的設定(<a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-unavailable">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#max-unavailable</a>))</p><p>開始測試之前，先來檢視一下 deployment 的細節：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe deployment/nginx-deployment</span><br><span class="line">Name:                   nginx-deployment</span><br><span class="line">.... (略)</span><br><span class="line">RollingUpdateStrategy:  25% max unavailable, 25% max surge</span><br></pre></td></tr></table></figure><p>可以看到關於 Proportional Scaling 的兩個重要設定，分別是：</p><ul><li><p><strong>25% max unavailable</strong></p></li><li><p><strong>25% max surge</strong></p></li></ul><p>接著以上面 rollback 的例子來說，如果當 replica 的數量比較大時，就比較容易看的出來，以下用個範例來說明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   10        10        10           10          23h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 container image version 指定到一個不存在的版本</span></span><br><span class="line">$ kubectl <span class="built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.91</span><br><span class="line"></span><br><span class="line"><span class="comment"># k8s 調整後 (目前因為 image version 問題，所以已經卡住)</span></span><br><span class="line">$ kubectl get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   10        13        5            8           23h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 兩個不同的 version 同時存在(template hash code 為 &quot;67594d6bf6&quot; &amp; &quot;58c7645486&quot;)</span></span><br><span class="line"><span class="comment"># 原本的 replicaset 中有 2(10-8) 個 pod 已經暫時被移除 (maxUnavailable &lt; 25%)</span></span><br><span class="line"><span class="comment"># 所有的 replicaset 的 pod 數量為 13(8+5)，沒有超過 max surge 定義的 25% =&gt; (13 - 10) / 13 約莫 24%</span></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-58c7645486   5         5         0         23h</span><br><span class="line">nginx-deployment-67594d6bf6   8         8         8         23h</span><br><span class="line">nginx-deployment-6fdbb596db   0         0         0         23h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以再度看一下 deployment 細節</span></span><br><span class="line"><span class="comment"># 可以發現無論何時，所有 pod 的總和都沒超過 13</span></span><br><span class="line"><span class="comment"># 也就是表示 max surge 低於 25%</span></span><br><span class="line">$ kubectl describe deployment/nginx-deployment</span><br><span class="line">Name:                   nginx-deployment</span><br><span class="line">....(略)</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason             Age   From                   Message</span><br><span class="line">  ----    ------             ----  ----                   -------</span><br><span class="line">  Normal  ScalingReplicaSet  9m    deployment-controller  Scaled up replica <span class="built_in">set</span> nginx-deployment-67594d6bf6 to 10</span><br><span class="line">  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica <span class="built_in">set</span> nginx-deployment-58c7645486 to 3</span><br><span class="line">  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled down replica <span class="built_in">set</span> nginx-deployment-67594d6bf6 to 8</span><br><span class="line">  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica <span class="built_in">set</span> nginx-deployment-58c7645486 to 5</span><br></pre></td></tr></table></figure><blockquote><p>這邊比較需要注意的是，<code>max surge</code> 的計算中，是以 <strong>新舊 replica 中所有的 pod 數量為分母</strong> 去計算出來的，而不是以原有設定的 10 replica。</p></blockquote><h1 id="暫停-amp-恢復-Deployment"><a href="#暫停-amp-恢復-Deployment" class="headerlink" title="暫停 &amp; 恢復 Deployment"></a>暫停 &amp; 恢復 Deployment</h1><p>k8s 允許使用者暫停某個 deployment 後續的任何變更，然後再恢復。這種功能有利於當有多個調整要套用到 deployment 上時，而不需要不斷的觸發 rollout。</p><p>首先來看看目前 k8s cluster 內部 deployment &amp; replicaset 的狀況：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         3         3            3           12s</span><br><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-67594d6bf6   3         3         3         15s</span><br></pre></td></tr></table></figure><p>確認了目前　deployment　的狀況後，接著透過 <code>rollout pause</code> 來將 deployment 的變更暫停：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 暫停 deployment/nginx-deployment 的 rollout (template 變更暫時不會觸發 rollout)</span></span><br><span class="line">$ kubectl rollout pause deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment paused</span><br><span class="line"></span><br><span class="line"><span class="comment"># 變更 deployment/nginx-deployment 中的 container image 設定</span></span><br><span class="line"><span class="comment"># 並將指令紀錄到 rollout history =&gt; --record</span></span><br><span class="line">$ kubectl <span class="built_in">set</span> image deployment/nginx-deployment nginx=nginx:1.9.1 --record</span><br><span class="line">deployment.extensions/nginx-deployment image updated</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 deployment/nginx-deployment 的 rollout history</span></span><br><span class="line">$ kubectl rollout <span class="built_in">history</span> deployment/nginx-deployment</span><br><span class="line">deployments <span class="string">&quot;nginx-deployment&quot;</span></span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 變更 deployment/nginx-deployment 中的 resource quita 設定</span></span><br><span class="line"><span class="comment"># 這個變更不會馬上觸發 rollout，因為已經被暫停</span></span><br><span class="line">$ kubectl <span class="built_in">set</span> resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi</span><br><span class="line">deployment.extensions/nginx-deployment resource requirements updated</span><br></pre></td></tr></table></figure><p>從暫停到目前為止，已經完成了上面的兩個變更，分別是：</p><ol><li><p>修改 container image version</p></li><li><p>設定 resource quota</p></li></ol><p>接著把 rollout 功能恢復：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 恢復 deployment/nginx-deployment 的 rollout</span></span><br><span class="line">$ kubectl rollout resume deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment resumed</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前 replicaset rollout 的即時狀態</span></span><br><span class="line"><span class="comment"># 仔細看下方的輸出，就可以發現 rollout 的過程都是會遵守 maxUnavailable &amp; maxSurge 的設定</span></span><br><span class="line">$ kubectl get rs -w</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-67594d6bf6   3         3         3         2m</span><br><span class="line">nginx-deployment-84bc848fd6   1         1         0         8s</span><br><span class="line">nginx-deployment-84bc848fd6   1         1         1         11s</span><br><span class="line">nginx-deployment-67594d6bf6   2         3         3         3m</span><br><span class="line">nginx-deployment-84bc848fd6   2         1         1         11s</span><br><span class="line">nginx-deployment-67594d6bf6   2         3         3         3m</span><br><span class="line">nginx-deployment-84bc848fd6   2         1         1         11s</span><br><span class="line">nginx-deployment-84bc848fd6   2         2         1         11s</span><br><span class="line">nginx-deployment-67594d6bf6   2         2         2         3m</span><br><span class="line">nginx-deployment-84bc848fd6   2         2         2         22s</span><br><span class="line">nginx-deployment-67594d6bf6   1         2         2         3m</span><br><span class="line">nginx-deployment-84bc848fd6   3         2         2         22s</span><br><span class="line">nginx-deployment-67594d6bf6   1         2         2         3m</span><br><span class="line">nginx-deployment-84bc848fd6   3         2         2         22s</span><br><span class="line">nginx-deployment-84bc848fd6   3         3         2         22s</span><br><span class="line">nginx-deployment-67594d6bf6   1         1         1         3m</span><br><span class="line">nginx-deployment-84bc848fd6   3         3         3         33s</span><br><span class="line">nginx-deployment-67594d6bf6   0         1         1         3m</span><br><span class="line">nginx-deployment-67594d6bf6   0         1         1         3m</span><br><span class="line">nginx-deployment-67594d6bf6   0         0         0         3m</span><br><span class="line">^C</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最後檢視一下完成狀態</span></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-deployment-67594d6bf6   0         0         0         3m</span><br><span class="line">nginx-deployment-84bc848fd6   3         3         3         52s</span><br></pre></td></tr></table></figure><p>最後記得一件事情，若是 rollout 目前已經暫停，那就連 rollback 也無法做了，因為 rollback 也會觸發 rollout 的發生。</p><h1 id="佈署狀態"><a href="#佈署狀態" class="headerlink" title="佈署狀態"></a>佈署狀態</h1><p>一個 deployment 整體生命周期中會有好幾個狀態，例如：</p><ul><li><p><strong>Progressing</strong></p></li><li><p><strong>Complete</strong></p></li><li><p><strong>Fail to progress</strong></p></li></ul><p>以下就針對上面3個狀態進行額外說明。</p><h2 id="Progressing"><a href="#Progressing" class="headerlink" title="Progressing"></a>Progressing</h2><p>當 deployment 在進行以下工作時，狀態會變更為 <strong>Progressing</strong>：</p><ul><li><p>建立一個 replicaset</p></li><li><p>scale up 新的 replicaset</p></li><li><p>scale down 舊的 replicaset</p></li><li><p>新的 pod 狀態變成 <strong>ready</strong> or <strong>available</strong></p></li></ul><h2 id="Complete"><a href="#Complete" class="headerlink" title="Complete"></a>Complete</h2><p>當以下的條件 or 狀態有達成時，k8s 會將 deployment 狀態變更為 <strong>Complete</strong>：</p><ul><li><p>所有與 deployment 相關的 replicaset 都已經更新到最新的 template 版本(表示已經達到 desired status)</p></li><li><p>所有相關的 pod 的狀態都是 available</p></li><li><p>沒有舊版本的 pod 還在執行中</p></li></ul><p>透過以下指令可以簡單的檢視目前 deployment 是否已經進入 complete 的狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 deployment rollout status</span></span><br><span class="line">$ kubectl rollout status deployment/nginx-deployment</span><br><span class="line">deployment <span class="string">&quot;nginx-deployment&quot;</span> successfully rolled out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若上一個指令的 exit code 為 0 表示 deployment 目前為 complete 狀態</span></span><br><span class="line">root@leon-k8s-node01:~<span class="comment"># echo $?</span></span><br><span class="line">0</span><br></pre></td></tr></table></figure><h2 id="Fail-to-progress"><a href="#Fail-to-progress" class="headerlink" title="Fail to progress"></a>Fail to progress</h2><p>有時會遇到 deployment 執行到一半發生問題 or 卡住而無法完成工作，而這些問題通常是以下情況引起的：</p><ul><li><p>Readiness probe 診斷失敗</p></li><li><p>container image pull 發生錯誤</p></li><li><p>權限不足</p></li><li><p>resource quota 不足 (namespace level)</p></li><li><p>超過 limit range 設定範圍 (pod level)</p></li><li><p>application 本身造成的錯誤</p></li></ul><p>其實 k8s 判定 deployment 為 fail to progress 的預設時間為 600 秒，如果想要提早確認 deployment 無法正常運作，可以透過設定 <strong><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#progress-deadline-seconds">.spec.progressDeadlineSeconds</a></strong> 來縮短 or 延長判定為 fail 的時間，可以直接定義在 deployment spec 中，也可以透過以下指令修改：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># progressDeadlineSeconds 預設為 600 秒，因此不會變更</span></span><br><span class="line">$ kubectl patch deployments/nginx-deployment -p <span class="string">&#x27;&#123; &quot;spec&quot;: &#123; &quot;progressDeadlineSeconds&quot;: 600 &#125;&#125;&#x27;</span></span><br><span class="line">deployment.extensions/nginx-deployment not patched</span><br><span class="line"></span><br><span class="line"><span class="comment">#　將 progressDeadlineSeconds 從 600 秒改為 300 秒</span></span><br><span class="line">$ kubectl patch deployments/nginx-deployment -p <span class="string">&#x27;&#123; &quot;spec&quot;: &#123; &quot;progressDeadlineSeconds&quot;: 300 &#125;&#125;&#x27;</span></span><br><span class="line">deployment.extensions/nginx-deployment patched</span><br></pre></td></tr></table></figure><p>當超過　progressDeadlineSeconds 設定值，就會有一筆新的 <strong>DeploymentCondition</strong> 資訊被加入到 deployment 的 <code>.status.conditions</code> 中，內容如下：</p><table><thead><tr><th>Type</th><th>Status</th><th>Reason</th></tr></thead><tbody><tr><td>Progressing</td><td>False</td><td>ProgressDeadlineExceeded</td></tr></tbody></table><p>當 deployment 已經被判定為 fail to prgress 後，除了回報 deployment 狀態外，k8s 就不會再額外多做什麼了。但若是有更上層的工具，依然還是可以利用這樣的狀態完成一些管理操作，例如：rollback 到前一個版本。</p><p>最後，即使是一個 failed deployment，當它的 template 變更時，依然還是會觸發 roll out 的進行，就跟一般正常的 deployment 是完全相同的。</p><h2 id="當-Deployment-發生-Failure-時怎辦？"><a href="#當-Deployment-發生-Failure-時怎辦？" class="headerlink" title="當 Deployment 發生 Failure 時怎辦？"></a>當 Deployment 發生 Failure 時怎辦？</h2><p>從上面可以看出，可能發生 fail to progress 的原因其實也不少，當遇到 deployment 無法正常佈署時，可以透過以下幾種方式來查 root cause：</p><h3 id="檢查-rollout-status"><a href="#檢查-rollout-status" class="headerlink" title="檢查 rollout status"></a>檢查 rollout status</h3><p>以上面 <strong>deployment/nginx-deployment</strong> 為例，利用以下語法可以檢查 rollout status：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 deploy/nginx-deployment 的 rollout status</span></span><br><span class="line">$ kubectl rollout status deploy/nginx-deployment</span><br><span class="line">Waiting <span class="keyword">for</span> rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">error: deployment <span class="string">&quot;nginx&quot;</span> exceeded its progress deadline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上一個指令回傳的 exit code 不等於 0</span></span><br><span class="line">$ <span class="built_in">echo</span> $?</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h3 id="輸出完整的-YAML-資訊"><a href="#輸出完整的-YAML-資訊" class="headerlink" title="輸出完整的 YAML 資訊"></a>輸出完整的 YAML 資訊</h3><p>透過 <code>kubectl describe</code> 檢視 deployment 狀況時，可能會出現以下資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe deployment nginx-deployment</span><br><span class="line">.....(略)</span><br><span class="line">Conditions:</span><br><span class="line">  Type            Status  Reason</span><br><span class="line">  ----            ------  ------</span><br><span class="line">  Available       True    MinimumReplicasAvailable</span><br><span class="line">  Progressing     True    ReplicaSetUpdated</span><br><span class="line">  ReplicaFailure  True    FailedCreate</span><br><span class="line">.....(略)</span><br></pre></td></tr></table></figure><p>只看到 <strong>FailedCreate</strong> 實在很難判定實際的問題到底是什麼，因此可以透過以下指令找出導致問題的詳細原因：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可看出每一個階段產生出的詳細 log 資訊</span></span><br><span class="line">$ kubectl get deployment nginx-deployment -o yaml</span><br><span class="line">.....(略)</span><br><span class="line">status:</span><br><span class="line">  availableReplicas: 2</span><br><span class="line">  conditions:</span><br><span class="line">  - lastTransitionTime: 2016-10-04T12:25:39Z</span><br><span class="line">    lastUpdateTime: 2016-10-04T12:25:39Z</span><br><span class="line">    message: Replica <span class="built_in">set</span> <span class="string">&quot;nginx-deployment-4262182780&quot;</span> is progressing.</span><br><span class="line">    reason: ReplicaSetUpdated</span><br><span class="line">    status: <span class="string">&quot;True&quot;</span></span><br><span class="line">    <span class="built_in">type</span>: Progressing</span><br><span class="line">  - lastTransitionTime: 2016-10-04T12:25:42Z</span><br><span class="line">    lastUpdateTime: 2016-10-04T12:25:42Z</span><br><span class="line">    message: Deployment has minimum availability.</span><br><span class="line">    reason: MinimumReplicasAvailable</span><br><span class="line">    status: <span class="string">&quot;True&quot;</span></span><br><span class="line">    <span class="built_in">type</span>: Available</span><br><span class="line">  - lastTransitionTime: 2016-10-04T12:25:39Z</span><br><span class="line">    lastUpdateTime: 2016-10-04T12:25:39Z</span><br><span class="line">    message: <span class="string">&#x27;Error creating: pods &quot;nginx-deployment-4262182780-&quot; is forbidden: exceeded quota:</span></span><br><span class="line"><span class="string">      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#x27;</span></span><br><span class="line">    reason: FailedCreate</span><br><span class="line">    status: <span class="string">&quot;True&quot;</span></span><br><span class="line">    <span class="built_in">type</span>: ReplicaFailure</span><br><span class="line">  observedGeneration: 3</span><br><span class="line">  replicas: 2</span><br><span class="line">  unavailableReplicas: 2</span><br><span class="line">.....(略)</span><br></pre></td></tr></table></figure><h1 id="Clean-up-Policy"><a href="#Clean-up-Policy" class="headerlink" title="Clean up Policy"></a>Clean up Policy</h1><p>這個部份主要設定的是 revision history 保留的數量，這個設定可以針對單一的 Deployment 進行設定，透過設定 <code>.spec.revisionHistoryLimit</code> 就可以指定 k8s 要保留幾份 Deployment 的 revision history，預設值為 <strong>10</strong>。</p><blockquote><p>若是設定為 0，將會喪失 rollback 的能力</p></blockquote><h1 id="如何撰寫正確的-Deployment"><a href="#如何撰寫正確的-Deployment" class="headerlink" title="如何撰寫正確的 Deployment"></a>如何撰寫正確的 Deployment</h1><p>以上介紹了很多 Deployment 的特性 &amp; 功能，但如果希望某些屬性在定義時就宣告完成，就必須要知道 deployment spec 要如何撰寫，以下是一個比較詳細的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># apiVersion, kind, metadata 3個欄位是必備的</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="comment"># deployment name</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment-spec</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="comment"># 用來指定當 new pod 要取代 old pod 時要如何進行</span></span><br><span class="line">    <span class="comment"># RollingUpdate: 會根據 maxUnavailable &amp; maxSurge 的設定，確保有足夠的 pod 可以提供服務</span></span><br><span class="line">    <span class="comment"># 才會慢慢的將 old pod 換成 new pod (default)</span></span><br><span class="line">    <span class="comment"># Recreate: 會先移除所有的 old pod 後，才會開始產生 new pod</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">RollingUpdate</span></span><br><span class="line">    <span class="attr">rollingUpdate:</span></span><br><span class="line">      <span class="comment"># 指定當 update 進行中時，可以失效(無法提供服務)的 pod 佔整體 pod 數量的比例(也可以是整數值)為多少</span></span><br><span class="line">      <span class="comment"># (default = 25%)</span></span><br><span class="line">      <span class="attr">maxUnavailable:</span> <span class="number">25</span><span class="string">%</span></span><br><span class="line">      <span class="comment"># 指定當 update 進行中時，pod 可以超過 desired status 定義數量的比例(也可以是整數值)</span></span><br><span class="line">      <span class="comment"># (default = 25%)</span></span><br><span class="line">      <span class="attr">maxSurge:</span> <span class="number">25</span><span class="string">%</span></span><br><span class="line">  <span class="comment"># 指定要建立多少 pod 副本(default = 1)</span></span><br><span class="line">  <span class="comment"># 實際情況少於此數字，則會增加 pod，反之則會殺掉 pod</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># 指定最長等待多少時間後，佈署依舊無法順利完成時，回報 &quot;failed progressing&quot; 的時間(秒)</span></span><br><span class="line">  <span class="comment"># (default = 600)</span></span><br><span class="line">  <span class="attr">progressDeadlineSeconds:</span> <span class="number">600</span></span><br><span class="line">  <span class="comment"># 設定若是沒有任何 pod crashing 的情況發生，被認為是可用狀態的最小秒數 (default = 0)</span></span><br><span class="line">  <span class="attr">minReadySeconds:</span> <span class="number">0</span></span><br><span class="line">  <span class="comment"># 設定 revision history 保留的數量</span></span><br><span class="line">  <span class="comment"># 建議可以根據 Deployment 更新的頻率來決定這個值要設定多少</span></span><br><span class="line">  <span class="comment"># 設定 0 將會無法進行 rollback</span></span><br><span class="line">  <span class="comment"># (default = 10)</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment"># 用來指定要用來監控並進行管理的 pod label 設定</span></span><br><span class="line">  <span class="comment"># 必須要與下面的 pod label(.spec.template.metadata.labels) 相符合</span></span><br><span class="line">  <span class="comment"># 在 apps/v1 版本中，&quot;.spec.selector&quot; 一旦設定後就無法變更了</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="comment"># .spec.template 其實就是 pod 的定義</span></span><br><span class="line">  <span class="comment"># 也是 .spec 中唯一必須要設定的欄位</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="comment"># pod metadata</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="comment"># 設定給 pod 的 label 資訊</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="comment"># restart policy  在 Delpoyment 中只能設定為 Always</span></span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br><span class="line">      <span class="comment"># 可看出這個 pod 只運行了一個 nginx container</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><blockquote><p>設定 deployment 時，必須注意<strong>不可以</strong>佈署帶有相同 label selector 的 Deployment, ReplicaSet 或是 ReplicationController</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments - Kubernetes</a></p></li><li><p><a href="https://jimmysong.io/kubernetes-handbook/concepts/deployment.html">Deployment · Kubernetes Handbook - Kubernetes中文指南/云原生应用架构实践手册 by Jimmy Song(宋净超)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] ReplicaSet 介紹</title>
      <link href="/blog/Kubernetes/k8s-ReplicaSet-Overview/"/>
      <url>/blog/Kubernetes/k8s-ReplicaSet-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-ReplicaSet"><a href="#What-is-ReplicaSet" class="headerlink" title="What is ReplicaSet ?"></a>What is ReplicaSet ?</h1><p><img src="/blog/images/kubernetes/k8s-replicaset.png" alt="ReplicaSet"></p><p>ReplicaSet 是用來確保在 k8s 中，在資源允許的前提下，指定的 pod 的數量會跟使用者所期望的一致，也就是所謂的 desired status。</p><p>而 ReplicaSet 其實是 ReplicationController 的進化版，其中的差別僅在於 ReplicaSet 支援 set-based label selector，而 ReplicationController 僅支援 equality-based label selector。</p><h1 id="如何使用-ReplicaSet"><a href="#如何使用-ReplicaSet" class="headerlink" title="如何使用 ReplicaSet?"></a>如何使用 ReplicaSet?</h1><p>官方建議 ReplicaSet 要搭配 Deployment 一起來使用，原因如下：</p><ul><li><p>若是有 <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rolling-update">rolling update</a> 的需求 只有在 Deployment 有相關的 kubectl 指令可以協助，單單使用 ReplicaSet 是沒有的</p></li><li><p>Deployment 是個更上層的抽象概念，也因此支援了更多好用的 feature，因此官方才會建議不要單獨使用 ReplicaSet，而是使用 Deployment，並將 ReplicaSet 的資訊設定到 Deployment 的 spec 中</p></li></ul><h1 id="ReplicaSet-定義說明"><a href="#ReplicaSet-定義說明" class="headerlink" title="ReplicaSet 定義說明"></a>ReplicaSet 定義說明</h1><h2 id="範例介紹"><a href="#範例介紹" class="headerlink" title="範例介紹"></a>範例介紹</h2><p>以下用一個簡單的範例來介紹 ReplicaSet 的結構定義：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># apiVersion, kind, metadata 是必備欄位</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="comment"># replicaset 也可以定義 label</span></span><br><span class="line">  <span class="comment"># 一般會與 .spec.template.metadata.labels 設定相同，但不同其實也沒差</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">guestbook</span></span><br><span class="line">    <span class="attr">tier:</span> <span class="string">frontend</span></span><br><span class="line"><span class="comment"># 以下透過 spec 設定 replicaset 的規格 </span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 要產生幾份副本(沒設定則預設為 1)</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="comment"># 設定 label selector，用來選擇產生副本用的 pod</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">tier:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">matchExpressions:</span></span><br><span class="line">      <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">tier</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">frontend</span>]&#125;</span><br><span class="line">  <span class="comment"># .spec.template 是 .spec 中唯一的必要欄位</span></span><br><span class="line">  <span class="comment"># 其實就是 pod template</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="comment"># .spec.template.metadata.labels 必須符合 .spec.selector 中的設定才行</span></span><br><span class="line">      <span class="comment"># 否則 API server 會拒絕產生此物件</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">guestbook</span></span><br><span class="line">        <span class="attr">tier:</span> <span class="string">frontend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="comment"># 所有 .spec.template.spec.restartPolicy 的設定，僅能設定為 Always (同時也是預設值)</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">php-redis</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">gcr.io/google_samples/gb-frontend:v3</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">100Mi</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GET_HOSTS_FROM</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">dns</span></span><br><span class="line">          <span class="comment"># If your cluster config does not include a dns service, then to</span></span><br><span class="line">          <span class="comment"># instead access environment variables to find service host</span></span><br><span class="line">          <span class="comment"># info, comment out the &#x27;value: dns&#x27; line above, and uncomment the</span></span><br><span class="line">          <span class="comment"># line below.</span></span><br><span class="line">          <span class="comment"># value: env</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><h2 id="使用上需要注意的事情"><a href="#使用上需要注意的事情" class="headerlink" title="使用上需要注意的事情"></a>使用上需要注意的事情</h2><p>在設定 ReplicaSet 的時候，有幾點是必須注意的：</p><ul><li><p><code>.spec.template.metadata.labels</code> 的設定必須符合 <code>.spec.selector</code>，否則 API server 會拒絕產生此物件</p></li><li><p>從 v1.9 後，apiVersion 已經從 <code>apps/v1beta2</code> 改為 <code>apps/v1</code></p></li><li><p>當 ReplicaSet 物件建立後，不要建立帶有完全相同 label 組合設定的 pod, deployment 或是其他 replicaset，這樣會造成運作上的混淆 (k8s 並不會阻止你這麼做….) </p></li><li><p>如果有建立多個帶有相同 label selector 的 controller，<strong>刪除</strong>這件事情你就要自己動手了</p></li><li><p><code>.spec.replicas</code> 若沒有設定就預設為 <strong>1</strong></p></li><li><p>如果要運行一個執行完工作就自動終止的 pod，就不要使用 ReplicaSet，而是要改用 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Job</a></p></li><li><p>如果要運行一個 machine level 的功能(例如：monitoring, logging)，確保 pod lifetime 與 machine lifetime 一致，且希望這個 pod 可以比其他 pod 更早啟動，也不要使用 ReplicaSet，而是要改用 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a></p></li></ul><h1 id="ReplicaSet-相關操作"><a href="#ReplicaSet-相關操作" class="headerlink" title="ReplicaSet 相關操作"></a>ReplicaSet 相關操作</h1><h2 id="刪除-ReplicaSet-amp-相關的-Pod"><a href="#刪除-ReplicaSet-amp-相關的-Pod" class="headerlink" title="刪除 ReplicaSet &amp; 相關的 Pod"></a>刪除 ReplicaSet &amp; 相關的 Pod</h2><p>透過 <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete">kubectl delete</a> 刪除，k8s 中的 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/">Garbage Controller</a> 會自動的刪除相對應的 pod。</p><p>但若是透過 REST API k的方式，則必須要將 <code>propagationPolicy</code> 設定為 <strong>Foreground</strong> 才可以：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl proxy --port=8080</span><br><span class="line"></span><br><span class="line">$ curl -X DELETE  <span class="string">&#x27;localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend&#x27;</span> \</span><br><span class="line">&gt; -d <span class="string">&#x27;&#123;&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Foreground&quot;&#125;&#x27;</span> \</span><br><span class="line">&gt; -H <span class="string">&quot;Content-Type: application/json&quot;</span></span><br></pre></td></tr></table></figure><h2 id="僅刪除-ReplicaSet"><a href="#僅刪除-ReplicaSet" class="headerlink" title="僅刪除 ReplicaSet"></a>僅刪除 ReplicaSet</h2><p>若只要刪除 ReplicaSet 而不影響 pod，透過 <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete">kubectl delete</a> 搭配 <code>--cascade=false</code> 參數就可以完成。</p><p>若使用 REST API，則必須要將 <code>propagationPolicy</code> 設定為 <strong>Orphan</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl proxy --port=8080</span><br><span class="line"></span><br><span class="line">$ curl -X DELETE  <span class="string">&#x27;localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend&#x27;</span> \</span><br><span class="line">&gt; -d <span class="string">&#x27;&#123;&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Orphan&quot;&#125;&#x27;</span> \</span><br><span class="line">&gt; -H <span class="string">&quot;Content-Type: application/json&quot;</span></span><br></pre></td></tr></table></figure><h2 id="將-ReplicaSet-與-Pod-隔離"><a href="#將-ReplicaSet-與-Pod-隔離" class="headerlink" title="將 ReplicaSet 與 Pod 隔離"></a>將 ReplicaSet 與 Pod 隔離</h2><p>通常會這麼做的目的大多在於進行 debugging 或是 data recovery。</p><p>作法不困難，只要修改 pod label，讓 ReplicaSet 的 label selector 選不到該 pod 即可；但同時要搭配調整 ReplicaSet 中的 <code>.spec.replicas</code> 的設定，不然新的 pod 又會被產生出來。</p><h2 id="ReplicaSet-Scaling"><a href="#ReplicaSet-Scaling" class="headerlink" title="ReplicaSet Scaling"></a>ReplicaSet Scaling</h2><p>調整 <code>.spec.replicas</code> 設定即可，k8s 會自動維護使用者所指定的 desired status。</p><h2 id="與-HPA-Horizontal-Pod-Autoscaler-Target-的搭配"><a href="#與-HPA-Horizontal-Pod-Autoscaler-Target-的搭配" class="headerlink" title="與 HPA(Horizontal Pod Autoscaler Target) 的搭配"></a>與 HPA(Horizontal Pod Autoscaler Target) 的搭配</h2><p>透過與 <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler Target(HPA)</a> 搭配，可以指定 ReplicaSet Scaling 的上下限範圍，在資源使用率達到指定門檻時，讓 k8s 自動進行 scale out/in，以下是個簡單範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">autoscaling/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">HorizontalPodAutoscaler</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">frontend-scaler</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># 指定要搭配的 resource object，這裡是 ReplicaSet</span></span><br><span class="line">  <span class="attr">scaleTargetRef:</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">frontend</span></span><br><span class="line">  <span class="comment"># 指定 scale in/out 的 pod 數量為 3~10 個</span></span><br><span class="line">  <span class="attr">minReplicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">maxReplicas:</span> <span class="number">10</span></span><br><span class="line">  <span class="comment"># 資源使用率門檻為 CPU 使用率 50%</span></span><br><span class="line">  <span class="attr">targetCPUUtilizationPercentage:</span> <span class="number">50</span></span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet - Kubernetes</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Pod 的設計 &amp; 相關運作機制</title>
      <link href="/blog/Kubernetes/k8s-Pod-Overview/"/>
      <url>/blog/Kubernetes/k8s-Pod-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="What-is-Pod"><a href="#What-is-Pod" class="headerlink" title="What is Pod ?"></a>What is Pod ?</h2><p>Pod 是在 k8s 最基本的組成單位(也是最小的可佈署單位)，實際在 k8s 上運行的很多 resource object 都是以 pod 型式存在，因此了解 pod 運作的機制會讓我們對 k8s 有更深入的了解。</p><p>Pod 雖然是最基本的組成單位，但其實在設計上隱藏的相當多的細節，它封裝了許多不同的資源，也因此每個 pod 都有以下特性：</p><ul><li><p>包含一到多個 container</p></li><li><p>同一個 pod 都 container 都共享相同的檔案系統 &amp; volume … 等資源</p></li><li><p>container 共享相同的 network namespace(container 之間可以透過 <code>localhost</code> + <code>port number</code> 互相通訊)，且有獨一無二的 IP address</p></li><li><p>container 之間也可以透過進程間通信，例如：SystemV or POSIX shared memory</p></li><li><p>container 共享 pod 中的 volume resource</p></li><li><p>pod 中的 container 總是被同時調度 &amp; 有共同的運行環境</p></li></ul><blockquote><p>Pod 中共同的運行環境包含 Linux 的namespace，cgroup 和其他可能的隔絕環境，這部份跟 Docker container 是一樣的。不過在 pod 的環境中，每個 container 中可能還會因為 application 的管理策略會有不同的隔離方式</p></blockquote><p>Pod 在 k8s 中有兩種主要的使用方式：</p><ol><li><p>只運行一個 container: 這就類似在一個 container 外再包一個 wrapper</p></li><li><p>運行多個需要協同合作的 container: 這些 container 除了協同合作外，還共享相同的 network &amp; storage 資源，其它與 application 相關的資源可能會另外由一個 sidecar container 來負責處理</p></li></ol><blockquote><p>讓多個 container 同時運行在同一個 pod 裏面，算是比較進階的應用，除非確定有多個容器之間緊密合作的需求才考慮使用這樣的模式</p></blockquote><p>以下就是一個類似的應用範例：(一個 container 提供 web 服務，另一個則是負責 container management，從外面取得特定資料回來，並存到共享的 volume 中)</p><p><img src="/blog/images/kubernetes/k8s_multi-containers-in-a-pod.png" alt="Multiple Containers in a Pod"></p><h2 id="單獨使用-Pod-需注意的事項"><a href="#單獨使用-Pod-需注意的事項" class="headerlink" title="單獨使用 Pod 需注意的事項"></a>單獨使用 Pod 需注意的事項</h2><p>在 k8s 中直接使用獨立的 pod 是沒問題的，但並不建議這麼做，理由大概有以下幾個：</p><ul><li><p>獨立的 pod 若是發生問題時(例如: node failure)，k8s 不會協助恢復其正常的狀態</p></li><li><p>若 pod 所在的 worker node 因為資源不足或是進入維護狀態時，pod 不會被自動移到其他正常的 node 並重新啟動</p></li></ul><p>因此在 k8s 中，提供了一個更 high level 的抽象概念 <strong>controller</strong> 來處理上面的問題，而使用者在使用上也是應該透過這些 controller 來管理 pod。</p><h2 id="Pod-amp-Controller"><a href="#Pod-amp-Controller" class="headerlink" title="Pod &amp; Controller"></a>Pod &amp; Controller</h2><p>Controller 可以協助使用者完成以下的工作：</p><ul><li><p>建立 &amp; 管理多個 pod</p></li><li><p>replication 管理</p></li><li><p>rolling upgrade/rollback</p></li><li><p>self healing</p></li></ul><p>而在 k8s 中，屬於 controller 的 resource object 很多，詳細的清單可以參考<a href="https://godleon.github.io/blog/2018/08/31/Kubernetes/k8s-CoreConcept-ResourceObject-Overview/#Controller-%E9%A1%9E%E5%9E%8B%E7%9A%84-Resource-Object">此篇文章</a>。</p><p>基本上，controller 使用的是稱為 <strong>Pod Template</strong> 的資訊來管理 pod 相關的資源，而 pod template 其實就是整個 pod 的 spec 資訊，其中可能還包含了其他 resource object 的資訊，例如：ReplicationController, Jobs, DaemonSet … 等等。</p><h2 id="設計-Pod-的原因"><a href="#設計-Pod-的原因" class="headerlink" title="設計 Pod 的原因"></a>設計 Pod 的原因</h2><p>可能會有人想要知道，為什麼在 k8s 中不直接使用 <strong>Container</strong> 為單位而是用 <strong>Pod</strong> 呢? 主要有兩個原因：</p><h3 id="方便管理"><a href="#方便管理" class="headerlink" title="方便管理"></a>方便管理</h3><p>Pod 是由 <strong>multiple cooperating processes</strong> 這個 pattern 所產生出來的一種模型，將多個原本各自獨立的元素變成了一個緊密結合的服務單位，這種設計方式簡化了佈署佈署 &amp; 管理工作，並提供了一個 high level 的抽象概念來管理這些各自獨立的元素。</p><p>也因為設計 pod 作為 deployment unit，使得像是 <strong>Horizontal Scaling</strong>、<strong>Replication</strong>、<strong>Coordinated Replication</strong>、<strong>Resource Sharing</strong>、<strong>Dependency Management</strong> 這些功能在 k8s 中都可以自動被處理。</p><h3 id="資源共享-amp-通訊"><a href="#資源共享-amp-通訊" class="headerlink" title="資源共享 &amp; 通訊"></a>資源共享 &amp; 通訊</h3><p>如上面所提到的，透過 pod 的設計，讓同一個 pod 中的 container 可以很方便的共享 network &amp; volume 等 application 使用時相當重要的資源。</p><h2 id="終止-Pod"><a href="#終止-Pod" class="headerlink" title="終止 Pod"></a>終止 Pod</h2><p>在 k8s 中，當一個 pod 被指定要終止時，k8s 會儘量確保 container 可以優雅地按照正常的流程停止，沒必要不會直接送 <strong>SIGKILL</strong> 去清除對應的 process，以下是一個 pod 終止時的流程範例：</p><ol><li><p>使用者送出指令要刪除 pod，預設的寬限期(<strong>grace period</strong>)是 30 秒</p></li><li><p>超過寬限期後，pod 狀態在 API server 會被更新為 <strong>dead</strong></p></li><li><p>接著同步執行以下工作：</p><p> 3.1 此時若是在 CLI 中列出 pod，狀態會顯示 <strong>terminating</strong></p><p> 3.2 當 kubelet 發現 pod 被標記為 <strong>terminating</strong> 狀態時(目前尚在寬限期內)，開始停止 pod 的流程：</p><pre><code> 3.2.1 如果在pod中 定義了 [preStop hook](https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-details)，此時會被呼叫；如果 preStop hook 在超過寬限期過後依然在運行，第二步會再增加 2 秒的寬限期 3.2.2 向 Pod 中的 process 發送 **TERM** 信號；</code></pre><p> 3.3 該 Pod 會從 service 的 endpoint list 中移除，replication controller 不會再進行管理；此時若是有終止比較慢的 pod，也不會繼續處理 load balancer 轉發過來的流量</p></li><li><p>過了寬限期後，將會向 Pod 中還在運作的 process 發送 <strong>SIGKILL</strong> 來清除 process。</p></li><li><p>kublete 會在 API server 中設定 grace period 為 0，表示完成 pod 的刪工作已經完成。此時 Pod 已經在 在 API server 中消失，CLI 也無法看見</p></li></ol><p>預設的 grace period 為 30 秒，而使用 <code>kubectl delete</code> 指令可以直接透過 <code>--grace-period=&lt;seconds&gt;</code> 參數來改變這個預設值，若設定為 <strong>0</strong> 則是強制刪除 pod。</p><blockquote><p>在 v1.5 之後，若要使用 <code>--grace-period=0</code> 就必須要搭配 <code>--force</code> 參數一起使用</p></blockquote><p>另外，強制刪除 pod 會讓 API server 直接清除該 pod 的所有資訊，不會等待 kubelet 將 resource 清理完成，在使用上會有潛在的風險；這個功能若是要應用在 StatefulSet 中的 pod，建議參考<a href="https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/">官網文章 deleting Pods from a StatefulSet</a> 來處理。</p><h2 id="Privileged-Mode"><a href="#Privileged-Mode" class="headerlink" title="Privileged Mode"></a>Privileged Mode</h2><p>k8s v1.1 之後開始可以透過 pod spec 中的 <strong>SecurityContext</strong> 中的 <code>privileged</code> 屬性，讓 pod 以 privileged mode 的狀態下運行，如果使用者要透過 pod 操作 network stack or 存取特定裝置時相當好用；讓 container 中的 process 幾乎可以跟 container 外部的 process 有著幾乎相同的存取權限。</p><h1 id="Pod-Lifecycle"><a href="#Pod-Lifecycle" class="headerlink" title="Pod Lifecycle"></a>Pod Lifecycle</h1><p>Volume 跟 pod 有相同的生命週期(以 pod UID 為準)。當 Pod 因為某種原因被刪除或者被新創建但相同的 Pod 取代(UID 已經變更)，它相關的資源（例如：volume）也會被銷毀和再創建一個新的。</p><h2 id="Pod-Phase"><a href="#Pod-Phase" class="headerlink" title="Pod Phase"></a>Pod Phase</h2><p>當使用 <code>kubectl get pod</code> 命令嘗試取得目前 namespace 中的 pod 列表，可以看到列表中有個欄位為 <strong>STATUS</strong>，顯示著目前 pod 的狀態，而這個狀態資訊其實是一個名稱為 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#podstatus-v1-core">PodStatus</a> 的物件中的 <strong>phase</strong> 欄位所提供，根據 pod 狀況不同會有以下的值：</p><table><thead><tr><th>Value</th><th>Description</th></tr></thead><tbody><tr><td>Pending</td><td>Pod 已被 Kubernetes 系統接受，但有一個或者多個 container image 尚未被建立完成。這一段時間包含了調度 Pod 的時間和通過網絡下載 image 的時間，需要一段時間來處理</td></tr><tr><td>Running</td><td>Pod 已經綁定到某個 node 上，相關的 container 都已經建立完成；且至少有個 container 正在運行 or 正處於 starting or restarting 的狀態下</td></tr><tr><td>Succeeded</td><td>Pod 中的所有 container 都被成功終止，並且不會再重啟</td></tr><tr><td>Failed</td><td>Pod 中的所有 container 都被終止，但至少有一個因為任務失敗而終止(表示退出時的狀態碼不為 0 or 被系統終止)</td></tr><tr><td>Unknown</td><td>因為某些原因無法取得 Pod 的狀態，通常是因為與 Pod 所在 node 通訊失敗</td></tr></tbody></table><p><img src="/blog/images/kubernetes/kubernetes-pod-life-cycle-status.jpg" alt="Pod Life Cycle"></p><h2 id="Pod-Condition"><a href="#Pod-Condition" class="headerlink" title="Pod Condition"></a>Pod Condition</h2><p>接著來看看 pod condition，以下透過 <code>kubectl describe pod/kubernetes-dashboard-xyz -n kube-system</code> 指令細部檢視 <strong>kubernetes-dashboard-xyz</strong> 這個 pod 的資訊可以看到類似一下內容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Name:               kubernetes-dashboard-xyz</span><br><span class="line">Namespace:          kube-system</span><br><span class="line">...(以下略)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             True </span><br><span class="line">  ContainersReady   True </span><br><span class="line">  PodScheduled      True </span><br><span class="line">...(以下略)</span><br></pre></td></tr></table></figure><p>仔細看看在 Conditions 的部份，系統提供了我們上面這些資訊，代表的意義是：</p><ul><li><p><code>Initialized</code>: 所有的 <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">init container</a> 是否都有成功啟動</p></li><li><p><code>Ready</code>: 是否已經可接受 service request 或是可加入到 load balancer 中</p></li><li><p><code>ContainersReady</code>: 在 pod 中的所有 container 是否都已經 ready</p></li><li><p><code>PodScheduled</code>: pod 是否已經被 schedule 到特定的 node</p></li></ul><h2 id="診斷-Container-有無正常運行-Probe"><a href="#診斷-Container-有無正常運行-Probe" class="headerlink" title="診斷 Container 有無正常運行 - Probe"></a>診斷 Container 有無正常運行 - Probe</h2><h3 id="What-is-Probe"><a href="#What-is-Probe" class="headerlink" title="What is Probe?"></a>What is Probe?</h3><p>probe(探針)的設計目的在於定期診斷 container 是否持續正常運作中，由 kubelet 執行。而執行方式就是 kubelet 呼叫實作在 container 中的特定 <a href="https://godoc.org/k8s.io/kubernetes/pkg/api/v1#Handler">handler</a> 來取得狀態資訊，目前 handler 可分為以下三類：</p><ol><li><p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#execaction-v1-core">ExecAction</a>: 在 container 內執行特定命令。如果命令結束時 exit code 為 0 則視為診斷成功</p></li><li><p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#tcpsocketaction-v1-core">TCPSocketAction</a>: 對 pod 上的特定 port 進行 TCP check，若是 port 是 open 的狀態，則視為診斷成功</p></li><li><p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#httpgetaction-v1-core">HTTPGetAction</a>: 對指定的 pod IP+Port 執行 HTTP Get 請求。如果 HTTP response status code 大於等於200 且小於 400，則視為診斷成功</p></li></ol><p>當每個 probe 完成後，會有三種結果，分別是 <code>Success</code>(通過診斷)、<code>Failure</code>(未通過診斷) &amp; <code>Unknown</code>(診斷失敗 &amp; 不採取任何行動) </p><h3 id="Probe-種類"><a href="#Probe-種類" class="headerlink" title="Probe 種類"></a>Probe 種類</h3><p>目前 kubelet 可以根據使用者的設定，來執行以下兩種的 probe 並根據結果做出反應：</p><ul><li><code>livenessProbe</code>: 診斷 pod 是否處於 running 狀態。若診斷沒有通過，kueblet 就會殺掉 pod 並根據 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">restart policy</a> 進行後續動作。此外，如果 pod spec 中沒有提供 livenessProbe 的設定，則預設為診斷成功。</li></ul><p>以下是一個 livenessProbe 的設定範例</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">test:</span> <span class="string">liveness</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-http</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/server</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/liveness</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="comment"># when &quot;host&quot; is not defined, &quot;PodIP&quot; will be used</span></span><br><span class="line">        <span class="comment"># host: my-host</span></span><br><span class="line">        <span class="comment"># when &quot;scheme&quot; is not defined, &quot;HTTP&quot; scheme will be used. Only &quot;HTTP&quot; and &quot;HTTPS&quot; are allowed</span></span><br><span class="line">        <span class="comment"># scheme: HTTPS</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/healthz</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">        <span class="attr">httpHeaders:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">X-Custom-Header</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">Awesome</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line">      <span class="attr">timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">liveness</span></span><br></pre></td></tr></table></figure><ul><li><code>readinessProbe</code>: 診斷 pod 是否可以接受 service request。若診斷沒有通過，與這個 pod 相關的 service endpoint 資訊都會被 controller 移除。在 initial delay 之前 readiness 狀態會被預設為 failure。此外，如果 pod spec 中沒有提供 readinessProbe 的設定，則預設為診斷成功。</li></ul><h3 id="何時使用這些-Probe"><a href="#何時使用這些-Probe" class="headerlink" title="何時使用這些 Probe?"></a>何時使用這些 Probe?</h3><p>使用 probe 大概有幾個原則可以遵循：</p><ul><li><p>如果 container 有設計成當遇到問題的時候會自己 crash，不會卡死，那就不用特別設定 livenessProbe，kubelet 會自動根據 restart policy 來進行後續處理</p></li><li><p>如果希望 pod 可以在 probe 診斷失敗的時候被 kill 並 restart，那就要把 <code>restartPolicy</code> 設定為 <strong>Always</strong> or <strong>OnFailure</strong></p></li><li><p>如果希望 pod 進入正常服務狀態的時候才開始接收 service request 的話，那就設定 <code>readinessProbe</code>，只有當診斷成功時，k8s 才會把 service request 送給 pod</p></li><li><p>若 container 啟動時需要處理大量資料 or 設定檔 or 進行 migration 的檢查，此時就需要設定 <code>readinessProbe</code>，當診斷成功後才開始接收 service request</p></li><li><p>如果只是要在 pod 刪除時不接收任何 service request，其實不需要設定任何的 probe</p></li></ul><h2 id="Pod-readiness-gate"><a href="#Pod-readiness-gate" class="headerlink" title="Pod readiness gate"></a>Pod readiness gate</h2><p>這是在 v1.11 後開始推出一個稱為 <strong><a href="https://github.com/kubernetes/community/blob/master/keps/sig-network/0007-pod-ready%2B%2B.md">Pod ready++</a></strong> 的功能(在 v1.11 中還是 alpha)，目的是要透過開放讓額外的 feedback or signal 放到 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#podstatus-v1-core">Pod Status</a> 中，來強化 Pod readiness 的診斷功能。</p><p>有了這個功能後，就可以在 pod spec 中定義要額外診斷的 <strong>status.conditions</strong>(預設值為 <code>False</code>)，而 pod condition 的定義必須是以 key/value 的型式，以下是一個範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">readinessGates:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">conditionType:</span> <span class="string">&quot;www.example.com/feature-1&quot;</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">conditions:</span></span><br><span class="line">    <span class="comment"># 內建的 pod condition 資訊</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">Ready</span></span><br><span class="line">      <span class="attr">status:</span> <span class="string">&quot;True&quot;</span></span><br><span class="line">      <span class="attr">lastProbeTime:</span> <span class="literal">null</span></span><br><span class="line">      <span class="attr">lastTransitionTime:</span> <span class="number">2018-01-01T00:00:00Z</span></span><br><span class="line">    <span class="comment"># 額外的 pod condition 資訊</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">&quot;www.example.com/feature-1&quot;</span></span><br><span class="line">      <span class="attr">status:</span> <span class="string">&quot;False&quot;</span></span><br><span class="line">      <span class="attr">lastProbeTIme:</span> <span class="literal">null</span></span><br><span class="line">      <span class="attr">lastTransitionTime:</span> <span class="number">2018-01-01T00:00:00Z</span></span><br><span class="line">  <span class="attr">containerStatuses:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerID:</span> <span class="string">docker://abcd...</span></span><br><span class="line">      <span class="attr">ready:</span> <span class="literal">true</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>目前 <strong>kubectl patch</strong> 命令不支援變更 object status，所以要塞入額外的 pod condition 資訊必須使用 <a href="https://kubernetes.io/docs/reference/using-api/client-libraries/">k8s client library</a> 來完成。</p><p>因此有了 readinessGates 之後，只有在同時滿足以下兩個條件下，pod 才會被診斷為 ready：</p><ol><li><p>pod 中所有的 container 狀態皆為 Ready</p></li><li><p>所有在 pod spec 中定義的 ReadinessGates 的狀態皆為 True</p></li></ol><p>若要啟用這個功能，必須要在啟用 k8s 元件時，在 <strong><a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">–feature-gates</a></strong> 參數中把 <code>PodReadinessGates</code> 設定為 True。</p><h2 id="Restart-Policy"><a href="#Restart-Policy" class="headerlink" title="Restart Policy"></a>Restart Policy</h2><p>關於 restart policy 有幾個重點：</p><ul><li><p>有 <code>Always</code>(預設值), <code>OnFailure</code>, <code>Never</code> 三種不同的 restart policy</p></li><li><p>restart policy 套用的範圍是 pod 中的所有 container，而不是某一個</p></li><li><p>restart policy 僅會透過使用同一個 node 上的 kubelet 來重起 container</p></li><li><p>失敗的 container 會由 kubelet 重新啟動，但每次的延遲時間都會加長(10秒，20秒，40秒…)，延遲時間上限為 5 分鐘，並在 container 成功執行十分鐘後重置</p></li><li><p>一旦 pod 被綁定到特定的 node 後，就不會在綁定到另一個 node</p></li></ul><h2 id="Pod-Lifetime"><a href="#Pod-Lifetime" class="headerlink" title="Pod Lifetime"></a>Pod Lifetime</h2><p>一般來說，除非有使用者 or controller 介入，不然 pod 不會消失；而目前有三種手段可以來控制 pod 的存活：</p><ul><li><p>一次性的工作，使用 <a href="https://kubernetes.io/docs/concepts/jobs/run-to-completion-finite-workloads/">Job</a> resource object　來處理</p><blockquote><p>務必記得使用 Job 時，<strong>restartPolicy</strong> 必須設定為 <code>OnFailure</code> or <code>Never</code></p></blockquote></li><li><p>若希望 pod 不要被意外終止，使用 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">ReplicationController</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> 或是 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> 來確保 desired status 可以被維持</p><blockquote><p>ReplicationController 只有在 restartPolicy 設定為 <code>Always</code> 才會正常的運作</p></blockquote></li><li><p>若在每一台機器中都需要運行的 pod，使用 <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a></p></li></ul><p>總之，透過上面的 resource object 來控制 pod 的存活，不要單獨佈署 pod。</p><h2 id="Pod-Status-Restart-Policy-的情境範例"><a href="#Pod-Status-Restart-Policy-的情境範例" class="headerlink" title="Pod Status + Restart Policy 的情境範例"></a>Pod Status + Restart Policy 的情境範例</h2><p>這部份可以參考官網的 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states">Pod - Example states</a> 文件，有列出相當多的情境範例可參考，了解在不同的情境下，k8s 會做出的反應。</p><h1 id="Init-Container"><a href="#Init-Container" class="headerlink" title="Init Container"></a>Init Container</h1><h2 id="What-is-Init-Container"><a href="#What-is-Init-Container" class="headerlink" title="What is Init Container ?"></a>What is Init Container ?</h2><p>Init Container 是用來指定在 app container 開始運行之前(<strong>此時 pod network &amp; volume 資源都已經初始化完成</strong>)啟動起來完成某些特定的工作之用，因此 Init Container 也可能包含一些 app container image 中所沒有的工具或是 script。</p><p>一個 Pod 中能夠包含多個 app container 同時運行，同時也可以有多個 init container 來處理在 app container 開始運行前需要完成的特殊工作，而在 init container 工作執行完成之前，pod 的狀態都會顯示為 <strong>Initializing</strong>。</p><p>init container 跟一般的 container 幾乎相同，除了以下幾點：</p><ul><li><p>init container 總是會執行到結束為止</p></li><li><p>若定義多個 init container，會依序一個一個執行，且必須等到前一個 init container 執行成功後，才會輪到下一個</p></li></ul><blockquote><p>如果任何一個 init container 工作執行失敗，k8s 就會重新啟動 pod 並再度啟動所有的 init container 繼續執行工作，除非 <strong>restartPolicy</strong> 設定為 <code>Never</code></p></blockquote><h2 id="Init-Container-可以做哪些事情"><a href="#Init-Container-可以做哪些事情" class="headerlink" title="Init Container 可以做哪些事情 ?"></a>Init Container 可以做哪些事情 ?</h2><p>由於 init container 跟 app container 使用的是不同的 container image，因此在使用上會有一些優點，例如：</p><ul><li><p>可以打包一些特殊的工具並執行，而這些工具 &amp; 工作內容可能因為安全性的關係而沒有放進 app container 中</p></li><li><p>也可以包含一些常用的工具(例如：sed, awk, python, dig … 等等)，而為了這些簡單工具再包一個 container image 很沒必要</p></li><li><p>將 application image 的 builder &amp; deployer 分開，bulder 可以根據 init &amp; app container 的需要建立不同的 image，而不是將所有東西都打包在同一個 image 中，因此 deployer 的工作就會相對單純點</p></li><li><p>由於同一個 pod 的 container 中，mount namespace 還是分開的，因此 init container 可以設計用來存取 app container 不能存取的 secret</p></li><li><p>可提供一些簡單的方式來阻止 or 延緩 app container 啟動，直到某些條件達成為止</p></li></ul><p>以下是一些使用情境的介紹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 執行某個 shell command 直到 service 啟動為止</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..100&#125;; <span class="keyword">do</span> sleep 1; <span class="keyword">if</span> dig myservice; <span class="keyword">then</span> <span class="built_in">exit</span> 0; <span class="keyword">fi</span>; <span class="keyword">done</span>; <span class="built_in">exit</span> 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將這個 pod 註冊到遠端的 server</span></span><br><span class="line">curl -X POST http://<span class="variable">$MANAGEMENT_SERVICE_HOST</span>:<span class="variable">$MANAGEMENT_SERVICE_PORT</span>/register -d ‘instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)’</span><br><span class="line"></span><br><span class="line"><span class="comment"># 單純的等待一會兒</span></span><br><span class="line">sleep 60</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將某個 git repository 複製下來放到 volume 上</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 value(例如：POD_IP) 放到設定檔中，並執行特定的 template tool(例如：Jinja2) 來動態的產生給 app container 使用的設定檔</span></span><br></pre></td></tr></table></figure><h2 id="範例說明"><a href="#範例說明" class="headerlink" title="範例說明"></a>範例說明</h2><p>接著是一個簡單的使用範例參考：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myapp-pod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">myapp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">myapp-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;echo The app is running! &amp;&amp; sleep 3600&#x27;</span>]</span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">init-myservice</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&#x27;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">init-mydb</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&#x27;</span>]</span><br></pre></td></tr></table></figure><p>看到上述的範例可能有點模糊，不曉得要等待什麼 service，因此要搭配下面的 service 定義來看：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myservice</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9376</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mydb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">9377</span></span><br></pre></td></tr></table></figure><p>如此一來就很清楚了，只要當 service 設定完成，nslookup 可以找到對應的 IP 後，在 init container 中的回圈就會跳出 &amp; 結束工作</p><p>更多的使用範例可以參考以下網址：</p><ul><li><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#creating-a-pod-that-has-an-init-container">Configure Pod Initialization - Kubernetes</a></p></li><li><p><a href="https://blog.qikqiak.com/post/pod-init-container/">初始化容器-blog.qikqiak.com|阳明的博客|Kubernetes|Docker|Python|Golang|Cloud Native</a></p></li></ul><h2 id="使用上需注意的地方"><a href="#使用上需注意的地方" class="headerlink" title="使用上需注意的地方"></a>使用上需注意的地方</h2><ul><li><p>若要修改 init container 的設定，只能針對 image 的欄位進行修改，而且修改後會等同於 restart pod</p></li><li><p>因為 init container 可能會 restart, retired 或是 re-execute，因此在 init container 中的執行程式要確保達成 idempotent 的特性(重複執行會得到相同結果)</p></li><li><p>不支援 readiness probe</p></li><li><p>可透過 <strong>activeDeadlineSeconds</strong> &amp; <strong>livenessProbe</strong> 兩個方式來確保 init container 不會永遠的掛掉</p></li><li><p>init container 與 app container 不能設定相同的 name</p></li></ul><h2 id="Resource-管理"><a href="#Resource-管理" class="headerlink" title="Resource 管理"></a>Resource 管理</h2><p>若是使用者有設定 resource quota，init container 同樣也會受到影響，而資源限制的規則大概如下：</p><ul><li><p>init container 最上層的 resource request 就是有效的 <strong>init request/limit</strong></p></li><li><p>若 pod 的層級有設定有效的 request/limit，則優先權大於 app container request/limit 的總和 &amp; 有效的 init request/limit</p></li><li><p>scheduling 是根據有效的 request/limit 進行的，這表示 init container 可以用到所有 request/limit 所定義的資源 (因為此時 pod 還用不到任何資源)</p></li><li><p>pod 的 QoS tier 跟 init/app container 的 QoS tier 是相同的</p></li></ul><h1 id="Pod-Preset"><a href="#Pod-Preset" class="headerlink" title="Pod Preset"></a>Pod Preset</h1><h2 id="What-is-PodPreset"><a href="#What-is-PodPreset" class="headerlink" title="What is PodPreset ?"></a>What is PodPreset ?</h2><p>PodPreset 是個很有意思的 resource object，主要是用來在 pod 建立的時候，放入一些使用者自行額外定義的資訊，這些資訊可以是 <code>secret</code>, <code>volume</code>, <code>volume mount</code> 或是 <code>環境變數</code> …. 等等。</p><p>在什麼情況下需要使用 PodPreset 呢? 當你有一批透過相同 template 產生出來的 pod，但卻希望他們可以根據特定的資訊(例如：<strong>環境變數</strong>)來產生不一樣的行為時，就可以透過 <strong>PodPreset</strong> 搭配 <strong>label selector</strong> 將特定的資訊放到帶有特殊 key/value 的 label 的 pod 中。</p><h2 id="PodPreset-是如何運作的"><a href="#PodPreset-是如何運作的" class="headerlink" title="PodPreset 是如何運作的?"></a>PodPreset 是如何運作的?</h2><p>首先要知道，PodPreset 是 k8s <strong>admission controller</strong> 所提供的功能，因此要使用必須要在 kube-apiserver 啟動時把這個功能選項(<code>--enable-admission-plugins=PodPreset</code>)加入才會有，當此功能啟動後，k8s 就會在收到 pod creation request，運行以下流程：</p><ul><li><p>取得目前 namespace 中已經存在的 PodPreset</p></li><li><p>檢查是否有符合任何 PodPreset 中 label selector 的定義</p></li><li><p>若有符合的 PodPreset，就將 PodPreset 中定義的資訊合併到 pod 中</p></li><li><p>若是合併 PodPreset 資訊時發生錯誤，會拋出一個 merge error 並記錄，但並不影響 pod 的建立過程(只是沒包含 PodPreset 定義的資訊)</p></li><li><p>若成功，PodPreset 就會在 pod spec 中增加一筆 annotation 資訊，內容如下：</p><blockquote><p><strong>podpreset.admission.kubernetes.io/podpreset-&lt;pod-preset name&gt;: “&lt;resource version&gt;”</strong></p></blockquote></li></ul><p>由於 PodPreset 這個機制是搭配 label selector 完成的，因此 PodPreset 中定義的資訊可以同時套用到 0 個或多個 pod；反之多個 pod 也可以因為含有多個 label 而被多個 PodPreset 套用，因此兩者是多對多的關係。</p><p>最後還需要注意的是，若 PodPreset 中定義的資訊類型屬於 <strong>Env</strong>, <strong>EnvFrom</strong> 或是 <strong>VolumeMounts</strong>，則會被修改的是 container spec；但若是 <strong>Volume</strong>，則是 pod spec 被修改。</p><blockquote><p>PodPreset 對 init container 無效</p></blockquote><h2 id="如何啟用-PodPreset"><a href="#如何啟用-PodPreset" class="headerlink" title="如何啟用 PodPreset ?"></a>如何啟用 PodPreset ?</h2><p>要在 k8s cluster 啟動 PodPreset 的功能，需要在 <strong>kube-apiserver</strong> 啟動的參數中，額外加入兩組設定：()</p><ol><li><p><code>--runtime-config=xxx=true,yyy=true,zzz=true,settings.k8s.io/v1alpha1=true</code></p></li><li><p><code>--enable-admission-plugins=xxx,yyy,zzz,PodPreset</code></p></li></ol><p>接著就可以在需要使用此功能的 namespace 定義 PodPreset resource object 來使用了!</p><h2 id="使用範例"><a href="#使用範例" class="headerlink" title="使用範例"></a>使用範例</h2><ul><li><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/">Inject Information into Pods Using a PodPreset - Kubernetes</a></p></li><li><p><a href="https://blog.qikqiak.com/post/how-to-use-podpreset-in-kubernetes/">kubernetes PodPreset 的使用-blog.qikqiak.com|阳明的博客|Kubernetes|Docker|Python|Golang|Cloud Native</a></p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod Overview - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">Pod Lifecycle - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">Init Containers - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/#creating-a-pod-that-has-an-init-container">Configure Pod Initialization - Kubernetes</a></p></li><li><p><a href="https://blog.qikqiak.com/post/pod-init-container/">初始化容器-blog.qikqiak.com|阳明的博客|Kubernetes|Docker|Python|Golang|Cloud Native</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/podpreset/">Pod Preset - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/inject-data-application/podpreset/">Inject Information into Pods Using a PodPreset - Kubernetes</a></p></li><li><p><a href="https://blog.qikqiak.com/post/how-to-use-podpreset-in-kubernetes/">kubernetes PodPreset 的使用-blog.qikqiak.com|阳明的博客|Kubernetes|Docker|Python|Golang|Cloud Native</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Resource Object 概觀</title>
      <link href="/blog/Kubernetes/k8s-CoreConcept-ResourceObject-Overview/"/>
      <url>/blog/Kubernetes/k8s-CoreConcept-ResourceObject-Overview/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Resource-Object"><a href="#What-is-Resource-Object" class="headerlink" title="What is Resource Object?"></a>What is Resource Object?</h1><p>在 k8s 中，定義了各式各樣的 resource object(or <code>API object</code>)，每個 resource object 都是一個 persistent entity，用來表示目前 cluster 的特定狀態 &amp; 運作行為，例如：</p><ul><li><p>有哪些 containerized application 正在運行?</p></li><li><p>承上，在哪些 worker nodes 上運行?</p></li><li><p>這些 application 可用的 resource 為何?</p></li><li><p>application 運作時的規範，例如：restart policy, fault-tolerance</p></li></ul><p>此外，每個 resource object 都包含了 spec &amp; status 兩個部份，其中 spec 是用來描述 object 要以什麼樣的形式(規格 or 內容)呈現，而 status 則是用來描述這個 object 希望由 K8s 所維持的狀態(desired state)。</p><blockquote><p>關於 spec &amp; status 的細節，可以參考官網的 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/api-conventions.md">API Conventions</a></p></blockquote><h1 id="Kubernetes-提供了那些-Resource-Object-Type"><a href="#Kubernetes-提供了那些-Resource-Object-Type" class="headerlink" title="Kubernetes 提供了那些 Resource Object Type?"></a>Kubernetes 提供了那些 Resource Object Type?</h1><p>k8s 提供的 resource object 種類相當多，以下根據運用的類型作成以下四大分類：</p><table><thead><tr><th>Category</th><th>Resource Object Type Name</th></tr></thead><tbody><tr><td>Workload</td><td>Pod, HorizontalPodAutoscaler</td></tr><tr><td>Controller</td><td>ReplicaSet, ReplicationController, Deployment, StatefulSet, DaemonSet, Job, CronJob</td></tr><tr><td>Service Discovery</td><td>Service, Ingress</td></tr><tr><td>Authentication &amp; Authorization</td><td>ServiceAccount, RBAC(Role, ClusterRole, RoleBinding, ClusterRoleBinding)</td></tr><tr><td>Storage</td><td>Volume, PersistentVolume, StorageClass, Secret, ConfigMap</td></tr><tr><td>Policy</td><td>NetworkPolicy, SecurityContext, ResourceQuota, LimitRange</td></tr><tr><td>Extension</td><td>CustomResourceDefinitions</td></tr></tbody></table><p>從上面的分類可以看出，作為一個 multiple node 的 container orchestration platform，k8s 為了將 container 進行有效的管理，將很多管理概念抽象化，儘量隱藏底層複雜的實作細節，讓使用者可以根據自己的需求，專注在特定的 resource object 上，進而有效率的完成工作。</p><h1 id="Workload-類型的-Resource-Object"><a href="#Workload-類型的-Resource-Object" class="headerlink" title="Workload 類型的 Resource Object"></a>Workload 類型的 Resource Object</h1><p>以下針對各種不同的 Resource Object Type 作個簡單介紹：</p><h2 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h2><p>Pod 是 k8s 中的最小運作單位，要了解 Pod 是什麼，則必須看看官網的定義：</p><blockquote><p>A pod (as in a pod of whales or pea pod) is a group of one or more containers (such as Docker containers), with shared storage/network, and a specification for how to run the containers.</p></blockquote><p>從上述的定義可以知道幾件事情：</p><ol><li><p>一個 pod 中可以有一個或多個 container</p></li><li><p>在同一個 pod 的 container 共享相同的 storage &amp; network(因此在同一個 Pod 裡面的 containers，可以用 local port numbers 來互相溝通)</p></li><li><p>pod 的 spec 宣告中還必須包含 container 要如何運作</p></li></ol><h1 id="Controller-類型的-Resource-Object"><a href="#Controller-類型的-Resource-Object" class="headerlink" title="Controller 類型的 Resource Object"></a>Controller 類型的 Resource Object</h1><h2 id="ReplicationController"><a href="#ReplicationController" class="headerlink" title="ReplicationController"></a>ReplicationController</h2><p>用途是在確保同一時間內，有指定數量的 pod 在運行，可隨時進行 scale out/in。</p><p>在 k8s v1.2 後，就開始由 <strong>Deployment</strong> + <strong>ReplicaSet</strong> 取代，為了就是要準備支援在 v1.3 後出現的 Auto Scaling 的功能。</p><h2 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h2><p>可以將 ReplicaSet 視為進化版的 ReplicationController，其中兩者最大的差別在於 ReplicaSet 可以被更多種不同的 label selector 支援。</p><blockquote><p>ReplicationController 僅能支援 <strong>=</strong> 以及 <strong>!=</strong> 兩種，而 ReplicaSet 還支援了 chain &amp; subset 等更複雜的 selector 功能</p></blockquote><p>雖然 ReplicaSet 提供了更強大的 selector 功能，但不建議單獨使用，而是透過與 Deployment 來搭配使用。</p><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><p>Deployment 其實就是 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pod</a> + <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>，同樣的，使用者對 k8s 宣告 Deployment 的 desired status，接著 Deployment controller 就會在資源允許的範圍內幫你達成。</p><blockquote><p>請不要直接控制包在 Deployment 內部的 ReplicaSet，應該是要透過調整 Deployment 來改變實際 workload 運作的行為</p></blockquote><p>然而 Deployment 可以幫我們達成以下幾件事情：</p><ul><li><p>部署一個應用服務(application)，並確保維持在 desired status</p></li><li><p>將 applications 升級到某個特定版本，同時也可以降級到某個特定版本</p></li><li><p>zero downtime upgrade</p></li><li><p>當問題發生時，可以快速的 rollback</p></li></ul><p>當然 Deployment 的功能不只上面這些，之後有機會會用獨立篇幅來介紹。</p><h2 id="StatefulSets"><a href="#StatefulSets" class="headerlink" title="StatefulSets"></a>StatefulSets</h2><p>StatefulSets 是在 v1.9 後正式釋出的功能，顧名思義，主要功能就是作為 stateful application 的管理，例如：Database。</p><p>與 Deployment Pod 相比較之下，有些相同也有些不同的地方，首先說明相同的地方：</p><ul><li>與 Deployment 相同，都是透過 container spec 來描述 desired status</li></ul><p>而與 Deployment Pod 不同的地方則是：</p><ul><li><p>StatefulSets 會為每一個 pod 維護一個固定的識別資訊，而 Deployment 則不會</p></li><li><p>承上，StatefulSets pod 雖然都是從同樣的 spec 產生出來，但卻不是等價關係，因此無法互相替代對方的任務</p></li><li><p>每個 StatefulSets Pod 都有一個永久性的識別資訊，不會因為被 reschedule 而改變</p></li></ul><p>StatefulSet 的管理是由 k8s 中名稱為 StatefulSet Controller 的元件來負責。</p><h2 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h2><p>由 daemon controller 管理，確認每個 node 上都會有運行一個指定的 pod，而當有新的 node 加入到 cluster 中時，k8s 也會自動幫新的 node 加上一個 daemonset pod；若是 node 被移除時，當然這些 pod 也都會被刪除掉。</p><p>典型的應用如下：</p><ul><li><p>運行 storage daemon，例如: glusterd, ceph … 等等</p></li><li><p>運行蒐集 log 用的 daemon，例如: fluentd, logstash … 等等</p></li><li><p>運行監控用的 daemon，例如: <a href="https://github.com/prometheus/node_exporter">Prometheus Node Exporter</a>, collectd … 等等</p></li></ul><h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><p>Job 很直覺，跟 Linux 中的 <code>at</code> 是類似的概念。</p><p>而在 k8s 中，job 會建立一個 or 多個 pod，並確保有特定數量的 pod 執行成功，條件達成後，job 就會進入完成狀態，此時 job 以及相關的 pod 都會被刪除。</p><h2 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h2><p>跟 Linux 相同，CronJob 也是在特定的時間 or 週期性的特定時間運行 job；在使用上，時間格式的定義也是與 Linux 相同。</p><p>典型的運用像是資料庫的備份、發送郵件 … 等等</p><h2 id="TTL-Controller"><a href="#TTL-Controller" class="headerlink" title="TTL Controller"></a>TTL Controller</h2><p>k8s 可以讓使用者執行很多一次性的工作，但執行完成後，即使過程中的 log 或是 resource object 本身都已經不再需要了，它們都還是會一直留在 k8s 上，於是就會看到很多狀態為 Complete(or Failed) 的 resource object，久而久之為了環境不要過度雜亂，可能一段時間就要清理一次，不論是手動 or 寫 script 完成，但其實這是一件挺麻煩的事情。</p><p>而在 v1.12 之後，k8s 新增了 TTL controller 這個新功能，目的就是透過限制已經執行完成的 resource object 的生命周期，藉此來達成自動清理的目的；這個設計其實是針對多種不同的 resource object，但在 v1.12 中，僅支援 Job 而已，未來可能或擴充支援到 Pod or Custom Resource。</p><blockquote><p>TTL 在 v1.12 版本中仍是 alpha 而已，且必須透過啟用 <code>TTLAfterFinished</code> feature gate 才可使用</p></blockquote><h1 id="Service-Discovery-類型的-Resource-Object"><a href="#Service-Discovery-類型的-Resource-Object" class="headerlink" title="Service Discovery 類型的 Resource Object"></a>Service Discovery 類型的 Resource Object</h1><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>Service 是為了讓<strong>應該要送到 pod 處理的網路流量可以正確的送達</strong>而抽象出來的一個概念，為了達到此目的，需要具備幾個條件：</p><p>DNS 服務 (沒人會想用 IP 存取服務)</p><p>網路服務 (kube-proxy 在 pod 上層提供了一個 Load Balance 的服務)</p><p>以下是一個典型的 service 應用：</p><p><img src="/blog/images/kubernetes/k8s-service.svg" alt="Kubernetes Service Example"></p><ul><li><p>每個 service 會帶有一個 VIP</p></li><li><p>在 cluster 內部可以透過 <strong>&lt;service_name&gt;.&lt;namespace&gt;.cluster.local</strong> 這個 domain name 存取到</p></li><li><p>若要在 cluster 外部存取 service，則必須將 service 的 type 設定為 <strong>NodePort</strong> or <strong>LoadBalancer</strong>(只有在公有雲有效)</p></li><li><p>service 的流量要導到那些 pod，是用 Label 來定義</p></li><li><p>kube-proxy 會主動產生相對應的 iptables rule，讓 traffic 可以平均的分流到指定的 pod</p></li></ul><h2 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h2><p>k8s 在沒有 Ingress 之前，要從外部存取 k8s 的 service，僅能透過 <strong>NodePort</strong> 的方式來達成(在公有雲可以使用 <strong>LoadBalancer</strong>)，架構大概如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  internet</span><br><span class="line">      |</span><br><span class="line">------------</span><br><span class="line">[ Services ]</span><br></pre></td></tr></table></figure><p>Ingress 在 v1.1 後出現，目的就是要解決外部存取 k8s service 的問題，因此架構變成如下圖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> internet</span><br><span class="line">     |</span><br><span class="line">[ Ingress ]</span><br><span class="line">--|-----|--</span><br><span class="line">[ Services ]</span><br></pre></td></tr></table></figure><p>Ingress 負責的事情主要被定義為下面幾項：</p><ul><li><p>give services externally-reachable urls</p></li><li><p>load balance traffic</p></li><li><p>SSL Termination</p></li><li><p>offer name based virtual hosting</p></li></ul><h1 id="權限管理類型的-Resource-Object"><a href="#權限管理類型的-Resource-Object" class="headerlink" title="權限管理類型的 Resource Object"></a>權限管理類型的 Resource Object</h1><h2 id="Service-Account"><a href="#Service-Account" class="headerlink" title="Service Account"></a>Service Account</h2><p>在 k8s 中一個 pod 可以正常開始運作與否，取決於這個 pod 有沒有足夠的權限，在 k8s 中每個 pod 運作時的權限，則是以 service account 這個 resource object 來表示。</p><p>但在 k8s 中，真正決定 pod 可不可以被 schedule 到 worker node 的是 API server，而要跟 API server，必須要有合法的 token，從哪裡來呢?</p><p>首先要知道幾件事情：</p><ul><li><p>每個 pod 都會屬於特定的 namespace</p></li><li><p>每個 namespace 在建立之初，會有一個名稱為 <strong>default</strong> 的 service account 會同時被建立</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get serviceaccount</span><br><span class="line">NAME      SECRETS   AGE</span><br><span class="line">default   1         18h</span><br></pre></td></tr></table></figure><ul><li>每個 service account 會帶有一個 <code>secret</code> resource object</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe serviceaccount/default</span><br><span class="line">Name:                default</span><br><span class="line">Namespace:           default</span><br><span class="line">Labels:              &lt;none&gt;</span><br><span class="line">Annotations:         &lt;none&gt;</span><br><span class="line">Image pull secrets:  &lt;none&gt;</span><br><span class="line">Mountable secrets:   default-token-zmx94</span><br><span class="line">Tokens:              default-token-zmx94</span><br><span class="line">Events:              &lt;none&gt;</span><br></pre></td></tr></table></figure><ul><li>上述的 secret 帶有與 API server 認証用的 token 資訊</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe secret/default-token-zmx94</span><br><span class="line">Name:         default-token-zmx94</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name=default</span><br><span class="line">              kubernetes.io/service-account.uid=b9881486-abf6-11e8-9824-66712a0dd587</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">ca.crt:     1090 bytes</span><br><span class="line">namespace:  7 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tem14OTQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImI5ODgxNDg2LWFiZjYtMTFlOC05ODI0LTY2NzEyYTBkZDU4NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.l8xziA1DiH_u2UUSPmbo3Gry5DqjRn9qHG6SAiu83BkHmWNXaY_K1-55Ta_Uq3m1I2LCkMxCmjL_Aoau_efHLMqbT7nCJHuG78r9mVPOuy2OyhFLPniKE3-QteY_53smOFV5p-DathRP1zzbdzsiasigjKOlLn_2TDFMcy9ZvAyOzm20MqiQE31CVrdiMWPOD3pl0AerWnRkEsoxdIJ70RK8pc0Z65FQ3uILCrGSMlwr5FPl_cxw6nj2yTm7UeONPvHskIG2uzedUi9dX7LsnXj__SorWizuFc1G8aN3RyzXUgstXo0h6x_tOwINAk7f9LHPBhQJ7zBpWGNC3hxR8w</span><br></pre></td></tr></table></figure><p>因此藉由上面的說明，可以了解 namespace, pod, service account, secret, token, 與 API server 之間的關係</p><h2 id="RBAC-機制"><a href="#RBAC-機制" class="headerlink" title="RBAC 機制"></a>RBAC 機制</h2><p>RBAC 是目前 k8s 預設會啟動的 authorization module。</p><p>在 RBAC API 中定義了 resource target，用來描述使用者以及 resource 之間的權限關係：</p><ul><li><p><strong>Role</strong>：定義在特定 namespace 下的 resource 的存取權限</p></li><li><p><strong>RoleBinding</strong>： 設定哪些使用者(or service account)與 role 綁定而擁有存取權限</p></li><li><p><strong>ClusterRole</strong>：定義在整個 k8s cluster 下的 resource 的存取權限</p></li><li><p><strong>ClusterRoleBinding</strong>：設定哪些使用者(or service account)與 role 綁定而擁有存取權限</p></li></ul><p>了解 RBAC 的運作機制，在使用一些 k8s extension(例如：Helm) 的時候會有幫助，可以了解這些 extension 是如何在 k8s 中取得合法的運作權限。</p><h1 id="Storage-管理類型的-Resource-Object"><a href="#Storage-管理類型的-Resource-Object" class="headerlink" title="Storage 管理類型的 Resource Object"></a>Storage 管理類型的 Resource Object</h1><h2 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h2><p>由於 container 的生命周期是短暫的，並且可能會隨時被 reschedule 到其他的 node 上面運行，這看似資源有效的被管理的前提下，同時也衍生出了 application 資料儲存的問題，而 <strong>volume</strong> 就是在 k8s 中被提出來的一個抽象概念，要來解決資料儲存的問題，並隱藏底層複雜的 storage 相關細節。</p><p>類似於 Docker 中的 volume 概念，k8s 支援的 volume 有相當多種，例如：hostPath, iSCSI, NFS, GlusterFS, Ceph … 等等，詳細清單可以參考<a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes">官網的列表</a>。</p><h2 id="PersistentVolume-amp-PersistentVolumeClaim"><a href="#PersistentVolume-amp-PersistentVolumeClaim" class="headerlink" title="PersistentVolume &amp; PersistentVolumeClaim"></a>PersistentVolume &amp; PersistentVolumeClaim</h2><p>有了 volume 被設計出來的目的後，接著進入到在 k8s 實作的細節；由於 storage 的管理一直是一個獨立且複雜的課題這個部份，因此k8s 提出了 <strong>PersistentVolume(PV)</strong> &amp; <strong>PersistentVolumeClaim(PVC)</strong> 兩個抽象概念來實現永久性資料的儲存，藉由提供標準的 API，讓 storage 的管理與應用抽象化。</p><p>其中，PersistentVolume(簡稱 <code>PV</code>) resource 在概念上可以視為可讓 pod 掛載的 storage，後面的實作(NFS, iCSCI, RBD … etc)都已經被標準的 API 隱藏，管理者可以透過標準的宣告方式佈署所需要的 storage resource；而 PersistentVolume 的 lifecycle 會跟著使用 PersistentVolume 的 pod 走，當所有 pod 都消失，PersistentVolume resource 也會跟著消失，但儲存在上面的資料依然會存在。</p><p>PersistentVolumeClaim(簡稱 <code>PVC</code>) resource 則是來自於使用者的 request，概念上類似 pod，只是 pod 使用 worker node 上的運算資源，可限定 CPU &amp; Memory；而 PersistentVolumeClaim 使用的則是 PV，可限定 storage 的 size &amp; access mode(read/write or read-only)。</p><p>然而其實 pod, PV, PVC 之間的關係是這樣的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pod &lt;---&gt; PersistentVolumeClaim(PVC) &lt;---&gt; PersistentVolume(PV)</span><br></pre></td></tr></table></figure><ol><li><p>Pod 中定義要使用哪個 PVC，其中包含了一些 storage requirement</p></li><li><p>PVC 在 k8s cluster 中尋找可用的 PV 並掛載</p></li></ol><p>透過以上的分工，讓 Administrator &amp; Developer 的權責可以劃分的很清楚，而 Developer 也不需要了解底層 storage 設定的細節，就像以下這張圖：</p><p><img src="/blog/images/kubernetes/k8s_pod-pvc-pv.png" alt="Kubernetes - Pod, PVC, PV"></p><h2 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h2><p>PV + PVC 解決了永久性資料儲存的問題，但同時也帶來了以下問題：</p><ul><li><p>如果我常常需要產生 PV，又要刪除它呢 ?</p></li><li><p>可以不要一直去煩 storage manager 嗎? 每次產生新的 PV 都需要請他建立對應的 RBD image</p></li><li><p>產生 PV 的動作可以自動完成嗎 ?</p></li><li><p>每次都要手動建立 RBD image 哪有 cloud native ?</p></li></ul><p>而 Storage Class 就是以上問題的解答，簡單的概念圖如下：</p><p><img src="/blog/images/kubernetes/k8s_storageclass.png" alt="Kubernetes - StorageClass"></p><p>而 Storage Class 的功能就是要<code>動態</code> &amp; <code>自動</code>的把上述的事情給自動化，其詳細的運作流程如下圖：</p><p><img src="/blog/images/kubernetes/dynamic-volume-provision.jpg" alt="Kubernetes Persistent Volume Provisioning"></p><ol><li><p>設定 persistent volume provisioner</p></li><li><p>k8s cluster 管理者建立 Storage Class，並指定要使用的 PV provisioner</p></li><li><p>使用者建立 PVC，指定要使用的 Storage Class</p></li><li><p>Storage Class 使用 provisioner 在實際的 storage 上產生 volume，並建立 PV 與其繫結</p></li><li><p>將 PV 與 使用者 PVC 進行繫結</p></li><li><p>使用者建立 pod，並使用 PVC 取得外部的儲存空間</p></li></ol><blockquote><p><strong>以上的重點在於，管理者再也不用辛苦的手動建立儲存空間，並設定 PV 與其繫結了，這個部份都交給 StorageClass 自動處理</strong></p></blockquote><h2 id="Secret"><a href="#Secret" class="headerlink" title="Secret"></a>Secret</h2><p>Secret 是用來解決在 k8s 中密碼、token，key 或是任何敏感資訊的管理問題，使用者不用把這些敏感資訊定義在 image or pod 的 spec 設定中，而是透過 volume 掛載的方式或是環境變數的方式在 pod 中使用。</p><p>上面提到在 service account 中，就是利用了 secret 來儲存與 API server 溝通用的 token；而一般使用者也可以定義 <strong>Opaque</strong> 類型(以 base64 的方式編碼)的 secret 來儲存需要隱藏的敏感性資料。</p><h2 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h2><p>所有的 application 在開發上，總是會遇到設定檔(configuration)處理的問題，最典型的就是在不同的環境中(testing, staging, production)需要連結到不同的資料庫，當然也就會有不同的資料庫連線資訊。</p><p>若是將 configuration 打包到 container image 中，那有多少個環境就要打包多少個 image，其實這是相當沒有效益的，因此在 k8s 中，提出了 <strong>ConfigMap</strong> 的概念來解決這個問題。</p><p>ConfigMap 可以包含的資訊其實沒什麼太多限制，可以保存單一屬性，也可以是完整的 configuration 內容，或是一個 json 格式的資料也都可以，儲存的方式是以 key/value 的型式；使用上類似 secret，若是處理非機密資料的話是相當合適的。</p><h1 id="Policy-類型的-Resource-Object"><a href="#Policy-類型的-Resource-Object" class="headerlink" title="Policy 類型的 Resource Object"></a>Policy 類型的 Resource Object</h1><h2 id="NetworkPolicy"><a href="#NetworkPolicy" class="headerlink" title="NetworkPolicy"></a>NetworkPolicy</h2><p>Network Policy 定義了 pod 之間是否可以互相通訊，以及到各個不同的 endpoint 之間是否可以互相通訊。而 policy 的套用同樣也是透過 label selector 來篩選。</p><h2 id="SecurityContext"><a href="#SecurityContext" class="headerlink" title="SecurityContext"></a>SecurityContext</h2><p>當我們要在 k8s 使用可能不受信任的 container image 時，可能會希望限制該 pod or container 可以運作的權限 or 範圍，而在 k8s 中，Security Context 就是用來定義這樣的限制，來確保他容器不受其影響 &amp; 保護系統。</p><p>在 k8s 中提供了三種配置 Security Context 的方法：</p><ul><li><p>Container-level Security Context：僅套用到指定的容器</p></li><li><p>Pod-level Security Context：套用到 Pod 內所有容器以及 Volume</p></li><li><p>Pod Security Policies（PSP）：套用到 k8s cluster 內部所有 Pod 以及 Volume</p></li></ul><h2 id="ResourceQuota"><a href="#ResourceQuota" class="headerlink" title="ResourceQuota"></a>ResourceQuota</h2><p>當一個 k8s cluster 中同時有多個使用者 or 團隊同時使用 &amp; 硬體資源有限時，ResourceQuota 就是管理者可以用來控制資源利用量的工具。</p><p>ResourceQuota 的套用是以 namespace 為單位，預設是沒有任何 quota 的，而每個 namespace 也只能有一個 ResourceQuota 物件來管理資源的使用量，目前可管理的資源類型有以下三種：</p><ul><li><p><strong>Compute Resource</strong>: 這個包含 CPU &amp; Memory</p></li><li><p><strong>Storage</strong>: 可用來限制每個 namespace 可用的儲存空間</p></li><li><p><strong>Object Count</strong>: 在 v1.9 正式釋出的功能，主要用來限制特定 resource object type 的數量，例如：<code>count/persistentvolumeclaims</code>, <code>count/deployments.apps</code></p></li></ul><h2 id="LimitRange"><a href="#LimitRange" class="headerlink" title="LimitRange"></a>LimitRange</h2><p>LimitRange 跟 ResourceQuota 同樣也是用來限制資源使用量，但 LimitRange 主要是用來設定資源申請的上下限範圍限制的預設值。</p><p>兩種資源控制的機制都是以 namespace 為單位，其中 <code>ResourceQuota</code> 是用來設定在 namespace 所有 pod 佔用資源的 request &amp; limit，而 <code>LimitRange</code> 則是用來設定 namespace 中 pod 的預設使用資源的 request &amp; limit。</p><h1 id="Extension-類型的-Resource-Object"><a href="#Extension-類型的-Resource-Object" class="headerlink" title="Extension 類型的 Resource Object"></a>Extension 類型的 Resource Object</h1><h2 id="CustomResourceDefinitions"><a href="#CustomResourceDefinitions" class="headerlink" title="CustomResourceDefinitions"></a>CustomResourceDefinitions</h2><p>k8s 原生的 resource type 有 pod, deployment, daemonset, service … 等等，但若這些 resource type 並不完全符合使用者需求時，使用者可以自行定義所謂的 Custom Resource &amp; 相對應的 controller 來進行進階的 container 應用，而在操作使用上同樣也可以透過 kubectl 來進行管理。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod Overview - Kubernetes</a></p></li><li><p><a href="https://ithelp.ithome.com.tw/articles/10193232">[Day 5] 在 Minikube 上跑起你的 Docker Containers - Pod &amp; kubectl 常用指令 - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天</a></p></li><li><p><a href="https://ithelp.ithome.com.tw/articles/10194152">[Day 8] 還在用Replication Controller嗎？不妨考慮Deployment - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployments - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Jobs - Run to Completion - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Configure Service Accounts for Pods - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/">Managing Service Accounts - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">Using RBAC Authorization - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">Configure a Pod to Use a ConfigMap - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Network Policies - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Configure a Security Context for a Pod or Container - Kubernetes</a></p></li><li><p><a href="https://github.com/feiskyer/kubernetes-handbook/blob/master/zh/concepts/security-context.md">Security Context 和 Pod Security Policy· feiskyer/kubernetes-handbook</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">Resource Quotas - Kubernetes</a></p></li><li><p><a href="https://jimmysong.io/posts/kubernetes-resourcequota-limitrange-management/">Kubernetes中的ResourceQuota和LimitRange配置资源限额 - 宋净超的博客|Cloud Native Developer Advocate|jimmysong.io</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources - Kubernetes</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] 如何管理 Resource Object</title>
      <link href="/blog/Kubernetes/k8s-CoreConcept-Wokring-with-Objects/"/>
      <url>/blog/Kubernetes/k8s-CoreConcept-Wokring-with-Objects/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Kubernetes-Resource-Object"><a href="#What-is-Kubernetes-Resource-Object" class="headerlink" title="What is Kubernetes Resource Object?"></a>What is Kubernetes Resource Object?</h1><p>k8s object 是存在於 k8s 系統中的 persisten entity，用來描述 k8s cluster 中的狀態，例如：</p><ul><li><p>那一個 containerized application 正在運作?</p></li><li><p>承上，在哪些 worker nodes 上運行?</p></li><li><p>那些資源是可提供給 application 使用</p></li><li><p>套用在這些 application 上的規則，例如：restart policy, upgrade, fault-tolerance</p></li></ul><p>每個 Object 都包含了 <strong>spec</strong> &amp; <strong>status</strong> 兩個部份，其中 spec 是用來描述 object 要以什麼樣的形式(規格 or 內容)呈現，而 status 則是用來描述這個 object 希望由 K8s 所維持的狀態(<strong>desired state</strong>)。</p><p>以下舉個 deployment 的例子來說明：(可透過 YAML or JSON 呈現)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定使用那個版本的 API 來建立此 object</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="comment"># 建立什麼樣的 resource object</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="comment"># 用來辨識此物件的資訊，可能包含&quot;name&quot;, &quot;UID&quot;, &quot;namespace&quot; .... 等資訊</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line"><span class="comment"># spec 則是用來描述物件被生成的細節</span></span><br><span class="line"><span class="comment"># 每個不同的 resource object 都有不同的 spec 定義</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><blockquote><p>若是透過 <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#create">kubectl create -f FILENAME</a> 建立物件，檔案的資料格式可以是 JSON or YAML 格式；但若是呼叫 API 直接建立物件，就只能使用 JSON 格式資料</p></blockquote><p>上面是一個 deployment object 的定義，其中的 spec 部份則清楚的定義這個 deployment 的規格(使用 nginx:1.7.9 作為 image 來運行 container)；而 status 則是希望有 3 份 replica，因此 K8s 會協助維持 3 份 replica 的狀態。</p><p>在前一篇文章提到，在 K8s 上各式各樣(<strong>kind</strong>)的 resource，其實就是以一個一個 object 存在於 K8s ecosystem 中，而每個 object 都各自擁有自己的 spec &amp; status。</p><p>以下提供兩種 resource object 的 spec 定義文件提供參考：</p><ul><li><p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#deploymentspec-v1-apps">Deployment</a></p></li><li><p><a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.11/#podspec-v1-core">Pod</a></p></li></ul><h1 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h1><h2 id="What-is-Node"><a href="#What-is-Node" class="headerlink" title="What is Node?"></a>What is Node?</h2><p>Node 在 k8s cluster 中的一個 worker node，以前稱為 <code>minion</code>，可以是實體機 or VM。在每個 node 中都包含數個服務元件，用來管理 pod 在上面運作的行為 &amp; 狀態，這些管理用的 service 包含以下幾個：</p><ul><li><p>container time: 可以是 <a href="https://www.docker.com/">Docker</a>, <a href="https://coreos.com/rkt/">rkt</a>, <a href="https://github.com/opencontainers/runc">runc</a> 或是其他任何符合 <a href="https://github.com/opencontainers/runtime-spec">OCI 規範</a>的 container runtime</p></li><li><p>kubelet: 負責接收來自 API server 的命令、維護 container 的生命周期，並負責 CSI(Container Storage Interface) &amp; CNI(Container Network Interface) 的管理</p></li><li><p>kube-proxy: 負責為 service 提供 cluster 內部的 service discovery &amp; load balance</p></li></ul><h2 id="Node-Status"><a href="#Node-Status" class="headerlink" title="Node Status"></a>Node Status</h2><p>為了可以讓 master 確保每個 node 是否正常運作，Node 本身就要自己回報狀態資訊，包含以下內容：</p><h3 id="Address"><a href="#Address" class="headerlink" title="Address"></a>Address</h3><ul><li><p>HostName: 由系統的 kernel 主動回報的 資訊，可以被 kubelet 中的 <code>--hostname-override</code> 參數取代</p></li><li><p>ExternalIP：可被 cluster 外部存取的 IP</p></li><li><p>InternalIP: cluster 內部的 IP，無法從外部存取</p></li></ul><h3 id="Condition"><a href="#Condition" class="headerlink" title="Condition"></a>Condition</h3><p>以下的狀態包含了所有用來描述運行中的 node 的狀態：</p><ul><li><p>OutOfDisk: <code>True</code> 表示沒有足夠的磁碟空間可以正常運作</p></li><li><p>Ready: <code>True</code> 表示可正常運作並接受 pod scheduling，<code>False</code> 則表示不健康，<code>Unknown</code> 表示 node 沒有在 <strong>node-monitor-grace-period</strong>(預設 40 秒) 限制時間內主動回報訊息給 node controller</p></li><li><p>MemoryPressure: <code>True</code> 表示剩餘記憶體很低</p></li><li><p>PIDPressure: <code>True</code> 表示存在太多 process</p></li><li><p>DiskPressure: <code>True</code> 表示磁碟空間所剩無幾</p></li><li><p>NetworkUnavailable: <code>True</code> 表示 node 網路沒有正確設定</p></li><li><p>ConfigOK: <code>True</code> 表示 kubelet 有被正確的設定好</p></li></ul><h3 id="Capacity"><a href="#Capacity" class="headerlink" title="Capacity"></a>Capacity</h3><p>用來描述 node 上資源的可用程度，包含：</p><ul><li><p>CPU</p></li><li><p>Memory</p></li><li><p>可同時運行的 Pod 數量上限</p></li></ul><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><p>關於 Node 的一般資訊，由 kubelet 蒐集，例如：</p><ul><li><p>OS name</p></li><li><p>kernel version</p></li><li><p>Kubernetes version (包含 kubelet &amp; kube-proxy)</p></li><li><p>Docker version (如果安裝的 container runtime 是 Docker)</p></li></ul><h1 id="Name"><a href="#Name" class="headerlink" title="Name"></a>Name</h1><p>每個在 K8s cluster 上的 object 都可以用 Name or UID 來識別，而在同一時間，每一種 <strong>kind(resource type)</strong> 的 object name 不能重複，例如：不能有兩個 name 同為 nginx 的 pod。</p><p>而每個 resource 都可以透過 API 進行存取，以 pod 為例，resource url 可能像是 <code>/api/v1/pods/some-name</code>。當然，為 object 取名也是有學問的，若是要研究此部份可參考<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/identifiers.md">官方文件</a>。</p><h1 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h1><h2 id="What-is-namespace"><a href="#What-is-namespace" class="headerlink" title="What is namespace ?"></a>What is namespace ?</h2><p>假設有很多不同的使用者同時使用相同的 K8s cluster，K8s 提供了 <code>namespace</code> 讓一個 K8s cluster 上可區分出多個不同的 virtual cluster。</p><blockquote><p>通常用來區分不同 team 的 user 或是不同的專案</p></blockquote><p>大部分的 resource 都會隸屬於某個 namespace，不同 namespace 的 resource name 可以相同，每個使用者可以自訂一個自己的 namespace，並在此 namespace 中自訂自己的 resource。</p><blockquote><p>有些特殊的 resource(e.g., node, persistent volume) 不屬於任何 namespace</p></blockquote><p>而管理者也可以透過 <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">resource quota</a> 來定義每個 namespace 可使用的實體資源總量，亦可以 namespace 為單位設定 ccontrol policy。</p><p>而若是要在同一個 namespace 為不同的 resource 進行分門別類，則可搭配 <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Label</a> 來完成。</p><h2 id="檢視-cluster-中的-namespace"><a href="#檢視-cluster-中的-namespace" class="headerlink" title="檢視 cluster 中的 namespace"></a>檢視 cluster 中的 namespace</h2><p>此外，namespace 其實也是 K8s resource 的一種，只是透過不同 resource 的搭配，可以達成資源隔離的效果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get ns</span><br><span class="line">NAME          STATUS    AGE</span><br><span class="line">default       Active    1d</span><br><span class="line">kube-system   Active    1d</span><br><span class="line">kube-public   Active    1d</span><br></pre></td></tr></table></figure><p>當 K8s 安裝完成後，預設就會有以下 3 個 namespace：</p><ul><li><p><strong>default</strong>：若沒有指定 namespace 的 resource object 則會被歸類在 default</p></li><li><p><strong>kube-system</strong>：這個 namespace 內的 resource object 則都是屬於 K8s 系統本身所擁有的</p></li><li><p><strong>kube-public</strong>：在這個 namespace 中的 resource object 是可以被所有使用者所存取(visible &amp; readable)的(也包含未認証的使用者)，通常是在 cluster level 被使用</p></li></ul><h2 id="使用-namespace"><a href="#使用-namespace" class="headerlink" title="使用 namespace"></a>使用 namespace</h2><p>使用 namespace 的方式也很簡單，以下是幾個範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得指定 namespace 的 pod 清單</span></span><br><span class="line">$ kubectl --namespace=&lt;insert-namespace-name-here&gt; get pods</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改使用 kubectl 時預設的 namespace (若不修改，預設為 default)</span></span><br><span class="line">$ kubectl config set-context $(kubectl config current-context) --namespace=&lt;insert-namespace-name-here&gt;</span><br></pre></td></tr></table></figure><h2 id="namespace-amp-DNS-的關係"><a href="#namespace-amp-DNS-的關係" class="headerlink" title="namespace &amp; DNS 的關係"></a>namespace &amp; DNS 的關係</h2><p>當 <a href="https://kubernetes.io/docs/concepts/services-networking/service/">service</a> 被建立後，為了讓 cluster 內部可以透過 domain name 存取 service，相對應的 <a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">DNS entry</a> 也同時會被建立，而這個 DNS entry 的格式如下：</p><blockquote><p>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</p></blockquote><p>其中 <code>cluster.local</code> 是當初建立 cluster 的時候給入的設定，可以修改</p><blockquote><p>若是在同一個 namespace 中的 pod，可直接透過 <strong><service-name></strong> 就存取到 service，不需指定完整的 FQDN</p></blockquote><h2 id="不屬於任何-namespace-的-resource"><a href="#不屬於任何-namespace-的-resource" class="headerlink" title="不屬於任何 namespace 的 resource"></a>不屬於任何 namespace 的 resource</h2><p>雖然大部分的 resource 都有所屬的 namespace，但也有一些 resource 是例外，通常這些都是 cluster level 的 resource，例如：<strong>namespace</strong>、<strong>ClusterRole</strong>、<strong>ClusterRoleBinding</strong> …. 等等。</p><p>詳細的清單可以透過以下指令查詢：</p><blockquote><p>kubectl api-resources –namespaced=false</p></blockquote><h1 id="Labels-and-Selectors"><a href="#Labels-and-Selectors" class="headerlink" title="Labels and Selectors"></a>Labels and Selectors</h1><h2 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h2><p>Label 是個 key/value 的組合，使用者可以隨意為 object 賦予附加上自訂的 label(一個 or 多個)，作為每個 object 的屬性，來達成類似群組 or 分類的效果；此外，還可以透過 selector 進行 object 的選取。</p><blockquote><p>在 k8s 中有個 <strong>annotation</strong> 的機制，同樣也是 key/value 的型式 &amp; 為 object 增加不同的屬性資料，但 annotation 可乘載的資料較為大量，甚至可以是結構性的資料或是一段 script，但也就無法透過 select 來進行屬性的過濾</p></blockquote><p>以下是幾個 Label 設定的範例：</p><ul><li><p>“<strong>release</strong>“ : “stable”, “<strong>release</strong>“ : “canary”</p></li><li><p>“<strong>environment</strong>“ : “dev”, “<strong>environment</strong>“ : “qa”, “<strong>environment</strong>“ : “production”</p></li><li><p>“<strong>tier</strong>“ : “frontend”, “<strong>tier</strong>“ : “backend”, “<strong>tier</strong>“ : “cache”</p></li><li><p>“<strong>partition</strong>“ : “customerA”, “<strong>partition</strong>“ : “customerB”</p></li><li><p>“<strong>track</strong>“ : “daily”, “<strong>track</strong>“ : “weekly”</p></li></ul><blockquote><p>一個 object 可以有多個 label，而同樣的 label 設定可以套在不同的 object 上面，因此 label &amp; resource object 之間其實是多對多的關係，然後仰賴後續介紹的 selector 來進行 resource 的過濾</p></blockquote><h2 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h2><p>select 顧名思義，就是用來過濾 label 之用的，那要如何使用呢? 以下也有幾個 cli 範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -l environment=production,tier=frontend</span><br><span class="line"></span><br><span class="line">$ kubectl get pods -l <span class="string">&#x27;environment in (production),tier in (frontend)&#x27;</span></span><br><span class="line"></span><br><span class="line">$ kubectl get pods -l <span class="string">&#x27;environment in (production, qa)&#x27;</span></span><br><span class="line"></span><br><span class="line">$ kubectl get pods -l <span class="string">&#x27;environment,environment notin (frontend)&#x27;</span></span><br></pre></td></tr></table></figure><p>若是透過 YAML 宣告：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">selector:</span></span><br><span class="line">  <span class="attr">matchLabels:</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">matchExpressions:</span></span><br><span class="line">    <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">tier</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">cache</span>]&#125;</span><br><span class="line">    <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">environment</span>, <span class="attr">operator:</span> <span class="string">NotIn</span>, <span class="attr">values:</span> [<span class="string">dev</span>]&#125;</span><br></pre></td></tr></table></figure><p>最後，也可以指定 schedule pod 到特定的 node。(使用 <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">nodeSelector</a>)</p><p>以下有個簡單範例，將 workload 丟到有強力顯示卡的機器上執行：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將這個 pod schedule 到帶有 &quot;accelerator=nvidia-tesla-p100&quot; 上的 node 上執行</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cuda-test</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cuda-test</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">&quot;k8s.gcr.io/cuda-vector-add:v0.1&quot;</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">limits:</span></span><br><span class="line">          <span class="attr">nvidia.com/gpu:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">accelerator:</span> <span class="string">nvidia-tesla-p100</span></span><br></pre></td></tr></table></figure><h1 id="進階的-Label-用法"><a href="#進階的-Label-用法" class="headerlink" title="進階的 Label 用法"></a>進階的 Label 用法</h1><h2 id="針對-Application-管理的-Label-使用方式"><a href="#針對-Application-管理的-Label-使用方式" class="headerlink" title="針對 Application 管理的 Label 使用方式"></a>針對 Application 管理的 Label 使用方式</h2><p>除了 kubectl &amp; dashboard 之外，若想要視覺化的管理 k8s object，透過套用 common set label 來為不同的 object 增加更多的 metadata &amp; 管理資訊是相當有用的，特別是對那些想自己開發管理工具的人來說。</p><p>k8s 原生就不是一個 PaaS，因此在設計上就不是以 <strong>Application</strong> 的角度為主來進行架構設計 &amp; 開發，因此若是要針對 Application 進行深入的管理，透過使用 <strong>app.kubernetes.io</strong> 作為 prefix 的 label or annotation 就是一種作法。</p><blockquote><p>一般來說 label 是 private to user，使用 <code>app.kubernetes.io</code> 的原因就是確保這樣子的 label 不會影響到 custom user label</p></blockquote><h2 id="相關的-Label-列表"><a href="#相關的-Label-列表" class="headerlink" title="相關的 Label 列表"></a>相關的 Label 列表</h2><p>透過 <code>app.kubernetes.io</code> prefix，有以下的 label 組合可以使用：</p><table><thead><tr><th>Key</th><th>Description</th><th>Example</th><th>Type</th></tr></thead><tbody><tr><td>app.kubernetes.io/name</td><td>Application 名稱</td><td>mysql</td><td>string</td></tr><tr><td>app.kubernetes.io/instance</td><td>Application Instance 的識別用名稱</td><td>wordpress-abcxzy</td><td>string</td></tr><tr><td>app.kubernetes.io/version</td><td>Application目前的版本號</td><td>5.7.21</td><td>string</td></tr><tr><td>app.kubernetes.io/component</td><td>在 Application 架構中的元件</td><td>database</td><td>string</td></tr><tr><td>app.kubernetes.io/part-of</td><td>更上層的 Application 名稱</td><td>wordpress</td><td>string</td></tr><tr><td>app.kubernetes.io/managed-by</td><td>用來管理 Application 佈署 &amp; 操作的工具</td><td>helm</td><td>string</td></tr></tbody></table><p>使用上面表格中的資訊，轉換成可設定在 k8s resource object 中的 YAML 格式，就會變成如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">&quot;mysql&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/instance:</span> <span class="string">&quot;wordpress-abcxzy&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/version:</span> <span class="string">&quot;5.7.21&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/component:</span> <span class="string">&quot;database&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">&quot;wordpress&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">&quot;helm&quot;</span></span><br></pre></td></tr></table></figure><h2 id="當-Application-被多次佈署到-Kubernetes-Cluster-時"><a href="#當-Application-被多次佈署到-Kubernetes-Cluster-時" class="headerlink" title="當 Application 被多次佈署到 Kubernetes Cluster 時"></a>當 Application 被多次佈署到 Kubernetes Cluster 時</h2><p>以上述的例子來說，一個 wordpress application 可以被佈署多次到 k8s cluster 中，每一次的佈署都稱為 <strong>Application Instance</strong>，雖然每個 application name 都是 <strong>wordpress</strong>，但每個 application instance 都會有各自不同的 instance ID，例如：</p><ul><li><p>app.kubernetes.io/instance: “wordpress-abcxzy”</p></li><li><p>app.kubernetes.io/instance: “wordpress-defstu”</p></li></ul><p>透過不同的 instance 名稱，就可以知道同一個 application，可能被不同的使用者佈署，或是在不同的專案同時被使用</p><h2 id="範例"><a href="#範例" class="headerlink" title="範例"></a>範例</h2><p>以下是使用 Helm 安裝 wordpress(Web + DB) 時，label 設定的範例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Web 以 Deployment 的形式佈署</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">wordpress</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/instance:</span> <span class="string">wordpress-abcxzy</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/version:</span> <span class="string">&quot;4.9.4&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">helm</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/component:</span> <span class="string">server</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">wordpress</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用來提供外部連線處理的 Service，串到後面的 Web 服務</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">wordpress</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/instance:</span> <span class="string">wordpress-abcxzy</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/version:</span> <span class="string">&quot;4.9.4&quot;</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">helm</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/component:</span> <span class="string">server</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">wordpress</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DB(MySQL) 則是以 StatefulSet 的形式佈署</span></span><br><span class="line"><span class="comment"># 並可看出與那個 application instance 相關</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/instance:</span> <span class="string">wordpress-abcxzy</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">helm</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/component:</span> <span class="string">database</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">wordpress</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/version:</span> <span class="string">&quot;5.7.21&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用來提供 DB 連線介面給 Web(wordpress) 的 service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/instance:</span> <span class="string">wordpress-abcxzy</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/managed-by:</span> <span class="string">helm</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/component:</span> <span class="string">database</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">wordpress</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/version:</span> <span class="string">&quot;5.7.21&quot;</span></span><br></pre></td></tr></table></figure><p>從上面 Label <strong>app.kubernetes.io/instance</strong> 設定的都是相同的 value 來看，就可以很明確的看出，這幾個 resource object 構成了一個完成的 Wordpress 服務。</p><h1 id="Annotations"><a href="#Annotations" class="headerlink" title="Annotations"></a>Annotations</h1><h2 id="What-is-Annotations"><a href="#What-is-Annotations" class="headerlink" title="What is Annotations ?"></a>What is Annotations ?</h2><p>Annotations 跟 上面提到的 label 相同，是以 key/value 的型式儲存資料，但不同的是，Annotations 並不是儲存用來辨識 resource object 用的，主要就是作為 metadata 來當類似”<strong>註解</strong>“的方式使用，大概有幾個特點：</p><ul><li><p>並非用來辨識 resource object 之用</p></li><li><p>註解的資料量通常是較大的</p></li><li><p>資料可以是較為複雜的結構化資料(structured data)，當然也可以是非結構化的資料</p></li><li><p>tool or library 等 client 都可以存取此 metadata</p></li></ul><h2 id="如何設定-Annotations"><a href="#如何設定-Annotations" class="headerlink" title="如何設定 Annotations"></a>如何設定 Annotations</h2><p>以下是簡單一點的 JSON 格式範例：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;metadata&quot;: &#123;</span><br><span class="line">  &quot;annotations&quot;: &#123;</span><br><span class="line">    &quot;key1&quot; : &quot;value1&quot;,</span><br><span class="line">    &quot;key2&quot; : &quot;value2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以是複雜的資料格式：(以 YAML 格式示範)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cafe-ingress-with-annotations</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">nginx.org/proxy-connect-timeout:</span> <span class="string">&quot;30s&quot;</span></span><br><span class="line">    <span class="attr">nginx.org/proxy-read-timeout:</span> <span class="string">&quot;20s&quot;</span></span><br><span class="line">    <span class="attr">nginx.org/client-max-body-size:</span> <span class="string">&quot;4m&quot;</span></span><br><span class="line">    <span class="attr">nginx.org/location-snippets:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">if</span> <span class="string">($ssl_client_verify</span> <span class="string">=</span> <span class="string">SUCCESS)</span> &#123;</span><br><span class="line">            <span class="string">set</span> <span class="string">$auth_basic</span> <span class="string">off;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">if</span> <span class="string">($ssl_client_verify</span> <span class="type">!=</span> <span class="string">SUCCESS)</span> &#123;</span><br><span class="line">            <span class="string">set</span> <span class="string">$auth_basic</span> <span class="string">&quot;Restricted&quot;</span><span class="string">;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="string">auth_basic</span> <span class="string">$auth_basic;</span></span><br><span class="line">        <span class="string">auth_basic_user_file</span> <span class="string">&quot;/var/run/secrets/nginx.org/auth-basic-file&quot;</span><span class="string">;</span></span><br><span class="line">    <span class="attr">nginx.org/server-snippets:</span> <span class="string">|</span></span><br><span class="line">        <span class="string">ssl_verify_client</span> <span class="string">optional;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="string">.......</span> <span class="string">(以下略)</span></span><br></pre></td></tr></table></figure><p>然而，Annotations 的設定不一定是由使用者套用到 resource object 上的，也可以是透過 auto-sizing or auto-scaling system 來自動生成相關 metadata 並套用到 object 上，讓 control manager 透過監控這一類的資訊來調整 object 狀態的變更。</p><h2 id="那些資訊適合用-Annotation-呈現"><a href="#那些資訊適合用-Annotation-呈現" class="headerlink" title="那些資訊適合用 Annotation 呈現?"></a>那些資訊適合用 Annotation 呈現?</h2><p>以下幾個例子說明：</p><ul><li><p>auto-sizing or auto-scaling system 自動生成的資訊</p></li><li><p>build, release 或是與 image 相關的資訊，例如：timestamp, release ID, Git branch, PR numbers, image hash, registry address … 等等</p></li><li><p>與 client library/tool 相關且用來 debug 用的資訊，例如：name, version, build information</p></li><li><p>tool/system 的來源資訊，例如：URL</p></li><li><p>負責人聯絡資訊，例如：電話, E-Mail ….等等</p></li></ul><h1 id="Field-Selectors"><a href="#Field-Selectors" class="headerlink" title="Field Selectors"></a>Field Selectors</h1><h2 id="What-is-Field-Selectors"><a href="#What-is-Field-Selectors" class="headerlink" title="What is Field Selectors?"></a>What is Field Selectors?</h2><p>k8s 除了提供了上面使用 label 的方式搜尋對應的 resource object 之外，還另外支援了另外一種稱為 “<strong>Field Selectors</strong>“ 的搜尋方式。</p><p>透過 Field Selectors，使用者可以根據 object 中的 <code>Field</code> 作為搜尋的依據，以下是幾個範例：</p><ul><li><p>metadata.name=my-service</p></li><li><p>metadata.namespace!=default</p></li><li><p>status.phase=Pending</p></li></ul><p>實際操作的範例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜尋目前 status 為 running 的 pod</span></span><br><span class="line">$ kubectl get pods --field-selector status.phase=Running</span><br></pre></td></tr></table></figure><h2 id="那些-Field-支援搜尋功能"><a href="#那些-Field-支援搜尋功能" class="headerlink" title="那些 Field 支援搜尋功能"></a>那些 Field 支援搜尋功能</h2><p>每種不同的 resource type 支援不同的 Field，但所有的 resource type 都支援以下兩種：</p><ol><li><p>metadata.name</p></li><li><p>metadata.namespace</p></li></ol><p>若是使用者操作時針對不支援的 Field 做搜尋，就會出現類似下面的錯誤訊息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system --field-selector foo.bar=baz</span><br><span class="line">No resources found.</span><br><span class="line">Error from server (BadRequest): Unable to find &#123;<span class="string">&quot;&quot;</span> <span class="string">&quot;v1&quot;</span> <span class="string">&quot;pods&quot;</span>&#125; that match label selector <span class="string">&quot;&quot;</span>, field selector <span class="string">&quot;foo.bar=baz&quot;</span>: field label not supported: foo.bar</span><br></pre></td></tr></table></figure><h2 id="進階搜尋"><a href="#進階搜尋" class="headerlink" title="進階搜尋"></a>進階搜尋</h2><h3 id="Supported-operators"><a href="#Supported-operators" class="headerlink" title="Supported operators"></a>Supported operators</h3><p>除了上面示範的 <strong>=</strong>(or <strong>==</strong>) 之外，同時也支援了 **!=**，但也僅此兩種而已，不像 label 支援 subset 的搜尋</p><h3 id="Chained-selectors"><a href="#Chained-selectors" class="headerlink" title="Chained selectors"></a>Chained selectors</h3><p>還可以將多個搜尋條件作結合，就是類似 <strong>AND</strong> 的效果，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜尋 kube-system namespace 中，狀態為 running &amp; restartPolicy 為 Always 的 pod</span></span><br><span class="line">$ kubectl get pods -n kube-system --field-selector=status.phase==Running,spec.restartPolicy=Always</span><br></pre></td></tr></table></figure><h3 id="Multiple-resource-types"><a href="#Multiple-resource-types" class="headerlink" title="Multiple resource types"></a>Multiple resource types</h3><p>也可以一次針對多個 resource type 進行搜尋，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同時搜尋名稱為 myweb 的 pod &amp; service</span></span><br><span class="line">$ kubectl get pod,service --field-selector metadata.name=myweb</span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/">Understanding Kubernetes Objects - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/">Names - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">Namespaces - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">Labels and Selectors - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/">Annotations - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/">Field Selectors - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/">Recommended Labels - Kubernetes</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CKA(Certified Kubernetes Administrator) 學習資源整理</title>
      <link href="/blog/Kubernetes/k8s-CKA-resources/"/>
      <url>/blog/Kubernetes/k8s-CKA-resources/</url>
      
        <content type="html"><![CDATA[<p>本篇文章蒐集與 CKA(Certified Kubernetes Administrator) 考試範圍的相關學習資源整理….會根據學習進度陸續補上</p><h1 id="Installation-Configuration-amp-Validation"><a href="#Installation-Configuration-amp-Validation" class="headerlink" title="Installation, Configuration &amp; Validation"></a>Installation, Configuration &amp; Validation</h1><h2 id="內容範圍"><a href="#內容範圍" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Design a Kubernetes cluster.</p></li><li><p>Install Kubernetes masters and nodes, including the use of TLS bootstrapping.</p></li><li><p>Configure secure cluster communications.</p></li><li><p>Configure a Highly-Available Kubernetes cluster.</p></li><li><p>Know where to get the Kubernetes release binaries.</p></li><li><p>Provision underlying infrastructure to deploy a Kubernetes cluster.</p></li><li><p>Choose a network solution.</p></li><li><p>Choose your Kubernetes infrastructure configuration.</p></li><li><p>Run end-to-end tests on your cluster.</p></li><li><p>Analyse end-to-end tests results.</p></li><li><p>Run Node end-to-end tests</p></li></ul><h2 id="官網資料"><a href="#官網資料" class="headerlink" title="官網資料"></a>官網資料</h2><ul><li><a href="https://kubernetes.io/docs/setup/scratch/">Creating a Custom Cluster from Scratch - Kubernetes</a></li></ul><h2 id="其他參考資料"><a href="#其他參考資料" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><hr><h1 id="Core-Concept"><a href="#Core-Concept" class="headerlink" title="Core Concept"></a>Core Concept</h1><h2 id="內容範圍-1"><a href="#內容範圍-1" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Understand the Kubernetes API primitives.</p></li><li><p>Understand the Kubernetes cluster architecture.</p></li><li><p>Understand Services and other network primitives.</p></li></ul><h2 id="官網資料-1"><a href="#官網資料-1" class="headerlink" title="官網資料"></a>官網資料</h2><ul><li><p><a href="https://kubernetes.io/docs/concepts/architecture/nodes/">Nodes - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/">Master-Node communication - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">Concepts Underlying the Cloud Controller Manager - Kubernetes</a></p></li><li><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md">Kubernetes Design and Architecture</a></p></li></ul><h2 id="其他參考資料-1"><a href="#其他參考資料-1" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><ul><li><p><a href="https://jimmysong.io/kubernetes-handbook/concepts/">Kubernetes架构 · Kubernetes Handbook - Kubernetes中文指南/云原生应用架构实践手册 by Jimmy Song(宋净超)</a></p></li><li><p><a href="https://kubernetes.feisky.xyz/zh/architecture/architecture.html">架构原理 · Kubernetes Handbook</a></p></li><li><p><a href="https://kubernetes.feisky.xyz/zh/architecture/architecture.html">Kubernetes指南 - 架构原理 · Kubernetes Handbook</a></p></li></ul><hr><h1 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h1><h2 id="內容範圍-2"><a href="#內容範圍-2" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Understand the networking configuration on the cluster nodes.</p></li><li><p>Understand Pod networking concepts.</p></li><li><p>Understand service networking.</p></li><li><p>Deploy and configure network load balancer.</p></li><li><p>Know how to use Ingress rules.</p></li><li><p>Know how to configure and use the cluster DNS.</p></li><li><p>Understand CNI</p></li></ul><h2 id="官網資料-2"><a href="#官網資料-2" class="headerlink" title="官網資料"></a>官網資料</h2><h3 id="Services"><a href="#Services" class="headerlink" title="Services"></a>Services</h3><ul><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services - Kubernetes</a></p></li><li><p><a href="https://kubernetes.io/zh/docs/concepts/services-networking/service/">Service - Kubernetes (中文)</a></p></li><li><p><a href="https://blog.xuite.net/champyen/champ/5396649-Stream+Control+Transmission+Protocol+-+SCTP">Stream Control Transmission Protocol - SCTP @ 網路黑貓 :: 隨意窩 Xuite日誌</a></p></li></ul><h2 id="其他參考資料-2"><a href="#其他參考資料-2" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><h3 id="Services-1"><a href="#Services-1" class="headerlink" title="Services"></a>Services</h3><ul><li><p><a href="https://ieevee.com/tech/2017/01/20/k8s-service.html">谈谈kubernets的service组件的Virtual IP</a></p></li><li><p><a href="https://my.oschina.net/jxcdwangtao/blog/839650">kube-proxy工作原理 - WaltonWang’s Blog - 开源中国</a></p></li><li><p><a href="https://www.jianshu.com/p/8a61de3f8be9">LVS原理介绍 - 简书</a></p></li><li><p><a href="https://ieevee.com/tech/2017/03/22/k8s-headless-service.html">kubernets: Headless Services</a></p></li><li><p><a href="https://www.hwchiu.com/kubernetes-service-ii.html">[Kubernetes] How to Implement Kubernetes Service - ClusterIP | Hwchiu Learning Note</a></p></li><li><p><a href="https://www.hwchiu.com/kubernetes-service-iii.html">[Kubernetes] How to Implement Kubernetes Service - NodePort | Hwchiu Learning Note</a></p></li><li><p><a href="https://www.hwchiu.com/kubernetes-service-iiii.html">[Kubernetes] How Implemete Kubernetes Service - SessionAffinity | Hwchiu Learning Note</a></p></li><li><p><a href="http://dockone.io/article/4884">http://dockone.io/article/4884</a></p></li><li><p><a href="https://akomljen.com/kubernetes-tips-part-1/">Kubernetes Tips - Part 1</a></p></li></ul><h3 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h3><ul><li><p><a href="https://www.hwchiu.com/netfilter-eiptables-i.html">[netfilter] Introduction to ebtables | Hwchiu Learning Note</a></p></li><li><p><a href="https://www.hwchiu.com/netfilter-eiptables-iai.html">[netfilter] Introduction to iptables | Hwchiu Learning Note</a></p></li></ul><hr><h1 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h1><h2 id="內容範圍-3"><a href="#內容範圍-3" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Understand persistent volumes and know how to create them.</p></li><li><p>Understand access modes for volumes.</p></li><li><p>Understand persistent volume claims primitive.</p></li><li><p>Understand Kubernetes storage objects.</p></li><li><p>Know how to configure applications with persistent storage.</p></li></ul><h2 id="官網資料-3"><a href="#官網資料-3" class="headerlink" title="官網資料"></a>官網資料</h2><h2 id="其他參考資料-3"><a href="#其他參考資料-3" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><ul><li><p><a href="https://my.oschina.net/jxcdwangtao/blog/1934004">深度解析Kubernetes Local Persistent Volume - WaltonWang’s Blog - 开源中国</a></p></li><li><p><a href="https://github.com/kubernetes-incubator/external-storage">kubernetes-incubator/external-storage: External storage plugins, provisioners, and helper libraries</a></p></li></ul><h3 id="GlusterFS"><a href="#GlusterFS" class="headerlink" title="GlusterFS"></a>GlusterFS</h3><ul><li><p><a href="http://www.l-penguin.idv.tw/book/Gluster-Storage_GitBook/">Gluster Storage System</a></p></li><li><p><a href="https://github.com/heketi/heketi">heketi/heketi: RESTful based volume management framework for GlusterFS</a></p></li><li><p><a href="https://github.com/heketi/heketi/blob/master/docs/admin/readme.md">Heketi Documentation - Administration</a></p></li><li><p><a href="https://github.com/heketi/heketi/blob/master/docs/admin/topology.md">Prepare Heketi Topology</a></p></li><li><p><a href="https://www.cnblogs.com/breezey/p/8849466.html">独立部署GlusterFS+Heketi实现Kubernetes共享存储 - breezey - 博客园</a></p></li><li><p>[GlusterFS 操作備忘 « Jamyy’s Weblog](<a href="http://jamyy.us.to/blog/2014/03/6220.html">http://jamyy.us.to/blog/2014/03/6220.html</a></p></li></ul><hr><h1 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a>Scheduling</h1><h2 id="內容範圍-4"><a href="#內容範圍-4" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Use label selectors to schedule Pods.</p></li><li><p>Understand the role of DaemonSets.</p></li><li><p>Understand how resource limits can affect Pod scheduling.</p></li><li><p>Understand how to run multiple schedulers and how to configure Pods to use them.</p></li><li><p>Manually schedule a pod without a scheduler.</p></li><li><p>Display scheduler events.</p></li><li><p>Know how to configure the Kubernetes scheduler</p></li></ul><h2 id="官網資料-4"><a href="#官網資料-4" class="headerlink" title="官網資料"></a>官網資料</h2><h2 id="其他參考資料-4"><a href="#其他參考資料-4" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><hr><h1 id="Logging-amp-Monitoring"><a href="#Logging-amp-Monitoring" class="headerlink" title="Logging &amp; Monitoring"></a>Logging &amp; Monitoring</h1><h2 id="內容範圍-5"><a href="#內容範圍-5" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Understand how to monitor all cluster components.</p></li><li><p>Understand how to monitor applications.</p></li><li><p>Manage cluster component logs.</p></li><li><p>Manage application logs.</p></li></ul><h2 id="官網資料-5"><a href="#官網資料-5" class="headerlink" title="官網資料"></a>官網資料</h2><h2 id="其他參考資料-5"><a href="#其他參考資料-5" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><hr><h1 id="Application-Lifecycle-Management"><a href="#Application-Lifecycle-Management" class="headerlink" title="Application Lifecycle Management"></a>Application Lifecycle Management</h1><h2 id="內容範圍-6"><a href="#內容範圍-6" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Understand Deployments and how to perform rolling updates and rollbacks.</p></li><li><p>Know various ways to configure applications.</p></li><li><p>Know how to scale applications.</p></li><li><p>Understand the primitives necessary to create a self-healing application.</p></li></ul><h2 id="官網資料-6"><a href="#官網資料-6" class="headerlink" title="官網資料"></a>官網資料</h2><h2 id="其他參考資料-6"><a href="#其他參考資料-6" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><hr><h1 id="Cluster-Maintenance"><a href="#Cluster-Maintenance" class="headerlink" title="Cluster Maintenance"></a>Cluster Maintenance</h1><h2 id="內容範圍-7"><a href="#內容範圍-7" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Understand Kubernetes cluster upgrade process.</p></li><li><p>Facilitate operating system upgrades.</p></li><li><p>Implement backup and restore methodologies</p></li></ul><h2 id="官網資料-7"><a href="#官網資料-7" class="headerlink" title="官網資料"></a>官網資料</h2><h2 id="其他參考資料-7"><a href="#其他參考資料-7" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><hr><h1 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h1><h2 id="內容範圍-8"><a href="#內容範圍-8" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Know how to configure authentication and authorization.</p></li><li><p>Understand Kubernetes security primitives.</p></li><li><p>Know to configure network policies.</p></li><li><p>Create and manage TLS certificates for cluster components.</p></li><li><p>Work with images securely.</p></li><li><p>Define security contexts.</p></li><li><p>Secure persistent key value store.</p></li><li><p>Work with role-based access control.</p></li></ul><h2 id="官網資料-8"><a href="#官網資料-8" class="headerlink" title="官網資料"></a>官網資料</h2><h2 id="其他參考資料-8"><a href="#其他參考資料-8" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><hr><h1 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h1><h2 id="內容範圍-9"><a href="#內容範圍-9" class="headerlink" title="內容範圍"></a>內容範圍</h2><ul><li><p>Troubleshoot application failure.</p></li><li><p>Troubleshoot control plane failure.</p></li><li><p>Troubleshoot worker node failure.</p></li><li><p>Troubleshoot networking.</p></li></ul><h2 id="官網資料-9"><a href="#官網資料-9" class="headerlink" title="官網資料"></a>官網資料</h2><h2 id="其他參考資料-9"><a href="#其他參考資料-9" class="headerlink" title="其他參考資料"></a>其他參考資料</h2><hr><h1 id="完整學習資料整理"><a href="#完整學習資料整理" class="headerlink" title="完整學習資料整理"></a>完整學習資料整理</h1><ul><li><p><a href="https://jimmysong.io/kubernetes-handbook/">Kubernetes Handbook——Kubernetes中文指南/云原生应用架构实践手册</a></p></li><li><p><a href="https://github.com/walidshaari/Kubernetes-Certified-Administrator">walidshaari/Kubernetes-Certified-Administrator</a></p></li></ul><blockquote><p>裏面有熱心網友整理了相當多寶貴的資料….</p></blockquote><ul><li><a href="https://linuxacademy.com/linux/training/course/name/certified-kubernetes-administrator-preparation-course">LiNUX Courses - Certified Kubernetes Administrator (CKA)</a></li></ul><blockquote><p>沒付費可以有七天的無限制存取權限，用來快速的了解整體內容 or 做複習衝刺應該會有幫助</p></blockquote><ul><li><a href="https://github.com/nikovirtala/Certified-Kubernetes-Administrator-CKA">nikovirtala/Certified-Kubernetes-Administrator-CKA</a></li></ul><hr><h1 id="線上課程"><a href="#線上課程" class="headerlink" title="線上課程"></a>線上課程</h1><ul><li><p><a href="https://kubernetes.io/docs/tutorials/online-training/overview/">Overview of Kubernetes Online Training - Kubernetes</a></p></li><li><p><a href="https://linuxacademy.com/linux/training/course/name/certified-kubernetes-administrator-preparation-course">LiNUX Courses - Certified Kubernetes Administrator (CKA)</a></p></li><li><p><a href="https://www.udemy.com/kubernetes-cka-on-cloud/">Kubernetes On The Cloud &amp; The CNCF CKA Certification | Udemy</a></p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.youtube.com/watch?v=tqr581_bBM0&list=PLrKlqis8aRHbMYWrNTsbx_MD9zJxjRLIc">K8S CKA Study Prep</a></p></li><li><p><a href="https://www.youtube.com/watch?v=zeS6OyDoy78&list=PLX-REfIYjgkzkWPRlLxSTuGn743cKD1GH">CKA Study</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] Cluster Architecture</title>
      <link href="/blog/Kubernetes/k8s-CoreConcept-Cluster-Architecture/"/>
      <url>/blog/Kubernetes/k8s-CoreConcept-Cluster-Architecture/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes-的前身-Google-Borg"><a href="#Kubernetes-的前身-Google-Borg" class="headerlink" title="Kubernetes 的前身 - Google Borg"></a>Kubernetes 的前身 - Google Borg</h1><p>Kubernetes 源自於 Google 內部的 Borg，因此看看原生的 Borg 會有助於從大方向了解 Kubernets 架構中的各個 component 是如何協同運作的，以下是 Borg 的系統架構：</p><p><img src="/blog/images/kubernetes/cka-k8s-borg-architecture.png" alt="Google Borg System Architecture"></p><p>從上面的架構圖可以看出幾個重點：</p><h2 id="BorgMaster"><a href="#BorgMaster" class="headerlink" title="BorgMaster"></a>BorgMaster</h2><p>BorgMaster 是整個系統的大腦，用來負責與系統中的不同元件進行溝通，確保系統可以正常運作。</p><h2 id="Borglet"><a href="#Borglet" class="headerlink" title="Borglet"></a>Borglet</h2><p>這是在每個 host 中負責管理 container 生命周期的元件，接收來自 BorgMaster 的命令並進行相對應的操作。</p><h2 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h2><p>負責進行任務的調度，根據不同應用的需求，將 workload 調度到不同的機器去執行。</p><h2 id="Paxos-persistent-store"><a href="#Paxos-persistent-store" class="headerlink" title="Paxos (persistent store)"></a>Paxos (persistent store)</h2><p>由於整體系統在設計上就是預期會有某些 node 在某些時候會發生故障，因此當要儲存系統運作狀態時，就不會考慮儲存在自身的儲存空間上；因此 Paxos 就是在整個系統中負責儲存系統運作狀態的地方。</p><h2 id="borgcfg"><a href="#borgcfg" class="headerlink" title="borgcfg"></a>borgcfg</h2><p>用來操作 Borg 系統的 CLI 所使用的設定檔</p><h1 id="Kubernets-架構"><a href="#Kubernets-架構" class="headerlink" title="Kubernets 架構"></a>Kubernets 架構</h1><p>Kubernetes 的架構設計上也極為類似 Borg，如下圖所示：</p><p><img src="/blog/images/kubernetes/CKA-CoreConcept-k8s-cluster-architecture-high-level.jpg" alt="Kubernetes Architecture"></p><p>在把上面複雜的架構圖抽象化一點：</p><p><img src="/blog/images/kubernetes/k8s-architecture.png" alt="Abstract Kubernetes Architecture"></p><p>從上圖可以看出以下結論：</p><ol><li><p>不論是透過 CLI or UI，都只能透過呼叫 API 的方式與 Kubernetes 溝通</p></li><li><p>master node 收到 API request 後，針對使用者需求對後面的 worker node 進行指定的工作(透過 kubelet)</p></li><li><p>worker node 到 master node 的溝通，僅能透過 API server 作為窗口，kubelet 是無法存取到 master node 上的其他元件</p></li><li><p>唯一可以存取 etcd 的只有 REST service(kube-apiserver)</p></li><li><p>外部的到 worker node 的 traffic 則是由 kube-proxy 來負責處理，將流量導到正確的 pod 上</p></li></ol><p>接著以下用不同 node type 的角度來切入進行細部檢視：</p><h2 id="Master-Node"><a href="#Master-Node" class="headerlink" title="Master Node"></a>Master Node</h2><p><img src="/blog/images/kubernetes/kubernetes-master-arch.png" alt="Abstract Kubernetes Architecture"></p><p>在 master node 上有幾個相當重要的 component 分別是：</p><h3 id="API-server"><a href="#API-server" class="headerlink" title="API server"></a>API server</h3><p>此為所有資源操作的唯一入口，並負責 Authentication、Authorization、Access Control、API registration &amp; discovery。</p><h3 id="Scheduler-1"><a href="#Scheduler-1" class="headerlink" title="Scheduler"></a>Scheduler</h3><p>負責進行任務的調度，根據不同應用的需求，將 workload 調度到不同的機器去執行。</p><h3 id="Controller-Manager"><a href="#Controller-Manager" class="headerlink" title="Controller Manager"></a>Controller Manager</h3><p>負責維護 cluster 的狀態，包含 failure detection、auto scale in/out、rolling update …. 等等。</p><h3 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h3><p>負責保存整個 cluster 的運行狀態</p><blockquote><p>若 cluster 的 node 數量龐大，etcd 也有可能會被獨立於 master 之外來單獨安裝。</p></blockquote><h2 id="Worker-Node"><a href="#Worker-Node" class="headerlink" title="Worker Node"></a><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node">Worker Node</a></h2><p>在 Kubernetes 中，worker node 也稱為 <strong>minion</strong>，可以是實體機或是 VM，以下是 worker node 的架構圖：</p><p><img src="/blog/images/kubernetes/kubernetes-node-arch.png" alt="Abstract Kubernetes Architecture"></p><h3 id="Container-Runtime"><a href="#Container-Runtime" class="headerlink" title="Container Runtime"></a>Container Runtime</h3><p>負責 container image 的管理並透過 <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md">CRI(Container Runtime Interface)</a> 來管理 pod 的運行。 </p><p>大部份時候會以 Docker 作為 container runtime，但其實可以作為 container runtime 的除了 Docker 之外，還有以下幾個選項：</p><ul><li><p><a href="https://github.com/rkt/rkt">rkt - the pod-native container engine</a></p></li><li><p><a href="https://github.com/kubernetes-incubator/cri-o">CRI-O - OCI-based implementation of Kubernetes Container Runtime Interface</a></p></li><li><p><a href="https://github.com/kubernetes/frakti">Frakti - The hypervisor-based container runtime for Kubernetes</a></p></li></ul><h3 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h3><p>負責接收來自 API server 的命令、維護 container 的生命周期，並負責 CSI(Container Storage Interface) &amp; CNI(Container Network Interface) 的管理。</p><h3 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h3><p>在 Kubernetes 中，<a href="https://kubernetes.io/docs/concepts/services-networking/service/"><strong>service</strong></a> 將多個 pod 抽象化成一個單一群組，並為此群組提供一個 virtual IP，並提供其他 pod or service 進行存取。</p><p>而 kube-proxy 就是負責在 host 中針對 service 與 pod 的配置設定，產生相對應的 iptables rule，讓送到 service virtual IP 的流量可以正確的導向 service 後端的 pod 中。</p><p>因此總結來說，kube-proxy 就是負責為 service 提供 cluster 內部的 service discovery &amp; load balance</p><h1 id="核心元件之間的通訊"><a href="#核心元件之間的通訊" class="headerlink" title="核心元件之間的通訊"></a>核心元件之間的通訊</h1><p>接著來探討一下 Kubernetes 核心元件之間的通信細節，以下面的圖片進行說明：</p><p><img src="/blog/images/kubernetes/CKA-CoreConcept-k8s-cluster-architecture-high-level.jpg" alt="Kubernetes Architecture"></p><h2 id="External-lt-–-gt-Master"><a href="#External-lt-–-gt-Master" class="headerlink" title="External &lt;–&gt; Master"></a>External &lt;–&gt; Master</h2><ul><li>管理者從外面僅能透過 API server 進行對 Kubernetes cluster 的管理 (傳送 JSON 格式的資料)</li></ul><h2 id="Inside-Master"><a href="#Inside-Master" class="headerlink" title="Inside Master"></a>Inside Master</h2><ul><li><p>不論是負責管理 cluster 狀態的 controller manager，或是負責任務調度的 scheduler，要調整 worker 的工作狀態，都必須透過送 request 給 API server，再由 API server 統一對 host 上的 kubelet 進行操作 (這幾個元件之間傳送的資料格式為 <a href="https://developers.google.com/protocol-buffers/" title="protobuf">protobuf</a>)</p></li><li><p>在 master node 中的元件，唯一會對 etcd 服務進行 cluster 狀態存取的僅有 API server，雙方使用 gRPC 進行通訊</p></li></ul><h2 id="Master-lt-–-gt-Worker"><a href="#Master-lt-–-gt-Worker" class="headerlink" title="Master &lt;–&gt; Worker"></a>Master &lt;–&gt; Worker</h2><ul><li>所有對 worker node 上的狀態變更，都是由 master node 上的 API server 所發過來，並由 kubelet 接收後進行處理 (透過 <a href="https://developers.google.com/protocol-buffers/" title="protobuf">protobuf</a>) 資料格式傳送資料)</li></ul><h2 id="Inside-Worker"><a href="#Inside-Worker" class="headerlink" title="Inside Worker"></a>Inside Worker</h2><ul><li><p>kubelet 透過 CRI(Container Runtime Interface)，控制 container runtime 進行 pod 的管理</p></li><li><p>contain runtime 則是透過符合 OCI(Open Container Initiative) 介面，在 OS 中實際的產生出相對應的 container</p></li><li><p>kubelet 透過 CNI(Container Network Interface)，呼叫 CNI plugin 來進行 pod network 的設定</p></li></ul><h1 id="Add-Ons"><a href="#Add-Ons" class="headerlink" title="Add-Ons"></a>Add-Ons</h1><p>除了上面在 master &amp; worker node 上的核心元件外，還有一些 add-on 可以讓 Kubernetes 運作的更好，例如：</p><ul><li><p><strong>DNS</strong>：目前以 <a href="https://github.com/coredns/coredns">CoreDNS</a> 為主，以前則是 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/kube-dns">kube-dns</a></p></li><li><p><strong>Ingress controller</strong>：負責讓外部的流量可以正確的導到 Kubernetes 內部的 service</p></li><li><p><strong>Resource Monitoring</strong>：原先是由 <a href="https://github.com/kubernetes/heapster">Heapster</a> 來負責，但由於 <a href="https://prometheus.io/">Prometheus</a> 從 CNCF 變成正式專案後，在 resource monitoring 這個部份開始推薦使用 Prometheus 來實現</p></li><li><p><strong>Dashboard</strong>：為了提供使用者更好的體驗，除了 <a href="https://github.com/kubernetes/dashboard/">Kubernetes 官方正式維護的 Dashboard</a> 之外，也可以考慮其他的選項，例如：<a href="https://cockpit-project.org/guide/latest/feature-kubernetes.html">Cockpit</a> or <a href="https://rancher.com/">Rancher</a>(透過 import Kubernetes Clusters 的功能)</p></li></ul><p>當然上面的 add-on 並不是全部，詳細的 add-on 清單可以到 <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons">Kubernetes 的專案</a>中去查詢。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/architecture/">Kubernetes Architecture - Kubernetes</a></p></li><li><p><a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md">Kubernetes Design and Architecture</a></p></li><li><p><a href="https://jimmysong.io/kubernetes-handbook/concepts/">Kubernetes架构 · Kubernetes Handbook - Kubernetes中文指南/云原生应用架构实践手册 by Jimmy Song(宋净超)</a></p></li><li><p><a href="https://kubernetes.feisky.xyz/zh/architecture/architecture.html">架构原理 · Kubernetes Handbook</a></p></li><li><p><a href="https://kubernetes.feisky.xyz/zh/architecture/architecture.html">Kubernetes指南 - 架构原理 · Kubernetes Handbook</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Core Concept </tag>
            
            <tag> CKA Core Concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Docker] MACVLAN Network 簡介</title>
      <link href="/blog/Docker/docker-network-macvlan/"/>
      <url>/blog/Docker/docker-network-macvlan/</url>
      
        <content type="html"><![CDATA[<p>此篇文章介紹 Docker MACVLAN Network 及其運作方式</p><h1 id="環境介紹"><a href="#環境介紹" class="headerlink" title="環境介紹"></a>環境介紹</h1><p>以下的測試將會在以下環境進行：</p><ul><li><p>OS：<code>Ubuntu 18.04</code></p></li><li><p>Docker: <code>18.03.1-ce</code></p></li></ul><p>網卡配置：</p><ul><li><p>eth0：<code>10.103.19.0/24</code></p></li><li><p>eth1：<code>trunk port (10.103.[17-18].0/24)</code></p></li></ul><h1 id="Prerequisites-amp-Limitations"><a href="#Prerequisites-amp-Limitations" class="headerlink" title="Prerequisites &amp; Limitations"></a>Prerequisites &amp; Limitations</h1><p>使用 MACVLAN 功能，在環境上是有所限制的：</p><ol><li><p>大多數的 cloud provider 是不支援這個功能的，因為這會需要使用到實體的網路設備</p></li><li><p>需要 Linux kernel 3.9 以上 or 4.0 以上</p></li><li><p>承上，所以僅支援 Linux</p></li><li><p>網卡必須開啟 <code>promiscuous mode</code>，網卡才可以設定多個 MAC address 上去</p></li></ol><h2 id="簡易檢測"><a href="#簡易檢測" class="headerlink" title="簡易檢測"></a>簡易檢測</h2><p>MACVLAN 這個功能需要 Linux Kernel(<code>v3.9–3.19</code> and <code>4.0+</code>) 的支援，為了確保 MACVLAN 可用，必須做以下檢查：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 掛載模組</span></span><br><span class="line">$ modprobe macvlan</span><br><span class="line"><span class="comment"># 列出目前已經掛載的模組</span></span><br><span class="line">$ lsmod | grep macvlan</span><br><span class="line">macvlan                24576  0</span><br></pre></td></tr></table></figure><p>上面若是有指令發生錯誤，或是第二個指令沒有回傳任何結果，就表示該 host 上的 Linux kernel 不支援 MACVLAN 的功能。</p><h1 id="MACVLAN-簡介"><a href="#MACVLAN-簡介" class="headerlink" title="MACVLAN 簡介"></a>MACVLAN 簡介</h1><p>MACVLAN 允許你在主機的一個 NIC 上配置多個虛擬的 NIC，這些 NIC 有自己獨立的 MAC 地址，也可以配置上 IP address 進行通訊。在 MACVLAN 下的 VM 或者 container 的網路和Host 都在同一個網段中，共享同一個 broadcast domain。</p><h1 id="Bridge-v-s-MACVLAN"><a href="#Bridge-v-s-MACVLAN" class="headerlink" title="Bridge v.s. MACVLAN"></a>Bridge v.s. MACVLAN</h1><h2 id="Bridge"><a href="#Bridge" class="headerlink" title="Bridge"></a>Bridge</h2><p>Bridge 有以下特點：</p><ul><li><p>Bridge 是 layer 2 設備，僅用來處理 layer 2 的通訊</p></li><li><p>Bridge 使用 MAC address table 來決定網路封包要怎麼 forward</p></li><li><p>Bridge 會從 host 之間的通訊中的封包中學習 MAC address</p></li><li><p>可以是硬體設備，也可以是純軟體(例如：Linux Bridge)</p></li></ul><p>以下是一個在 Linux Host 上，多個 VM 使用 bridge 相互通訊的狀況：</p><p><img src="http://hicu.be/wp-content/uploads/2016/03/linux-bridge.png" alt="Bridge Example"></p><h2 id="MACVLAN"><a href="#MACVLAN" class="headerlink" title="MACVLAN"></a>MACVLAN</h2><p>MACVLAN 有以下特點：</p><ul><li><p>可讓使用者在同一張實體網卡上設定多個 Layer 2 address (一般就是 MAC address)</p></li><li><p>承上，帶有上述設定的 MAC address 的網卡稱為 sub interface；=而實體網卡則稱為 parent interface</p></li><li><p>可在 parent/sub interface 上設定的不只是 MAC address，IP address 同樣也是可以被設定</p></li><li><p>sub interface 無法直接與 parent interface 通訊 (帶有 sub interface 的 VM or container 無法與 host 直接通訊)</p></li><li><p>承上，若 VM or container 需要與 host 通訊，那就必須額外建立一個 sub interface 給 host 用</p></li><li><p>sub interface 通常以 <code>mac0@eth0</code> 的形式來命名以方便區別</p></li></ul><p>以下用張圖來解釋一下設定 MACVLAN 後的樣子：</p><p><img src="http://hicu.be/wp-content/uploads/2016/03/linux-macvlan.png" alt="MACVLAN"></p><h1 id="MACVLAN-Modes"><a href="#MACVLAN-Modes" class="headerlink" title="MACVLAN Modes"></a>MACVLAN Modes</h1><p>MACVLAN 共支援四種模式，分別是：</p><h2 id="Private"><a href="#Private" class="headerlink" title="Private"></a>Private</h2><p><img src="http://hicu.be/wp-content/uploads/2016/03/linux-macvlan-private-mode.png" alt="MACVLAN Private Mode"></p><p>在 private mode 下，sub interface 之間無法相互通訊</p><h2 id="VEPA"><a href="#VEPA" class="headerlink" title="VEPA"></a>VEPA</h2><p><img src="http://hicu.be/wp-content/uploads/2016/03/linux-macvlan-802.1qbg-vepa-mode.png" alt="MACVLAN VEPA Mode"></p><p>在此 VEPA mode 下， sub interface 的通訊必須透過外部的 switch 來完成，而且此 switch 必須支援 <code>IEEE 802.1Qbg</code> 協定。</p><p>VM or container 之間的通訊透過外部的 switch，因此廠商可以在外部的 switch 上針對此類的流量進行優化設定，以達到更好的效能。</p><h2 id="Bridge-1"><a href="#Bridge-1" class="headerlink" title="Bridge"></a>Bridge</h2><p><img src="http://hicu.be/wp-content/uploads/2016/03/linux-macvlan-bridge-mode.png" alt="MACVLAN Bridge Mode"></p><p>sub interface 之間的通訊在 host 之間完成(類似上面使用 Linux bridge 時 VM or container 之間的通訊方式)，且不用 Linux bridge，因此也沒有 MAC learning，也不需要 STP，因此效能比起使用 Linux bridge 好上很多。</p><h2 id="Passthru"><a href="#Passthru" class="headerlink" title="Passthru"></a>Passthru</h2><p><img src="http://hicu.be/wp-content/uploads/2016/03/linux-macvlan-passthru-mode.png" alt="MACVLAN Passthru Mode"></p><p>直接把實體網卡分配給單一 sub interface，因此使用此 sub interface 的 VM or container 可以自行修改網卡的 MAC address or 相關參數。 </p><h1 id="在-Docker-上使用-MACVLAN"><a href="#在-Docker-上使用-MACVLAN" class="headerlink" title="在 Docker 上使用 MACVLAN"></a>在 Docker 上使用 MACVLAN</h1><p>使用 MACVLAN 的 container 會有以下幾個特點：</p><ol><li><p>由於 container 的 interface 與 host NIC 連接，因此即使沒有 iptables or port mapping 的相關設定，就可以連外(只要 gateway 設定正確即可)</p></li><li><p>效能比起 bridge 方式相對好</p></li><li><p>若 container 很多，可能造成 IP 耗盡的狀況</p></li><li><p>需要自行管理眾多不同的 MAC address</p></li></ol><h2 id="Bridge-Example"><a href="#Bridge-Example" class="headerlink" title="Bridge Example"></a>Bridge Example</h2><p>建立 container 並檢視結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根據 parent NIC 的網路配置，建立一個 MACVLAN network</span></span><br><span class="line">$ docker network create -d macvlan \</span><br><span class="line">  --subnet=10.103.19.0/24 \</span><br><span class="line">  --gateway=10.103.19.1 \</span><br><span class="line">  -o parent=eth0 \</span><br><span class="line">  my-macvlan-net</span><br><span class="line">8e9ae1c6629de975356184f8d6959a490dbaf351a8ed5334ba050049a9a5c3d2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認 docker MACVLAN network 已經被正確建立</span></span><br><span class="line">$ docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">a1c7b6f80389        bridge              bridge              <span class="built_in">local</span></span><br><span class="line">dc2f51e1056f        host                host                <span class="built_in">local</span></span><br><span class="line">8e9ae1c6629d        my-macvlan-net      macvlan             <span class="built_in">local</span></span><br><span class="line">f28460d3a620        none</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用一個 container，並使用上面建立的網路</span></span><br><span class="line">$ docker run --rm -itd \</span><br><span class="line">  --network my-macvlan-net \</span><br><span class="line">  --name my-macvlan-alpine \</span><br><span class="line">  alpine:latest \</span><br><span class="line">  ash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 container 詳細資訊</span></span><br><span class="line">$ docker container inspect my-macvlan-alpine</span><br><span class="line">[</span><br><span class="line">... (略)</span><br><span class="line"><span class="string">&quot;Networks&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;my-macvlan-net&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;IPAMConfig&quot;</span>: null,</span><br><span class="line">        <span class="string">&quot;Links&quot;</span>: null,</span><br><span class="line">        <span class="string">&quot;Aliases&quot;</span>: [</span><br><span class="line">            <span class="string">&quot;bb4d75902a4e&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="string">&quot;NetworkID&quot;</span>: <span class="string">&quot;8e9ae1c6629de975356184f8d6959a490dbaf351a8ed5334ba050049a9a5c3d2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;5e88fd818f4c294332e31caaf084f053b7de950281154f4f8e3587aaa3fc5ba4&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Gateway&quot;</span>: <span class="string">&quot;10.103.19.1&quot;</span>,</span><br><span class="line">        <span class="string">&quot;IPAddress&quot;</span>: <span class="string">&quot;10.103.19.2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;IPPrefixLen&quot;</span>: 24,</span><br><span class="line">        <span class="string">&quot;IPv6Gateway&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;GlobalIPv6Address&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;GlobalIPv6PrefixLen&quot;</span>: 0,</span><br><span class="line">        <span class="comment"># 這個 MAC address 與 host eth0 的 MAC address 並不相同 (新產生的)</span></span><br><span class="line">        <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:0a:67:13:02&quot;</span>,</span><br><span class="line">        <span class="string">&quot;DriverOpts&quot;</span>: null</span><br><span class="line">    &#125;</span><br><span class="line">... (略)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>最後從 container 內部來檢視一下網路情況：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 IP，跟上面看到的是相同的</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it my-macvlan-alpine ip addr show eth0</span><br><span class="line">25: eth0@if2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 02:42:0a:67:13:02 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.103.19.2/24 brd 10.103.19.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 routing information</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it my-macvlan-alpine ip route</span><br><span class="line">default via 10.103.19.1 dev eth0 </span><br><span class="line">10.103.19.0/24 dev eth0 scope link  src 10.103.19.2 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試對 gateway 的通訊</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it my-macvlan-alpine ping -c 2 10.103.19.1</span><br><span class="line">PING 10.103.19.1 (10.103.19.1): 56 data bytes</span><br><span class="line">64 bytes from 10.103.19.1: seq=0 ttl=64 time=0.309 ms</span><br><span class="line">64 bytes from 10.103.19.1: seq=1 ttl=64 time=0.317 ms</span><br><span class="line"></span><br><span class="line">--- 10.103.19.1 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.309/0.313/0.317 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試連外通訊 &amp; DNS resolution</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it my-macvlan-alpine ping -c 2 www.google.com</span><br><span class="line">PING www.google.com (216.58.200.36): 56 data bytes</span><br><span class="line">64 bytes from 216.58.200.36: seq=0 ttl=54 time=7.995 ms</span><br><span class="line">64 bytes from 216.58.200.36: seq=1 ttl=54 time=7.876 ms</span><br><span class="line"></span><br><span class="line">--- www.google.com ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 7.876/7.935/7.995 ms</span><br></pre></td></tr></table></figure><h2 id="802-1Q-Trunked-Bridge-Example"><a href="#802-1Q-Trunked-Bridge-Example" class="headerlink" title="802.1Q Trunked Bridge Example"></a>802.1Q Trunked Bridge Example</h2><p>建立 container 並檢視結果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># interface 預設是 down，因此要把它啟用</span></span><br><span class="line">$ ifconfig ens19 up</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 802.1Q MACVLAN network</span></span><br><span class="line">$ docker network create -d macvlan \</span><br><span class="line">  --subnet=10.103.18.0/24 \</span><br><span class="line">  --gateway=10.103.18.1 \</span><br><span class="line">  -o parent=ens19.1318 \</span><br><span class="line">  my-8021q-1318-macvlan-net</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 docker network</span></span><br><span class="line">$ docker network ls</span><br><span class="line">NETWORK ID          NAME                        DRIVER              SCOPE</span><br><span class="line">e9a149acc74d        bridge                      bridge              <span class="built_in">local</span></span><br><span class="line">dc2f51e1056f        host                        host                <span class="built_in">local</span></span><br><span class="line">3cf96bd7e0d1        my-8021q-1318-macvlan-net   macvlan             <span class="built_in">local</span></span><br><span class="line">ef98674855e0        my-macvlan-net              macvlan             <span class="built_in">local</span></span><br><span class="line">f28460d3a620        none                        null                <span class="built_in">local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用上面的 MACVLAN 建立 container</span></span><br><span class="line">$ docker run --rm -itd \</span><br><span class="line">  --network my-8021q-1318-macvlan-net \</span><br><span class="line">  --name my-8021q-1318-macvlan-alpine \</span><br><span class="line">  alpine:latest \</span><br><span class="line">  ash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 container 詳細內容</span></span><br><span class="line">$ docker container inspect my-8021q-1318-macvlan-alpine</span><br><span class="line">[</span><br><span class="line">.... (略)</span><br><span class="line">        <span class="string">&quot;NetworkSettings&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Bridge&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;SandboxID&quot;</span>: <span class="string">&quot;a09cfbaa619c387b2647fd8236a28b4a40ca7473048faf5c1ca723897f82e001&quot;</span>,</span><br><span class="line">            <span class="string">&quot;HairpinMode&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">            <span class="string">&quot;LinkLocalIPv6Address&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;LinkLocalIPv6PrefixLen&quot;</span>: 0,</span><br><span class="line">            <span class="string">&quot;Ports&quot;</span>: &#123;&#125;,</span><br><span class="line">            <span class="string">&quot;SandboxKey&quot;</span>: <span class="string">&quot;/var/run/docker/netns/a09cfbaa619c&quot;</span>,</span><br><span class="line">            <span class="string">&quot;SecondaryIPAddresses&quot;</span>: null,</span><br><span class="line">            <span class="string">&quot;SecondaryIPv6Addresses&quot;</span>: null,</span><br><span class="line">            <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Gateway&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;GlobalIPv6Address&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;GlobalIPv6PrefixLen&quot;</span>: 0,</span><br><span class="line">            <span class="string">&quot;IPAddress&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;IPPrefixLen&quot;</span>: 0,</span><br><span class="line">            <span class="string">&quot;IPv6Gateway&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Networks&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;my-8021q-1318-macvlan-net&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;IPAMConfig&quot;</span>: null,</span><br><span class="line">                    <span class="string">&quot;Links&quot;</span>: null,</span><br><span class="line">                    <span class="string">&quot;Aliases&quot;</span>: [</span><br><span class="line">                        <span class="string">&quot;0192eb5284c1&quot;</span></span><br><span class="line">                    ],</span><br><span class="line">                    <span class="string">&quot;NetworkID&quot;</span>: <span class="string">&quot;3cf96bd7e0d18dac332dc160aecdc212d3fe46a03622ca208a6ac3cb90c2debe&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;8f44064144c68f31b273973b61375b5c41c6bbd9e9d9b5d1e6a3034101fdeba2&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;Gateway&quot;</span>: <span class="string">&quot;10.103.18.1&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;IPAddress&quot;</span>: <span class="string">&quot;10.103.18.2&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;IPPrefixLen&quot;</span>: 24,</span><br><span class="line">                    <span class="string">&quot;IPv6Gateway&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;GlobalIPv6Address&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;GlobalIPv6PrefixLen&quot;</span>: 0,</span><br><span class="line">                    <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:0a:67:12:02&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;DriverOpts&quot;</span>: null</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">.... (略)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>最後從 container 內部來檢視一下網路情況：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 container IP 資訊，跟上面的詳細資訊相同</span></span><br><span class="line">$ docker <span class="built_in">exec</span> my-8021q-1318-macvlan-alpine ip addr show eth0</span><br><span class="line">6: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 02:42:0a:67:12:02 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.103.18.2/24 brd 10.103.18.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 routing information</span></span><br><span class="line">$ docker <span class="built_in">exec</span> my-8021q-1318-macvlan-alpine ip route</span><br><span class="line">default via 10.103.18.1 dev eth0 </span><br><span class="line">10.103.18.0/24 dev eth0 scope link  src 10.103.18.2 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試與 gateway 的通訊</span></span><br><span class="line">$ docker <span class="built_in">exec</span> my-8021q-1318-macvlan-alpine ping -c 2 10.103.18.1</span><br><span class="line">PING 10.103.18.1 (10.103.18.1): 56 data bytes</span><br><span class="line">64 bytes from 10.103.18.1: seq=0 ttl=64 time=0.379 ms</span><br><span class="line">64 bytes from 10.103.18.1: seq=1 ttl=64 time=0.290 ms</span><br><span class="line"></span><br><span class="line">--- 10.103.18.1 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.290/0.334/0.379 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試對外通訊 &amp; DNS resolution</span></span><br><span class="line">$ docker <span class="built_in">exec</span> my-8021q-1318-macvlan-alpine ping -c 2 www.google.com</span><br><span class="line">PING www.google.com (172.217.160.68): 56 data bytes</span><br><span class="line">64 bytes from 172.217.160.68: seq=0 ttl=54 time=7.284 ms</span><br><span class="line">64 bytes from 172.217.160.68: seq=1 ttl=54 time=7.223 ms</span><br><span class="line"></span><br><span class="line">--- www.google.com ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 7.223/7.253/7.284 ms</span><br></pre></td></tr></table></figure><h1 id="結論"><a href="#結論" class="headerlink" title="結論"></a>結論</h1><p>Bridge 是 docker 中提供的預設方式，但使用者如果希望 container 的網路可以跟 host 的環境放在一起，選擇 MACVLAN 也是一種不錯的方式。</p><p>Docker 提供了非常多的網路設定選項，可以讓使用者根據需求選擇合適的方案；弄清楚每一種不同的 network driver 合適的場景、優缺點以及使用方式，container technology 一定可以在提供開發者以及維運人員更多的彈性。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://blog.csdn.net/jincm13/article/details/50351268">圖解幾個與Linux網絡虛擬化相關的虛擬網卡-VETH/MACVLAN/MACVTAP/IPVLAN - CSDN博客</a></p></li><li><p><a href="http://hicu.be/bridge-vs-macvlan">Bridge vs Macvlan – HiCube</a></p></li><li><p><a href="https://docs.docker.com/network">Configure Network | Docker Documentation</a></p></li><li><p><a href="https://docs.docker.com/network/macvlan/">Use Macvlan networks | Docker Documentation</a></p></li><li><p><a href="http://lyt0112.pixnet.net/blog/post/306813547">linux 網絡虛擬化： macvlan @ 小廷的部落格 :: 痞客邦 ::</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Docker] Bridge Network 簡介</title>
      <link href="/blog/Docker/docker-network-bridge/"/>
      <url>/blog/Docker/docker-network-bridge/</url>
      
        <content type="html"><![CDATA[<p>此篇文章介紹 Docker Bridge Network 及其運作方式</p><h1 id="環境介紹"><a href="#環境介紹" class="headerlink" title="環境介紹"></a>環境介紹</h1><p>以下的測試將會在以下環境進行：</p><ul><li><p>OS：<code>Ubuntu 18.04</code></p></li><li><p>Docker: <code>18.03.1-ce</code></p></li></ul><h1 id="檢視目前-Docker-Network-狀態"><a href="#檢視目前-Docker-Network-狀態" class="headerlink" title="檢視目前 Docker Network 狀態"></a>檢視目前 Docker Network 狀態</h1><p>以下是在 Ubuntu 18.04 上剛安裝好 docker 18.03.1 後的 docker network 狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">a1c7b6f80389        bridge              bridge              <span class="built_in">local</span></span><br><span class="line">dc2f51e1056f        host                host                <span class="built_in">local</span></span><br><span class="line">f28460d3a620        none                null                <span class="built_in">local</span></span><br></pre></td></tr></table></figure><p>從上面可以看出，剛安裝好 docker 的 Linux 主機，會預設帶有 3 個 driver，分別就是 <code>bridge</code>、<code>host</code> 以及 <code>null</code>。</p><p>接著以下將會仔細的介紹一下 bridge network 的運作方式。</p><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>先來個 Docker Bridge Network 的觀念釋疑：</p><ul><li><p><a href="https://docs.docker.com/network/bridge/" title="Docker Bridge Network">bridge network</a> 其實是利用的是 Docker Host 上的 software bridge 來達成讓 container 可以連外的目的，並透過此 bridge 可以讓同一個 bridge 的 container 之間相互進行通訊；而且 docker bridge driver 會自動在 Host 上設定相對應的 rule(iptables, network namespace)，讓 container 的網路可以正確的被使用。</p></li><li><p><a href="https://docs.docker.com/network/bridge/" title="Docker Bridge Network">bridge network</a> 是處理在單一 docker daemon 上運行的 container 之間的相互通訊，若是要處理多個 docker daemon 上的 container 通訊，則必須要使用 <a href="https://docs.docker.com/network/overlay/" title="Docker Overlay Network">overlay network</a></p></li></ul><p>下圖是 bridge network 的架構</p><p><img src="/blog/images/docker/bridge_network.jpg" alt="docker bridge network"></p><h1 id="使用預設的-Bridge-Network-docker0"><a href="#使用預設的-Bridge-Network-docker0" class="headerlink" title="使用預設的 Bridge Network (docker0)"></a>使用預設的 Bridge Network (docker0)</h1><p>安裝好 docker 後，其實 docker 就為我們準備好了一個 bridge <code>docker0</code> 用來當作 container 的對外 software bridge，以下的範例會先使用 docker0 來做示範。</p><h2 id="建立-container"><a href="#建立-container" class="headerlink" title="建立 container"></a>建立 container</h2><p>首先先建立所需要的 container 並檢視網路狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 啟動 alpine linux container 1 (alpine1)</span></span><br><span class="line">$ docker run -dit --name alpine1 alpine ash</span><br><span class="line">b75464efd30bf09e118450de2abaeb35549e732bac5805671f1e6e97cd970897</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 alpine linux container 2 (alpine2)</span></span><br><span class="line">$ docker run -dit --name alpine2 alpine ash</span><br><span class="line">16aa4a102b14de3e151cb5e19522925bb4e143034d5b2aac4ce239c79716b703 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前 software bridge status (需安裝 bridge-utils 套件)</span></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name        bridge id              STP enabled        interfaces</span><br><span class="line">docker0            8000.024284431eec      no                vethbddc381</span><br><span class="line">                                                        vethee2a421</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 docker bridge network 狀態</span></span><br><span class="line">$ docker network inspect bridge</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># 使用的 docker network 名稱為 &quot;bridge&quot;</span></span><br><span class="line">        <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Id&quot;</span>: <span class="string">&quot;a1c7b6f8038999f034b8e64ae66885fa8094a020a306ce4f5b5692d7230890b0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Created&quot;</span>: <span class="string">&quot;2018-07-09T01:23:03.98371109Z&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Scope&quot;</span>: <span class="string">&quot;local&quot;</span>,</span><br><span class="line">        <span class="comment"># 使用的 driver 為 bridge</span></span><br><span class="line">        <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">        <span class="string">&quot;EnableIPv6&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;IPAM&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;default&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Options&quot;</span>: null,</span><br><span class="line">            <span class="string">&quot;Config&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Subnet&quot;</span>: <span class="string">&quot;172.17.0.0/16&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;Gateway&quot;</span>: <span class="string">&quot;172.17.0.1&quot;</span></span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Internal&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Attachable&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Ingress&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;ConfigFrom&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Network&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;ConfigOnly&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Containers&quot;</span>: &#123;</span><br><span class="line">            <span class="comment"># 此為 alpine2 所使用的網路 (對應到上面的 container ID)</span></span><br><span class="line">            <span class="string">&quot;16aa4a102b14de3e151cb5e19522925bb4e143034d5b2aac4ce239c79716b703&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine2&quot;</span>,</span><br><span class="line">                <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;4dc3947583ece828dd4371351501e7d0ba9fa149ac5373ea4ddb9466d333b85d&quot;</span>,</span><br><span class="line">                <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:ac:11:00:03&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.17.0.3/16&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv6Address&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="comment"># 此為 alpine1 所使用的網路 (對應到上面的 container ID)</span></span><br><span class="line">            <span class="string">&quot;b75464efd30bf09e118450de2abaeb35549e732bac5805671f1e6e97cd970897&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine1&quot;</span>,</span><br><span class="line">                <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;ea0138a2c51812f3f142db086e1690f6696e36ff972c826727be30e3c8b8cb41&quot;</span>,</span><br><span class="line">                <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:ac:11:00:02&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.17.0.2/16&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv6Address&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Options&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.default_bridge&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.enable_icc&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.enable_ip_masquerade&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.host_binding_ipv4&quot;</span>: <span class="string">&quot;0.0.0.0&quot;</span>,</span><br><span class="line">            <span class="comment"># 使用的 bridge 名稱</span></span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.name&quot;</span>: <span class="string">&quot;docker0&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.driver.mtu&quot;</span>: <span class="string">&quot;1500&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Labels&quot;</span>: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>而目前的網路架構變成如下：</p><p><img src="/blog/images/docker/docker-bridge-network-1.png" alt="docker bridge network"></p><h2 id="測試-container-網路"><a href="#測試-container-網路" class="headerlink" title="測試 container 網路"></a>測試 container 網路</h2><p>了解上面的網路架構之後，接著來測試 container 網路：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 container IP</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ip addr show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試連外</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ping -c 3 www.google.com</span><br><span class="line">PING www.google.com (216.58.200.228): 56 data bytes</span><br><span class="line">64 bytes from 216.58.200.228: seq=0 ttl=54 time=2.112 ms</span><br><span class="line">64 bytes from 216.58.200.228: seq=1 ttl=54 time=2.166 ms</span><br><span class="line">64 bytes from 216.58.200.228: seq=2 ttl=54 time=2.417 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試連線到另外一個 container (使用 IP)</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ping -c 3 172.17.0.3</span><br><span class="line">PING 172.17.0.3 (172.17.0.3): 56 data bytes</span><br><span class="line">64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.121 ms</span><br><span class="line">64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.071 ms</span><br><span class="line">64 bytes from 172.17.0.3: seq=2 ttl=64 time=0.068 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試連線到另外一個 container (使用 domain name) =&gt; 不通</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ping -c 3 alpine2</span><br><span class="line">ping: bad address <span class="string">&#x27;alpine2&#x27;</span></span><br></pre></td></tr></table></figure><p>從上面的測試可以看出，其實兩個 container 由於使用相同的 bridge(<code>docker0</code>) 因此 IP 可以互通，但使用 domain name 卻無法，表示互相不認識對方。</p><h1 id="使用自訂的-Bridge-Network"><a href="#使用自訂的-Bridge-Network" class="headerlink" title="使用自訂的 Bridge Network"></a>使用自訂的 Bridge Network</h1><h2 id="自訂-docker-network"><a href="#自訂-docker-network" class="headerlink" title="自訂 docker network"></a>自訂 docker network</h2><p>這次同樣是使用 bridge driver，但透過 <code>docker network create</code> 指令建立一個自訂名稱的 docker network，這邊取名為 <code>alpine-net</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">$ docker network create --driver bridge alpine-net</span><br><span class="line">2575acac8e781004cb0dc5c5b020c0252a4bc4cdf18dfc661fe3ffcde30fd0b2</span><br><span class="line"></span><br><span class="line">$ docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">2575acac8e78        alpine-net          bridge              <span class="built_in">local</span></span><br><span class="line">a1c7b6f80389        bridge              bridge              <span class="built_in">local</span></span><br><span class="line">dc2f51e1056f        host                host                <span class="built_in">local</span></span><br><span class="line">f28460d3a620        none                null                <span class="built_in">local</span></span><br><span class="line"></span><br><span class="line">$ docker network inspect alpine-net</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># docker bridge network 名稱改變了</span></span><br><span class="line">        <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine-net&quot;</span>,</span><br><span class="line">        <span class="comment"># 此 ID 會對應到新產生的 bridge</span></span><br><span class="line">        <span class="string">&quot;Id&quot;</span>: <span class="string">&quot;2575acac8e781004cb0dc5c5b020c0252a4bc4cdf18dfc661fe3ffcde30fd0b2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Created&quot;</span>: <span class="string">&quot;2018-07-09T03:05:05.505539609Z&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Scope&quot;</span>: <span class="string">&quot;local&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">        <span class="string">&quot;EnableIPv6&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;IPAM&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;default&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Options&quot;</span>: &#123;&#125;,</span><br><span class="line">            <span class="string">&quot;Config&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Subnet&quot;</span>: <span class="string">&quot;172.18.0.0/16&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;Gateway&quot;</span>: <span class="string">&quot;172.18.0.1&quot;</span></span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Internal&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Attachable&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Ingress&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;ConfigFrom&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Network&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;ConfigOnly&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Containers&quot;</span>: &#123;&#125;,</span><br><span class="line">        <span class="string">&quot;Options&quot;</span>: &#123;&#125;,</span><br><span class="line">        <span class="string">&quot;Labels&quot;</span>: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增了一個 software bridge</span></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name            bridge id                STP enabled      interfaces</span><br><span class="line">br-2575acac8e78        8000.0242a2569c8d        no        </span><br><span class="line">docker0                8000.024284431eec        no</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此 bridge 所使用的網段為 172.18.0.1/16，跟 docker0 的不同</span></span><br><span class="line">$ ip a</span><br><span class="line">.... (略)</span><br><span class="line">14: br-2575acac8e78: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default </span><br><span class="line">    link/ether 02:42:a2:56:9c:8d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.0.1/16 brd 172.18.255.255 scope global br-2575acac8e78</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>從上面的結果看的出來，新建立的 docker network(<code>alpine-net</code>) 會被分配使用另一個不同的網段，與原先的 docker0 使用的並不相同。</p><h2 id="建立-container-1"><a href="#建立-container-1" class="headerlink" title="建立 container"></a>建立 container</h2><p>這裡一共要建立4個 container，分別是 <code>alpine1</code>、<code>alpine2</code>、<code>alpine3</code>、<code>alpine4</code>，並進行以下的網路設定：</p><table><thead><tr><th align="center">Container</th><th align="left">Docker Network</th></tr></thead><tbody><tr><td align="center">alpine1</td><td align="left">alpine-net</td></tr><tr><td align="center">alpine2</td><td align="left">alpine-net</td></tr><tr><td align="center">alpine3</td><td align="left">docker0</td></tr><tr><td align="center">alpine4</td><td align="left">alpine-net + docker0</td></tr></tbody></table><p>並透過以下指令來完成：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 啟用 container，使用 alpine-net</span></span><br><span class="line">$ docker run -dit --name alpine1 --network alpine-net alpine ash</span><br><span class="line">010ba5c3e71a723f8c980bb00bb87c97bc41b073163415705006ce5d0071e97c</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用 container，使用 alpine-net</span></span><br><span class="line">$ docker run -dit --name alpine2 --network alpine-net alpine ash</span><br><span class="line">aecb66274c3b3ec9109ff4eaa582ad4708ed871428d3b4894f9d04330aabcbc4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用 container，使用 default bridge docker0</span></span><br><span class="line">$ docker run -dit --name alpine3 alpine ash</span><br><span class="line">3c602d26fcdcc8c0145d4d2d159b20b5b760c1b74393c7b5f7be060f4f8822ac</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用 container，使用 alpine-net</span></span><br><span class="line">$ docker run -dit --name alpine4 --network alpine-net alpine ash</span><br><span class="line">83133e6103d63513116e0fb791efea64b016424ca73357ba3fed41b7ea92df95</span><br><span class="line"></span><br><span class="line"><span class="comment"># 額外將 alpine4 網路在與 default bridge docker0 接上</span></span><br><span class="line">$ docker network connect bridge alpine4</span><br></pre></td></tr></table></figure><p>接著確認一下相關的網路設定是不是都有被正確建立：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 veth device</span></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name            bridge id                STP enabled     interfaces</span><br><span class="line">br-2575acac8e78        8000.0242a2569c8d        no                veth19aee42</span><br><span class="line">                                                            veth403e609</span><br><span class="line">                                                            veth439dc39</span><br><span class="line">docker0                8000.024284431eec        no                veth1c78e99</span><br><span class="line">                                                            veth9857cc8</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 docker bridge - bridge(docker0)</span></span><br><span class="line">$ docker network inspect bridge</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># docker network 名稱</span></span><br><span class="line">        <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Id&quot;</span>: <span class="string">&quot;a1c7b6f8038999f034b8e64ae66885fa8094a020a306ce4f5b5692d7230890b0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Created&quot;</span>: <span class="string">&quot;2018-07-09T01:23:03.98371109Z&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Scope&quot;</span>: <span class="string">&quot;local&quot;</span>,</span><br><span class="line">        <span class="comment"># 使用的 driver 為 bridge</span></span><br><span class="line">        <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">        <span class="string">&quot;EnableIPv6&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;IPAM&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;default&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Options&quot;</span>: null,</span><br><span class="line">            <span class="string">&quot;Config&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Subnet&quot;</span>: <span class="string">&quot;172.17.0.0/16&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;Gateway&quot;</span>: <span class="string">&quot;172.17.0.1&quot;</span></span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Internal&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Attachable&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Ingress&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;ConfigFrom&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Network&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;ConfigOnly&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Containers&quot;</span>: &#123;</span><br><span class="line">            <span class="comment"># 此為 alpine3 所使用的網路 (對應到上面的 container ID)</span></span><br><span class="line">            <span class="string">&quot;3c602d26fcdcc8c0145d4d2d159b20b5b760c1b74393c7b5f7be060f4f8822ac&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine3&quot;</span>,</span><br><span class="line">                <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;7bd6ae1d8a28a554f57f9937953716cc5f62ea8bd669fc744d3aeffacf05f3c1&quot;</span>,</span><br><span class="line">                <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:ac:11:00:02&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.17.0.2/16&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv6Address&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="comment"># 此為 alpine4 所使用的網路 (對應到上面的 container ID)</span></span><br><span class="line">            <span class="string">&quot;83133e6103d63513116e0fb791efea64b016424ca73357ba3fed41b7ea92df95&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine4&quot;</span>,</span><br><span class="line">                <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;2bb63005cf4bcb037d644ab230023e65e8ee9e00cf4d98b3fcf9526c14790492&quot;</span>,</span><br><span class="line">                <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:ac:11:00:03&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.17.0.3/16&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv6Address&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Options&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.default_bridge&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.enable_icc&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.enable_ip_masquerade&quot;</span>: <span class="string">&quot;true&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.host_binding_ipv4&quot;</span>: <span class="string">&quot;0.0.0.0&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.bridge.name&quot;</span>: <span class="string">&quot;docker0&quot;</span>,</span><br><span class="line">            <span class="string">&quot;com.docker.network.driver.mtu&quot;</span>: <span class="string">&quot;1500&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Labels&quot;</span>: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 docker bridge - alpine-net</span></span><br><span class="line">$ docker network inspect alpine-net</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># docker network 名稱</span></span><br><span class="line">        <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine-net&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Id&quot;</span>: <span class="string">&quot;2575acac8e781004cb0dc5c5b020c0252a4bc4cdf18dfc661fe3ffcde30fd0b2&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Created&quot;</span>: <span class="string">&quot;2018-07-09T03:05:05.505539609Z&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Scope&quot;</span>: <span class="string">&quot;local&quot;</span>,</span><br><span class="line">        <span class="comment"># 使用的 driver 為 bridge</span></span><br><span class="line">        <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;bridge&quot;</span>,</span><br><span class="line">        <span class="string">&quot;EnableIPv6&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;IPAM&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Driver&quot;</span>: <span class="string">&quot;default&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Options&quot;</span>: &#123;&#125;,</span><br><span class="line">            <span class="string">&quot;Config&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Subnet&quot;</span>: <span class="string">&quot;172.18.0.0/16&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;Gateway&quot;</span>: <span class="string">&quot;172.18.0.1&quot;</span></span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Internal&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Attachable&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Ingress&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;ConfigFrom&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;Network&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;ConfigOnly&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">        <span class="string">&quot;Containers&quot;</span>: &#123;</span><br><span class="line">            <span class="comment"># 此為 alpine1 所使用的網路 (對應到上面的 container ID)</span></span><br><span class="line">            <span class="string">&quot;010ba5c3e71a723f8c980bb00bb87c97bc41b073163415705006ce5d0071e97c&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine1&quot;</span>,</span><br><span class="line">                <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;f1357dd263dea947c4cd203fd8031be40fae4840f6994dd5e11d6790a038569c&quot;</span>,</span><br><span class="line">                <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:ac:12:00:02&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.18.0.2/16&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv6Address&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="comment"># 此為 alpine4 所使用的網路 (對應到上面的 container ID)</span></span><br><span class="line">            <span class="string">&quot;83133e6103d63513116e0fb791efea64b016424ca73357ba3fed41b7ea92df95&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine4&quot;</span>,</span><br><span class="line">                <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;cf6bb33270eca6d40d4d953cea49302d210bc8ba995f04cdcf138344d5378d2f&quot;</span>,</span><br><span class="line">                <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:ac:12:00:04&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.18.0.4/16&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv6Address&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="comment"># 此為 alpine2 所使用的網路 (對應到上面的 container ID)</span></span><br><span class="line">            <span class="string">&quot;aecb66274c3b3ec9109ff4eaa582ad4708ed871428d3b4894f9d04330aabcbc4&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;Name&quot;</span>: <span class="string">&quot;alpine2&quot;</span>,</span><br><span class="line">                <span class="string">&quot;EndpointID&quot;</span>: <span class="string">&quot;28a237e9fe55f133459140c9e5265470e34baeec43c2c0f6c32d5d539b694403&quot;</span>,</span><br><span class="line">                <span class="string">&quot;MacAddress&quot;</span>: <span class="string">&quot;02:42:ac:12:00:03&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv4Address&quot;</span>: <span class="string">&quot;172.18.0.3/16&quot;</span>,</span><br><span class="line">                <span class="string">&quot;IPv6Address&quot;</span>: <span class="string">&quot;&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">&quot;Options&quot;</span>: &#123;&#125;,</span><br><span class="line">        <span class="string">&quot;Labels&quot;</span>: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>由於 alpine4 特別與兩個 network 連接，因此來檢視一下 alpine4 的網路狀況：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 從下面的輸出可以看出，由於 alpine4 接到了兩個 software bridge，因此會有兩張網卡顯示出來</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine4 ip a</span><br><span class="line">..... (略)</span><br><span class="line">21: eth0@if22: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 02:42:ac:12:00:04 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.18.0.4/16 brd 172.18.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">23: eth1@if24: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP </span><br><span class="line">    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.3/16 brd 172.17.255.255 scope global eth1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>而目前的網路架構變成如下圖：</p><p><img src="/blog/images/docker/docker-bridge-network-custom.png" alt="customize docker bridge network"></p><h2 id="測試-container-網路-1"><a href="#測試-container-網路-1" class="headerlink" title="測試 container 網路"></a>測試 container 網路</h2><p>最後要來測試一下 container 之間的連通性：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#　測試與 alpine2 通訊</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ping -c 2 alpine2</span><br><span class="line">PING alpine2 (172.18.0.3): 56 data bytes</span><br><span class="line">64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.095 ms</span><br><span class="line">64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.076 ms</span><br><span class="line"></span><br><span class="line">--- alpine2 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.076/0.085/0.095 ms</span><br><span class="line"></span><br><span class="line"><span class="comment">#　測試與 alpine4 通訊</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ping -c 2 alpine4</span><br><span class="line">PING alpine4 (172.18.0.4): 56 data bytes</span><br><span class="line">64 bytes from 172.18.0.4: seq=0 ttl=64 time=0.145 ms</span><br><span class="line">64 bytes from 172.18.0.4: seq=1 ttl=64 time=0.102 ms</span><br><span class="line"></span><br><span class="line">--- alpine4 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.102/0.123/0.145 ms</span><br><span class="line"></span><br><span class="line"><span class="comment">#　測試與 alpine3 通訊</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ping -c 2 alpine3</span><br><span class="line">ping: bad address <span class="string">&#x27;alpine3&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#　以 IP 的方式測試與 alpine3 通訊</span></span><br><span class="line">$　docker <span class="built_in">exec</span> -it alpine1 ping -c 2 172.17.0.2</span><br><span class="line">PING 172.17.0.2 (172.17.0.2): 56 data bytes</span><br><span class="line"></span><br><span class="line">--- 172.17.0.2 ping statistics ---</span><br><span class="line">2 packets transmitted, 0 packets received, 100% packet loss</span><br><span class="line"></span><br><span class="line"><span class="comment">#　測試與 alpine1(自己) 通訊</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ping -c 2 alpine1</span><br><span class="line">PING alpine1 (172.18.0.2): 56 data bytes</span><br><span class="line">64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.052 ms</span><br><span class="line">64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.079 ms</span><br><span class="line"></span><br><span class="line">--- alpine1 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.052/0.065/0.079 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試連外能力</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine1 ping -c 2 www.google.com</span><br><span class="line">PING www.google.com (172.217.160.68): 56 data bytes</span><br><span class="line">64 bytes from 172.217.160.68: seq=0 ttl=53 time=7.252 ms</span><br><span class="line">64 bytes from 172.217.160.68: seq=1 ttl=53 time=7.329 ms</span><br><span class="line"></span><br><span class="line">--- www.google.com ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 7.252/7.290/7.329 ms</span><br></pre></td></tr></table></figure><p>從上面的測試可以得出以下結論：</p><ol><li><p><strong>在自訂的 docker network 中，container 可以透過 domain name 的方式與其他在同一個 docker network 下的 container 通訊</strong></p></li><li><p>不同 docker network 之間的 container 網路是隔離無法相互通訊的，不論是透過 domain name or IP</p></li></ol><blockquote><p>第一個結論是因為 docker 對於自訂的 docker network 會提供 <strong>automatic service discovery</strong> 的功能，讓 container 之間可以透過 name 來與對方通訊</p></blockquote><p>另外，若是在同時與 <code>docker0</code> 與 <code>alpine-net</code> 兩個 software bridge 相接的 <strong>alpine4</strong> container 上執行測試會是什麼結果呢?</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可透過 name 與 alpine-net bridge 下的 container(alpine1) 通訊 </span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine4 ping -c 2 alpine1</span><br><span class="line">PING alpine1 (172.18.0.2): 56 data bytes</span><br><span class="line">64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.093 ms</span><br><span class="line">64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.077 ms</span><br><span class="line"></span><br><span class="line">--- alpine1 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.077/0.085/0.093 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 無法透過 name 與 docker0 bridge 下的 container(alpine3) 通訊 </span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine4 ping -c 2 alpine3</span><br><span class="line">ping: bad address <span class="string">&#x27;alpine3&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但還是可透過 IP 與 docker0 bridge 下的 container(alpine3) 通訊</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine4 ping -c 2 172.17.0.2</span><br><span class="line">PING 172.17.0.2 (172.17.0.2): 56 data bytes</span><br><span class="line">64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.140 ms</span><br><span class="line">64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.092 ms</span><br><span class="line"></span><br><span class="line">--- 172.17.0.2 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.092/0.116/0.140 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試連外能力</span></span><br><span class="line">$ docker <span class="built_in">exec</span> -it alpine4 ping -c 2 www.google.com</span><br><span class="line">PING www.google.com (172.217.160.68): 56 data bytes</span><br><span class="line">64 bytes from 172.217.160.68: seq=0 ttl=53 time=7.429 ms</span><br><span class="line">64 bytes from 172.217.160.68: seq=1 ttl=53 time=7.361 ms</span><br><span class="line"></span><br><span class="line">--- www.google.com ping statistics ---</span><br><span class="line">2 packets transmitted, 2 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 7.361/7.395/7.429 ms</span><br></pre></td></tr></table></figure><p>從 alpine4 上的測試可以發現以下結論：</p><ol><li><p>在 <code>docker0</code> bridge 下的 container，都無法透過 domain name 相互溝通</p></li><li><p>承上，使用 IP 通訊是沒問題的</p></li></ol><p>有了以下測試結果，若要使用 domain name 處理 container 之間的通訊時，就記得必須建立一個新的 docker network 來處理才可以正常通訊。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://docs.docker.com/network">Configure Network | Docker Documentation</a></p></li><li><p><a href="https://docs.docker.com/engine/swarm/">Swarm mode overview | Docker Documentation</a></p></li><li><p><a href="https://blog.docker.com/2016/12/understanding-docker-networking-drivers-use-cases/">Understanding Docker Networking Drivers and their use cases - Docker Blog</a></p></li><li><p><a href="https://docs.docker.com/network/network-tutorial-standalone/">Bridge Network Tutorial | Docker Documentation</a></p></li><li><p><a href="http://blog.daocloud.io/docker-source-code-analysis-part7-first/">Docker源码分析（七）：Docker Container网络 （上） - DaoCloud</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker 網路簡介</title>
      <link href="/blog/Docker/docker-network-overview/"/>
      <url>/blog/Docker/docker-network-overview/</url>
      
        <content type="html"><![CDATA[<p>剛開始使用 docker 的人可能不會對於 docker network 的細節有過多的研究，但若是要將 docker 的功能發揮的淋漓盡致，docker network 相關的知識可是必須要必備的。</p><p>以下將會介紹目前 docker 預設支援的網路類型，包含：</p><ul><li><p>bridge</p></li><li><p>host</p></li><li><p>overlay</p></li><li><p>macvlan</p></li><li><p>none</p></li></ul><p>讓使用者可以知道在何種情況下，啟用 container 時應該要選擇哪一種網路類型。</p><h1 id="Network-Drivers"><a href="#Network-Drivers" class="headerlink" title="Network Drivers"></a>Network Drivers</h1><p>為了讓 docker 支援多種網路類型，docker networking subsystem 被設計成是以可抽換 driver 的形式來運作，可根據使用者需求置換不同的 driver 來達到不同的網路設定目的；其中有以下5個 driver 已經是預設存在的，用來提供核心的網路功能：</p><ul><li><p>bridge</p></li><li><p>host</p></li><li><p>overlay</p></li><li><p>macvlan</p></li><li><p>none </p></li></ul><h1 id="Bridge-Network"><a href="#Bridge-Network" class="headerlink" title="Bridge Network"></a>Bridge Network</h1><p>此為預設的 network driver(沒特別指定就是這個)，通常會用在當 container 都是以 standalone 的方式運作並與其他的 container 相互溝通時，架構圖如下：</p><p><img src="https://cdn-images-1.medium.com/max/1060/0*cMUND9w1bO1o5sPe.png" alt="docker network - bridge"></p><ul><li><p>docker 會新增一個 software bridge 作為 container 網路對外的出口，預設名稱為 <code>docker0</code></p></li><li><p>docker0 會與 host 中的對外網卡(上圖為 <code>eth0</code>)相連，藉此取得對外連線的能力</p></li><li><p>每個 container 會使用一個 veth device 與 docker0 相連，因此具備連外能力</p></li></ul><h1 id="Host-Network"><a href="#Host-Network" class="headerlink" title="Host Network"></a>Host Network</h1><p>此模式的網路有以下特點：</p><ul><li><p>不使用獨立的網路(並非獨立的 network stack)，而是與 Docker Host 使用相同的網路</p></li><li><p>目前這個模式僅在 Linux 上有效，無法在 Mac or Windows 上使用</p></li><li><p>在 Docker 17.06 版以後可以在 swarm service 上使用此模式，但同一個 port 只能被用一次</p></li><li><p>container 如果沒有對外開 port 提供服務，其實設定為 host mode 並沒有意義</p></li></ul><p>網路架構如下圖：</p><p><img src="https://success.docker.com/api/images/.%2Frefarch%2Fnetworking%2Fimages%2Fhost-driver.png" alt="docker network - host"></p><h1 id="Overlay-Network"><a href="#Overlay-Network" class="headerlink" title="Overlay Network"></a>Overlay Network</h1><p>overlay network 是藉由將多個 docker daemon 連結起來，並啟用 swarm service 來讓多個 container 可以相互溝通，有以下特點：</p><ul><li><p>需要 Docker Swarm</p></li><li><p>可讓在不同 host 上的 standalone container 互相溝通</p></li><li><p>可讓 standalone container 與 swarm service 互相溝通</p></li></ul><p>由於 overlay mode 需要 Docker Swarm，因此會先忽略不深入討論，因為未來的學習方向會以 Kubernetes 為主。</p><p>Overlay network 的網路架構如下圖：</p><p><img src="http://img.scoop.it/1nNoIXGkJiDax7l5g5GxH7nTzqrqzN7Y9aBZTaXoQ8Q=" alt="docker network - overlay"></p><h1 id="MACVLAN-Network"><a href="#MACVLAN-Network" class="headerlink" title="MACVLAN Network"></a>MACVLAN Network</h1><p>macvlan network 可以讓使用者直接分配實體網卡的 MAC address 給特定的 container，讓 container 可以透過實體的網卡使用網路；有些需要獨立使用實體網卡的老舊應用程式，可能會用到 macvlan 來設定網路。</p><p>換個角度思考，若是要將上述老舊應用程式的網路與 Host 網路隔開，多加一張實體網卡並設定 macvlan 是個不錯的方式。</p><p>此外，若是實體網路接的是 switch 上的 trunk port，也可以透過 MACVLAN driver 將不同的 VLAN 分配給不同的 container 使用，而 MACVLAN 的網路架構如下：</p><p><img src="http://img.scoop.it/zD6OR5JZu3qF9dxWL79Gc7nTzqrqzN7Y9aBZTaXoQ8Q=" alt="docker network - macvlan"></p><h1 id="None"><a href="#None" class="headerlink" title="None"></a>None</h1><p>不設定 container 的網路，因此 container 無法對外通訊</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://docs.docker.com/network">Configure Network | Docker Documentation</a></p></li><li><p><a href="https://docs.docker.com/engine/swarm/">Swarm mode overview | Docker Documentation</a></p></li><li><p><a href="https://blog.docker.com/2016/12/understanding-docker-networking-drivers-use-cases/">Understanding Docker Networking Drivers and their use cases - Docker Blog</a></p></li><li><p><a href="http://blog.daocloud.io/docker-source-code-analysis-part7-first/">Docker源码分析（七）：Docker Container网络 （上） - DaoCloud</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] 設定 Ingress Controller (以 Traefik 為例)</title>
      <link href="/blog/Kubernetes/k8s-Install-traefik-as-Ingress-Controller/"/>
      <url>/blog/Kubernetes/k8s-Install-traefik-as-Ingress-Controller/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前提到的 <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod</a> &amp; <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> 都會各自擁有一個 IP address 供存取，但這些 IP 僅有在 K8s cluster 內部才有辦法存取的到，但若要在 K8s cluster 上提供對外服務呢?</p><p>而目前可讓外部存取 K8s 上服務的方式主要有三種：</p><ol><li><p><strong>Service NodePort</strong>: 這方法會在每個 master &amp; worker node 上都開啟指定的 port number (這樣其實造成不少資源浪費)</p></li><li><p><strong>Service LoadBalancer</strong>: 只有在 GCP or AWS 這類的 public cloud 平台才有用的功能</p></li><li><p><strong><a href="https://kubernetes.io/docs/concepts/services-networking/ingress/" title="Kubernetes Ingress">Ingress</a> 就是被設計來處理這類的問題</strong>：即為此篇文章要談的主角</p></li></ol><p>首先，在沒有 Ingress 之前，從外面到 K8s cluster 的流量會如下圖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">  internet</span><br><span class="line">      |</span><br><span class="line">------------</span><br><span class="line">[ Services ]</span><br></pre></td></tr></table></figure><blockquote><p>但在原有的模式下，如果是在 GCP or AWS 使用 K8s 的話，還可以搭配 LoadBalancer 的 service type 動態取得 LB 對外提供服務；但如果是自己架設 K8s，那就只能透過 Service NodePort 的方式讓使用者從外部存取運行在 K8s 上的服務。</p></blockquote><p>所以 Ingress 在 K8s v1.1 之後就應運而生了，而 Ingress 其實就是一堆 rule 的集合，讓外面進來的網路流量可以正確的被導到後方的 Service，架構變成如下圖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> internet</span><br><span class="line">     |</span><br><span class="line">[ Ingress ]</span><br><span class="line">--|-----|--</span><br><span class="line">[ Services ]</span><br></pre></td></tr></table></figure><p>在官網文件中提到 Ingress 可以負責以下工作：</p><ul><li><p>give services externally-reachable urls</p></li><li><p>load balance traffic</p></li><li><p>SSL Termination</p></li><li><p>offer name based virtual hosting</p></li></ul><p>看的出來可以做到的功能實在很多，非常值得好好研究一下。</p><h1 id="在使用-Ingress-之前"><a href="#在使用-Ingress-之前" class="headerlink" title="在使用 Ingress 之前"></a>在使用 Ingress 之前</h1><p>看起來 Ingress 就是處理 inbound traffic 的銀彈，但直接在 YAML 中宣告 Ingress resource 並建立並不會有任何效果，必須搭配 <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-controllers" title="Kubernetes Ingress Controller">Ingress controller</a> 才有辦法讓設定真正的生效，而不同於其他的 controller，Ingress controller 是屬於 <code>kube-controller-manager</code> 的一部份，而且會自動的跟著 cluster 一起啟動。</p><blockquote><p>目前 <a href="https://git.k8s.io/ingress-nginx/README.md">K8s 官方 GitHub repository</a> 中提供的則是以 nginx 來作為 Ingress controller。</p></blockquote><p>但我找到另外一個也開發的相當不錯的 HTTP reverse proxy 作為 Ingress Controller，名稱為 <a href="https://docs.traefik.io/" title="Træfik">Træfik</a>，它可以與相當多的平台或軟體進行整合，例如：Docker, Docker Swarm, Marathon, Consul, etcd, Rancher, Amazon …. 等等，當然 Kubernetes 也是其中之一。</p><p>而為什麼要選擇 <a href="https://docs.traefik.io/" title="Træfik">Træfik</a> 呢? 來看看[官網][<a href="https://docs.traefik.io/" title="Træfik">traefik</a>]提供的 feature 列表:</p><ul><li><p>Continuously updates its configuration (No restarts!)</p></li><li><p>Supports multiple load balancing algorithms</p></li><li><p>Provides HTTPS to your microservices by leveraging Let’s Encrypt (wildcard certificates support)</p></li><li><p>Circuit breakers, retry</p></li><li><p>High Availability with cluster mode (beta)</p></li><li><p>See the magic through its clean web UI</p></li><li><p>Websocket, HTTP/2, GRPC ready</p></li><li><p>Provides metrics (Rest, Prometheus, Datadog, Statsd, InfluxDB)</p></li><li><p>Keeps access logs (JSON, CLF)</p></li><li><p>Fast</p></li><li><p>Exposes a Rest API</p></li><li><p>Packaged as a single binary file (made with :heart: with go) and available as a tiny official docker image</p></li></ul><p><a href="https://docs.traefik.io/" title="Træfik">Træfik</a> 功能真的是相當強大阿! 當然要給它試試看….</p><p>因此接下來將會以 <a href="https://docs.traefik.io/" title="Træfik">Træfik</a> 作為 Kubernetes Ingress Controller 作示範。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME            STATUS    ROLES          AGE       VERSION</span><br><span class="line">kube-ingress0   Ready     ingress,node   6d        v1.10.4</span><br><span class="line">kube-ingress1   Ready     ingress,node   6d        v1.10.4</span><br><span class="line">kube-master0    Ready     master         6d        v1.10.4</span><br><span class="line">kube-master1    Ready     master         6d        v1.10.4</span><br><span class="line">kube-master2    Ready     master         6d        v1.10.4</span><br><span class="line">kube-worker0    Ready     node           6d        v1.10.4</span><br><span class="line">kube-worker1    Ready     node           6d        v1.10.4</span><br><span class="line">kube-worker2    Ready     node           6d        v1.10.4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 Ingress Node 加上 lable</span></span><br><span class="line">$ kubectl label node kube-ingress0 k8s-app=traefik-ingress-lb</span><br><span class="line">$ kubectl label node kube-ingress1 k8s-app=traefik-ingress-lb</span><br></pre></td></tr></table></figure><p>若要檢視目前每個 node 所帶的 label 可使用以下指令：</p><blockquote><p>kubectl get nodes –show-labels</p></blockquote><h1 id="佈署-Traefik-Ingress-Controller"><a href="#佈署-Traefik-Ingress-Controller" class="headerlink" title="佈署 Træfik Ingress Controller"></a>佈署 Træfik Ingress Controller</h1><h2 id="設定-RBAC"><a href="#設定-RBAC" class="headerlink" title="設定 RBAC"></a>設定 RBAC</h2><p>首先要讓 Træfik 有相對應的權限，將會進行以下設定：</p><ol><li><p>為 Træfik 建立一個 cluster role(<code>traefik-ingress-controller</code>)，並給定足夠的權限</p></li><li><p>為 Træfik 建立一個 service account(<code>traefik-ingress-controller</code>)，並繫結(透過 cluster role binding <code>traefik-ingress-controller</code>)到上述的 cluster role 以取得權限</p></li></ol><p>準備檔案 <code>traefik-rbac.yaml</code>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-ingress-controller</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">services</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">endpoints</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">secrets</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">extensions</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ingresses</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-ingress-controller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-ingress-controller</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-ingress-controller</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><blockquote><p>kubectl apply -f traefik-rbac.yaml</p></blockquote><p>或是也可以直接套用 traefik 最新的程式碼：</p><blockquote><p>kubectl apply -f <a href="https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-rbac.yaml">https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-rbac.yaml</a></p></blockquote><h2 id="設定以-DaemonSet-的方式執行"><a href="#設定以-DaemonSet-的方式執行" class="headerlink" title="設定以 DaemonSet 的方式執行"></a>設定以 DaemonSet 的方式執行</h2><p>traefik 可以用 Deployment or DaemonSet 的方式進行佈署，兩者的差異比較可以參考<a href="https://docs.traefik.io/user-guide/kubernetes/#deploy-trfik-using-a-deployment-or-daemonset">官網說明</a>，以下是使用 DaemonSet 安裝所需要的 YAML 設定(<code>traefik-ds.yaml</code>)：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-ingress-controller</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-ingress-controller</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">traefik-ingress-controller</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">60</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">traefik</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">hostPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">admin</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">        <span class="attr">securityContext:</span></span><br><span class="line">          <span class="attr">capabilities:</span></span><br><span class="line">            <span class="attr">drop:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">ALL</span></span><br><span class="line">            <span class="attr">add:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">NET_BIND_SERVICE</span></span><br><span class="line">        <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--api</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--kubernetes</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--logLevel=INFO</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-ingress-service</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">admin</span></span><br></pre></td></tr></table></figure><p>套用上述設定：</p><blockquote><p>kubectl apply -f traefik-ds.yaml</p></blockquote><p>或是也可以直接套用 traefik 最新的程式碼：</p><blockquote><p>kubectl apply -f <a href="https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-ds.yaml">https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-ds.yaml</a></p></blockquote><p>到此階段，traefik ingress controller 已經佈署完成，接著就是定義 ingress 並與其繫結。</p><h2 id="佈署-Traefik-Dashboard"><a href="#佈署-Traefik-Dashboard" class="headerlink" title="佈署 Traefik Dashboard"></a>佈署 Traefik Dashboard</h2><p>traefik 有個不錯的優點就是提供了一個還蠻不錯看的 Dashboard，當然也是需要透過設定 ingress 才有辦法連到，因此準備以下檔案(<code>traefik-ui.yaml</code>)：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-web-ui</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">traefik-ingress-lb</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-web-ui</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">traefik-ui.k8s.example.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">traefik-web-ui</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><blockquote><p>kubectl apply -f traefik-ui.yaml</p></blockquote><p>除此之外，還有 DNS 的部份需要設定；在上面的 ingress host 設定是 <code>traefik-ui.k8s.example.com</code>，由於這個 domain name 是虛擬的，因此我們必須修改 client 端的 DNS 設定(<code>/etc/hosts</code>)，將 worker node 的 IP 指到這個 domain name。</p><p>在我的實驗環境中，worker node 一共有三台，分別是 **10.103.10.[61-63]**，因此在 client 的 <code>/etc/hosts</code> 中加入以下設定</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.103.10.61  traefik-ui.k8s.example.com</span><br><span class="line">10.103.10.62  traefik-ui.k8s.example.com</span><br><span class="line">10.103.10.63  traefik-ui.k8s.example.com</span><br></pre></td></tr></table></figure><p>完成之後就可以透過 <a href="http://traefik-ui.k8s.example.com/">http://traefik-ui.k8s.example.com</a> 連到 Traefik Dashboard 了!</p><h1 id="佈署網站-amp-設定-Ingress"><a href="#佈署網站-amp-設定-Ingress" class="headerlink" title="佈署網站 &amp; 設定 Ingress"></a>佈署網站 &amp; 設定 Ingress</h1><p>在這邊會另外建立一個 <code>ingress-test</code> 作為測試用的 namespace，並設定為 kubectl 的 default namespace：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立新的 namespace 並改為預設值</span></span><br><span class="line">$ kubectl create ns ingress-test</span><br><span class="line">$ kubectl config set-context $(kubectl config current-context) --namespace=ingress-test</span><br></pre></td></tr></table></figure><h2 id="設定-DNS"><a href="#設定-DNS" class="headerlink" title="設定 DNS"></a>設定 DNS</h2><p>接下來的範例會需要有相對應的 DNS 設定，首先必須確認兩件事情：</p><ol><li><p>traefik DaemonSet 所存在的機器 IP (一般就是 worker node)</p></li><li><p>可控制的 domain name</p></li></ol><p>在本環境中，worker node 一共有三台，分別是 **10.103.10.[61-63]**，將會用到的 domain name 為：</p><ul><li><p>stilton.k8s.example.com</p></li><li><p>cheddar.k8s.example.com</p></li><li><p>wensleydale.k8s.example.com</p></li></ul><p>由於上述的並非真的由我控制的 domain，因此我們必須修改 client 端的 DNS 設定(<code>/etc/hosts</code>) 如下：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.103.10.61  stilton.k8s.example.com cheddar.k8s.example.com wensleydale.k8s.example.com</span><br><span class="line">10.103.10.62  stilton.k8s.example.com cheddar.k8s.example.com wensleydale.k8s.example.com</span><br><span class="line">10.103.10.63  stilton.k8s.example.com cheddar.k8s.example.com wensleydale.k8s.example.com</span><br></pre></td></tr></table></figure><p>接著以下會分為 <strong>Name-based Routing</strong> &amp; <strong>Path-bsaed Routing</strong> 進行示範：</p><h2 id="Name-based-Routing"><a href="#Name-based-Routing" class="headerlink" title="Name-based Routing"></a>Name-based Routing</h2><h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><p>準備 Deployment 檔案(<code>cheese-deployments.yaml</code>)：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stilton</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">    <span class="attr">cheese:</span> <span class="string">stilton</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">      <span class="attr">task:</span> <span class="string">stilton</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">        <span class="attr">task:</span> <span class="string">stilton</span></span><br><span class="line">        <span class="attr">version:</span> <span class="string">v0.0.1</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cheese</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">errm/cheese:stilton</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cheddar</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">    <span class="attr">cheese:</span> <span class="string">cheddar</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">      <span class="attr">task:</span> <span class="string">cheddar</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">        <span class="attr">task:</span> <span class="string">cheddar</span></span><br><span class="line">        <span class="attr">version:</span> <span class="string">v0.0.1</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cheese</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">errm/cheese:cheddar</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">wensleydale</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">    <span class="attr">cheese:</span> <span class="string">wensleydale</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">      <span class="attr">task:</span> <span class="string">wensleydale</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">        <span class="attr">task:</span> <span class="string">wensleydale</span></span><br><span class="line">        <span class="attr">version:</span> <span class="string">v0.0.1</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cheese</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">errm/cheese:wensleydale</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><blockquote><p>kubectl apply -f cheese-deployments.yaml</p></blockquote><p>或是可以直接套用官網提供的 Deployment 範例：</p><blockquote><p>kubectl apply -f <a href="https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-deployments.yaml">https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-deployments.yaml</a></p></blockquote><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>接著準備 Service 檔案(<code>cheese-services.yaml</code>)：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stilton</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">    <span class="attr">task:</span> <span class="string">stilton</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cheddar</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">    <span class="attr">task:</span> <span class="string">cheddar</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">wensleydale</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">traefik.backend.circuitbreaker:</span> <span class="string">&quot;NetworkErrorRatio() &gt; 0.5&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">cheese</span></span><br><span class="line">    <span class="attr">task:</span> <span class="string">wensleydale</span></span><br></pre></td></tr></table></figure><blockquote><p>kubectl apply -f cheese-services.yaml</p></blockquote><p>或是可以直接套用官網提供的 Deployment 範例：</p><blockquote><p>kubectl apply -f <a href="https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-services.yaml">https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-services.yaml</a></p></blockquote><h3 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h3><p>最後則是 Ingress 的設定(<code>name-based-routing_cheese-ingress.yaml</code>)：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cheese</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">stilton.k8s.example.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">stilton</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="string">http</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">cheddar.k8s.example.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">cheddar</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="string">http</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">wensleydale.k8s.example.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">wensleydale</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="string">http</span></span><br></pre></td></tr></table></figure><blockquote><p>kubectl apply -f name-based-routing_cheese-ingress.yaml</p></blockquote><p>最後就可以透過以下3個連結連到不同的 Ingress -&gt; Service -&gt; Deployment：</p><ul><li><p><a href="http://stilton.k8s.example.com/">http://stilton.k8s.example.com</a></p></li><li><p><a href="http://cheddar.k8s.example.com/">http://cheddar.k8s.example.com</a></p></li><li><p><a href="http://wensleydale.k8s.example.com/">http://wensleydale.k8s.example.com</a></p></li></ul><h2 id="Path-bsaed-Routing"><a href="#Path-bsaed-Routing" class="headerlink" title="Path-bsaed Routing"></a>Path-bsaed Routing</h2><p>這個在 DNS 設定部份就簡單多了，在以下範例中，只要設定一個 DNS record(<code>cheese.k8s.example.com</code>) 即可，因此 DNS 設定(<code>/etc/hosts</code>)必須加入以下資訊：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.103.10.61  cheese.k8s.example.com</span><br><span class="line">10.103.10.62  cheese.k8s.example.com</span><br><span class="line">10.103.10.63  cheese.k8s.example.com</span><br></pre></td></tr></table></figure><p>而在 <strong>Deployment</strong> &amp; <strong>Service</strong> 的部份跟 Name-based Routing 是相同的，而這裡的 ingress 設定(<code>path-based-routing_cheese-ingress.yaml</code>)則是跟上面不同：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cheeses</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">traefik</span></span><br><span class="line">    <span class="attr">traefik.frontend.rule.type:</span> <span class="string">PathPrefixStrip</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">cheeses.k8s.example.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/stilton</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">stilton</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="string">http</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/cheddar</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">cheddar</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="string">http</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/wensleydale</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">wensleydale</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="string">http</span></span><br></pre></td></tr></table></figure><blockquote><p>kubectl apply -f path-based-routing_cheese-ingress.yaml</p></blockquote><p>最後就可以透過以下3個連結連到不同的 Ingress -&gt; Service -&gt; Deployment：</p><ul><li><p><a href="http://cheeses.k8s.example.com/stilton">http://cheeses.k8s.example.com/stilton</a></p></li><li><p><a href="http://cheeses.k8s.example.com/cheddar">http://cheeses.k8s.example.com/cheddar</a></p></li><li><p><a href="http://cheeses.k8s.example.com/wensleydale">http://cheeses.k8s.example.com/wensleydale</a></p></li></ul><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/concepts/services-networking/ingress">Ingress - Kubernetes</a></p></li><li><p><a href="https://docs.traefik.io/">Træfik</a></p></li><li><p><a href="https://ithelp.ithome.com.tw/articles/10196261">[Day 19] 在 Kubernetes 中實現負載平衡 - Ingress Controller - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Networking </tag>
            
            <tag> CKA Networking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Kubernetes] 設定 StorageClass (以 Ceph RBD 為例)</title>
      <link href="/blog/Kubernetes/k8s-Config-StorageClass-with-Ceph-RBD/"/>
      <url>/blog/Kubernetes/k8s-Config-StorageClass-with-Ceph-RBD/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>安裝完 k8s cluster 後，接著緊接下來要解決的問題就是 “資料儲存” 的問題。</p><p>由於 container 的生命周期跟 VM 不同，而且在 k8s cluster 上，container 可能會因為某些原因在不同的機器之間遷移，因此”<strong>資料儲存</strong>“這件事情就變得要謹慎考慮。</p><p>此篇文章將會介紹如何使用 Ceph RBD 為 k8s cluster 提供 block device 的 persistent volume。此外由於本文是著重在 k8s 與 Ceph 的整合，因此就不會再 k8s &amp; Ceph 的安裝著墨，而是直接切入整合時的設定。</p><h1 id="環境說明"><a href="#環境說明" class="headerlink" title="環境說明"></a>環境說明</h1><p>Ceph: <code>Luminous</code></p><p>Kubernetes：<code>1.10.4</code></p><h1 id="設定-Ceph-cluster"><a href="#設定-Ceph-cluster" class="headerlink" title="設定 Ceph cluster"></a>設定 Ceph cluster</h1><p>首先要在 Ceph cluster 完成以下幾件事情：</p><ol><li><p>建立一個給 k8s cluster 用的 pool (例如： <code>kube</code>)</p></li><li><p>設定 pool 相關參數 (replica 數量，Quota …. etc)</p></li><li><p>新增使用該 pool 的使用者帳號(例如： <code>kube</code>)，並指定該 pool 的存取權限</p></li></ol><p>接著我們透過以下指令完成上面的工作：(必須以 Ceph 管理者的權限進行設定)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立名稱為 &quot;kube&quot; 的 pool</span></span><br><span class="line">$ ceph osd pool create kube 128</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定停用 replication 功能</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> kube size 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 pool 最大容量為 10GB</span></span><br><span class="line">$ ceph osd pool set-quota kube max_bytes $((<span class="number">10</span> * <span class="number">1024</span> * <span class="number">1024</span> * <span class="number">1024</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立使用該 pool 的使用者帳號</span></span><br><span class="line">$ ceph auth get-or-create client.kube mon <span class="string">&#x27;allow r&#x27;</span> osd <span class="string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=kube&#x27;</span></span><br><span class="line">[client.kube]</span><br><span class="line">    key = [YOUR_KEY]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 Ceph admin keyring</span></span><br><span class="line">$ ceph auth get-key client.admin | base64</span><br><span class="line">QVFEcytKaGF6UVlmRmhBQWJOZTNaZjYvaFVFdkhpRVVQejJOWFE9PQ==</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 Ceph user keyring</span></span><br><span class="line">$ ceph auth get-key client.kube | base64</span><br><span class="line">QVFDTmp6RmJONy9wRkJBQUZxN3QzQnVLaTJpb2YwR0dDZEJ2dEE9PQ==</span><br></pre></td></tr></table></figure><p>為了測試接下來要示範的 persistent volume，再多建立一個名稱為 <code>ceph-image</code> 的 RBD image：</p><blockquote><p>rbd create kube/ceph-image –size 4096 –image-format 2 –image-feature layering</p></blockquote><p><strong>Ceph 預設的 Crush Map 設定會讓 k8s 與 Ceph 的整合發生問題，因此要先調整 Crush Map，詳情可以參考下方的<a href="#%E9%9A%9C%E7%A4%99%E6%8E%92%E9%99%A4">障礙排除</a>。</strong></p><h1 id="設定-Kubernetes"><a href="#設定-Kubernetes" class="headerlink" title="設定 Kubernetes"></a>設定 Kubernetes</h1><h2 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h2><p>由於在 mount Ceph RBD image 之前，kubelet 會檢查 image 的狀態，因此在 k8s worker node 都需要額外進行以下調整：</p><ol><li><p>安裝 <code>ceph-common</code> 套件</p></li><li><p>將 ceph admin keyring 放到 <code>/etc/ceph/ceph.keyring</code></p></li></ol><h2 id="獨立的-namespace"><a href="#獨立的-namespace" class="headerlink" title="獨立的 namespace"></a>獨立的 namespace</h2><p>為了讓系統環境維持乾淨，接著建立一個獨立的 namespace(名稱為 <code>ceph-rbd-pv-lab</code>) 來進行以下測試：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 namespace</span></span><br><span class="line">$ kubectl create namespace ceph-rbd-pv-lab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定預設的 namespace</span></span><br><span class="line">$ kubectl config set-context $(kubectl config current-context) --namespace==ceph-rbd-pv-lab</span><br></pre></td></tr></table></figure><h1 id="使用-Ceph-RBD-作為-Persistent-Volume"><a href="#使用-Ceph-RBD-作為-Persistent-Volume" class="headerlink" title="使用 Ceph RBD 作為 Persistent Volume"></a>使用 Ceph RBD 作為 Persistent Volume</h1><p>準備檔案 <code>ceph-pv.yaml</code>，內容如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定義 secret，儲存 Ceph admin keyring</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-secret-admin</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">key:</span> <span class="string">QVFEcytKaGF6UVlmRmhBQWJOZTNaZjYvaFVFdkhpRVVQejJOWFE9PQ==</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義 PV，指定到上面建立的 ceph rbd image</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">rbd:</span></span><br><span class="line">    <span class="attr">monitors:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">10.103</span><span class="number">.2</span><span class="number">.24</span><span class="string">:6789</span></span><br><span class="line">    <span class="attr">pool:</span> <span class="string">kube</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ceph-image</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">admin</span></span><br><span class="line">    <span class="attr">secretRef:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">ceph-secret-admin</span></span><br><span class="line">    <span class="attr">fsType:</span> <span class="string">ext4</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義 PVC，與上面的 PV 進行繫結</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-claim</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span>     </span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義使用 PVC 的 pod</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-pod1</span>           </span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-busybox</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span>          </span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;sleep&quot;</span>, <span class="string">&quot;60000&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-vol1</span>       </span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/usr/share/busybox</span> </span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-vol1</span>         </span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">ceph-claim</span></span><br></pre></td></tr></table></figure><p>套用以上設定：</p><blockquote><p>kubectl create -f ceph-pv.yaml</p></blockquote><p>過一陣子檢查 pod 狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pod 已經處於正常運作的狀態</span></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME        READY     STATUS    RESTARTS   AGE</span><br><span class="line">ceph-pod1   1/1       Running   0          2m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 成功掛載在 /usr/share/busybox 目錄下</span></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it ceph-pod1 -- df -h | grep <span class="string">&#x27;/usr/share/busybox&#x27;</span></span><br><span class="line">/dev/rbd0                 3.8G      8.0M      3.6G   0% /usr/share/busybox</span><br></pre></td></tr></table></figure><h1 id="使用-StorageClass-動態生成-Persistent-Volume"><a href="#使用-StorageClass-動態生成-Persistent-Volume" class="headerlink" title="使用 StorageClass 動態生成 Persistent Volume"></a>使用 StorageClass 動態生成 Persistent Volume</h1><h2 id="What’s-StorageClass"><a href="#What’s-StorageClass" class="headerlink" title="What’s StorageClass?"></a>What’s StorageClass?</h2><p>不曉得有沒有人想過以下問題：</p><ul><li><p>如果我常常需要產生 PV，又要刪除它呢 ?</p></li><li><p>可以不要一直去煩 storage manager 嗎? 每次產生新的 PV 都需要請他建立對應的 RBD image</p></li><li><p>產生 PV 的動作可以自動完成嗎 ?</p></li><li><p>每次都要手動建立 RBD image 哪有 cloud native ?</p></li></ul><p>而 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes" title="Kubernetes Storage Class">Kubernetes StorageClass</a> 就是以上問題的解答，其流程如下圖：</p><p><img src="/blog/images/kubernetes/dynamic-volume-provision.jpg" alt="Kubernetes Persistent Volume Provisioning"></p><ol><li><p>設定 persistent volume provisioner (Ceph RBD 已經被 k8s 支援)</p></li><li><p>k8s ckuster 管理者建立 Storage Class，並指定要使用的 PV provisioner(這裡使用 <code>kubernetes.io/rbd</code>)</p></li><li><p>使用者建立 PVC，指定要使用的 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes" title="Kubernetes Storage Class">StorageClass</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/storage/storage-classes" title="Kubernetes Storage Class">StorageClass</a> 使用 provisioner 在實際的 storage 上產生 volume，並建立 PV 與其繫結</p></li><li><p>將 PV 與 使用者 PVC 進行繫結</p></li><li><p>使用者建立 pod，並使用 PVC 取得外部的儲存空間</p></li></ol><blockquote><p><strong>以上的重點在於，管理者再也不用辛苦的手動建立儲存空間，並設定 PV 與其繫結了，這個部份都交給 StorageClass 自動處理</strong></p></blockquote><h2 id="在-Kubernetes-上設定-StorageClass"><a href="#在-Kubernetes-上設定-StorageClass" class="headerlink" title="在 Kubernetes 上設定 StorageClass"></a>在 Kubernetes 上設定 StorageClass</h2><p>準備以下設定檔，名稱為 <code>ceph-storageclass.yml</code>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># admin secret 要定義在 cluster level (對應 PV 的生成)</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-secret-admin</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">key:</span> <span class="string">QVFEcytKaGF6UVlmRmhBQWJOZTNaZjYvaFVFdkhpRVVQejJOWFE9PQ==</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">kubernetes.io/rbd</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># user secret 要定義在 namespace level (對應 PVC 的生成 &amp; 繫結)</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-secret-user</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">key:</span> <span class="string">QVFDTmp6RmJONy9wRkJBQUZxN3QzQnVLaTJpb2YwR0dDZEJ2dEE9PQ==</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">kubernetes.io/rbd</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># user secret 要定義在 namespace level (對應 PVC 的生成 &amp; 繫結)</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-dynamic</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">     <span class="attr">storageclass.beta.kubernetes.io/is-default-class:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/rbd</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">monitors:</span> <span class="number">10.103</span><span class="number">.2</span><span class="number">.24</span><span class="string">:6789</span></span><br><span class="line">  <span class="attr">adminId:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">adminSecretName:</span> <span class="string">ceph-secret-admin</span></span><br><span class="line">  <span class="attr">adminSecretNamespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">pool:</span> <span class="string">kube</span></span><br><span class="line">  <span class="attr">userId:</span> <span class="string">kube</span></span><br><span class="line">  <span class="attr">userSecretName:</span> <span class="string">ceph-secret-user</span></span><br><span class="line">  <span class="attr">fsType:</span> <span class="string">ext4</span></span><br><span class="line">  <span class="attr">imageFormat:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">  <span class="attr">imageFeatures:</span> <span class="string">&quot;layering&quot;</span></span><br></pre></td></tr></table></figure><p>套用上述設定檔即可完成：</p><blockquote><p>kubectl create -f ceph-storageclass.yml</p></blockquote><p>從以上可以看出，其實 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes" title="Kubernetes Storage Class">StorageClass</a> 的內容就是在實際的 storage 進行操作所必要的資訊(包含 IP，帳號、密碼…..等等)，敏感資訊的部份就透過 <strong>secret</strong> 放進去。</p><p>此外，在建立 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes" title="Kubernetes Storage Class">StorageClass</a> 的時候，就可以額外帶上建立 RBD 時的部份參數，以上面的設定為例，包含了：</p><ul><li><p>fsType: ext4</p></li><li><p>imageFormat: “2”</p></li><li><p>imageFeatures: “layering”</p></li></ul><p>如此一來當 RBD image 被建立時，就會是以在 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes" title="Kubernetes Storage Class">StorageClass</a> 中所定義的規格去產生。</p><h2 id="驗證動態-Persistent-Volume-的生成"><a href="#驗證動態-Persistent-Volume-的生成" class="headerlink" title="驗證動態 Persistent Volume 的生成"></a>驗證動態 Persistent Volume 的生成</h2><p>接著來驗證 persistent volume 的自動生成，透過以下的設定(名稱為 <code>storageclass-test.yml</code>)來完成：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">ceph-dynamic</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ceph-pod1</span>           </span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-busybox</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span>          </span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;sleep&quot;</span>, <span class="string">&quot;60000&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-vol1</span>       </span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/usr/share/busybox</span> </span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ceph-vol1</span>         </span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">ceph-pvc</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 套用上面的設定</span></span><br><span class="line">$ kubectl create -f storageclass-test.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 PVC 設定</span></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">ceph-pvc   Bound     pvc-426e7cc8-79b1-11e8-81d0-56277529c641   2Gi        RWO            ceph-dynamic   6s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認 pod 已經正常的執行中</span></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME        READY     STATUS    RESTARTS   AGE</span><br><span class="line">ceph-pod1   1/1       Running   0          20m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視與上面的 PVC 繫結的 PV 內容</span></span><br><span class="line">$ kubectl describe pv/pvc-426e7cc8-79b1-11e8-81d0-56277529c641</span><br><span class="line">Name:            pvc-426e7cc8-79b1-11e8-81d0-56277529c641</span><br><span class="line">....(略)</span><br><span class="line">StorageClass:    ceph-dynamic</span><br><span class="line">Status:          Bound</span><br><span class="line">Claim:           ceph-rbd-pv-lab/ceph-pvc</span><br><span class="line">....(略)</span><br><span class="line">Source:</span><br><span class="line">    Type:          RBD (a Rados Block Device mount on the host that shares a pod<span class="string">&#x27;s lifetime)</span></span><br><span class="line"><span class="string">    CephMonitors:  [10.103.2.24:6789]</span></span><br><span class="line"><span class="string">    RBDImage:      kubernetes-dynamic-pvc-4273f5b9-79b1-11e8-9115-3a01c1a41f09</span></span><br><span class="line"><span class="string">    FSType:        ext4</span></span><br><span class="line"><span class="string">    RBDPool:       kube</span></span><br><span class="line"><span class="string">    RadosUser:     kube</span></span><br><span class="line"><span class="string">    Keyring:       /etc/ceph/keyring</span></span><br><span class="line"><span class="string">    SecretRef:     &amp;&#123;ceph-secret-user &#125;</span></span><br><span class="line"><span class="string">    ReadOnly:      false</span></span><br><span class="line"><span class="string">Events:            &lt;none&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到有一個 RBD image <code>kubernetes-dynamic-pvc-4273f5b9-79b1-11e8-9115-3a01c1a41f09</code> 被產生出來。</p><p>接著回到 Ceph cluster 上查詢一下是不是有相對應的 object 被產生出來：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 的確有個 &quot;kubernetes-dynamic-pvc&quot; 開頭的 object 被產生出來了!</span></span><br><span class="line">$ rbd -p kube ls</span><br><span class="line">ceph-image</span><br><span class="line">kubernetes-dynamic-pvc-4273f5b9-79b1-11e8-9115-3a01c1a41f09</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 RBD image 的細節</span></span><br><span class="line">$ rbd info kube/kubernetes-dynamic-pvc-4273f5b9-79b1-11e8-9115-3a01c1a41f09</span><br><span class="line">rbd image <span class="string">&#x27;kubernetes-dynamic-pvc-4273f5b9-79b1-11e8-9115-3a01c1a41f09&#x27;</span>:</span><br><span class="line">    size 2048 MB <span class="keyword">in</span> 512 objects</span><br><span class="line">    order 22 (4096 kB objects)</span><br><span class="line">    block_name_prefix: rbd_data.bdce80238e1f29</span><br><span class="line">    format: 2</span><br><span class="line">    features: layering</span><br><span class="line">    flags: </span><br><span class="line">    create_timestamp: Wed Jun 27 10:24:46 2018</span><br></pre></td></tr></table></figure><p>從上面 RBD image 的內容可以看出，<a href="https://kubernetes.io/docs/concepts/storage/storage-classes" title="Kubernetes Storage Class">StorageClass</a> 的確是有幫我們根據上面的規格產生出一個 RBD image 與 PV，並相互繫結後，最後再與 PVC 繫結。</p><h1 id="障礙排除"><a href="#障礙排除" class="headerlink" title="障礙排除"></a>障礙排除</h1><h2 id="feature-set-mismatch-1"><a href="#feature-set-mismatch-1" class="headerlink" title="feature set mismatch (1)"></a>feature set mismatch (1)</h2><p>在 pod event 中看到以下的訊息：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MountVolume.WaitForAttach failed for volume &quot;ceph-pv&quot; : rbd: map failed exit status 110, rbd output: 2018-06-26 05:44:18.264219 7f50a90e4100 -1 did not load config file, using default settings.</span><br><span class="line"></span><br><span class="line">rbd: sysfs write failed</span><br><span class="line"></span><br><span class="line">In some cases useful info is found in syslog - try &quot;dmesg | tail&quot; or so.</span><br><span class="line">rbd: map failed: (110) Connection timed out</span><br></pre></td></tr></table></figure><p>於是到 worker node 上執行 <code>dmesg</code>，看到了很多類似以下的訊息：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">........ (略)</span><br><span class="line">libceph: mon0 10.103.2.24:6789 feature set mismatch, my 106b84a842a42 &lt; server&#x27;s 40106b84a842a42, missing 400000000000000</span><br><span class="line">libceph: mon0 10.103.2.24:6789 missing required protocol features</span><br><span class="line">........ (略)</span><br></pre></td></tr></table></figure><h3 id="解決方法"><a href="#解決方法" class="headerlink" title="解決方法"></a>解決方法</h3><p>這個問題只要修改 Ceph 的 crush map 即可，修改的方式到 Ceph 的 admin node 上執行以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd crush tunables legacy</span><br><span class="line"></span><br><span class="line">$ ceph osd crush reweight-all</span><br></pre></td></tr></table></figure><blockquote><p>其實也可以將 kernel 升級到 4.5 以上解決此問題</p></blockquote><h2 id="feature-set-mismatch-2"><a href="#feature-set-mismatch-2" class="headerlink" title="feature set mismatch (2)"></a>feature set mismatch (2)</h2><p>這個問題同樣是 <strong>feature set mismatch</strong>，會出現以下訊息：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable&quot;.</span><br></pre></td></tr></table></figure><p>這個問題需要在建立 RBD image 時就要設定好特定的 feature，因此回到 Ceph admin node 上，先以正常的方式建立 RBD image</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以正常的方式建立 rbd image</span></span><br><span class="line">$ rbd create kube/ceph-image --size 1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示 image feature</span></span><br><span class="line">$ rbd info kube/ceph-image</span><br><span class="line">rbd image <span class="string">&#x27;ceph-image&#x27;</span>:</span><br><span class="line">    size 1024 MB <span class="keyword">in</span> 256 objects</span><br><span class="line">    order 22 (4096 kB objects)</span><br><span class="line">    block_name_prefix: rbd_data.ba3aa02ae8944a</span><br><span class="line">    format: 2</span><br><span class="line">    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">    flags: </span><br><span class="line">    create_timestamp: Tue Jun 26 14:08:35 2018</span><br></pre></td></tr></table></figure><p>其中 <code>exclusive-lock</code>, <code>object-map</code>, <code>fast-diff</code>, <code>deep-flatten</code> 這些 feature 都要拿掉才行….</p><p>因此我們改成以下指令建立 RBD image：</p><blockquote><p>rbd create kube/ceph-image –size 1024 –image-format 2 –image-feature layering</p></blockquote><p>上面的問題就解決了!</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p>[<a href="https://hk.saowen.com/a/edc90fdb04b4cc267bcbcbae056e2a8b0e9d73ac05c81d05c1f72bf295112a13]">https://hk.saowen.com/a/edc90fdb04b4cc267bcbcbae056e2a8b0e9d73ac05c81d05c1f72bf295112a13]</a>(kubernetes持久化存儲Ceph RBD - 掃文資訊)</p></li><li><p><a href="https://tw.saowen.com/a/aa7346a600499cbde6551ef13c685b37b4148c6d5b3d03dcff6b48d431c7a10a">Kubernetes儲存之Persistent Volumes簡介 - 掃文資訊</a></p></li><li><p><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#ceph-rbd">Storage Classes - Kubernetes</a></p></li><li><p><a href="http://int32bit.me/2016/05/19/Ceph-Pool%E6%93%8D%E4%BD%9C%E6%80%BB%E7%BB%93/">Ceph Pool操作总结 - int32bit的博客 | int32bit Blog</a></p></li><li><p><a href="http://docs.ceph.com/docs/mimic/rados/operations/user-management/">User Management — Ceph Documentation</a></p></li><li><p><a href="http://docs.ceph.com/docs/argonaut/man/8/rados/">rados – rados object storage utility — Ceph documentation</a></p></li><li><p><a href="https://bugs.launchpad.net/charm-ceph-mon/+bug/1716735">Bug #1716735 “ceph with luminous can’t be used with KRBD with Xe…” : Bugs : OpenStack ceph-mon charm</a></p></li><li><p><a href="http://cephnotes.ksperis.com/blog/2014/01/21/feature-set-mismatch-error-on-ceph-kernel-client/">Feature Set Mismatch Error on Ceph Kernel Client - CephNotes</a></p></li><li><p><a href="https://kairen.github.io/2018/02/11/ceph/luminous-crush-issue/">Ceph Luminous CRUSH map 400000000000000 問題 | KaiRen’s Blog</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> Kubernetes </tag>
            
            <tag> CKA </tag>
            
            <tag> Kubernetes Storage </tag>
            
            <tag> CKA Storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>佈署 &amp; 存取 Kubernetes Dashboard</title>
      <link href="/blog/Kubernetes/k8s-Deploy-and-Access-Dashboard/"/>
      <url>/blog/Kubernetes/k8s-Deploy-and-Access-Dashboard/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>剛安裝完 Kubernetes，第一個會想到的大概就是 “<strong>入口網站在哪裡</strong>“?</p><p>k8s 無疑是個非常強大的 container orchestration platform，但它預設並沒有附上一個精美的 UI(因為不是每個人都需要 UI，畢竟多少還是會消耗掉一些資源)，設計者提供給使用者 &amp; 開發者完全自由的使用方式。</p><p><strong>因此! 有需要就自己設計 &amp; 安裝吧!</strong></p><h1 id="安裝-Dashboard"><a href="#安裝-Dashboard" class="headerlink" title="安裝 Dashboard"></a>安裝 Dashboard</h1><p>自己設計 dashboard 當然是無法，但找個現成的就挺簡單了。</p><p>若是使用 [Kubespray][Kubespray] 安裝的 k8s cluster，dashboard 是預設會安裝好的；若要確認 dashboard 是否會被安裝，可以在 <code>group_vars/k8s-cluster.yml</code> 這個檔案中，加入以下設定：</p><blockquote><p>dashboard_enabled: true</p></blockquote><p>當 k8s cluster 安裝好後，dashboard 也會被一併安裝完成。</p><p>若是沒有呢? 那其實也很簡單，進入 master node 中，透過 [kubectl][kubectl] 執行以下指令安裝：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 安裝最新版的 k8s dasdboard (namespace = kube-system)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢查 dashboard 是否安裝完成</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl get deployment -n kube-system | grep dashboard</span></span><br><span class="line">kubernetes-dashboard   1         1         1            1           5d</span><br></pre></td></tr></table></figure><p>當系統出現 <code>kubernetes-dashboard</code> 這個 deployment resource 時，就表示安裝完成了。</p><h1 id="存取-Dashboard"><a href="#存取-Dashboard" class="headerlink" title="存取 Dashboard"></a>存取 Dashboard</h1><p>由於 [kubectl][kubectl] 並不在本地端，因此使用 <code>kubectl proxy</code> 也沒太大意義，因此這邊直接就直接使用 master node 上的入口進入，使用以下的連結:</p><blockquote><p><a href="https://first_master:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login">https://first_master:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login</a></p></blockquote><p>進入後會出現以下畫面：</p><p><img src="/blog/images//kubernetes/dashboard-login-page.png" alt="Kubernetes Dashboard Login Page"></p><p>登入方式有兩種，分別是：</p><ul><li><p>Kubeconfig</p></li><li><p>Token</p></li></ul><p>以下介紹如何使用 Token 來登入 Dashboard。</p><h1 id="建立可登入-Dashboard-的使用者"><a href="#建立可登入-Dashboard-的使用者" class="headerlink" title="建立可登入 Dashboard 的使用者"></a>建立可登入 Dashboard 的使用者</h1><p>剛安裝好的 k8s cluster，上面已經存在有很多 Service Account，但這些都是 k8s cluster 中的各種服務，<strong>並非真實的使用者</strong>。</p><p>但由於目前 k8s cluster 中並沒有與任何的帳號系統(OpenLDAP / Windows AD / OpenID …. etc) 進行連結，因此以下還是會以建立 Service Account 的方式來模擬建立一個新的使用者進行示範:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 建立一個名稱為 &quot;admin-user&quot; 的 Service Account，用來模擬管理者</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># cat &lt;&lt;EOF | kubectl create -f -</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 為了取得最大權限，預計會將 &quot;admin-user&quot; 與 role &quot;cluster-admin&quot; 進行繫結</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl describe clusterrole/cluster-admin</span></span><br><span class="line">Name:         cluster-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate=<span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources  Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------  -----------------  --------------  -----</span><br><span class="line">  *.*        []                 []              [*]</span><br><span class="line">             [*]                []              [*]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 將 &quot;admin-user&quot; 與 role &quot;cluster-admin&quot; 進行繫結，建立名稱為 &quot;admin-user&quot; 的 ClusterRoleBinding</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># cat &lt;&lt;EOF | kubectl create -f -</span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視 ClusterRoleBinding &quot;admin-user&quot; 的內容</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl describe ClusterRoleBinding/admin-user</span></span><br><span class="line">Name:         admin-user</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Role:</span><br><span class="line">  Kind:  ClusterRole</span><br><span class="line">  Name:  cluster-admin</span><br><span class="line">Subjects:</span><br><span class="line">  Kind            Name        Namespace</span><br><span class="line">  ----            ----        ---------</span><br><span class="line">  ServiceAccount  admin-user  kube-system</span><br></pre></td></tr></table></figure><p>當 Service Account 被建立完成時，Kubernetes Token Controller 就會自動的為其產生一個 <strong>secret</strong> resource(名稱為 <code>[SERVICE_ACCOUNT_NAME]-token-[RANDOM_STRING]</code>)，並使用裡面的 token 作為與 API server 認證用，可以用以下指令察看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視建立 service account 時產生的 secret</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl get secret -n kube-system | grep admin-user</span></span><br><span class="line">admin-user-token-572j8                           kubernetes.io/service-account-token   3         11m</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視 secret 細節，並取得 token 內容</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &#x27;&#123;print $1&#125;&#x27;)</span></span><br><span class="line">Name:         admin-user-token-572j8</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name=admin-user</span><br><span class="line">              kubernetes.io/service-account.uid=ad79cce8-7453-11e8-b6b3-065296fdbf18</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">ca.crt:     1090 bytes</span><br><span class="line">namespace:  11 bytes</span><br><span class="line">token:      [TOKEN_CONTENT]</span><br></pre></td></tr></table></figure><h1 id="登入-Dashboard"><a href="#登入-Dashboard" class="headerlink" title="登入 Dashboard"></a>登入 Dashboard</h1><p>透過上面的 secret 取得 token 後，就可以選擇 login page 上的 <code>Token</code> 選項登入，進入後可以看到以下畫面：</p><p><img src="/blog/images/kubernetes/dashboard-overview-page.png" alt="Kubernetes Dashboard Overview Page"></p><p>接著就可以以 Cluster Admin 的身份進行操作了。</p><h1 id="後記"><a href="#後記" class="headerlink" title="後記"></a>後記</h1><p>由於 k8s 內部是不儲存使用者帳號的，它設計可以外接不同的帳號認證系統。因此若是要真的執行嚴格的權限管理，並不是像上面的建立一個擁有 cluster admin 權限的 Service Account(因為 Service Account 顧名思義是給 service 用的)，而是要與外部的認證系統串接(例如：OpenID, LDAP … etc)。</p><p>因此之後將會找時間補上與外部認證系統串接這個部份，將使用者帳號獨立在 k8s cluster 外部。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://github.com/kubernetes/dashboard">kubernetes/dashboard: General-purpose web UI for Kubernetes clusters</a></p></li><li><p><a href="https://github.com/kubernetes/dashboard/wiki/Creating-sample-user">Creating sample user · kubernetes/dashboard Wiki</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解 Kubernetes 中的授權機制</title>
      <link href="/blog/Kubernetes/k8s-API-Authorization/"/>
      <url>/blog/Kubernetes/k8s-API-Authorization/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在進行授權(Authorization) 之前，一定要先完成認證(Authentication) 的過程，而認證的說明可以參考<a href="https://godleon.github.io/blog/2018/06/19/Kubernetes/k8s-API-Authentication/">上一篇文章</a>。</p><p>k8s 對於 REST API request 的屬性要求是通用的，因此可以與企業中 or 公有雲平台中現存的管理控制系統進行整合。</p><p>而在 k8s 中，負責對 API request 進行授權的是 API server，藉由檢查所有的 <strong>request attribute</strong> 並查詢所有的 policy 來決定是否允許或拒絕 API request 的存取。</p><blockquote><p>預設情況下所有權限都是預設關閉的</p></blockquote><p>k8s 支援同時多個授權機制，並且為循序式的進行檢查，若有任何一個授權機制通過檢查，就會馬上允許存取，若是沒有通過任何的授權機制檢查，此 API request 就會被拒絕存取；而被拒絕存取的 API request 都會收到 HTTP 403 error。</p><h1 id="名詞釋疑"><a href="#名詞釋疑" class="headerlink" title="名詞釋疑"></a>名詞釋疑</h1><h2 id="NameSpace"><a href="#NameSpace" class="headerlink" title="NameSpace"></a>NameSpace</h2><p>用來進行資源的虛擬隔離之用，在 k8s 剛建立好的初期會有預設的 <code>default</code> &amp; <code>kube-system</code> 兩個 namespace</p><h2 id="User-Account-amp-Service-Account"><a href="#User-Account-amp-Service-Account" class="headerlink" title="User Account &amp; Service Account"></a>User Account &amp; Service Account</h2><p>User Account &amp; Service Account 是用來區別作用的範圍，但會有以下的差別：</p><ol><li><p>User Account 是用來辨識使用者，而 Service Account 則是用來辨識 k8s 中的各種服務</p></li><li><p>User Account 對應人的身份，因此是跨 namespace；而 Service Account 對應的是 service 的身份，因此是與 namespace 有相關</p></li></ol><h1 id="Request-Attributes"><a href="#Request-Attributes" class="headerlink" title="Request Attributes"></a>Request Attributes</h1><p>上面有提到，API server 會檢查 <strong>API request attribute</strong> 來決定是否進行授權存取，在 k8s 有以下 API request attributes：</p><ul><li><p>user, group, extra(額外的 key/value 資訊)</p></li><li><p>API、Request Path(/api, /healthz), API request verb(get, list, create, update, patch, watch … etc)</p></li><li><p>HTTP request verb(get, post, put, delete)</p></li><li><p>Resource, Subresource (存取 resource 時需要的資訊)</p></li><li><p>Namespace</p></li><li><p><a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups">API group</a> (存取 resource 時需要的資訊)</p></li></ul><h1 id="Authorization-Modules"><a href="#Authorization-Modules" class="headerlink" title="Authorization Modules"></a>Authorization Modules</h1><p>目前 k8s 支援以下幾種 Authorization Modules：</p><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a><a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">Node</a></h2><p>此模組是為了授權在每一個 node 上的 kubelet 所發出的 API request 所設計出來的，讓 kubelet 的 API request 可進行特定的權限控制；例如對於以下的 resource 的讀取的權限：</p><ul><li><p>services</p></li><li><p>endpoints</p></li><li><p>nodes</p></li><li><p>pods</p></li><li><p>secrets / configmaps / presistent volume (claim)</p></li></ul><p>另外透過 NodeRestriction 的 plugin，可以針對以下 resource 進行寫入：</p><ul><li><p>node (status)</p></li><li><p>pod (status)</p></li><li><p>event</p></li></ul><blockquote><p>未來可能還會繼續透過 Node Authorization 來對於 kubelet 進行更細膩的權限控管</p></blockquote><h2 id="ABAC-Attribute-based-access-control"><a href="#ABAC-Attribute-based-access-control" class="headerlink" title="ABAC (Attribute-based access control)"></a><a href="https://kubernetes.io/docs/reference/access-authn-authz/abac/">ABAC (Attribute-based access control)</a></h2><p>此種方式就是在 master node 上保留一份 policy 文件，指定不同的使用者對於 resource 的存取權限，不彈性也不容易擴充，修改了 policy 文件之後還需要重新啟動 master node。</p><h2 id="RBAC-Role-based-access-control"><a href="#RBAC-Role-based-access-control" class="headerlink" title="RBAC (Role-based access control)"></a><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC (Role-based access control)</a></h2><p>RBAC 是目前 k8s 預設會啟動的 authorization module。</p><p>在 RBAC API 中定義了 resource target，用來描述使用者以及 resource 之間的權限關係：</p><ul><li><p>Role：定義在特定 namespace 下的 resource 的存取權限</p></li><li><p>RoleBinding： 設定哪些使用者(or service account)與 role 綁定而擁有存取權限</p></li><li><p>ClusterRole：定義在整個 k8s cluster 下的 resource 的存取權限</p></li><li><p>ClusterRoleBinding：設定哪些使用者(or service account)與 role 綁定而擁有存取權限</p></li></ul><p>接著以下以 role <strong>kubernetes-dashboard-minimal</strong> 做個簡單的示範：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">root@kube-master0:~<span class="comment"># kubectl get role --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                                             AGE</span><br><span class="line">default       pod-reader                                       4d</span><br><span class="line">kube-public   system:controller:bootstrap-signer               5d</span><br><span class="line">kube-system   extension-apiserver-authentication-reader        5d</span><br><span class="line">kube-system   kubernetes-dashboard-minimal                     5d</span><br><span class="line">..... (略)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl get rolebinding --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                                             AGE</span><br><span class="line">kube-public   system:controller:bootstrap-signer               5d</span><br><span class="line">kube-system   kubernetes-dashboard-minimal                     5d</span><br><span class="line">..... (略)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視 role &quot;kubernetes-dashboard-minimal&quot; 中有那些 resource 的存取權限</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl describe role/kubernetes-dashboard-minimal -n kube-system</span></span><br><span class="line">Name:         kubernetes-dashboard-minimal</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">&quot;apiVersion&quot;</span>:<span class="string">&quot;rbac.authorization.k8s.io/v1&quot;</span>,<span class="string">&quot;kind&quot;</span>:<span class="string">&quot;Role&quot;</span>,<span class="string">&quot;metadata&quot;</span>:&#123;<span class="string">&quot;annotations&quot;</span>:&#123;&#125;,<span class="string">&quot;name&quot;</span>:<span class="string">&quot;kubernetes-dashboard-minimal&quot;</span>,<span class="string">&quot;namespace&quot;</span>:<span class="string">&quot;kube-system&quot;</span>...</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources       Non-Resource URLs  Resource Names                     Verbs</span><br><span class="line">  ---------       -----------------  --------------                     -----</span><br><span class="line">  configmaps      []                 [kubernetes-dashboard-settings]    [get update]</span><br><span class="line">  configmaps      []                 []                                 [create]</span><br><span class="line">  secrets         []                 [kubernetes-dashboard-certs]       [get update delete]</span><br><span class="line">  secrets         []                 [kubernetes-dashboard-key-holder]  [get update delete]</span><br><span class="line">  secrets         []                 []                                 [create]</span><br><span class="line">  services        []                 [heapster]                         [proxy]</span><br><span class="line">  services/proxy  []                 [heapster]                         [get]</span><br><span class="line">  services/proxy  []                 [http:heapster:]                   [get]</span><br><span class="line">  services/proxy  []                 [https:heapster:]                  [get]</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視目前與 role &quot;kubernetes-dashboard-minimal&quot; 相關連的使用者(名稱為 &quot;kubernetes-dashboard&quot; 的 service account)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl describe rolebinding/kubernetes-dashboard-minimal -n kube-system</span></span><br><span class="line">Name:         kubernetes-dashboard-minimal</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubectl.kubernetes.io/last-applied-configuration=&#123;<span class="string">&quot;apiVersion&quot;</span>:<span class="string">&quot;rbac.authorization.k8s.io/v1&quot;</span>,<span class="string">&quot;kind&quot;</span>:<span class="string">&quot;RoleBinding&quot;</span>,<span class="string">&quot;metadata&quot;</span>:&#123;<span class="string">&quot;annotations&quot;</span>:&#123;&#125;,<span class="string">&quot;name&quot;</span>:<span class="string">&quot;kubernetes-dashboard-minimal&quot;</span>,<span class="string">&quot;namespace&quot;</span>:<span class="string">&quot;kube-...</span></span><br><span class="line"><span class="string">Role:</span></span><br><span class="line"><span class="string">  Kind:  Role</span></span><br><span class="line"><span class="string">  Name:  kubernetes-dashboard-minimal</span></span><br><span class="line"><span class="string">Subjects:</span></span><br><span class="line"><span class="string">  Kind            Name                  Namespace</span></span><br><span class="line"><span class="string">  ----            ----                  ---------</span></span><br><span class="line"><span class="string">  ServiceAccount  kubernetes-dashboard  kube-system</span></span><br></pre></td></tr></table></figure><p>接著以 clusterole <strong>system:controller:deployment-controller</strong> 做示範：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視目前 k8s cluster 中的 ClusteRole 資訊</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl get clusterrole</span></span><br><span class="line">NAME                                                                   AGE</span><br><span class="line">admin                                                                  5d</span><br><span class="line">calico-node                                                            5d</span><br><span class="line">cluster-admin                                                          5d</span><br><span class="line">..... (略)</span><br><span class="line">system:controller:deployment-controller                                5d</span><br><span class="line">..... (略)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視 &quot;system:controller:deployment-controller&quot; ClusteRole 的 resource 存取權限</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl describe clusterrole/system:controller:deployment-controller</span></span><br><span class="line">Name:         system:controller:deployment-controller</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate=<span class="literal">true</span></span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                          Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------                          -----------------  --------------  -----</span><br><span class="line">  events                             []                 []              [create patch update]</span><br><span class="line">  pods                               []                 []              [get list update watch]</span><br><span class="line">  deployments.apps                   []                 []              [get list update watch]</span><br><span class="line">  deployments.apps/finalizers        []                 []              [update]</span><br><span class="line">  deployments.apps/status            []                 []              [update]</span><br><span class="line">  replicasets.apps                   []                 []              [create delete get list patch update watch]</span><br><span class="line">  deployments.extensions             []                 []              [get list update watch]</span><br><span class="line">  deployments.extensions/finalizers  []                 []              [update]</span><br><span class="line">  deployments.extensions/status      []                 []              [update]</span><br><span class="line">  replicasets.extensions             []                 []              [create delete get list patch update watch]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視目前 k8s cluster 中的 ClusteRoleBinding 資訊</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl get clusterrolebinding</span></span><br><span class="line">NAME                                                   AGE</span><br><span class="line">..... (略)</span><br><span class="line">system:controller:deployment-controller                5d</span><br><span class="line">..... (略)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢視目前 k8s cluster 中與 &quot;system:controller:deployment-controller&quot; ClusteRoleBinding 連結的使用者(or service account)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl describe clusterrolebinding/system:controller:deployment-controller</span></span><br><span class="line">Name:         system:controller:deployment-controller</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate=<span class="literal">true</span></span><br><span class="line">Role:</span><br><span class="line">  Kind:  ClusterRole</span><br><span class="line">  Name:  system:controller:deployment-controller</span><br><span class="line">Subjects:</span><br><span class="line">  Kind            Name                   Namespace</span><br><span class="line">  ----            ----                   ---------</span><br><span class="line">  ServiceAccount  deployment-controller  kube-system</span><br></pre></td></tr></table></figure><p>API server 已經預先建立了一系列的 ClusterRole &amp; ClusteRoleBinding，名稱都以 <strong>system</strong> 開頭，並涵蓋了維持系統正常服務所需要的所有權限，因此整個 k8s cluster 都可以在 RBAC 的基礎下進行權限的管理與授權。</p><blockquote><p>這也代表著隨意對 system 開頭的 ClusteRole 修改，可能會造成系統無法正常運作。</p></blockquote><h2 id="Webhook"><a href="#Webhook" class="headerlink" title="Webhook"></a><a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/">Webhook</a></h2><p>此模式是管理者在外部提供 HTTPS 授權服務，並設定 API server 透過與外部服務互動的方式進行授權。</p><h1 id="檢查-API-存取權限"><a href="#檢查-API-存取權限" class="headerlink" title="檢查 API 存取權限"></a>檢查 API 存取權限</h1><p>kubectl 提供了 <code>auth can-i</code> 命令，可用來查詢當前的使用者有沒有特定 resource 的存取權限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢查有無在 &quot;dev&quot; namespace 中建立 deployment 的權限</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl auth can-i create deployments --namespace dev</span></span><br><span class="line">yes</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢查有無在 &quot;prod&quot; namespace 中建立 deployment 的權限</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl auth can-i create deployments --namespace prod</span></span><br><span class="line">yes</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 檢查有無在 &quot;dev&quot; namespace 中以 dave 的身份來列出 secret 的權限</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl auth can-i list secrets --namespace dev --as dave</span></span><br><span class="line">yes</span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/">Authorization Overview - Kubernetes</a></p></li><li><p><a href="https://zhangchenchen.github.io/2017/08/17/kubernetes-authentication-authorization-admission-control/">Kubernetes– 漫谈kubernetes 中的认证 &amp; 授权 &amp; 准入机制 | Solar</a></p></li><li><p><a href="http://cizixs.com/2017/06/16/kubernetes-authentication-and-authorization">kubernetes 權限管理 – Cizixs Writes Here</a></p></li><li><p><a href="https://k8smeetup.github.io/docs/admin/authorization/rbac/">https://k8smeetup.github.io/docs/admin/authorization/rbac/</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubernetes Security </tag>
            
            <tag> CKA </tag>
            
            <tag> CKA Security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>了解 Kubernetes 中的認證機制</title>
      <link href="/blog/Kubernetes/k8s-API-Authentication/"/>
      <url>/blog/Kubernetes/k8s-API-Authentication/</url>
      
        <content type="html"><![CDATA[<p>由於 Kubernetes 整體就是以 API 為基礎來設計的，因此要了解認證機制也就等同於要了解 k8s API server 的認證機制。</p><h1 id="了解-Kubernetes-API-認證機制"><a href="#了解-Kubernetes-API-認證機制" class="headerlink" title="了解 Kubernetes API 認證機制"></a>了解 Kubernetes API 認證機制</h1><p>k8s 提供了多種的認證方式，管理者可以設定多種認證機制共存,只要任何一種通過就算通過。</p><p>以 k8s 中各服務之間的認證為例，是以 x509 的認證，同時也是最嚴格的認證方式。透過 Kubespray 安裝好 k8s 後，我們可以在 master node 上檢視一下 api server 的啟動設定：(<code>/etc/kubernetes/manifests/kube-apiserver.manifest</code>)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-apiserver</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="string">........(略)</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">ClusterFirst</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-apiserver</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">gcr.io/google-containers/hyperkube:v1.10.4</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">800m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">2000M</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">256M</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/hyperkube</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">apiserver</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--advertise-address=10.103.10.51</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--etcd-servers=https://10.103.10.51:2379,https://10.103.10.52:2379,https://10.103.10.53:2379</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--etcd-cafile=/etc/ssl/etcd/ssl/ca.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--etcd-certfile=/etc/ssl/etcd/ssl/node-kube-master0.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--etcd-keyfile=/etc/ssl/etcd/ssl/node-kube-master0-key.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--insecure-bind-address=127.0.0.1</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--bind-address=0.0.0.0</span></span><br><span class="line">    <span class="string">........(略)</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--client-ca-file=/etc/kubernetes/ssl/ca.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--kubelet-client-certificate=/etc/kubernetes/ssl/node-kube-master0.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--kubelet-client-key=/etc/kubernetes/ssl/node-kube-master0-key.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--service-account-key-file=/etc/kubernetes/ssl/service-account-key.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--secure-port=6443</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--insecure-port=8080</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--storage-backend=etcd3</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--authorization-mode=Node,RBAC</span></span><br><span class="line">    <span class="string">........(略)</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.pem</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client-key.pem</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">    <span class="string">........(略)</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="string">........(略)</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/etc/kubernetes</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">kubernetes-config</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ssl-certs-host</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/etc/ssl</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">usr-share-ca-certificates</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/usr/share/ca-certificates</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/etc/ssl/etcd/ssl</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">etcd-certs</span></span><br></pre></td></tr></table></figure><p>從上面可以看出：</p><ol><li><p>與各個服務相互認證所使用的憑證 (<code>*.pem</code>)</p></li><li><p>與本地端的其他服務則是透過 <code>--insecure-bind-address=127.0.0.1</code> &amp; <code>--insecure-port=8080</code> 來溝通</p></li><li><p>憑證存放的位置其實就是在各個 node 的本機上 (<code>hostPath</code> 定義)</p></li><li><p>目前此 k8s cluster 支援的認證方式為 <strong><a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">Node</a></strong> &amp; <strong><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC</a></strong></p></li><li><p>pod 內部服務需要與 api server 互動時的認證方式，使用 <strong><a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/">Service Account Token</a></strong> 的方式，此處對應的設定是 <code>--service-account-key-file=/etc/kubernetes/ssl/service-account-key.pem</code></p></li></ol><p>Kubespray 預設設置了 <code>X509</code>, <code>Node</code> &amp; <code>RBAC</code> 三種認證方式，但 Kubernetes 還支援了相當多其他的認證方式，例如：</p><ul><li><p><code>Static Token File</code></p></li><li><p><code>Bootstrap Tokens</code></p></li><li><p><code>Static Password File</code></p></li><li><p><code>OpenID Connect Tokens</code></p></li><li><p><code>Webhook Token Authentication</code></p></li><li><p><code>Keystone Password</code></p></li></ul><p>以下會針對比較重要的認證機制詳細說明。 </p><h1 id="X509-Client-Certs"><a href="#X509-Client-Certs" class="headerlink" title="X509 Client Certs"></a>X509 Client Certs</h1><p>從 Kubespray 預設的安裝設定中的 API server 啟動參數中，可以看到與 X509 相關設定參數有：</p><ul><li><p><code>client-ca-file</code></p></li><li><p><code>tls-private-key-file</code></p></li><li><p><code>tls-cert-file</code></p></li></ul><p>由於只有在 k8s cluster 內的 node 才會有這些憑證檔案，因此這樣的認證方式是在 cluster 內部不同 node 之間的服務相互存取時使用。</p><p>但若是與 API server 同在本地端(localhost)的服務要存取 API 呢? 就直接 pass 了…..(像是 <code>kube-scheduler</code> &amp; <code>kube-controller-manager</code> 都屬於這一類的服務)</p><p>可看到 <code>--insecure-bind-address=127.0.0.1</code> &amp; <code>--insecure-port=8080</code> 兩個設定，就可以看出在本地端內部服務存取 API server 時，使用的是 <strong><a href="http://localhost:8080/">http://localhost:8080</a></strong></p><h1 id="Service-Account"><a href="#Service-Account" class="headerlink" title="Service Account"></a>Service Account</h1><p>Service Account 本身在 k8s 中是屬於 resource 的一種。我們可以透過以下指令取得 Service Account 清單：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 每一個 namespace 都有一個名稱為 &quot;default&quot; 的 service account</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl get serviceaccount --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                                 SECRETS   AGE</span><br><span class="line">default       default                              1         4d</span><br><span class="line">kube-public   default                              1         4d</span><br><span class="line">kube-system   attachdetach-controller              1         4d</span><br><span class="line">kube-system   cronjob-controller                   1         4d</span><br><span class="line">kube-system   daemon-set-controller                1         4d</span><br><span class="line">kube-system   default                              1         4d</span><br><span class="line">kube-system   deployment-controller                1         4d</span><br><span class="line">.....(略)</span><br></pre></td></tr></table></figure><p>從上面清單可看出，每個 namespace 中都會有一個名稱為 <code>default</code> 的 service account，接著可以來拆解這個 <code>default</code> service account 有什麼內容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 名稱為 &quot;default&quot; 的 service account 其實就是帶著用來存取 API server 時認證用的 token</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl describe serviceaccount/default -n kube-system</span></span><br><span class="line">Name:                default</span><br><span class="line">Namespace:           kube-system</span><br><span class="line">Labels:              &lt;none&gt;</span><br><span class="line">Annotations:         &lt;none&gt;</span><br><span class="line">Image pull secrets:  &lt;none&gt;</span><br><span class="line">Mountable secrets:   default-token-nkl69</span><br><span class="line">Tokens:              default-token-nkl69</span><br><span class="line">Events:              &lt;none&gt;</span><br></pre></td></tr></table></figure><p>最後來看看上面 service account token 所帶的內容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># 此 token 內容載明了 token 本身之外，還有 namespace, type ... 等資訊</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">root@kube-master0:~<span class="comment"># kubectl describe secret/default-token-nkl69 -n kube-system</span></span><br><span class="line">Name:         default-token-nkl69</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name=default</span><br><span class="line">              kubernetes.io/service-account.uid=fd2e5d64-6f9c-11e8-b6b3-065296fdbf18</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">namespace:  11 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLW5rbDY5Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmZDJlNWQ2NC02ZjljLTExZTgtYjZiMy0wNjUyOTZmZGJmMTgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.cMC6jKcj1vmDhQnxlsHzop1t6W3qeG-xhEAZyBuzagUgb-2f06ZxI0UvIQ-qb3mkXyjtCdhw-Hn4PpvJ56HtvCBMYwfaP-ii4ord0aZfhqIRynlFuj-cc2qvhewaGwC84Yj7awMj6rv9yQGMgFBEL0roaLgoVAyYqpbJ6B2ig4cBlQQnTbiewYXdGoWGzb3wtl2Ii6E9nZ6ANPxLI4dhwbAxVNVAR4tojRiukQldSnI0ItX-Iwx1Djd5FK3FCgxY1soo682sLE-_NJeF8KfVPAWtg1049dSXe1iNQ3k-AO3pfEGDBaAq4WACuYhMtfL2iYlZRxS5SEt8Mwz3aVW1ng</span><br><span class="line">ca.crt:     1090 bytes</span><br></pre></td></tr></table></figure><blockquote><p>Service Account 的認證方式主要是由 k8s 自行管理，當我們建立一個 namespace 時，就自動會產生一個名稱為 <code>default</code> 的 service account 並帶有新建立的 token；而未來在此 namespace 中所產生的 pod，都會自動使用此 token 與 API server 進行認證。</p></blockquote><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">Authenticating - Kubernetes</a></p></li><li><p><a href="https://zhangchenchen.github.io/2017/08/17/kubernetes-authentication-authorization-admission-control/">Kubernetes– 漫谈kubernetes 中的认证 &amp; 授权 &amp; 准入机制 | Solar</a></p></li><li><p>[<a href="https://kairen.github.io/2018/04/15/kubernetes/k8s-integration-ldap/]">https://kairen.github.io/2018/04/15/kubernetes/k8s-integration-ldap/]</a>(整合 OpenLDAP 進行 Kubernetes 身份認證 | KaiRen’s Blog)</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubernetes Security </tag>
            
            <tag> CKA </tag>
            
            <tag> CKA Security </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Proxmox VE] 使用 cloud-init 快速產生 VM</title>
      <link href="/blog/Proxmox/Proxmox-use-cloud-init-to-provision-vm/"/>
      <url>/blog/Proxmox/Proxmox-use-cloud-init-to-provision-vm/</url>
      
        <content type="html"><![CDATA[<p>本文將會介紹如何在 Proxmox 上使用 cloud-init 產生 VM</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>使用 <a href="https://www.proxmox.com/en/proxmox-ve">Proxmox</a> 也有一段時間了，這套免費的 KVM virtualization platform 真是佛心來的，好用穩定又有 Web GUI，但唯一的缺憾就是要作 Infrastructure as code 真的有點困難，因為他本身並不具備 cloud-init 的功能。</p><p>還好這個功能終於在 5.2 的時候被支援了，不過看起來還是很陽春，但基本使用上應該還算足夠。</p><blockquote><p>希望未來會有 scheduler 也被開發出來，不然我都要自己指定 host 來擺 VM….</p></blockquote><h1 id="準備-Template-VM"><a href="#準備-Template-VM" class="headerlink" title="準備 Template VM"></a>準備 Template VM</h1><p>以下將以 <a href="https://cloud-images.ubuntu.com/xenial/current">ubuntu 16.04 cloud image</a> 做示範來準備 template VM</p><p>在準備 template VM 前必須有以下資訊：</p><ol><li><p>VM ID：這個不要跟其他 VM 重複到即可，下面使用 <code>9999</code></p></li><li><p>CPU / Memory / Network 相關設定：這個部份就根據自己的環境 &amp; 需求調整</p></li><li><p>Storage：要選擇一個 Template VM disk 存放的 storage，以下以 <code>rbd_vm</code> 做示範 (官網的文章是直接放到 <code>local-lvm</code>)</p></li><li><p>SSH login key：cloud image 預設無法使用密碼登入，因此必須準備好登入用的 SSH Key(public)</p></li></ol><p>接著執行以下 script 即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> /tmp</span><br><span class="line"></span><br><span class="line">wget https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 CPU type 為 host, 共 8(2x4) vcpu, 8GB memory, 網路使用 vmbr1(tag 1310, 此為我自己設定的 trunk bridge)</span></span><br><span class="line">qm create 9999 --cpu cputype=host --sockets 2 --cores 4 --memory 8192 --net0 virtio,bridge=vmbr1,tag=1310</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 cloud image 匯入到指定的 storage 作為 template VM 的第一個 disk</span></span><br><span class="line">qm importdisk 9999 xenial-server-cloudimg-amd64-disk1.img rbd_vm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 VM 細節</span></span><br><span class="line"><span class="comment"># 設定 storage type</span></span><br><span class="line"><span class="comment"># 設定 cloud-init 的功能以 cd-rom 的形式掛載</span></span><br><span class="line"><span class="comment"># serial 一定要加，否則 cloud image 會無法正常開機</span></span><br><span class="line">qm <span class="built_in">set</span> 9999 --virtio0 rbd_vm:vm-9999-disk-1 --ide2 rbd_vm:cloudinit --boot c --bootdisk virtio0 --serial0 socket</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 SSH key (cloud image 預設是無法使用密碼登入，必須設定 SSH key)</span></span><br><span class="line">qm <span class="built_in">set</span> 9999 --sshkey &lt;your public ssk key path&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 轉換成 template VM</span></span><br><span class="line">qm template 9999</span><br></pre></td></tr></table></figure><p>執行完成後就會有一個 VM ID=9999 的 template VM 產生，從系統上可以看到類似以下資訊：</p><p><img src="/blog/images/proxmox/vm_hw_info.png" alt="Template VM Hardware Information"></p><p><img src="/blog/images//proxmox/vm_cloud-init_info.png" alt="Template VM cloud-init Information"></p><h1 id="產生-VM"><a href="#產生-VM" class="headerlink" title="產生 VM"></a>產生 VM</h1><p>接著就可以使用這個 template VM 來快速產生 VM 了，以下用個簡單的 script 來完成：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 複製 VM</span></span><br><span class="line">qm <span class="built_in">clone</span> 9999 1001 --name ubuntu1604-1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 storage size 擴大到 64GB</span></span><br><span class="line">qm resize 1001 virtio0 64G; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 網路設定 (以下是對應到上面的 tag 1310，請自行修改以對應自己的環境)</span></span><br><span class="line">qm <span class="built_in">set</span> 1001 --ipconfig0 ip=10.103.10.51/24,gw=10.103.10.1 --nameserver <span class="string">&#x27;8.8.8.8 1.1.1.1&#x27;</span></span><br><span class="line"></span><br><span class="line">qm start 1001</span><br></pre></td></tr></table></figure><p>以上的 script 修改一下，放到 loop 中，一下子要產生多個 VM 其實就是一件很簡單的事情了!</p><h1 id="其他限制"><a href="#其他限制" class="headerlink" title="其他限制"></a>其他限制</h1><p>目前 cloud-init 這個功能無法在 CentOS cloud image 上使用</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://pve.proxmox.com/wiki/Cloud-Init_Support">Cloud-Init Support - Proxmox VE</a></p></li><li><p><a href="https://pve.proxmox.com/pve-docs/qm.1.html">qm(1)</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Proxmox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Proxmox </tag>
            
            <tag> cloud-init </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[OpenShift] 如何計算在 Jenkins pipeline 中工作所花費的時間</title>
      <link href="/blog/OpenShift/OpenShift-Howto-Calculate-Time-Duration-in-Jenkins/"/>
      <url>/blog/OpenShift/OpenShift-Howto-Calculate-Time-Duration-in-Jenkins/</url>
      
        <content type="html"><![CDATA[<p>最近的工作都是使用 OpenShift + Jenkins 在進行 CI/CD 的相關工作，在時間的顯示上遇到了兩個問題：</p><ol><li><p>Jenkins 系統上使用的是 UTC 時間，希望改成 Taiwan 時間(+8:00)</p></li><li><p>想要知道整個 pipeline 的工作完成後一共花了多少時間</p></li></ol><p>上網找了很多資料，拼拼湊湊寫出了以下的 groovy script 來完成這件事情：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat</span><br><span class="line"><span class="keyword">import</span> java.util.Calendar</span><br><span class="line"><span class="keyword">import</span> groovy.time.*</span><br><span class="line"></span><br><span class="line"><span class="comment">// human-readable format</span></span><br><span class="line"><span class="keyword">def</span> dateFormat = <span class="keyword">new</span> SimpleDateFormat(<span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// convert time from UTC to Taiwan Time(+8:00)</span></span><br><span class="line"><span class="keyword">def</span> startTime = <span class="keyword">new</span> Date((Calendar.getInstance()).getTimeInMillis() + (<span class="number">480</span> * <span class="number">60000</span>))</span><br><span class="line"><span class="keyword">def</span> strStartTime = dateFormat.format(startTime)</span><br><span class="line"></span><br><span class="line">sleep <span class="number">5</span></span><br><span class="line"></span><br><span class="line">openshift.withCluster() &#123;</span><br><span class="line">    stage(<span class="string">&#x27;Calculate Time Duration&#x27;</span>) &#123;</span><br><span class="line">        node &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">def</span> endTime = <span class="keyword">new</span> Date((Calendar.getInstance()).getTimeInMillis() + (<span class="number">480</span> * <span class="number">60000</span>))</span><br><span class="line">            strEndTime = dateFormat.format(endTime)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// calculate time duration</span></span><br><span class="line">            TimeDuration duration = TimeCategory.minus(endTime, startTime)</span><br><span class="line">            <span class="keyword">def</span> strDuration = String.format(<span class="string">&quot;%02d&quot;</span>, duration.getHours()) + <span class="string">&quot;:&quot;</span> + String.format(<span class="string">&quot;%02d&quot;</span>, duration.getMinutes()) + <span class="string">&quot;:&quot;</span> + String.format(<span class="string">&quot;%02d&quot;</span>, duration.getSeconds())</span><br><span class="line">            <span class="comment">// it&#x27;s necessary to set it to null for avoiding &quot;not serializable exception&quot;</span></span><br><span class="line">            duration = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">            echo <span class="string">&quot;Start Time = $&#123;strStartTime&#125;&quot;</span></span><br><span class="line">            echo <span class="string">&quot;End Time = $&#123;strEndTime&#125;&quot;</span></span><br><span class="line">            echo <span class="string">&quot;Time Duration = $&#123;strDuration&#125;&quot;</span></span><br><span class="line">        &#125; </span><br><span class="line">    &#125; <span class="comment">// ---------- End of stage(&#x27;Configure IAAS&#x27;)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>透過以下步驟，可以直接使用我放在 GitHub 上面的範例，直接建立一個 build job 來測試：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/godleon/openshift-jenkins-customization.git</span><br><span class="line"></span><br><span class="line">$ oc create -f openshift-objects/</span><br><span class="line"></span><br><span class="line">$ oc start-build calculate-time-duration</span><br></pre></td></tr></table></figure><p>接著到 Jenkins 系統內部就可以看到執行結果囉!</p><hr><h1 id="關於-Script-Approval-的處理"><a href="#關於-Script-Approval-的處理" class="headerlink" title="關於 Script Approval 的處理"></a>關於 Script Approval 的處理</h1><p>預設 Jenkins pipeline 會在 sandbox 的環境中執行，因此很多 grovvy or java method 都會無法使用，因此在 OpenShift 中需要對 Jenkins 進行客製化，預先設定 script whitelist，讓某些 method 可以在 pipeline 中使用，如何客製化 Jenkins 詳情可以參考之前寫過的文章。</p>]]></content>
      
      
      <categories>
          
          <category> OpenShift </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenShift </tag>
            
            <tag> Jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[OpenShift] 如何客製化 Jenkins Image</title>
      <link href="/blog/OpenShift/OpenShift-Howto-Customize-builtin-Jenkins-Image/"/>
      <url>/blog/OpenShift/OpenShift-Howto-Customize-builtin-Jenkins-Image/</url>
      
        <content type="html"><![CDATA[<p>本文章的主題在於如何將客製化 OpenShift 原生提供的 Jenkins image</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Openshift 在 DevOps 的方面的確是下了不少功夫，使用 OpenShift 的確可以讓 DevOps engineer 省下不少工，但相對的也有蠻多新東西要學的! XD</p><p>在 OpenShift 建立第一個 BuildConfig(pipeline) 時，它會自動幫使用者佈署一個用來驅動 build pipeline 的 Jenkins server(此 container 會由 OpenShift 本身納管，因此掛了會自己再起一個新的)，佈署的行為如下：</p><ol><li><p>使用 <code>openshift</code> namespace 中的 template <code>jenkins-ephemeral</code> 為範本產生 Jenkins server</p></li><li><p>此 tempate 使用的 image stream 為 <code>jenkins:latest</code></p></li><li><p>Image Stream <code>jenkins:latest</code> 則是指到 Red Hat 官方所提供的 Jenkins image(<strong>registry.access.redhat.com/openshift3/jenkins-2-rhel7</strong>)</p></li></ol><p>當 Jenkins server 佈署好之後，就會自動的與 OpenShift 本身的認証機制進行整合，完全不用使用者進行額外的設定，相關的 credential 也都會設定在 Jenkins server 內部，使用者需要做的就是開始設計 pipeline 就對了!</p><hr><h1 id="Jenkins-image-for-OpenShift"><a href="#Jenkins-image-for-OpenShift" class="headerlink" title="Jenkins image for OpenShift"></a>Jenkins image for OpenShift</h1><p>由於 Jenkins 並不是一個很容易入門上手的系統，因此為了提供使用者能更方便的使用 Jenkins，OpenShift 將 Jenkins 系統打包成 docker image，並加入了許多常用的 plugin 以及與 OpenShift 整合所需要的 plugin，還很貼心的做好相關整合的設定，讓使用者在建立 pipeline 時就會同時產生相對應的 Jenkins server 來使用，省略許多繁複且容易造成錯誤的設定過程。</p><p>目前使用者可能比較容易用到的 Jenkins image for OpenShift 可能會有以下兩種：</p><ul><li>Red Hat 版本：<code>registry.access.redhat.com/openshift3/jenkins-2-rhel7</code><br>本文章的主題在於如何將客製化 OpenShift 原生提供的 Jenkins image</li></ul><h1 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h1><p>Openshift 在 DevOps 的方面的確是下了不少功夫，使用 OpenShift 的確可以讓 DevOps engineer 省下不少工，但相對的也有蠻多新東西要學的! XD</p><p>在 OpenShift 建立第一個 BuildConfig(pipeline) 時，它會自動幫使用者佈署一個用來驅動 build pipeline 的 Jenkins server(此 container 會由 OpenShift 本身納管，因此掛了會自己再起一個新的)，佈署的行為如下：</p><ol><li><p>使用 <code>openshift</code> namespace 中的 template <code>jenkins-ephemeral</code> 為範本產生 Jenkins server</p></li><li><p>此 tempate 使用的 image stream 為 <code>jenkins:latest</code></p></li><li><p>Image Stream <code>jenkins:latest</code> 則是指到 Red Hat 官方所提供的 Jenkins image(<strong>registry.access.redhat.com/openshift3/jenkins-2-rhel7</strong>)</p></li></ol><p>當 Jenkins server 佈署好之後，就會自動的與 OpenShift 本身的認証機制進行整合，完全不用使用者進行額外的設定，相關的 credential 也都會設定在 Jenkins server 內部，使用者需要做的就是開始設計 pipeline 就對了!</p><hr><h1 id="Jenkins-image-for-OpenShift-1"><a href="#Jenkins-image-for-OpenShift-1" class="headerlink" title="Jenkins image for OpenShift"></a>Jenkins image for OpenShift</h1><p>由於 Jenkins 並不是一個很容易入門上手的系統，因此為了提供使用者能更方便的使用 Jenkins，OpenShift 將 Jenkins 系統打包成 docker image，並加入了許多常用的 plugin 以及與 OpenShift 整合所需要的 plugin，還很貼心的做好相關整合的設定，讓使用者在建立 pipeline 時就會同時產生相對應的 Jenkins server 來使用，省略許多繁複且容易造成錯誤的設定過程。</p><p>目前使用者可能比較容易用到的 Jenkins image for OpenShift 可能會有以下兩種：</p><ul><li><p>Red Hat 版本：<code>registry.access.redhat.com/openshift3/jenkins-2-rhel7</code></p></li><li><p>CentOS 版本：<code>openshift/jenkins-2-centos7</code> (位於 DockerHub 上)</p></li></ul><p>由於新版的 OpenShift pipeline plugin for Jenkins 目前僅在 CentOS 版本的 Jenkins image 支援，因此後面的範例將會以 CentOS 版本為主，並提供將原有的 Jenkins image 從 Red Hat 版本改成 CentOS 版本的方法。</p><hr><h1 id="安裝額外的-Jenkins-plugin"><a href="#安裝額外的-Jenkins-plugin" class="headerlink" title="安裝額外的 Jenkins plugin"></a>安裝額外的 Jenkins plugin</h1><p>由於 OpenShift 提供的 Jenkins image 本身就是一個具有 s2i 功能的 docker image，因此我們可以透過 s2i 的流程，將所需要安裝的 plugin 以 source code injection 的方式指定進來並安裝。</p><p>以下的範例將會進行以下的客製化：</p><ol><li><p>安裝 <strong>redmine</strong>, <strong>gitlab-plugin</strong>, <strong>testlink</strong> 三個 plugin (詳細版本可參考<a href="https://github.com/godleon/learning_openshift/blob/master/jenkins_customization/plugins.txt">原始碼</a>)</p></li><li><p>預先加入 <strong>scriptApproval.xml</strong> 檔案，讓某些 method 可在 sandbox 的環境中執行 (詳細清單可參考<a href="https://github.com/godleon/learning_openshift/blob/master/jenkins_customization/configuration/scriptApproval.xml">原始碼</a>)</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得原始碼</span></span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/godleon/openshift-jenkins-customization.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 ImageStream(custom-jenkins-2-centos7) BuildConfig(custom-jenkins-build)</span></span><br><span class="line">$ oc create -f openshift-jenkins-customization/openshift-objects/</span><br><span class="line"></span><br><span class="line">$ oc -n openshift start-build custom-jenkins-build</span><br><span class="line"></span><br><span class="line">$ oc -n openshift get pods</span><br><span class="line">NAME                           READY     STATUS      RESTARTS   AGE</span><br><span class="line">custom-jenkins-build-1-build   0/1       Completed   0          22m</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 build log</span></span><br><span class="line">$ oc -n openshift logs -f custom-jenkins-build-1-build</span><br></pre></td></tr></table></figure><blockquote><p>使用者必須先安裝 <a href="https://github.com/openshift/origin/releases">OpenShift CLI tool</a> 才可以執行上述的 oc 指令</p></blockquote><p>在最後一行指令中檢視 build log 時，就可以看見 OpenShift 透過 s2i 流程將我們在程式碼中所指定的 plugin 都已經安裝完成，此時我們就可以透過 ImageStream <code>custom-jenkins-2-centos7</code>(定義於 <strong>Jenkins_Customization/bc_custom-jenkins-build.yml</strong> 中) 來作為啟動 Jenkins server 的 default image。</p><hr><h1 id="更換原有的-Jenkins-Image"><a href="#更換原有的-Jenkins-Image" class="headerlink" title="更換原有的 Jenkins Image"></a>更換原有的 Jenkins Image</h1><p>今天在測試設計較為複雜的 Jenkins pipeline 時，發現 <a href="https://github.com/openshift/origin/tree/master/examples/jenkins/pipeline">OpenShift 在 GitHub 提供的範例</a> 無法正常的使用，會出現 openshift class 不存在的錯誤，後來仔細的查了一下，發現原來裡面的範例需要搭配 <a href="https://github.com/openshift/jenkins-client-plugin">OpenShift Jenkins Pipeline (DSL) Plugin</a> 一起使用。</p><p><strong>問題是，Red Hat 版本的 Jenkins image 並沒有內建這一個 plugin，此 plugin 目前僅有內建在 Centos 版本的 Jenkins image 中。</strong></p><p>而在上一個步驟中，我們使用了 CentOS 版本的 Jenkins image，並安裝了額外的 plugin 作為後續使用，因此以下便使用已經客製化完成的 Jenkins image 來作為啟動 Jenkins server 的 image。</p><p>為了更改自動佈署的 Jenkins server 所使用的 docker image，需要調整 <code>jenkins-ephemeral</code> 的內容，將 image 從 Red Hat 版本改到在上一個步驟完成的客製化 CentOS 版本，執行以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ oc edit template/jenkins-ephemeral -n openshift</span><br></pre></td></tr></table></figure><p>找到 <code>parameters</code> –&gt; <code>JENKINS_IMAGE_STREAM_TAG</code>，將 <strong>value</strong> 從 <code>jenkins:latest</code> 改為 <code>custom-jenkins-2-centos7:latest</code>，存檔即可。</p><p>經過了以上的設定，後面自動佈署出來的 Jenkins server 都會是 CentOS7 的版本，也會同時預載好 <a href="https://github.com/openshift/jenkins-client-plugin">OpenShift Jenkins Pipeline (DSL) Plugin</a> 以及在上一個步驟額外安裝好的 plugin。</p><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://github.com/jenkinsci/openshift-pipeline-plugin">OpenShift V3 Plugin for Jenkins (based on Kubernetes plugin)</a></p></li><li><p><a href="https://github.com/openshift/jenkins-client-plugin">OpenShift Jenkins Pipeline (DSL) Plugin (newly design)</a></p></li><li><p><a href="https://github.com/openshift/origin/tree/master/examples/jenkins/pipeline">Using Jenkins Pipelines with OpenShift @GitbHub</a></p></li><li><p><a href="https://docs.openshift.com/container-platform/3.6/using_images/other_images/jenkins.html">Jenkins - Other Images | Using Images | OpenShift Container Platform 3.6</a></p></li><li><p><a href="https://github.com/openshift/jenkins-sync-plugin/issues/57">Script approvals needed for changes to build config Jenkinsfile · Issue #57 · openshift/jenkins-sync-plugin</a></p></li><li><p><a href="https://github.com/fabric8io/jenkins-docker">fabric8io/jenkins-docker: docker file for a jenkins docker image</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> OpenShift </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenShift </tag>
            
            <tag> Jenkins </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[OpenShift] Concept - Image Stream</title>
      <link href="/blog/OpenShift/OpenShift-Concept-ImageStream/"/>
      <url>/blog/OpenShift/OpenShift-Concept-ImageStream/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Image-Stream"><a href="#What-is-Image-Stream" class="headerlink" title="What is Image Stream?"></a>What is Image Stream?</h1><p>每一個 image stream 代表著一個 Docker-formatted container image；它其實只是一個在 OpenShift 中內部對於 docker image 的命名方式，讓系統可以使用指定的名稱找到正確的 docker image 來使用。(類似 Docker 中對每個 image 使用 tag 來命名)</p><p>也因為有自己內部的命名方式，因此 Image Stream 就可以包含以下來源的 image：</p><ul><li><p>OpenShift 內部的 container registry</p></li><li><p>其他的 image stream</p></li><li><p>外部的 image repository (例如：DockerHub, CoreOS Quay)</p></li></ul><p>在 OpenShift 中，image stream 可與 Build &amp; Deployment 搭配完成特定的自動化功能；由於 Build &amp; Deployment 都可以監控特定 image stream，當 image stream 指向的 image 有新版產生時，可自動的進行特定的 build or deploy 的工作。</p><hr><h1 id="建立第一個-Image-Stream"><a href="#建立第一個-Image-Stream" class="headerlink" title="建立第一個 Image Stream"></a>建立第一個 Image Stream</h1><p>以下是一個 ImageStream 的定義範例：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ImageStream</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;myubuntu:xenial&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">tags:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&#x27;16.04&#x27;</span></span><br><span class="line">    <span class="attr">from:</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">DockerImage</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">ubuntu</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure><p>透過以上的 image stream 定義，在 OpenShift Build or Deployment 中，就可以使用 <code>myubuntu:xenial</code> 指定外部 DockerHub 中的 <code>ubuntu:16.04</code> image。</p><p>當以上 ImageStream 被建立後，我們可以查詢到以下資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ oc get is</span><br><span class="line">NAME       DOCKER REPO                                           TAGS      UPDATED</span><br><span class="line">myubuntu   docker-registry.default.svc:5000/leon-test/myubuntu   16.04     2 seconds ago</span><br></pre></td></tr></table></figure><p>若使用 <code>oc get is/myubuntu -o=yaml</code> 指令檢視 YAML 輸出，得到以下內容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ImageStream</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">openshift.io/image.dockerRepositoryCheck:</span> <span class="number">2017-11-06T04:12:59Z</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="number">2017-11-06T04:12:58Z</span></span><br><span class="line">  <span class="attr">generation:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">myubuntu</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">leon-test</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">&quot;9145705&quot;</span></span><br><span class="line">  <span class="attr">selfLink:</span> <span class="string">/oapi/v1/namespaces/leon-test/imagestreams/myubuntu</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">c59725ce-c2a8-11e7-a13b-faf564e56811</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">lookupPolicy:</span></span><br><span class="line">    <span class="attr">local:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">tags:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">annotations:</span> <span class="literal">null</span></span><br><span class="line">    <span class="attr">from:</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">DockerImage</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">ubuntu</span></span><br><span class="line">    <span class="attr">generation:</span> <span class="number">2</span></span><br><span class="line">    <span class="attr">importPolicy:</span> &#123;&#125;</span><br><span class="line">    <span class="attr">name:</span> <span class="string">&quot;16.04&quot;</span></span><br><span class="line">    <span class="attr">referencePolicy:</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Source</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">dockerImageRepository:</span> <span class="string">docker-registry.default.svc:5000/leon-test/myubuntu</span></span><br><span class="line">  <span class="attr">tags:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">items:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">created:</span> <span class="number">2017-11-06T04:12:59Z</span></span><br><span class="line">      <span class="attr">dockerImageReference:</span> <span class="string">ubuntu@sha256:152b4ccc429f6f28533aff625d8345baf1ba3808e9a99446e86b2bf3efa18571</span></span><br><span class="line">      <span class="attr">generation:</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">sha256:152b4ccc429f6f28533aff625d8345baf1ba3808e9a99446e86b2bf3efa18571</span></span><br><span class="line">    <span class="attr">tag:</span> <span class="string">&quot;16.04&quot;</span></span><br></pre></td></tr></table></figure><hr><h1 id="Image-Stream-Image"><a href="#Image-Stream-Image" class="headerlink" title="Image Stream Image"></a>Image Stream Image</h1><p>image stream image(簡稱 <strong>isimage</strong>) 是一種 virtual resource，讓使用者可以透過 isimage 從特定的 image stream 取得 image，isimage 以 <code>&lt;image stream name&gt;@&lt;image name&gt;</code> 的方式呈現，因此以上面的範例來看，image steam image 就會是：</p><blockquote><p>myubuntu@sha256:152b4ccc429f6f28533aff625d8345baf1ba3808e9a99446e86b2bf3efa18571</p></blockquote><hr><h1 id="Image-Stream-Tag"><a href="#Image-Stream-Tag" class="headerlink" title="Image Stream Tag"></a>Image Stream Tag</h1><p>image stream tag(簡稱 <strong>istag</strong>) 是一個指到上面 image stream image 的 name pointer，可指向 local or 外部的 image，此外 isiage 還包含了 image 內容變動的歷史紀錄，這樣的設計讓使用者可以在有需要的時候方便的進行 rollback。</p><p>istag 以 <code>&lt;image stream name&gt;:&lt;tag&gt;</code> 的方式呈現，因此以上面的範例來看， istag 就會是：</p><blockquote><p>　myubuntu:16.04</p></blockquote><hr><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>有了 Image Stream(is), Image Stream Image (isimage), 以及 Image Stream Tag(istag) 的觀念之後，下一個階段將會介紹如何在 OpenShift 中管理 Image。</p><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://docs.openshift.com/container-platform/3.6/architecture/core_concepts/builds_and_image_streams.html">Builds and Image Streams - Core Concepts | Architecture | OpenShift Container Platform 3.6</a></p></li><li><p><a href="https://docs.openshift.com/enterprise/3.0/architecture/core_concepts/builds_and_image_streams.html">Builds and Image Streams - Core Concepts | Architecture | OpenShift Enterprise 3.0</a></p></li><li><p><a href="https://github.com/openshift/origin/tree/master/examples/image-streams">OpenShift ImageStream Examples @GitHub</a></p></li><li><p><a href="https://docs.openshift.com/container-platform/3.6/dev_guide/managing_images.html">Managing Images | Developer Guide | OpenShift Container Platform 3.6</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> OpenShift </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenShift </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes 學習筆記</title>
      <link href="/blog/Kubernetes/Learning-Kubernetes/"/>
      <url>/blog/Kubernetes/Learning-Kubernetes/</url>
      
        <content type="html"><![CDATA[<p>此篇文章為研究 Kubernetes 時所留下的學習筆記索引</p><h1 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h1><h2 id="kubeconfig"><a href="#kubeconfig" class="headerlink" title="kubeconfig"></a>kubeconfig</h2><ul><li><a href="https://github.com/godleon/learning_kubernetes/blob/master/concept/howto_configure_kubeconfig.md">如何設定 kubeconfig 與 Kubernetes cluster 互動</a></li></ul><h2 id="Overview-amp-Components"><a href="#Overview-amp-Components" class="headerlink" title="Overview &amp; Components"></a>Overview &amp; Components</h2><ul><li><p><a href="https://github.com/godleon/learning_kubernetes/blob/master/concept/component_overview.md">組成元件概觀</a></p></li><li><p><a href="https://github.com/godleon/learning_kubernetes/blob/master/concept/overview.md">Kubernetes Overview</a></p></li></ul><h2 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h2><ul><li><p><a href="https://github.com/godleon/learning_kubernetes/blob/master/concept/network/service.md">Service</a></p></li><li><p><a href="https://github.com/godleon/learning_kubernetes/blob/master/concept/network/ingress.md">Ingress</a></p></li></ul><h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><ul><li><a href="https://github.com/godleon/learning_kubernetes/blob/master/concept/storage/volume.md">Volume, PersistentVolume &amp; PersistentVolumeClaim</a></li></ul><hr><h1 id="Operating"><a href="#Operating" class="headerlink" title="Operating"></a>Operating</h1><ul><li><p><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">kubectl Cheat Sheet - Kubernetes</a></p></li><li><p><a href="https://github.com/godleon/learning_kubernetes/blob/master/operation/basic.md">Kubernetes 基本操作</a></p></li></ul><h2 id="Persistent-Volume"><a href="#Persistent-Volume" class="headerlink" title="Persistent Volume"></a>Persistent Volume</h2><ul><li><a href="https://github.com/godleon/learning_kubernetes/blob/master/operation/use_PersistentVolume_NFS.md">使用 Persistent Volume - 以 NFS 為例</a></li></ul><h1 id="學習資源"><a href="#學習資源" class="headerlink" title="學習資源"></a>學習資源</h1><ul><li><a href="http://docs.kubernetes.org.cn/">Kubernetes(K8S)中文文档_Kubernetes中文社区</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ceph 簡單指令操作</title>
      <link href="/blog/Ceph/Ceph-Cheatsheet/"/>
      <url>/blog/Ceph/Ceph-Cheatsheet/</url>
      
        <content type="html"><![CDATA[<h1 id="Monitoring-and-Health"><a href="#Monitoring-and-Health" class="headerlink" title="Monitoring and Health"></a>Monitoring and Health</h1><p>剛安裝完，首先先檢查 ceph cluster 的狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 ceph cluster status</span></span><br><span class="line">$ ceph -s</span><br><span class="line">    cluster cd6fcb41-c373-48fb-aab3-f8d330a26ccb</span><br><span class="line">     health HEALTH_WARN</span><br><span class="line">            too few PGs per OSD (16 &lt; min 30)</span><br><span class="line">     monmap e1: 3 mons at &#123;ceph01=10.102.41.101:6789/0,ceph02=10.102.41.102:6789/0,ceph03=10.102.41.103:6789/0&#125;</span><br><span class="line">            election epoch 8, quorum 0,1,2 ceph01,ceph02,ceph03</span><br><span class="line">     osdmap e36: 12 osds: 12 up, 12 <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v84: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">            405 MB used, 2174 GB / 2174 GB avail</span><br><span class="line">                  64 active+clean</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>也可以用 <code>ceph -w</code> 檢視即時的狀態</p></blockquote><p>檢查健康狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ceph health detail</span><br><span class="line">HEALTH_WARN too few PGs per OSD (16 &lt; min 30)</span><br><span class="line">too few PGs per OSD (16 &lt; min 30)</span><br></pre></td></tr></table></figure><p>接著檢視目前 ceph cluster 提供了多少容量可用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視目前可用的容量、每個 pool 的使用狀況 &amp; quota 等資訊</span></span><br><span class="line">$ ceph df</span><br><span class="line">GLOBAL:</span><br><span class="line">    SIZE      AVAIL     RAW USED     %RAW USED </span><br><span class="line">    2174G     2174G         405M          0.02 </span><br><span class="line">POOLS:</span><br><span class="line">    NAME     ID     USED     %USED     MAX AVAIL     OBJECTS </span><br><span class="line">    rbd      0         0         0          724G           0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 連到 CRUSH tree, 顯示 weight, variance, capacity ... etc</span></span><br><span class="line">$ ceph osd df tree</span><br><span class="line">ID WEIGHT  REWEIGHT SIZE  USE    AVAIL %USE VAR  PGS TYPE NAME       </span><br><span class="line">-1 2.12384        - 2174G   405M 2174G 0.02 1.00   0 root default    </span><br><span class="line">-2 0.70795        -  724G   135M  724G 0.02 1.00   0     host ceph01 </span><br><span class="line"> 0 0.17699  1.00000  181G 35200k  181G 0.02 1.02  11         osd.0   </span><br><span class="line"> 3 0.17699  1.00000  181G 34708k  181G 0.02 1.00  19         osd.3   </span><br><span class="line"> 6 0.17699  1.00000  181G 34420k  181G 0.02 0.99  14         osd.6   </span><br><span class="line"> 8 0.17699  1.00000  181G 34336k  181G 0.02 0.99  20         osd.8   </span><br><span class="line">-3 0.70795        -  724G   135M  724G 0.02 1.00   0     host ceph03 </span><br><span class="line"> 1 0.17699  1.00000  181G 35568k  181G 0.02 1.03  17         osd.1   </span><br><span class="line"> 5 0.17699  1.00000  181G 34432k  181G 0.02 0.99  12         osd.5   </span><br><span class="line"> 9 0.17699  1.00000  181G 34272k  181G 0.02 0.99  18         osd.9   </span><br><span class="line">11 0.17699  1.00000  181G 34200k  181G 0.02 0.99  17         osd.11  </span><br><span class="line">-4 0.70795        -  724G   135M  724G 0.02 1.00   0     host ceph02 </span><br><span class="line"> 2 0.17699  1.00000  181G 35076k  181G 0.02 1.01  19         osd.2   </span><br><span class="line"> 4 0.17699  1.00000  181G 34652k  181G 0.02 1.00  18         osd.4   </span><br><span class="line"> 7 0.17699  1.00000  181G 34456k  181G 0.02 0.99  11         osd.7   </span><br><span class="line">10 0.17699  1.00000  181G 34280k  181G 0.02 0.99  16         osd.10  </span><br><span class="line">              TOTAL 2174G   405M 2174G 0.02                          </span><br></pre></td></tr></table></figure><h1 id="Working-with-Pools-and-OSDs"><a href="#Working-with-Pools-and-OSDs" class="headerlink" title="Working with Pools and OSDs"></a>Working with Pools and OSDs</h1><p>若要尋找單顆 OSD 的相關資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尋找 OSD physical location</span></span><br><span class="line">$ ceph osd find 1</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;osd&quot;</span>: 1,</span><br><span class="line">    <span class="string">&quot;ip&quot;</span>: <span class="string">&quot;10.102.41.103:6800\/63011&quot;</span>,</span><br><span class="line">    <span class="string">&quot;crush_location&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;host&quot;</span>: <span class="string">&quot;ceph03&quot;</span>,</span><br><span class="line">        <span class="string">&quot;root&quot;</span>: <span class="string">&quot;default&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示指定 OSD metadata</span></span><br><span class="line">$ ceph osd metadata 1</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;id&quot;</span>: 1,</span><br><span class="line">    <span class="string">&quot;arch&quot;</span>: <span class="string">&quot;x86_64&quot;</span>,</span><br><span class="line">    <span class="string">&quot;back_addr&quot;</span>: <span class="string">&quot;10.102.41.103:6801\/63011&quot;</span>,</span><br><span class="line">    <span class="string">&quot;backend_filestore_dev_node&quot;</span>: <span class="string">&quot;unknown&quot;</span>,</span><br><span class="line">    <span class="string">&quot;backend_filestore_partition_path&quot;</span>: <span class="string">&quot;unknown&quot;</span>,</span><br><span class="line">    <span class="string">&quot;ceph_version&quot;</span>: <span class="string">&quot;ceph version 10.2.5-37.el7cp (033f137cde8573cfc5a4662b4ed6a63b8a8d1464)&quot;</span>,</span><br><span class="line">    ......</span><br><span class="line">    <span class="string">&quot;osd_data&quot;</span>: <span class="string">&quot;\/var\/lib\/ceph\/osd\/ceph-1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;osd_journal&quot;</span>: <span class="string">&quot;\/var\/lib\/ceph\/osd\/ceph-1\/journal&quot;</span>,</span><br><span class="line">    <span class="string">&quot;osd_objectstore&quot;</span>: <span class="string">&quot;filestore&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>建立/移除 pool: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 pool, 名稱為 pve_image, pg 數量為 1024</span></span><br><span class="line">$ ceph osd pool create pve_images 1024</span><br><span class="line">pool <span class="string">&#x27;pve_images&#x27;</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示目前 pool 詳細狀態</span></span><br><span class="line">$ ceph osd pool ls detail</span><br><span class="line">pool 0 <span class="string">&#x27;rbd&#x27;</span> replicated size 3 min_size 2 crush_ruleset 0 object_hash rjenkins pg_num 64 pgp_num 64 last_change 1 flags hashpspool stripe_width 0</span><br><span class="line">pool 1 <span class="string">&#x27;pve_images&#x27;</span> replicated size 3 min_size 2 crush_ruleset 0 object_hash rjenkins pg_num 1024 pgp_num 1024 last_change 37 flags hashpspool stripe_width 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示指定 pool 的詳細資料</span></span><br><span class="line">$ ceph osd pool get pve_images all</span><br><span class="line">size: 3</span><br><span class="line">min_size: 2</span><br><span class="line">crash_replay_interval: 0</span><br><span class="line">pg_num: 1024</span><br><span class="line">pgp_num: 1024</span><br><span class="line">crush_ruleset: 0</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 pool (要重複 pool name 兩次還要加上那有趣的參數)</span></span><br><span class="line">$ ceph osd pool delete pve_images pve_images --yes-i-really-really-mean-it</span><br><span class="line">pool <span class="string">&#x27;pve_images&#x27;</span> removed</span><br></pre></td></tr></table></figure><p>調整現有 pool 的狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定 placement groups 的數量</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> rbd pg_num 384</span><br><span class="line"><span class="built_in">set</span> pool 0 pg_num to 384</span><br><span class="line"></span><br><span class="line"><span class="comment"># pgp =&gt; The effective number of placement groups to use when calculating data placement.</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> rbd pgp_num 384</span><br><span class="line"><span class="built_in">set</span> pool 0 pgp_num to 384</span><br><span class="line"></span><br><span class="line"><span class="comment"># 調整完就會從原本的 warning 狀態變成 health_ok 了!</span></span><br><span class="line">$ ceph -s</span><br><span class="line">    cluster cd6fcb41-c373-48fb-aab3-f8d330a26ccb</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: 3 mons at &#123;ceph01=10.102.41.101:6789/0,ceph02=10.102.41.102:6789/0,ceph03=10.102.41.103:6789/0&#125;</span><br><span class="line">            election epoch 8, quorum 0,1,2 ceph01,ceph02,ceph03</span><br><span class="line">     osdmap e47: 12 osds: 12 up, 12 <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v177: 384 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">            461 MB used, 2174 GB / 2174 GB avail</span><br><span class="line">                 384 active+clean</span><br></pre></td></tr></table></figure><h1 id="RBD-Block-Storage"><a href="#RBD-Block-Storage" class="headerlink" title="RBD Block Storage"></a>RBD Block Storage</h1><p>這個部份是用在把 Ceph 作為 block-based storage 時所需要了解的指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示目前的 RBD volume</span></span><br><span class="line">$ rbd ls</span><br><span class="line">vm-101-disk-1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示指定 RBD volume 的狀態</span></span><br><span class="line">$ rbd status vm-101-disk-1</span><br><span class="line">Watchers:</span><br><span class="line">    watcher=10.102.70.124:0/3361738208 client.143192 cookie=140498849842176</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示指定 RBD volume 的相關資訊</span></span><br><span class="line">$ rbd info vm-101-disk-1</span><br><span class="line">rbd image <span class="string">&#x27;vm-101-disk-1&#x27;</span>:</span><br><span class="line">    size 32768 MB <span class="keyword">in</span> 8192 objects</span><br><span class="line">    order 22 (4096 kB objects)</span><br><span class="line">    block_name_prefix: rbd_data.22f52238e1f29</span><br><span class="line">    format: 2</span><br><span class="line">    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">    flags: </span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除指定的 RBD volume</span></span><br><span class="line">$ rbd rm vm-101-disk-1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立名稱為 Jenkins_Data，大小為 40GB 的 RBD volume</span></span><br><span class="line">$ rbd create --pool rbd --image Jenkins_Data --size 40960</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 RBD volume 資訊</span></span><br><span class="line">$ rbd --image Jenkins_Data info</span><br><span class="line">rbd image <span class="string">&#x27;Jenkins_Data&#x27;</span>:</span><br><span class="line">    size 40960 MB <span class="keyword">in</span> 10240 objects</span><br><span class="line">    order 22 (4096 kB objects)</span><br><span class="line">    block_name_prefix: rbd_data.a71d6238e1f29</span><br><span class="line">    format: 2</span><br><span class="line">    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">    flags: </span><br><span class="line"></span><br><span class="line"><span class="comment"># 變更 RBD volume 大小，縮小要加上 &quot;--allow-shrink&quot; 以確保安全</span></span><br><span class="line">$ rbd --image Jenkins_Data resize --size 10240 --allow-shrink</span><br><span class="line">Resizing image: 100% complete...done.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認 RBD volume 容量正確變更</span></span><br><span class="line">$ rbd --image Jenkins_Data info</span><br><span class="line">rbd image <span class="string">&#x27;Jenkins_Data&#x27;</span>:</span><br><span class="line">    size 10240 MB <span class="keyword">in</span> 2560 objects</span><br><span class="line">    order 22 (4096 kB objects)</span><br><span class="line">    block_name_prefix: rbd_data.a71d6238e1f29</span><br><span class="line">    format: 2</span><br><span class="line">    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">    flags: </span><br><span class="line"></span><br><span class="line"><span class="comment"># 放大 RBD Volume</span></span><br><span class="line">$ rbd --image Jenkins_Data resize --size 20480</span><br><span class="line">Resizing image: 100% complete...done.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認 RBD volume 容量正確變更</span></span><br><span class="line">$ rbd --image Jenkins_Data info</span><br><span class="line">rbd image <span class="string">&#x27;Jenkins_Data&#x27;</span>:</span><br><span class="line">    size 20480 MB <span class="keyword">in</span> 5120 objects</span><br><span class="line">    order 22 (4096 kB objects)</span><br><span class="line">    block_name_prefix: rbd_data.a71d6238e1f29</span><br><span class="line">    format: 2</span><br><span class="line">    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line">    flags: </span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="Authentication-and-Authorization"><a href="#Authentication-and-Authorization" class="headerlink" title="Authentication and Authorization"></a>Authentication and Authorization</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視目前可使用 ceph cluster 的 user list</span></span><br><span class="line">$ ceph auth list</span><br><span class="line">installed auth entries:</span><br><span class="line"></span><br><span class="line">osd.0</span><br><span class="line">    key: ABxxxxxxx</span><br><span class="line">    caps: [mon] allow profile osd</span><br><span class="line">    caps: [osd] allow *</span><br><span class="line">......</span><br><span class="line">client.admin</span><br><span class="line">    key: ABxxxxxxx</span><br><span class="line">    caps: [mds] allow *</span><br><span class="line">    caps: [mon] allow *</span><br><span class="line">    caps: [osd] allow *</span><br><span class="line">client.bootstrap-mds</span><br><span class="line">    key: ABxxxxxxx</span><br><span class="line">    caps: [mon] allow profile bootstrap-mds</span><br><span class="line">client.bootstrap-osd</span><br><span class="line">    key: ABxxxxxxx</span><br><span class="line">    caps: [mon] allow profile bootstrap-osd</span><br><span class="line">client.bootstrap-rgw</span><br><span class="line">    key: ABxxxxxxx</span><br><span class="line">    caps: [mon] allow profile bootstrap-rgw</span><br></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://www.gitbook.com/book/tobegit3hub1/ceph_from_scratch/details">Ceph From Scratch · GitBook</a></p></li><li><p><a href="https://sabaini.at/pages/ceph-cheatsheet.html">Ceph Cheatsheet - sabaini.at</a></p></li><li><p><a href="http://michaelkang.blog.51cto.com/1553154/1698287">最新ceph集群常用命令梳理 - 康建华 - 51CTO技术博客</a></p></li><li><p><a href="http://docs.ceph.com/docs/jewel/rados/operations/pools/">Pools — Ceph Documentation</a></p></li><li><p><a href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement Groups — Ceph Documentation</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ceph </tag>
            
            <tag> SDS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] Template &amp; Snapshot 的運用</title>
      <link href="/blog/KVM/KVM-Template-And-Snapshot/"/>
      <url>/blog/KVM/KVM-Template-And-Snapshot/</url>
      
        <content type="html"><![CDATA[<p>此篇文章介紹如何使用 KVM 中的 template &amp; snapshot 功能</p><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>虛擬化技術有些非常吸引人使用的特性，例如：</p><ul><li><p>Fast Provisioning</p></li><li><p>Snapshots</p></li><li><p>不複雜的 backup &amp; recovery 方式</p></li></ul><p>以上這些特性都不是在實體環境上容易實現的，但透過 KVM 中的 template，可以實現 fast provisioning，而透過 snapshot 則可簡單實現 backup &amp; recovery。</p><hr><h1 id="VM-Templates"><a href="#VM-Templates" class="headerlink" title="VM Templates"></a>VM Templates</h1><p>template 有別於一般的 VM clone，clone 只是從其他 VM 複製成另外一個完整的 VM；而 template 則是可以作為其他 VM 的 master copy，並用來產生很多個 clone</p><h2 id="建立-template"><a href="#建立-template" class="headerlink" title="建立 template"></a>建立 template</h2><p>template 是由以存在的 VM 所轉換過來的，因此在建立 template 之前，我們必須先完成以下步驟：</p><ol><li><p>安裝 &amp; 設定 VM，確認上面已經安裝所需要的所有軟體套件</p></li><li><p>移除所有系統特定的設定，確保只與此 VM 相關的設定(例如：固定 IP)不會被複製到其他 VM 上</p></li><li><p>修改 VM 名稱讓其容易辨識，例如以 <strong>template</strong> 作為開頭</p></li></ol><h3 id="1-建立-Centos-Template"><a href="#1-建立-Centos-Template" class="headerlink" title="(1) 建立 Centos Template"></a>(1) 建立 Centos Template</h3><p>要建立 Linux template，必須透過 <code>virt-sysprep</code> 工具的協助。</p><p>這工具由 <code>libguestfs-tools-s</code> 套件所提供，可以移除 VM 中系統特定的資訊，以便於轉換成 template 之用；此外也可以客製化 VM，例如加上 SSH Key、加入使用者、設定 Logo …. 等等。</p><p>輸入 <code>virt-prep --help</code> 可以知道 virt-sysprep 支援哪些調整選項：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@ocp-kvm-host ~]<span class="comment"># virt-sysprep -help</span></span><br><span class="line">virt-sysprep: reset or unconfigure a virtual machine so clones can be made</span><br><span class="line"></span><br><span class="line"> virt-sysprep [--options] -d domname</span><br><span class="line"></span><br><span class="line"> virt-sysprep [--options] -a disk.img [-a disk.img ...]</span><br><span class="line"></span><br><span class="line">A short summary of the options is given below.  For detailed <span class="built_in">help</span> please</span><br><span class="line"><span class="built_in">read</span> the man page virt-sysprep(1).</span><br><span class="line"></span><br><span class="line">  -a file                             Add disk image file</span><br><span class="line">  --add file                          Add disk image file</span><br><span class="line">  -c uri                              Set libvirt URI</span><br><span class="line">  --chmod PERMISSIONS:FILE            Change the permissions of a file</span><br><span class="line">  --connect uri                       Set libvirt URI</span><br><span class="line">  -d domain                           Set libvirt guest name</span><br><span class="line">  --debug-gc                          Debug GC and memory allocations (internal)</span><br><span class="line">  --delete PATH                       Delete a file or directory</span><br><span class="line">  --domain domain                     Set libvirt guest name</span><br><span class="line">  --dry-run                           Perform a dry run</span><br><span class="line">  --dryrun                            Perform a dry run</span><br><span class="line">  --dump-pod                          Dump POD (internal)</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><p>從 help 中的說明可以看出，virt-sysprep 有兩個參數分別是 <code>-d</code> &amp; <code>-a</code>，其中 <code>-d</code> 所處理的對象是 VM，而 <code>-a</code> 所處理的對象則是獨立的 virtual disk。</p><p>以下使用對 VM 為範例來操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@ocp-kvm-host ~]<span class="comment"># virsh list --all</span></span><br><span class="line"> Id    Name                           State</span><br><span class="line">----------------------------------------------------</span><br><span class="line"> -     centos7                        shut off</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">[root@ocp-kvm-host ~]<span class="comment"># virt-sysprep -d centos7</span></span><br><span class="line">[   0.0] Examining the guest ...</span><br><span class="line">[  43.0] Performing <span class="string">&quot;abrt-data&quot;</span> ...</span><br><span class="line">[  43.0] Performing <span class="string">&quot;bash-history&quot;</span> ...</span><br><span class="line">........</span><br><span class="line">[  43.0] Performing <span class="string">&quot;udev-persistent-net&quot;</span> ...</span><br><span class="line">[  43.0] Performing <span class="string">&quot;utmp&quot;</span> ...</span><br><span class="line">[  43.0] Performing <span class="string">&quot;yum-uuid&quot;</span> ...</span><br><span class="line">[  43.0] Performing <span class="string">&quot;customize&quot;</span> ...</span><br><span class="line">[  43.0] Setting a random seed</span><br><span class="line">[  43.0] Performing <span class="string">&quot;lvm-uuids&quot;</span> ...</span><br></pre></td></tr></table></figure><blockquote><p>到此步驟時，VM template 已經準備完成；還有一點更重要的是，<strong>不要再啟動 template VM</strong>，否則將會失去之前 virt-sysprep 所完成的結果，甚至有可能會在使用 thin method(參考下方) 產生 VM 時發生問題 </p></blockquote><h3 id="2-建立-Windows-Template"><a href="#2-建立-Windows-Template" class="headerlink" title="(2) 建立 Windows Template"></a>(2) 建立 Windows Template</h3><p>要製作 template，可以透過 <code>virt-clone</code> + <code>virt-sysprep</code> 的方式</p><p>要透過 template 建立新的 VM，有以下兩種方式可以進行：</p><h2 id="透過-template-佈署-VM"><a href="#透過-template-佈署-VM" class="headerlink" title="透過 template 佈署 VM"></a>透過 template 佈署 VM</h2><ol><li><p><strong>thin method</strong></p><blockquote><p>此方式會以 template image 為 base image以 read-only(template VM image) 搭配 copy-on-write(新的 VM) 來處理，所需要的磁碟空間比較小，但需要確保 base image 可以被存取</p></blockquote></li><li><p><strong>clone method</strong></p><blockquote><p>此方式會完整複製一份 template image 作為 新 VM 的 image，會消耗較多的磁碟空間，但可以完全獨立不依賴原本的 base image</p></blockquote></li></ol><hr><h1 id="Snapshots"><a href="#Snapshots" class="headerlink" title="Snapshots"></a>Snapshots</h1><p>snapshot 是以檔案為基礎，用來表示 VM 在某個特定時間點的狀態，包含了相關設定檔 &amp; disk 資料；而透過 snapshot，管理者可以隨時將 VM 還原到當時建立 snapshot 時的狀態，而這功能在進行對於 VM 要進行重大變更前時特別好用。</p><p>此外，libvirt 還提供了 live snapshot 的功能，可以針對執行中的 VM 進行 snapshot，但這功能不建議使用在 I/O 工作頻繁的 VM 上，建議這類的 VM 還是 shutdown or suspend 之後再來做 snapshot 會較好。</p><p>libvirt 支援兩種 snapshop，分別如下：</p><h3 id="internal-snapshot"><a href="#internal-snapshot" class="headerlink" title="internal snapshot"></a>internal snapshot</h3><p>internal snapshot 的 snapshot 資訊會存在於同一個 qcow2 檔案中(before/after snapshot bit)，有以下的限制需要注意：</p><ul><li><p>僅支援 qcow2 format</p></li><li><p>當建立 snapshot 時，VM 會進入暫停狀態</p></li><li><p>無法使用在 LVM storage pool 上</p></li></ul><h3 id="external-snapshot"><a href="#external-snapshot" class="headerlink" title="external snapshot"></a>external snapshot</h3><p>external snapshot 是以 copy-on-write 的概念進行的，當 VM 進行 snapshot 後，system disk 就會進入 read-only 的模式，後續 VM guest 新增的資料就會放在 overlay disk image 上，以下有個圖示來說明：</p><p><img src="https://github.com/godleon/godleon.github.io/blob/master/_posts/images/2017/KVM-Template-And-Snapshot/copy-on-write_overlay-disk0-image.png?raw=true" alt="copy-on-write overlay disk image"></p><p>external snapshot 也有以下特點：</p><ol><li><p>overlay disk image 的起始大小為 0，最大可以到 original disk 的大小</p></li><li><p>base disk 可以是任何的格式(例如：raw, qcow2, 或是其他 libvirt 支援的格式)</p></li><li><p>overlay disk image 一定是 qcow2 格式</p></li></ol><h2 id="VM-Disk-Image-Format"><a href="#VM-Disk-Image-Format" class="headerlink" title="VM Disk Image Format"></a>VM Disk Image Format</h2><p>libvirt 支援很多種不同的 disk foramt，包含 iso, dmg, qcow2, raw, vmdk, vpc … 等等，但與 libvirt 搭配起來運作的最好的是 <strong>raw</strong> &amp; <strong>qcow2</strong> 兩種：</p><h3 id="1-raw"><a href="#1-raw" class="headerlink" title="(1) raw"></a>(1) <strong>raw</strong></h3><ul><li><p>效能最佳，performace overhead 最低，適合給有高度 I/O 需求的 VM 使用</p></li><li><p>完全佔據 disk 所指定的空間</p></li><li><p>沒有 snapshot &amp; compression 的功能</p></li></ul><h3 id="2-qcow2"><a href="#2-qcow2" class="headerlink" title="(2) qcow2"></a>(2) <strong>qcow2</strong></h3><ul><li><p>完全以 cloud 架構出發所設計的格式</p></li><li><p>支援 read-only backing、snapshot、compression、encryption、pre-allocation … 等功能</p></li></ul><h2 id="轉換-Disk-Image-Format"><a href="#轉換-Disk-Image-Format" class="headerlink" title="轉換 Disk Image Format"></a>轉換 Disk Image Format</h2><p>以下介紹如何透過 <code>qemu-img</code> 指令在 <strong>raw</strong> &amp; <strong>qcow2</strong> 之間進行轉換：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># raw -&gt; qcow2</span></span><br><span class="line">$ qemu-img convert -f raw -O qcow2 vm_disk.img vm_disk.qcow2</span><br><span class="line"></span><br><span class="line"><span class="comment"># qcow2 -&gt; raw</span></span><br><span class="line">$ qemu-img convert -f qcow2 -O ram vm_disk.qcow2 vm_disk.img</span><br></pre></td></tr></table></figure><h2 id="操作-Internal-Snapshot"><a href="#操作-Internal-Snapshot" class="headerlink" title="操作 Internal Snapshot"></a>操作 Internal Snapshot</h2><h3 id="1-建立-amp-檢視-snapshot"><a href="#1-建立-amp-檢視-snapshot" class="headerlink" title="(1) 建立 &amp; 檢視 snapshot"></a>(1) 建立 &amp; 檢視 snapshot</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">$ virsh snapshot-list rhel7.3</span><br><span class="line"> Name                 Creation Time             State</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 internal snapshot</span></span><br><span class="line">$ virsh snapshot-create rhel7.3</span><br><span class="line">Domain snapshot 1481514143 created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 internal snapshot (使用 snapshot-create-as 搭配 --name, --description 等參數)</span></span><br><span class="line"><span class="comment"># --atomic 可以確保 snapshot 是可以用的 (若是 snapshot 無法使用，建立時就會 fail)</span></span><br><span class="line">$ virsh snapshot-create-as rhel7.3 --name <span class="string">&quot;rhel7.3_snapshot_1&quot;</span> --description <span class="string">&quot;First named snapshot&quot;</span> --atomic</span><br><span class="line">Domain snapshot rhel7.3_snapshot_1 created</span><br><span class="line"></span><br><span class="line">$ virsh snapshot-list rhel7.3</span><br><span class="line"> Name                 Creation Time             State</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"> 1481514143           2016-12-12 11:42:23 +0800 shutoff</span><br><span class="line"> rhel7.3_snapshot_1   2016-12-14 08:32:23 +0800 shutoff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示 snapshot 資訊</span></span><br><span class="line">$ virsh snapshot-info rhel7.3 --snapshotname 1481514143</span><br><span class="line">Name:           1481514143</span><br><span class="line">Domain:         rhel7.3</span><br><span class="line">Current:        no</span><br><span class="line">State:          shutoff</span><br><span class="line">Location:       internal</span><br><span class="line">Parent:         -</span><br><span class="line">Children:       1</span><br><span class="line">Descendants:    1</span><br><span class="line">Metadata:       yes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視指定 VM 的 current snapshot 資訊</span></span><br><span class="line">$ virsh snapshot-current rhel7.3</span><br><span class="line">&lt;domainsnapshot&gt;</span><br><span class="line">  &lt;name&gt;rhel7.3_snapshot_1&lt;/name&gt;</span><br><span class="line">  &lt;description&gt;First named snapshot&lt;/description&gt;</span><br><span class="line">  &lt;state&gt;shutoff&lt;/state&gt;</span><br><span class="line">  &lt;parent&gt;</span><br><span class="line">    &lt;name&gt;1481514143&lt;/name&gt;</span><br><span class="line">  &lt;/parent&gt;</span><br><span class="line">  &lt;creationTime&gt;1481675543&lt;/creationTime&gt;</span><br><span class="line">  &lt;memory snapshot=<span class="string">&#x27;no&#x27;</span>/&gt;</span><br><span class="line">  &lt;disks&gt;</span><br><span class="line">    &lt;disk name=<span class="string">&#x27;vda&#x27;</span> snapshot=<span class="string">&#x27;internal&#x27;</span>/&gt;</span><br><span class="line">  &lt;/disks&gt;</span><br><span class="line">  &lt;domain <span class="built_in">type</span>=<span class="string">&#x27;kvm&#x27;</span>&gt;</span><br><span class="line">    ....</span><br><span class="line">  &lt;/domain&gt;</span><br><span class="line">&lt;/domainsnapshot&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示 snapshot XML 資訊</span></span><br><span class="line">$ virsh snapshot-dumpxml rhel7.3 --snapshotname 1481514143</span><br><span class="line">&lt;domainsnapshot&gt;</span><br><span class="line">  .... 與上面類似</span><br><span class="line">&lt;/domainsnapshot&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示 snapshot 的 parent snapshot (第一個 snapshot 沒有 parent snapshot)</span></span><br><span class="line">$ virsh snapshot-parent rhel7.3 --snapshotname 1481514143</span><br><span class="line">error: snapshot <span class="string">&#x27;1481514143&#x27;</span> has no parent</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二個 snapshot 就有 parent snapshot 了</span></span><br><span class="line">$ virsh snapshot-parent rhel7.3 --snapshotname rhel7.3_snapshot_1</span><br><span class="line">1481514143</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示每個 snapshot 的 parent</span></span><br><span class="line">$ virsh snapshot-list rhel7.3 --parent</span><br><span class="line"> Name                 Creation Time             State           Parent</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"> 1481514143           2016-12-12 11:42:23 +0800 shutoff         (null)</span><br><span class="line"> rhel7.3_snapshot_1   2016-12-14 08:32:23 +0800 shutoff         1481514143</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以樹狀結構顯示每個 snapshot 的關係</span></span><br><span class="line">$ virsh snapshot-list rhel7.3 --tree</span><br><span class="line">1481514143</span><br><span class="line">  |</span><br><span class="line">  +- rhel7.3_snapshot_1</span><br></pre></td></tr></table></figure><blockquote><p>若是在 VM 正在 running 的狀態下建立 snapshot，會需要多花一點時間，因為 VM 必須先進入到 <strong>pause</strong> 的狀態，當 snapshot 完成後才會回到 running 的狀態；而這一段時間的長短取決於 VM 當時耗用了多少記憶體，以及當時對記憶體存取的頻繁程度。</p></blockquote><p>另外還可以透過 <code>qemu-image</code> 工具來查詢 VM image 的狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢 image 的狀態</span></span><br><span class="line">$ qemu-img info /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.qcow2 </span><br><span class="line">image: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.qcow2</span><br><span class="line">file format: qcow2</span><br><span class="line">virtual size: 40G (42949672960 bytes)</span><br><span class="line">disk size: 572M</span><br><span class="line">cluster_size: 65536</span><br><span class="line">Snapshot list:</span><br><span class="line">ID        TAG                 VM SIZE                DATE       VM CLOCK</span><br><span class="line">1         1481514143                0 2016-12-12 11:42:23   00:00:00.000</span><br><span class="line">2         rhel7.3_snapshot_1        0 2016-12-14 08:32:23   00:00:00.000</span><br><span class="line">Format specific information:</span><br><span class="line">    compat: 0.10</span><br><span class="line">    refcount bits: 16</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢查 image 是否有錯誤</span></span><br><span class="line"><span class="comment"># 適合用在 running 過程中建立 snapshot 後，檢查有無錯誤發生</span></span><br><span class="line">$ qemu-img check /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.qcow2 </span><br><span class="line">No errors were found on the image.</span><br><span class="line">19467/655360 = 2.97% allocated, 93.71% fragmented, 90.63% compressed clusters</span><br><span class="line">Image end offset: 599916544</span><br></pre></td></tr></table></figure><h3 id="2-透過-snapshot-還原-VM"><a href="#2-透過-snapshot-還原-VM" class="headerlink" title="(2) 透過 snapshot 還原 VM"></a>(2) 透過 snapshot 還原 VM</h3><p>接著介紹如何透過 internal snapshot 還原 VM：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 從指定的 snapshot 還原 VM</span></span><br><span class="line">$ virsh snapshot-revert rhel7.3 --snapshotname rhel7.3_snapshot_1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 從指定的 snapshot 還原 &amp; 自動啟動 VM</span></span><br><span class="line">$ virsh snapshot-revert rhel7.3 --snapshotname rhel7.3_snapshot_1 --running</span><br></pre></td></tr></table></figure><h3 id="3-刪除-snapshot"><a href="#3-刪除-snapshot" class="headerlink" title="(3) 刪除 snapshot"></a>(3) 刪除 snapshot</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 刪除指定的 snapshot</span></span><br><span class="line">$ virsh snapshot-delete rhel7.3 1481514143</span><br><span class="line">Domain snapshot 1481514143 deleted</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示目前 snapshot 的狀態</span></span><br><span class="line">[root@ocp-kvm-host ~]<span class="comment"># virsh snapshot-list rhel7.3 --parent</span></span><br><span class="line"> Name                 Creation Time             State           Parent</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"> rhel7.3_snapshot_1   2016-12-14 08:32:23 +0800 shutoff         (null)</span><br></pre></td></tr></table></figure><h2 id="操作-External-Snapshot"><a href="#操作-External-Snapshot" class="headerlink" title="操作 External Snapshot"></a>操作 External Snapshot</h2><p>external snapshot 的原理是由 <code>overlay_image</code> &amp; <code>backing_file</code> 兩個所組成；其中 backing file 會變成 read-only，後續使用者針對 VM 的變更都會寫到 overlay_image 中。</p><p>而 external snapshot 比較有優勢的地方，在於支援各種不同的 disk image type(raw, qcow, vmdk … etc)，不僅有 qcow2 而已。</p><p>但由於目前 virsh 還不完全支援 external snapshot 的操作，這邊就先保留，等到 virsh 可以完整支援 external snapshot 之後再來補。</p><h3 id="1-建立-external-snapshot"><a href="#1-建立-external-snapshot" class="headerlink" title="(1) 建立 external snapshot"></a>(1) 建立 external snapshot</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ virsh list</span><br><span class="line"> Id    Name                           State</span><br><span class="line">----------------------------------------------------</span><br><span class="line"> 32    rhel7.3                        running</span><br><span class="line"></span><br><span class="line">$ virsh domblklist rhel7.3 --details</span><br><span class="line">Type       Device     Target     Source</span><br><span class="line">------------------------------------------------</span><br><span class="line">file       disk       vda        /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.qcow2</span><br><span class="line"></span><br><span class="line">$ virsh snapshot-create-as rhel7.3 ext_snapshot1 <span class="string">&quot;first external snapshot&quot;</span> --disk-only --atomic</span><br><span class="line">Domain snapshot ext_snapshot1 created</span><br><span class="line"></span><br><span class="line">$ virsh snapshot-list rhel7.3</span><br><span class="line">Name                 Creation Time             State</span><br><span class="line">------------------------------------------------------------</span><br><span class="line">ext_snapshot1        2016-12-31 21:00:23 +0800 disk-snapshot</span><br><span class="line"></span><br><span class="line">$ virsh snapshot-info rhel7.3 ext_snapshot1</span><br><span class="line">Name:           ext_snapshot1</span><br><span class="line">Domain:         rhel7.3</span><br><span class="line">Current:        yes</span><br><span class="line">State:          disk-snapshot</span><br><span class="line">Location:       external</span><br><span class="line">Parent:         -</span><br><span class="line">Children:       0</span><br><span class="line">Descendants:    0</span><br><span class="line">Metadata:       yes</span><br></pre></td></tr></table></figure><p>利用上面的命令就可以建立 VM external snapshot；此外，還有幾點需要注意：</p><ol><li><p>external snapshot 可以在 VM running 的狀態下建立(若是 disk I/O 頻繁的 VM，建議還是 shutdown 之後再作)</p></li><li><p><code>--disk-only</code> 表示只針對 disk 作 snapshot</p></li><li><p><code>--atomic</code> 會確保 snapshot 建立的工作執行成功後會得到一個可用的 snapshot；若是中途發生任何問題，就不會有 snapshot 的產生，也不會對原有的 VM 產生任何異動，藉此確保建立 snapshot 不會損壞原有的 VM</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視目前 VM disk 資訊</span></span><br><span class="line"><span class="comment"># 已經被 external disk 取代了!</span></span><br><span class="line">$ virsh domblklist rhel7.3 --details</span><br><span class="line">Type       Device     Target     Source</span><br><span class="line">------------------------------------------------</span><br><span class="line">file       disk       vda        /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot1</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 可看出原本的 disk 已經變成了 backing file</span></span><br><span class="line">$ qemu-img info /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot1</span><br><span class="line">image: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot1</span><br><span class="line">file format: qcow2</span><br><span class="line">virtual size: 40G (42949672960 bytes)</span><br><span class="line">disk size: 2.0M</span><br><span class="line">cluster_size: 65536</span><br><span class="line">backing file: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.qcow2</span><br><span class="line">backing file format: qcow2</span><br><span class="line">Format specific information:</span><br><span class="line">    compat: 1.1</span><br><span class="line">    lazy refcounts: <span class="literal">false</span></span><br><span class="line">    refcount bits: 16</span><br><span class="line">    corrupt: <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>從上面可看出，從此刻開始對 VM 的變更將會直接寫入 external snapshot，而原本的 disk image 變成了 backing file。</p><p>接著再建立兩個 external snapshot 試試看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ virsh snapshot-create-as rhel7.3 ext_snapshot2 <span class="string">&quot;second external snapshot&quot;</span> --disk-only --atomic</span><br><span class="line">Domain snapshot ext_snapshot2 created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 &quot;--quiesce&quot; 參數，確保連同記憶體內的資訊都一併進入到 snapshot 中</span></span><br><span class="line">$ virsh snapshot-create-as rhel7.3 ext_snapshot3 <span class="string">&quot;third external snapshot&quot;</span> --disk-only --quiesce</span><br><span class="line">Domain snapshot ext_snapshot3 created</span><br><span class="line"></span><br><span class="line">$ virsh snapshot-list rhel7.3</span><br><span class="line"> Name                 Creation Time             State</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"> ext_snapshot1        2016-12-31 21:00:23 +0800 disk-snapshot</span><br><span class="line"> ext_snapshot2        2016-12-31 22:16:42 +0800 disk-snapshot</span><br><span class="line"> ext_snapshot3        2016-12-31 22:17:53 +0800 disk-snapshot</span><br></pre></td></tr></table></figure><p>在第三個 snapshot 中使用了 <code>--quiesce</code> 參數，目的就是要讓尚未寫入 disk(記憶體中的資料) 一併進入到 snapshot 中，這樣就可以確保 snapshot 是最完整的狀態，但要使用這參數必須預先在 VM 上安裝 <code>qemu-guest agent</code> 才可以。</p><p>最後，每個 snapshot 的相依性可以使用下列命令觀察：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ qemu-img info --backing-chain /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot3 | grep backing</span><br><span class="line">backing file: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot2</span><br><span class="line">backing file format: qcow2</span><br><span class="line">backing file: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot1</span><br><span class="line">backing file format: qcow2</span><br><span class="line">backing file: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.qcow2</span><br><span class="line">backing file format: qcow2</span><br></pre></td></tr></table></figure><h3 id="2-從-external-snapshot-還原"><a href="#2-從-external-snapshot-還原" class="headerlink" title="(2) 從 external snapshot 還原"></a>(2) 從 external snapshot 還原</h3><p>external snapshot 看似不錯，但其實無法透過 virsh 指令直接還原 VM：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ virsh snapshot-revert rhel7.3 --snapshotname <span class="string">&quot;ext_snapshot3&quot;</span></span><br><span class="line">error: unsupported configuration: revert to external snapshot not supported yet</span><br></pre></td></tr></table></figure><p>但這並不代表沒辦法從 external snapshot 還原，只是要透過以下步驟來完成：(假設要還原到 <code>ext_snapshot2</code>)</p><ol><li><p>關閉 VM (<strong>這是必須的!</strong>)</p></li><li><p>檢查要還原的 external snapshot overlay image 有無損毀</p></li><li><p>若 snapshot 完整無誤，編輯 VM XML 定義檔，將 boot disk 指向 <code>ext_snapshot2</code></p></li><li><p>確認 external snapshot image 格式</p></li><li><p>從 VM XML 定義中移除原本的 disk，改成指定要還原的 <code>ext_snapshot2</code></p></li><li><p>透過 <code>domblklist</code> 參數確認 VM 使用的 disk 已經指向 ext_snapshot2</p></li><li><p>重新啟動 VM</p></li></ol><p>以下是實際操作步驟：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 關閉 VM</span></span><br><span class="line">$ virsh shutdown rhel7.3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 ext_snapshot2 的詳細路徑</span></span><br><span class="line">$ virsh snapshot-dumpxml rhel7.3 --snapshotname ext_snapshot2 | grep ext_snapshot2</span><br><span class="line">  &lt;name&gt;ext_snapshot2&lt;/name&gt;</span><br><span class="line">      &lt;<span class="built_in">source</span> file=<span class="string">&#x27;/var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot2&#x27;</span>/&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先使用 qemu-img 工具檢查 overlay image 有無損毀</span></span><br><span class="line">$ qemu-img check /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot2</span><br><span class="line">No errors were found on the image.</span><br><span class="line">11/655360 = 0.00% allocated, 36.36% fragmented, 0.00% compressed clusters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認 overlay image 格式</span></span><br><span class="line">$ qemu-img info /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot2 | grep backing</span><br><span class="line">backing file: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot1</span><br><span class="line">backing file format: qcow2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除原本的 disk</span></span><br><span class="line">$ virt-xml rhel7.3 --remove-device --disk target=vda</span><br><span class="line">Domain <span class="string">&#x27;rhel7.3&#x27;</span> defined successfully.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 換上要還原的 ext_snapshot2</span></span><br><span class="line">$ virt-xml rhel7.3 --add-device --disk /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot2,format=qcow2,bus=virtio</span><br><span class="line">Domain <span class="string">&#x27;rhel7.3&#x27;</span> defined successfully.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認目前 VM 所使用的 disk</span></span><br><span class="line">$ virsh domblklist rhel7.3</span><br><span class="line">Target     Source</span><br><span class="line">------------------------------------------------</span><br><span class="line">vda        /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新啟動 VM</span></span><br><span class="line">$ virsh start rhel7.3</span><br><span class="line">Domain rhel7.3 started</span><br></pre></td></tr></table></figure><h3 id="3-刪除-external-snapshot"><a href="#3-刪除-external-snapshot" class="headerlink" title="(3) 刪除 external snapshot"></a>(3) 刪除 external snapshot</h3><p>由於 virsh 不支援 external snapshot 的刪除，所以刪除 snapshot 就必須自己來了!</p><p>假設要移除所有的 snapshot，但又想要保留在 snapshot 上完整的變更，此時必須把 snapshot merge 到 base image 上，以下是操作步驟：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目前 disk 所使用的 snapshot</span></span><br><span class="line">$ virsh domblklist rhel7.3</span><br><span class="line">Target     Source</span><br><span class="line">------------------------------------------------</span><br><span class="line">vda        /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 snapshot overlay image 的相依關係</span></span><br><span class="line">$ qemu-img info --backing-chain /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot2 | grep backing</span><br><span class="line">backing file: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.ext_snapshot1</span><br><span class="line">backing file format: qcow2</span><br><span class="line">backing file: /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.qcow2</span><br><span class="line">backing file format: qcow2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 base image &amp; snapshot overlay image 合併!</span></span><br><span class="line">$ virsh blockcommit rhel7.3 vda --verbose --pivot --active</span><br><span class="line">Block commit: [100 %]</span><br><span class="line">Successfully pivoted</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可看出 VM disk 已經變成 base image</span></span><br><span class="line">$ virsh domblklist rhel7.3</span><br><span class="line">Target     Source</span><br><span class="line">------------------------------------------------</span><br><span class="line">vda        /var/lib/libvirt/images/rhel-guest-image-7.3-35.x86_64.qcow2</span><br><span class="line"></span><br><span class="line">$ virsh snapshot-list rhel7.3</span><br><span class="line"> Name                 Creation Time             State</span><br><span class="line">------------------------------------------------------------</span><br><span class="line"> ext_snapshot1        2016-12-31 21:00:23 +0800 disk-snapshot</span><br><span class="line"> ext_snapshot2        2016-12-31 22:16:42 +0800 disk-snapshot</span><br><span class="line"> ext_snapshot3        2016-12-31 22:17:53 +0800 disk-snapshot</span><br></pre></td></tr></table></figure><p>確認好 VM disk 已經不是指向 external snapshot 了，就可以開始進行刪除 snapshot 的動作，而刪除 external snapshot 必須從 metadata 下手，以下為操作範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定刪除從 ext_snapshot1 開始的所有 children snapshot 的 metadata &amp; image</span></span><br><span class="line">$ virsh snapshot-delete rhel7.3 ext_snapshot1 --children --metadata</span><br><span class="line">Domain snapshot ext_snapshot1 deleted</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認所有 external snapshot 都已經被刪除</span></span><br><span class="line">$ virsh snapshot-list rhel7.3</span><br><span class="line"> Name                 Creation Time             State</span><br><span class="line">------------------------------------------------------------</span><br></pre></td></tr></table></figure><h2 id="使用-snapshot-時所需的正確觀念"><a href="#使用-snapshot-時所需的正確觀念" class="headerlink" title="使用 snapshot 時所需的正確觀念"></a>使用 snapshot 時所需的正確觀念</h2><ol><li><p>不建議在 production 的環境中讓 VM 去 attach 之前做好的 snapshot 來使用</p></li><li><p>不要把 snapshot 當作是備份的方式，只是用來留下當時 VM 的狀態做後續使用而已</p></li><li><p>snapshot 不要保留太久，若確定不需要的就把 snapshot commit(for external snapshot) or 刪除</p></li><li><p>external snapshot 出現故障的機率比 internal snapshot 還低，因此建議優先使用 external snapshot</p></li><li><p>snapshot 數量要控制，太多的 snapshot 可能會倒置系統效能低落</p></li><li><p>建立 snapshot 前要先安裝 guest agent</p></li><li><p>建立 snapshot 確認都有帶上 <code>--quiesce</code> &amp;&amp; <code>--atomic</code> 兩個參數</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] VM Life Cycle 管理</title>
      <link href="/blog/KVM/KVM-Lifecycle-Management/"/>
      <url>/blog/KVM/KVM-Lifecycle-Management/</url>
      
        <content type="html"><![CDATA[<p>此篇文章將會介紹 VM 生命周期管理、Qemu Guest Agent、Virtual Video Card、Graphic Server … 等主題。</p><h1 id="VM-Lifecycle"><a href="#VM-Lifecycle" class="headerlink" title="VM Lifecycle"></a>VM Lifecycle</h1><p>在 KVM 中的 virtual machine 共會有以下幾種狀態：</p><ul><li><p><strong>Undefined</strong>：未定義</p></li><li><p><strong>Defined / Shutoff</strong>：已定義，libvirtd 已經知道有此 VM 存在，但狀態為關機中(Stopped or Shutdown)</p></li><li><p><strong>Running</strong>：執行中</p></li><li><p><strong>Shutdown</strong>：關機中</p></li><li><p><strong>Paused</strong>：暫停中，VM 記憶體中的資料暫時被保留，並且可以在 guest OS 無法知悉的狀況下回復</p></li><li><p><strong>Saved</strong>：VM 處於完全暫停的狀態，記憶體中的資料被存到一般檔案中，並存在於 persistent storage (可回復到進行 save 時的狀態)</p></li><li><p><strong>Idle</strong>：等待 I/O，或是因為沒有工作需要進行而休眠中</p></li><li><p><strong>Crashed</strong>：可能因為 QEMU process 被強制移除 or core dump 所造成的 VM 損毀</p></li><li><p><strong>Dying</strong>：在 shutdown 的過程中失敗所產生的況狀</p></li><li><p><strong>Pmsuspended</strong>：透過 guest OS 中的電源管理功能進行 suspend 後進入的狀態</p></li></ul><p>有了上面概念後，可以透過 virsh 來檢視目前 domain(virtual machine) 的狀態：</p><hr><h1 id="檢視-VM"><a href="#檢視-VM" class="headerlink" title="檢視 VM"></a>檢視 VM</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示所有 VM (包含關機中的)</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.3/system list --all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示特定狀態的 VM</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.3/system list --state-[running|paused|shutoff|]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示(未)包含 snapshot 的 VM</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.3/system list --[with|without]-snapshot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示(未)包含 managed save 狀態的 VM</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.3/system list --[with|without]-managed-save</span><br><span class="line"></span><br><span class="line"><span class="comment"># 僅顯示 uuid or name</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.3/system list --[uuid|name]</span><br></pre></td></tr></table></figure><blockquote><p>還有許多其他參數可用，使用者可透過 <code>virsh help</code> or <code>virsh help list</code> 來查詢更多使用方式</p></blockquote><hr><h1 id="操作-VM"><a href="#操作-VM" class="headerlink" title="操作 VM"></a>操作 VM</h1><p>了解 VM 有這麼多狀態之後，自然就會有相對應的操作了，virsh 提供以下幾項對 VM 的操作：</p><ul><li><p><strong>start</strong>：啟動 VM</p></li><li><p><strong>shutdown</strong>：關閉 VM (正常關機程序)</p></li><li><p><strong>reboot</strong>：重新啟動 VM</p></li><li><p><strong>reset</strong>：與 power cycle 相同效果</p></li><li><p><strong>save</strong>：將 VM 狀態儲存到檔案中，並關閉 VM</p></li><li><p><strong>restore</strong>：從指定的檔案將 VM 狀態回復為執行中</p></li><li><p><strong>suspend</strong>：暫停 VM 運作</p></li><li><p><strong>resume</strong>：回復 VM 運作</p></li><li><p><strong>destroy</strong>：直接刪除 QEMU process (類似直接拔掉電腦電源線的效果)</p></li><li><p><strong>create</strong>：使用指定的 XML 建立 VM，並啟動 VM</p></li><li><p><strong>define</strong>：使用指定的 XML 建立 VM，但不啟動 VM</p></li><li><p><strong>undefine</strong>：將 VM 從 libvird 的控制中移除</p></li></ul><hr><h1 id="QEMU-guest-agent"><a href="#QEMU-guest-agent" class="headerlink" title="QEMU guest agent"></a>QEMU guest agent</h1><p>在安裝 virtualbox or VMware 的 VM 時，安裝結束之後都會詢問要不要額外安裝 agent 在 guest VM 中，透過這個 agent，hypervisor 可以更有效率的管理每一個 VM；同樣的在 KVM 中，也是有相同的作法，稱為 <strong>QEMU guest agent</strong>。</p><p><strong>QEMU guest agent</strong> 是一個裝在 guest VM 中的套件，會以 service 的形式存在於背景，接著 service 就變成了 hypervisor &amp; guest OS 之間溝通的橋樑(channel)，hypervisor 會透過這個 channle 取得 VM 的資訊，以及對 VM 進行後續更多的操作；而兩者相互通訊的協定稱為 **Qemu Machine Protocol(QMP)**。</p><p>其中 Hypervisor 與 guest agent 是透過一個名稱為 <code>org.qemu.guest_agent.0</code> 的 virtio-serial channel 或是 isa-serial channel 來處理；而在 Hypervisor 這一端，相對應處理這些資訊交換的 socket file 則是存在於 <code>/var/lib/libvirt/qemu/channel/target</code> 目錄中，且同一個 socket file 可以同時被多個 VM 所共享，因此不會產生太多檔案。</p><p>若 guest VM 為 Linux，可安裝名稱為 <code>qemu-guest-agent</code> 的套件，若是 windows，則可以參考以下連結：</p><ul><li><p><a href="http://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers">WindowsGuestDrivers/Download Drivers - KVM</a></p></li><li><p><a href="https://fedoraproject.org/wiki/Windows_Virtio_Drivers">Windows Virtio Drivers - FedoraProject</a></p></li></ul><p>除了安裝 <code>qemu-guest-agent</code> 套件之外，還要確認 <code>qemu-guest-agent</code> service 是否有正確啟動：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl status qemu-guest-agent</span><br><span class="line">● qemu-guest-agent.service - QEMU Guest Agent</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/qemu-guest-agent.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Thu 2016-11-24 22:25:07 EST; 35min ago</span><br><span class="line">   ......</span><br></pre></td></tr></table></figure><p>確認服務沒有問題，我們就可以使用類似以下的命令在 KVM host 端取得 guest VM 的相關資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ virsh qemu-agent-command &lt;GUEST_VM_NAME&gt; <span class="string">&#x27;&#123;&quot;execute&quot;: &quot;guest-info&quot;&#125;&#x27;</span> --pretty</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;return&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;version&quot;</span>: <span class="string">&quot;2.3.0&quot;</span>,</span><br><span class="line">        <span class="string">&quot;supported_commands&quot;</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;enabled&quot;</span>: <span class="literal">true</span>,</span><br><span class="line">                <span class="string">&quot;name&quot;</span>: <span class="string">&quot;guest-get-memory-block-info&quot;</span>,</span><br><span class="line">                <span class="string">&quot;success-response&quot;</span>: <span class="literal">true</span></span><br><span class="line">            &#125;,</span><br><span class="line">.......</span><br></pre></td></tr></table></figure><blockquote><p>QMP 資料通訊使用 JSON format</p></blockquote><hr><h1 id="Virtual-Video-cards-amp-graphics"><a href="#Virtual-Video-cards-amp-graphics" class="headerlink" title="Virtual Video cards &amp; graphics"></a>Virtual Video cards &amp; graphics</h1><p>為了可以”看見” VM 的運作狀態，QEMU 需要提供兩個元素來達成這件任務：</p><ol><li><p><strong>virtual video card</strong>：讓每個 VM 都擁有一張虛擬的顯示卡</p></li><li><p>從遠端存取 VM 虛擬顯示卡的方式 or 協定</p></li></ol><h2 id="Virtual-Video-Card"><a href="#Virtual-Video-Card" class="headerlink" title="Virtual Video Card"></a>Virtual Video Card</h2><p>顯示卡的用途在於顯示圖形資料到顯示設備上，而虛擬顯示卡同樣也是為了這個目的而存在的。</p><p>然而在虛擬環境中當然沒有實體顯示卡，因此 QEMU 支援模擬以下幾種顯示卡：</p><ul><li><p><strong>Cirrus</strong>：libvirt 預設的顯示卡，可模擬 <strong>Cirrus Logic GD5446</strong> 顯示卡，Windows 95 之後的作業系統都支援這張顯示卡</p></li><li><p><strong>VGA</strong>：搭配 Bochs VBE extensions 的標準顯示卡，Windows XP 以後的作業系統可以支援(可設定 &gt;= 1280x1024x16 解析度 &amp; 大小的畫質)</p></li><li><p><strong>VMVGA</strong>：在 VGA 再更高階的虛擬顯示卡</p></li><li><p><strong>QXL</strong>：半虛擬化顯示卡，搭配在 VM 內安裝 QXL guest driver，可得到很不錯的顯示效果，而且是搭配 <strong>spice</strong> protocol 的最佳選擇</p></li></ul><p>在安裝 VM 時，libvirt 就會根據安裝的 OS，協助選擇一個最合適的顯示卡，通常安裝最近發佈的 OS，都會直接選配 QXL；若是 Windows 或是比較舊版的 Linux，可能就會使用 Cirrus。</p><h2 id="Graphics"><a href="#Graphics" class="headerlink" title="Graphics"></a>Graphics</h2><p>當 VM 有了顯示卡之後，接著需要一個可以存取圖形訊號的方法，在 KVM 中是透過 <strong>graphic server</strong> 的方式，目前提供了 <strong>VNC</strong> &amp; *<em>SPICE</em> 兩種 graphic server。</p><p>當 VM 確定好顯示卡之後，QEMU 就會啟動相對應的 Spice or VNC server，並與 VM 的顯示卡進行連接，以便讓外部的 client 可以使用圖形化的方式存取 VM。</p><h3 id="1-VNC"><a href="#1-VNC" class="headerlink" title="(1) VNC"></a>(1) VNC</h3><p>要為 VM 設定一個 VNC graphic server，可在 XML 定義檔中的 <code>&lt;device&gt;</code> section 內部加入類似以下的定義：(IP &amp; port 都可以自己定義)</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">&#x27;vnc&#x27;</span> <span class="attr">port</span>=<span class="string">&#x27;-1&#x27;</span> <span class="attr">autoport</span>=<span class="string">&#x27;yes&#x27;</span> <span class="attr">listen</span>=<span class="string">&#x27;192.168.122.1&#x27;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">listen</span> <span class="attr">type</span>=<span class="string">&#x27;address&#x27;</span> <span class="attr">address</span>=<span class="string">&#x27;192.168.122.1&#x27;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">graphics</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-SPICE"><a href="#2-SPICE" class="headerlink" title="(2) SPICE"></a>(2) SPICE</h3><p>SPICE(Simple Protocol for Independent Computing Environment) 僅有在 Linux 上支援，有以下特色：</p><ol><li><p>可以提供雙向的 audio</p></li><li><p>高效率的 2D 圖像繪製能力</p></li><li><p>可利用到 client 端的顯示卡的能力</p></li><li><p>支援加密、資料壓縮</p></li><li><p>支援透過網路的 USB passthrough</p></li></ol><p>因此若要規劃把 KVM 用在 VDI 的應用上，使用 <code>QXL</code> + <code>SPICE</code> 目前是最好的組合。</p><p>要為 VM 設定一個 SPICE graphic server，可在 XML 定義檔中的 <code>&lt;device&gt;</code> section 內部加入類似以下的定義：(IP &amp; port 都可以自己定義)</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">graphics</span> <span class="attr">type</span>=<span class="string">&#x27;spice&#x27;</span> <span class="attr">autoport</span>=<span class="string">&#x27;yes&#x27;</span> <span class="attr">listen</span>=<span class="string">&#x27;0.0.0.0&#x27;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">listen</span> <span class="attr">type</span>=<span class="string">&#x27;address&#x27;</span> <span class="attr">address</span>=<span class="string">&#x27;0.0.0.0&#x27;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">graphics</span>&gt;</span></span><br></pre></td></tr></table></figure><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Administration_Guide/sect-QEMU_Guest_Agent-Running_the_QEMU_guest_agent_on_a_Windows_guest.html">Virtualization Administration Guide &gt; Running the QEMU Guest Agent on a Windows Guest</a></p></li><li><p><a href="http://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers">WindowsGuestDrivers/Download Drivers - KVM</a></p></li><li><p><a href="https://fedoraproject.org/wiki/Windows_Virtio_Drivers">Windows Virtio Drivers - FedoraProject</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
            <tag> SPICE </tag>
            
            <tag> VNC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Git] Cheat Sheet</title>
      <link href="/blog/Git/Git-Cheat-Sheets/"/>
      <url>/blog/Git/Git-Cheat-Sheets/</url>
      
        <content type="html"><![CDATA[<h1 id="The-Basics"><a href="#The-Basics" class="headerlink" title="The Basics"></a>The Basics</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a Git repository in the current folder.</span></span><br><span class="line">$ git init</span><br><span class="line"></span><br><span class="line"><span class="comment"># View the status of each file in a repository.</span></span><br><span class="line">$ git status</span><br><span class="line"></span><br><span class="line"><span class="comment"># Stage a file for the next commit.</span></span><br><span class="line">$ git add &lt;file&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Commit the staged files with a descriptive message.</span></span><br><span class="line">$ git commit</span><br><span class="line"></span><br><span class="line"><span class="comment"># View a repository’s commit history.</span></span><br><span class="line">$ git <span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以單行的方式顯示 git log (commit log)</span></span><br><span class="line">$ git <span class="built_in">log</span> --oneline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以單行的方式顯示與指定檔案相關的 git log</span></span><br><span class="line">$ git <span class="built_in">log</span> --oneline &lt;file_name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 詳細列出所有 commit 的歷史紀錄</span></span><br><span class="line">$ git <span class="built_in">log</span> --graph --abbrev-commit --decorate --date=relative --all</span><br><span class="line">    </span><br><span class="line"><span class="comment"># Define the author name to be used in all repositories.</span></span><br><span class="line">$ git config --global user.name <span class="string">&quot;&lt;name&gt;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Define the author email to be used in all repositories. </span></span><br><span class="line">$ git config --global user.email &lt;email&gt;</span><br></pre></td></tr></table></figure><hr><h1 id="Undoing-Changes"><a href="#Undoing-Changes" class="headerlink" title="Undoing Changes"></a>Undoing Changes</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將 HEAD 指標移到指定的 commit or tag(sha1_checksum_prefix 可由 &quot;git log --oneline&quot; 取得)</span></span><br><span class="line"><span class="comment"># 若 git log 沒有加上 &quot;--all&quot; 參數，就會看不到比 HEAD 更新的 commit snapshot</span></span><br><span class="line">$ git checkout &lt;commit-id | tag-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 回到最新的 commit 紀錄(&quot;master&quot; 會固定指向最新一筆 commit snapshot)</span></span><br><span class="line">$ git checkout master</span><br><span class="line"></span><br><span class="line"><span class="comment"># 為目前最新的 commit 加上一個 annotated tag (通常 tag 用來作為正式 release 的標記用途)</span></span><br><span class="line">$ git tag -a &lt;tag-name&gt; -m <span class="string">&quot;&lt;description&gt;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出目前所有的 tag 紀錄</span></span><br><span class="line">$ git tag</span><br><span class="line"></span><br><span class="line"><span class="comment"># undo 指定的 commit (重要! 這並非是回到特定 commit 的概念)</span></span><br><span class="line"><span class="comment"># 即使 commit 被 undo，其實歷史紀錄還是永遠保留著，這是 git 的特性</span></span><br><span class="line">$ git revert &lt;commit-id&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消所有 tracked 的變更，回到最新的 commit (但不包含 untracked 的部分，例如：新增的檔案)</span></span><br><span class="line">$ git reset --hard</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 untracked files (與上一個指令搭配使用，此時應該回復到一個乾淨的 working directory，亦即為最新的 commit snapshot)</span></span><br><span class="line">$ git clean -f</span><br></pre></td></tr></table></figure><blockquote><p>需要注意 <code>git reset --hard</code> + <code>git clean -f</code> 的使用，是在 working directory 上生效，並不是在 commit snapshot 上，因此一旦 undo 之後，所有尚未 commit 的變更都會完全消失且無法追溯，請確定真的不要這些變更之後，再執行這兩個指令</p></blockquote><hr><h1 id="Branches"><a href="#Branches" class="headerlink" title="Branches"></a>Branches</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示所有的 branch (星號的部分表示目前所在的 branch)</span></span><br><span class="line">$ git branch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用目前的 working directory 作為基礎，新增 branch</span></span><br><span class="line">$ git branch &lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 HEAD 指標以及 working directory 移到指定的 branch</span></span><br><span class="line">$ git checkout &lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將指定的 branch 與目前所在的 branch(checked-out) 做合併</span></span><br><span class="line">$ git merge &lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刪除指定的 branch</span></span><br><span class="line">$ git branch -d &lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 強制刪除尚未合併的 branch (將會永遠遺失所有的檔案變更)</span></span><br><span class="line">$ git branch -D &lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 從 working directory 中移除 &amp; 停止追蹤指定的檔案</span></span><br><span class="line">$ git rm &lt;file&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改檔案名稱</span></span><br><span class="line">$ git mv &lt;old-filename&gt; &lt;new-filename&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用一行指令直接 commit 所有狀態為 tracked 的檔案，並指定 commit 內容</span></span><br><span class="line">$ git commit -a -m <span class="string">&quot;&lt;message&gt;&quot;</span></span><br></pre></td></tr></table></figure><p>以下 <strong>master</strong> branch 合併 <strong>css</strong> branch 的狀況稱為 <code>fast-forward merge</code>：</p><p><img src="http://rypress.com/tutorials/git/media/3-10.png" alt="fast-forward merge"></p><blockquote><p>因為 css branch 是從 master branch 中最新的 commit snapshot 所延伸出來，所以 master branch 合併 css branch 是不需要做什麼額外的判斷處理</p></blockquote><p>使用 branch 的基本原則：</p><ol><li><p>為每一個主要的新增功能，都使用 branch 的方式來完成</p></li><li><p>若無法為 branch 取一個實際的名稱(無法定義修改的內容為何)，就不要使用 branch</p></li></ol><p>!3-way merge](<a href="http://rypress.com/tutorials/git/media/4-1.png">http://rypress.com/tutorials/git/media/4-1.png</a>)</p><blockquote><p>3-way merge 在當要合併兩個擁有不同 commit snapshot 時會發生，此時 Git 會額外建立一個 merge commit snapshot，並同時指向兩個不同的 branch(如上圖中的 <strong>After</strong>，紅色圈圈表示這個 commit snapshot 同時來自 crazy &amp; master 兩個 branch)</p></blockquote><blockquote><p>fast-forward merge 不會在 project history 中看到，這是與 3-way merge 不同的地方</p></blockquote><hr><h1 id="Rebasing"><a href="#Rebasing" class="headerlink" title="Rebasing"></a>Rebasing</h1><p>當整個 git 專案越來越多 branch 時，可以透過 rebase 的方式來整理 branch，避免過於凌亂；此外，rebase 也有在不進行 merge 的情況下，取得最新版 master 的效果。</p><p>透過 rebase 可以把 branch 指向指定 branch 的最新 commit snapshot；舉例來說，可將多餘的 branch 重新 rebase 後指向 master 最新的 commit snapshot，如此一來後續進行 merge 時就會變成 <strong>fast-forward merge</strong>，相關的 commit snapshot 都變成了 linear history，這樣在後續檢視上會更為直覺。</p><blockquote><p>rebase 雖然可以讓整體專案的 branch 更為簡潔易讀，但在某些 rebase 操作上會移除(or 修改)某些 commit snapshot，因此後續就會無法還原當初 commit snapshot 的全貌，這也是 rebase 功能有所爭議的地方</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 branch-feature 是否有落後 branch-dev，有顯示 commit snapshot 則表示落後，可考慮進行 rebase</span></span><br><span class="line">$ git <span class="built_in">log</span> &lt;branch-feature&gt;..&lt;branch-dev&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將目前的 branch 的 root commit snapshot 指向 new-base 最新的 commit snapshot</span></span><br><span class="line">$ git rebase &lt;new-base&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以互動的方式進行 rebse 的設定，並可選擇對每個 commit snapshot 所要執行的動作</span></span><br><span class="line">$ git rebase -i &lt;new-base&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改已經存在的 commit snapshot，不產生新的 (搭配上面 rebase 編輯 commit snapshop list 時使用 edit/squash 等關鍵是)</span></span><br><span class="line">$ git commit --amend</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成修改了特定的 commit snapshot 資訊後，繼續進行 rebase 工作</span></span><br><span class="line">$ git rebase --<span class="built_in">continue</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 放棄目前的 rebase 結果並回到原先的狀態</span></span><br><span class="line">git rebase --abort</span><br><span class="line"></span><br><span class="line"><span class="comment"># 強制 git 不以 fast-forward 的方式進行 merge，藉此保留 branch 的相關資訊</span></span><br><span class="line">git merge --no-ff &lt;branch-name&gt;</span><br></pre></td></tr></table></figure><p>以下用圖說明執行 <code>git merge</code> 指令時有無加上 <code>--no-ff</code> 參數所產生的效果：</p><p><img src="https://i0.wp.com/farm6.static.flickr.com/5054/5488984566_359f74ecc2.jpg?resize=463,414&ssl=1" alt="git merge"></p><hr><h1 id="Rewriting-History"><a href="#Rewriting-History" class="headerlink" title="Rewriting History"></a>Rewriting History</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示本地端所有的 commit snapshot (以時間排序，包含已經被 reset 的 commit snapshot)</span></span><br><span class="line">$ git reflog</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 HEAD 指標往前移到第 n 的 commit snapshot，但不變更 working directory 中的內容</span></span><br><span class="line"><span class="comment"># 比 HEAD~&lt;n&gt; 還新的 commit snapshot 中的變更會保留</span></span><br><span class="line">$ git reset --mixed HEAD~&lt;n&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 HEAD 指標往前移到第 n 的 commit snapshot，但會變更 working directory 中的內容</span></span><br><span class="line"><span class="comment"># 比 HEAD~&lt;n&gt; 還新的 commit snapshot 中的變更會被移除</span></span><br><span class="line">$ git reset --hard HEAD~&lt;n&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示從 &lt;since&gt; &amp; &lt;until&gt; 兩個 branch 之間的 commit snapshot 紀錄</span></span><br><span class="line">$ git <span class="built_in">log</span> &lt;since&gt;..&lt;until&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 git log 顯示訊息中額外包含被修改的檔案資訊</span></span><br><span class="line">$ git <span class="built_in">log</span> --<span class="built_in">stat</span> </span><br></pre></td></tr></table></figure><hr><h1 id="Remotes"><a href="#Remotes" class="headerlink" title="Remotes"></a>Remotes</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 從遠端複製指定的 Git repository 回來</span></span><br><span class="line">$ git <span class="built_in">clone</span> &lt;remote-path&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出 remote repository 列表(若加上 -v 參數會顯示每個 remote repository 的詳細位址)</span></span><br><span class="line">$ git remote</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增一筆 remote repository 記錄</span></span><br><span class="line">$ git remote add &lt;remote-name&gt; &lt;remote-path&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 remote repository 的指定 branch (不進行 merge)</span></span><br><span class="line">$ git fetch &lt;remote-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># merge 指定的 remote repository branch 到目前所在的 branch</span></span><br><span class="line">$ git merge &lt;remote-name&gt;/&lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出 remote repository branch</span></span><br><span class="line">$ git branch -r</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 local repository branch 推送到 remote repository branch 進行更新</span></span><br><span class="line">$ git push &lt;remote-name&gt; &lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 為 remote repository 加上一個 tag</span></span><br><span class="line">$ git push &lt;remote-name&gt; &lt;tag-name&gt;</span><br></pre></td></tr></table></figure><hr><h1 id="Centralized-Workflows"><a href="#Centralized-Workflows" class="headerlink" title="Centralized Workflows"></a>Centralized Workflows</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 git repository，但沒有 working directory (只是用來存放檔案用)</span></span><br><span class="line">$ git init --bare &lt;repository-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 remote connection</span></span><br><span class="line">$ git remote rm &lt;remote-name&gt;</span><br></pre></td></tr></table></figure><hr><h1 id="Patch-Workflows"><a href="#Patch-Workflows" class="headerlink" title="Patch Workflows"></a>Patch Workflows</h1><p><img src="http://rypress.com/tutorials/git/media/10-2.png" alt="patch workflows"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 patch，包含了目前所在的 branch 有的，卻沒有在 &lt;branch-name&gt; branch 出現的 commit snapshot</span></span><br><span class="line">$ git format-patch &lt;branch-name&gt;</span><br><span class="line">    Create a patch <span class="keyword">for</span> each commit contained <span class="keyword">in</span> the current branch but not <span class="keyword">in</span> &lt;branch-name&gt;. You can also specify a commit ID instead of &lt;branch-name&gt;.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 套用  patch 到目前的 branch</span></span><br><span class="line">$ git am &lt; &lt;patch-file&gt;</span><br></pre></td></tr></table></figure><hr><h1 id="Tips-amp-Tricks"><a href="#Tips-amp-Tricks" class="headerlink" title="Tips &amp; Tricks"></a>Tips &amp; Tricks</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 備份目前的 branch，但只留下最新的一筆 commit snapshot</span></span><br><span class="line">$ git archive &lt;branch-name&gt; --format=zip --output=&lt;file&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整匯出指定的 branch 到指定檔案(會包含所有 commit snapshot 記錄)</span></span><br><span class="line">$ git bundle create &lt;file&gt; &lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 bundled repository 重新建立一個 repository 並 checkout 指定的 branch</span></span><br><span class="line">$ git <span class="built_in">clone</span> repo.bundle &lt;repo-dir&gt; -b &lt;branch-name&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暫時將變更隱藏，將目前的目錄變為乾淨的 working directory</span></span><br><span class="line">$ git stash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將隱藏的變更套用到 working directory</span></span><br><span class="line">$ git stash apply</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視兩個 commit snapshot 之間的差異</span></span><br><span class="line">$ git diff &lt;commit-id&gt;..&lt;commit-id&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前 working directory 與 staging area 的差異</span></span><br><span class="line">$ git diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前 staging area 與最新一筆 commit snapshot 之間的差異</span></span><br><span class="line">$ git diff --cached</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將指令檔案從 commit snapshot 中移到 staged snapshot 中</span></span><br><span class="line">$ git reset HEAD &lt;file&gt;</span><br><span class="line">    Unstage a file, but don’t alter the working directory or move the current branch.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 從指定的 commit snapshot 中取得指定檔案</span></span><br><span class="line">$ git checkout &lt;commit-id&gt; &lt;file&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 git command alias</span></span><br><span class="line">$ git config --global <span class="built_in">alias</span>.&lt;alias-name&gt; &lt;git-command&gt;</span><br></pre></td></tr></table></figure><p><strong>git reset</strong>：<br><img src="http://rypress.com/tutorials/git/media/11-1.png" alt="git reset"></p><p><strong>git checkout</strong>：<br><img src="http://rypress.com/tutorials/git/media/11-2.png" alt="git checkout"></p><hr><h1 id="Plumbing"><a href="#Plumbing" class="headerlink" title="Plumbing"></a>Plumbing</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示指定的 object 內容(其中 &lt;type&gt; 可以是 commit / tree / blob / tag)</span></span><br><span class="line">$ git cat-file &lt;<span class="built_in">type</span>&gt; &lt;object-id&gt;</span><br></pre></td></tr></table></figure><p><img src="http://rypress.com/tutorials/git/media/12-1.png" alt="commit and tree objects"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢指定的 object id 是屬於哪種 type</span></span><br><span class="line">$ git cat-file -t &lt;object-id&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出指定 tree 物件的內容</span></span><br><span class="line">$ git ls-tree &lt;tree-id&gt;</span><br><span class="line">040000 tree 5aa02e7f90df11621262d7fe91a9357bb44494aa    about</span><br><span class="line">100644 blob 4838a99c7bb4cc75941ff0a5e3a05fe4889570f9    blue.html</span><br><span class="line">.......</span><br><span class="line">100644 blob c9d942d8aadb84617d78455f8e2da25866c079a2    style.css</span><br><span class="line">100644 blob e9d1781fd949fd41d2439ae3824a293531bc38a5    yellow.html</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示指定 object id(這裡指的是 blob 物件) 的內容</span></span><br><span class="line">$ git cat-file blob e9d178</span><br></pre></td></tr></table></figure><p><img src="http://rypress.com/tutorials/git/media/12-2.png" alt="commit, tree, and blob objects"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 對 git object database 進行 garbage collection </span></span><br><span class="line">$ git gc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將指定的檔案從 working directory 移到 staged area (新增的檔案要額外加上 --add 參數)</span></span><br><span class="line">$ git update-index [--add] &lt;file&gt;</span><br><span class="line">    Stage the specified file, using the optional --add flag to denote a new untracked file.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 從目前的 index 中產生一個 tree 物件，並存入 object database 中</span></span><br><span class="line">$ git write-tree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 從指定的 tree object &amp; parent commit 中產生一個新的 commit 物件</span></span><br><span class="line">$ git commit-tree &lt;tree-id&gt; -p &lt;parent-id&gt; </span><br></pre></td></tr></table></figure><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://rypress.com/tutorials/git/index">Ry’s Git Tutorial - RyPress</a></p></li><li><p><a href="https://blog.yorkxin.org/2011/07/29/git-rebase">Git-rebase 小筆記 - Yu-Cheng Chuang’s Blog</a></p></li><li><p><a href="https://blog.wu-boy.com/2011/03/git-%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6-branch-model-%E5%88%86%E6%94%AF%E6%A8%A1%E7%B5%84%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%B4%B9/">Git 版本控制 branch model 分支模組基本介紹 | 小惡魔 - 電腦技術 - 工作筆記 - AppleBOY</a></p></li><li><p><a href="http://hungyuhei.github.io/2012/08/07/better-git-commit-graph-using-pull---rebase-and-merge---no-ff.html">洁癖者用 Git：pull –rebase 和 merge –no-ff</a></p></li><li><p><a href="https://speakerdeck.com/mouson/laradebut-number-03-cong-git-ru-men-dao-tuan-dui-he-zuo-kai-fa">Laradebut #03 從 git 入門到團隊合作開發 // Speaker Deck</a></p></li><li><p><a href="http://ithelp.ithome.com.tw/tags/articles/git?page=2">Tag 列表頁(git) - iT 邦幫忙::一起幫忙解決難題，拯救 IT 人的一天</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Git] Get Started with Git</title>
      <link href="/blog/Git/Get-Started-with-Git/"/>
      <url>/blog/Git/Get-Started-with-Git/</url>
      
        <content type="html"><![CDATA[<p>介紹開始使用 Git 前調教 config 的相關資訊</p><h1 id="初始化-Git-Repository"><a href="#初始化-Git-Repository" class="headerlink" title="初始化 Git Repository"></a>初始化 Git Repository</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化 git repository</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git init</span></span><br><span class="line">Initialized empty Git repository <span class="keyword">in</span> /root/git/git-lab/.git/</span><br></pre></td></tr></table></figure><hr><h1 id="Git-config"><a href="#Git-config" class="headerlink" title="Git config"></a>Git config</h1><h2 id="常用-Git-config-gitconfig"><a href="#常用-Git-config-gitconfig" class="headerlink" title="常用 Git config (~/.gitconfig)"></a>常用 Git config (~/.gitconfig)</h2><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[user]</span></span><br><span class="line">    <span class="attr">email</span> = godleon@gmail.com</span><br><span class="line">    <span class="attr">name</span> = godleon</span><br><span class="line"><span class="section">[core]</span></span><br><span class="line">    <span class="attr">editor</span> = vim</span><br><span class="line">    </span><br><span class="line">    <span class="attr">repositoryformatversion</span> = <span class="number">0</span></span><br><span class="line">    <span class="attr">filemode</span> = <span class="literal">true</span></span><br><span class="line">    <span class="attr">bare</span> = <span class="literal">false</span></span><br><span class="line">    <span class="attr">logallrefupdates</span> = <span class="literal">true</span></span><br><span class="line"><span class="section">[color]</span></span><br><span class="line">    <span class="attr">ui</span> = <span class="literal">true</span></span><br><span class="line"><span class="section">[alias]</span></span><br><span class="line">    <span class="attr">tree</span> = log --all --graph --decorate=short --color --format=format:<span class="string">&#x27;%C(bold blue)%h%C(reset) %C(auto)%d%C(reset)\n         %C(black)[%cr]%C(reset)  %x09%C(black)%an: %s %C(reset)&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">logs</span> = log --all --graph --decorate=short --color --format=format:<span class="string">&#x27;%C(bold blue)%h%C(reset)+%C(dim black)(%cr)%C(reset)+%C(auto)%d%C(reset)++\n+++       %C(bold black)%an%C(reset)%C(black): %s%C(reset)&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">stree</span> = !bash -c <span class="string">&#x27;&quot;                                                                             \</span></span><br><span class="line"><span class="string">    while IFS=+ read -r hash time branch message; do                                            \</span></span><br><span class="line"><span class="string">        timelength=$(echo \&quot;$time\&quot; | sed -r \&quot;s:[^ ][[]([0-9]&#123;1,2&#125;(;[0-9]&#123;1,2&#125;)?)?m::g\&quot;);     \</span></span><br><span class="line"><span class="string">        timelength=$(echo \&quot;16+$&#123;#time&#125;-$&#123;#timelength&#125;\&quot; | bc);                                 \</span></span><br><span class="line"><span class="string">        printf \&quot;%$&#123;timelength&#125;s    %s %s %s\n\&quot; \&quot;$time\&quot; \&quot;$hash\&quot; \&quot;$branch\&quot; \&quot;\&quot;;          \</span></span><br><span class="line"><span class="string">    done &lt; &lt;(git logs &amp;&amp; echo);&quot;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">logv</span> = log --all --graph --decorate=short --color --format=format:<span class="string">&#x27;%C(bold blue)%h%C(reset)+%C(dim black)(%cr)%C(reset)+%C(auto)%d%C(reset)++\n+++       %C(bold black)%an%C(reset)%C(black): %s%C(reset)&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">vtree</span> = !bash -c <span class="string">&#x27;&quot;                                                                             \</span></span><br><span class="line"><span class="string">    while IFS=+ read -r hash time branch message; do                                            \</span></span><br><span class="line"><span class="string">        timelength=$(echo \&quot;$time\&quot; | sed -r \&quot;s:[^ ][[]([0-9]&#123;1,2&#125;(;[0-9]&#123;1,2&#125;)?)?m::g\&quot;);     \</span></span><br><span class="line"><span class="string">        timelength=$(echo \&quot;16+$&#123;#time&#125;-$&#123;#timelength&#125;\&quot; | bc);                                 \</span></span><br><span class="line"><span class="string">        printf \&quot;%$&#123;timelength&#125;s    %s %s %s\n\&quot; \&quot;$time\&quot; \&quot;$hash\&quot; \&quot;$branch\&quot; \&quot;$message\&quot;;  \</span></span><br><span class="line"><span class="string">    done &lt; &lt;(git logv &amp;&amp; echo);&quot;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="Git-config-優先權"><a href="#Git-config-優先權" class="headerlink" title="Git config 優先權"></a>Git config 優先權</h2><p>Git config 的優先權如下：</p><ol><li><p><code>.git/config</code></p></li><li><p><code>~/.gitconfig</code></p></li><li><p><code>/etc/gitconfig</code></p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 &quot;.git/config&quot; 的內容</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># cat .git/config</span></span><br><span class="line">[core]</span><br><span class="line">        repositoryformatversion = 0</span><br><span class="line">        filemode = <span class="literal">true</span></span><br><span class="line">        bare = <span class="literal">false</span></span><br><span class="line">        logallrefupdates = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示目前有效的 git config</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git config -l</span></span><br><span class="line">core.repositoryformatversion=0</span><br><span class="line">core.filemode=<span class="literal">true</span></span><br><span class="line">core.bare=<span class="literal">false</span></span><br><span class="line">core.logallrefupdates=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 &#x27;/etc/gitconfig&#x27; 中的設定</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git config --system -l</span></span><br><span class="line">fatal: unable to <span class="built_in">read</span> config file <span class="string">&#x27;/etc/gitconfig&#x27;</span>: No such file or directory</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 ~/.gitconfig 中的設定</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git config --global -l</span></span><br><span class="line">fatal: unable to <span class="built_in">read</span> config file <span class="string">&#x27;/root/.gitconfig&#x27;</span>: No such file or directory</span><br></pre></td></tr></table></figure><h2 id="設定-amp-移除-git-config"><a href="#設定-amp-移除-git-config" class="headerlink" title="設定 &amp; 移除 git config"></a>設定 &amp; 移除 git config</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下的設定將會存在 ~/.gitconfig 中</span></span><br><span class="line"><span class="comment"># 若改成 &quot;--system&quot; 參數，則設定會存在 &#x27;/etc/gitconfig&#x27;</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git config --global user.name &#x27;godleon&#x27;</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git config --global user.email &#x27;godleon@gmail.com&#x27;</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git config --list</span></span><br><span class="line">user.name=godleon</span><br><span class="line">user.email=godleon@gmail.com</span><br><span class="line">core.repositoryformatversion=0</span><br><span class="line">core.filemode=<span class="literal">true</span></span><br><span class="line">core.bare=<span class="literal">false</span></span><br><span class="line">core.logallrefupdates=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 git config 設定</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git config --global --unset user.name</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git config --list</span></span><br><span class="line">user.email=godleon@gmail.com</span><br><span class="line">core.repositoryformatversion=0</span><br><span class="line">core.filemode=<span class="literal">true</span></span><br><span class="line">core.bare=<span class="literal">false</span></span><br><span class="line">core.logallrefupdates=<span class="literal">true</span></span><br></pre></td></tr></table></figure><h2 id="設定-Git-alias"><a href="#設定-Git-alias" class="headerlink" title="設定 Git alias"></a>設定 Git alias</h2><p>跟 Linux 的 alias 很像，可以自訂簡單的命令來替代複雜的指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@centos7 git-lab]<span class="comment"># git config alias.con &#x27;config -l&#x27;</span></span><br><span class="line"><span class="comment"># 此命令等同於 &quot;git config -l&quot; 的效果</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git con</span></span><br><span class="line">user.email=godleon@gmail.com</span><br><span class="line">core.repositoryformatversion=0</span><br><span class="line">core.filemode=<span class="literal">true</span></span><br><span class="line">core.bare=<span class="literal">false</span></span><br><span class="line">core.logallrefupdates=<span class="literal">true</span></span><br><span class="line">alias.con=config -l</span><br></pre></td></tr></table></figure><h2 id="設定-editor-amp-diff-tool"><a href="#設定-editor-amp-diff-tool" class="headerlink" title="設定 editor &amp; diff tool"></a>設定 editor &amp; diff tool</h2><ul><li><p><code>git config --global color.ui true</code>：加上顏色</p></li><li><p><code>git config --global core.editor vim</code>：把預設的文字編輯器改為 vim</p></li><li><p><code>git config --global diff.tool meld</code>：把預設的 diff tool 改為 meld (要先安裝 meld，且這是 GUI tool，不適用 text mode)</p></li><li><p><code>git difftool</code>：查詢檔案差異</p></li></ul><hr><h1 id="將檔案存入-Git-Repository"><a href="#將檔案存入-Git-Repository" class="headerlink" title="將檔案存入 Git Repository"></a>將檔案存入 Git Repository</h1><h2 id="gitignore"><a href="#gitignore" class="headerlink" title=".gitignore"></a>.gitignore</h2><p>Git 追蹤檔案的方式，會將檔案 &amp; 資料夾分成三類：</p><ol><li><p>untracked</p><blockquote><p>一開始所有檔案都是 untracked，執行 <code>git status</code> 可以看到 untracked 的檔案清單</p></blockquote></li><li><p>tracked</p><blockquote><p>當透過 <code>git add</code> 處理之後的檔案，狀態會變成 tracked，等待使用者進一步的 commit or cancel</p></blockquote></li><li><p>ignored</p><blockquote><p>會被 git 忽略處理的檔案，需要新增 <code>.gitignore</code> 檔案，並逐行宣告要忽略的檔案 (被設定忽略的檔案狀態不會變成 untracked)</p></blockquote></li></ol><p>以下為 <code>.gitignore</code> 設定範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可用萬用字元(# 開頭為註解)</span></span><br><span class="line">*.txt</span><br><span class="line"><span class="comment"># 不要忽略 note.txt 檔案</span></span><br><span class="line">!note.txt</span><br><span class="line"><span class="comment"># folder 資料夾 &amp; 資料夾內的檔案通通忽略</span></span><br><span class="line">folder</span><br></pre></td></tr></table></figure><h2 id="git-commit"><a href="#git-commit" class="headerlink" title="git commit"></a>git commit</h2><h3 id="1、從-tracked-復原到-untracked"><a href="#1、從-tracked-復原到-untracked" class="headerlink" title="1、從 tracked 復原到 untracked"></a>1、從 tracked 復原到 untracked</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入 file1.txt，狀態變成 tracked</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git add file1.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 從 git index 中移除，狀態變回 untracked</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git rm --cache file1.txt</span></span><br></pre></td></tr></table></figure><h3 id="2、從已經-commit-後的狀態回復原狀"><a href="#2、從已經-commit-後的狀態回復原狀" class="headerlink" title="2、從已經 commit 後的狀態回復原狀"></a>2、從已經 commit 後的狀態回復原狀</h3><p>首先先來看看如何顯示目前 commit 的狀態</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 此為自訂的 alias，請參考上方設定</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git tree</span></span><br><span class="line">* 72f32d4  (HEAD, master)</span><br><span class="line">|          [25 minutes ago]     root: commit of file1</span><br><span class="line">* fbaedbb</span><br><span class="line"></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git log --graph --oneline --decorate --date=relative --all</span></span><br><span class="line">* 72f32d4 (HEAD, master) commit of file1</span><br><span class="line">* fbaedbb First commit</span><br><span class="line"></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git log --graph --abbrev-commit --decorate --date=relative --all</span></span><br><span class="line">* commit 72f32d4 (HEAD, master)</span><br><span class="line">| Author: root &lt;godleon@gmail.com&gt;</span><br><span class="line">| Date:   23 minutes ago</span><br><span class="line">|</span><br><span class="line">|     commit of file1</span><br><span class="line">|</span><br><span class="line">* commit fbaedbb</span><br><span class="line">  Author: root &lt;godleon@gmail.com&gt;</span><br><span class="line">  Date:   9 hours ago</span><br><span class="line"></span><br><span class="line">      First commit</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消剛剛 commit 的狀態，將 file1.txt 狀態回復到 untracked 的狀態</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git reset HEAD^</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git status</span></span><br><span class="line"><span class="comment"># On branch master</span></span><br><span class="line"><span class="comment"># Untracked files:</span></span><br><span class="line"><span class="comment">#   (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#       .gitignore</span></span><br><span class="line"><span class="comment">#       file1.txt</span></span><br><span class="line"><span class="comment">#       file2.txt</span></span><br><span class="line">nothing added to commit but untracked files present (use <span class="string">&quot;git add&quot;</span> to track)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目前 HEAD 已經回到第一次  commit 的地方</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git tree</span></span><br><span class="line">* fbaedbb  (HEAD, master)</span><br></pre></td></tr></table></figure><hr><h1 id="比較檔案差異-amp-從-Git-Repository-取回檔案"><a href="#比較檔案差異-amp-從-Git-Repository-取回檔案" class="headerlink" title="比較檔案差異 &amp; 從 Git Repository 取回檔案"></a>比較檔案差異 &amp; 從 Git Repository 取回檔案</h1><h2 id="比較檔案差異"><a href="#比較檔案差異" class="headerlink" title="比較檔案差異"></a>比較檔案差異</h2><p>git 提供了很多方式來檢視不同版本間的檔案差異，主要就是使用 <code>git diff</code> &amp; <code>git difftool</code> 來完成此功能，以下用個簡單範例來說明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出目前的 commit 紀錄</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git tree</span></span><br><span class="line">* 81618ad  (HEAD, master)</span><br><span class="line">|          [2 hours ago]        godleon: 2nd commit <span class="keyword">for</span> file1.txt</span><br><span class="line">* fbaedbb</span><br><span class="line">           [2 days ago]         root: First commit</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目前 file1.txt 已經有修改</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git status</span></span><br><span class="line"><span class="comment"># On branch master</span></span><br><span class="line"><span class="comment"># Changes not staged for commit:</span></span><br><span class="line"><span class="comment">#   (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span></span><br><span class="line"><span class="comment">#   (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#       modified:   file1.txt</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Untracked files:</span></span><br><span class="line"><span class="comment">#   (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#       .gitignore</span></span><br><span class="line"><span class="comment">#       file2.txt</span></span><br><span class="line">no changes added to commit (use <span class="string">&quot;git add&quot;</span> and/or <span class="string">&quot;git commit -a&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比較本地端檔案 &amp; git repository 的差異</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git diff</span></span><br><span class="line">diff --git a/file1.txt b/file1.txt</span><br><span class="line">index 7cca94a..3aff906 100644</span><br><span class="line">--- a/file1.txt</span><br><span class="line">+++ b/file1.txt</span><br><span class="line">@@ -1,2 +1,3 @@</span><br><span class="line"> file1, line 1</span><br><span class="line"> file1, line2</span><br><span class="line">+file1, line3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將檔案 file1.txt 移到 staging 中</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git add file1.txt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比較 staging 與 HEAD 的版本差異</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git diff --cached file1.txt</span></span><br><span class="line">diff --git a/file1.txt b/file1.txt</span><br><span class="line">index 7cca94a..3aff906 100644</span><br><span class="line">--- a/file1.txt</span><br><span class="line">+++ b/file1.txt</span><br><span class="line">@@ -1,2 +1,3 @@</span><br><span class="line"> file1, line 1</span><br><span class="line"> file1, line2</span><br><span class="line">+file1, line3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比較不同 commit 之間指定檔案的差異</span></span><br><span class="line">[root@centos7 git-lab]<span class="comment"># git diff HEAD HEAD~1 file1.txt</span></span><br><span class="line">diff --git a/file1.txt b/file1.txt</span><br><span class="line">deleted file mode 100644</span><br><span class="line">index 7cca94a..0000000</span><br><span class="line">--- a/file1.txt</span><br><span class="line">+++ /dev/null</span><br><span class="line">@@ -1,2 +0,0 @@</span><br><span class="line">-file1, line 1</span><br><span class="line">-file1, line2</span><br></pre></td></tr></table></figure><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://github.com/geeeeeeeeek/git-recipes/wiki">git-recipes</a></p></li><li><p><a href="http://www-cs-students.stanford.edu/%7Eblynn/gitmagic/intl/zh_tw/index.html">Git Magic</a></p></li><li><p><a href="http://www.cnblogs.com/fanfan259/p/4810517.html">我所记录的git命令（非常实用） - 糖糖果 - 博客园</a></p></li><li><p><a href="https://ihower.tw/blog/archives/2622">Git 版本控制系統(3) 還沒 push 前可以做的事 | ihower { blogging }</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] libvirt &amp; Storage</title>
      <link href="/blog/KVM/KVM-libvirt-storage/"/>
      <url>/blog/KVM/KVM-libvirt-storage/</url>
      
        <content type="html"><![CDATA[<h1 id="建立-amp-使用-unmanaged-storage"><a href="#建立-amp-使用-unmanaged-storage" class="headerlink" title="建立 &amp; 使用 unmanaged storage"></a>建立 &amp; 使用 unmanaged storage</h1><p>建立 unmanaged storage 是幫 VM 增加 virtual disk 最快的方式，其中有兩種作法：</p><ol><li><p><strong>preallocated</strong>：效能好，但完全佔據磁碟空間</p></li><li><p><strong>thin-provisioned</strong>：效能較差，但僅佔據實際使用到的磁碟空間</p></li></ol><p>以下是兩種不同 image 的建立示範：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 產生 preallocated image</span></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=/tmp/dbvm_disk1.img bs=1G count=10</span><br><span class="line">10+0 records <span class="keyword">in</span></span><br><span class="line">10+0 records out</span><br><span class="line">10737418240 bytes (11 GB) copied, 8.32924 s, 1.3 GB/s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 產生 thin-provisioned image</span></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=/tmp/dbvm_disk1_seek.img bs=1G seek=10 count=0</span><br><span class="line">0+0 records <span class="keyword">in</span></span><br><span class="line">0+0 records out</span><br><span class="line">0 bytes (0 B) copied, 0.000307303 s, 0.0 kB/s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 image 資訊 (preallocated image 已經完全佔據磁碟空間)</span></span><br><span class="line">$ qemu-img info /tmp/dbvm_disk1.img </span><br><span class="line">image: /tmp/dbvm_disk1.img</span><br><span class="line">file format: raw</span><br><span class="line">virtual size: 10G (10737418240 bytes)</span><br><span class="line">disk size: 10G</span><br><span class="line"><span class="comment"># 查詢 image 資訊 (thin-provisioned image 並未預先佔據磁碟空間)</span></span><br><span class="line">$ qemu-img info /tmp/dbvm_disk1_seek.img </span><br><span class="line">image: /tmp/dbvm_disk1_seek.img</span><br><span class="line">file format: raw</span><br><span class="line">virtual size: 10G (10737418240 bytes)</span><br><span class="line">disk size: 0</span><br></pre></td></tr></table></figure><p>當 image 建立完成，可用以下指令直接掛載到執行中的 VM：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ virsh domblklist centos7</span><br><span class="line">Target     Source</span><br><span class="line">------------------------------------------------</span><br><span class="line">vda        /var/lib/libvirt/images/hdd/vmdisk/centos7.qcow2</span><br><span class="line"></span><br><span class="line"><span class="comment"># vdb =&gt; 指定 image 掛載為 vdb</span></span><br><span class="line"><span class="comment"># --live =&gt; 指定在執行中的 VM 掛載 image</span></span><br><span class="line"><span class="comment"># --config =&gt; 讓此掛載設定可以永久保留，不會因為 VM reboot 而消失</span></span><br><span class="line">$ virsh attach-disk centos7 /tmp/dbvm_disk1.img vdb --live --config</span><br><span class="line">Disk attached successfully</span><br><span class="line"></span><br><span class="line">$ virsh domblklist centos7</span><br><span class="line">Target     Source- [KVM XML 設定檔基本內容](http://www.tfcis.org/~lantw44/download/notes/cs/libvirt.domain(5))</span><br><span class="line">------------------------------------------------</span><br><span class="line">vda        /var/lib/libvirt/images/hdd/vmdisk/centos7.qcow2</span><br><span class="line">vdb        /tmp/dbvm_disk1.img</span><br></pre></td></tr></table></figure><hr><h1 id="建立-amp-使用-managed-storage"><a href="#建立-amp-使用-managed-storage" class="headerlink" title="建立 &amp; 使用 managed storage"></a>建立 &amp; 使用 managed storage</h1><p>為了讓 storage 有個統一標準的管理，除非是很臨時的測試目的需求，不然上面 unmanage 的作法就盡量少作囉!</p><p>libvirt 支援了相當多種的 storage pool，以下一一列出：</p><ul><li><p><code>-dir</code>：使用<strong>標準的檔案系統目錄</strong>儲存 virtual disk</p></li><li><p><code>-disk</code>：使用<strong>實體磁碟機</strong>來建立 virtual disk</p></li><li><p><code>-fs</code>：使用<strong>預設格式化好的磁碟分割</strong>來儲存 virtual disk</p></li><li><p><code>-netfs</code>：使用 <strong>network-shared storage</strong>(例如：NFS) 來儲存 virtual disk</p></li><li><p><code>-gluster</code>：使用 <strong>glusterfs</strong> 儲存 virtual disk</p></li><li><p><code>-iscsi</code>：使用 <strong>iscsi storage</strong> 儲存 virtual disk</p></li><li><p><code>-scsi</code>：使用 <strong>本地端 scsi storage</strong> 儲存 virtual disk </p></li><li><p><code>-lvm</code>：使用 <strong>LVM volume group</strong> 儲存 virtual disk</p></li><li><p><code>-rbd</code>：使用 <strong>Ceph storage</strong> 儲存 virtual disk </p></li></ul><blockquote><p>在 libvirt 中，managed storage 是以 <code>pool</code> + <code>volume</code> 所組合而成</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢 pool 列表</span></span><br><span class="line">$ $ virsh -c qemu+ssh://root@10.20.190.2/system pool-list</span><br><span class="line"> Name                 State      Autostart </span><br><span class="line">-------------------------------------------</span><br><span class="line"> default              active     yes       </span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 pool 的詳細資訊</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-info default</span><br><span class="line">Name:           default</span><br><span class="line">UUID:           0896bbcd-a502-4ed8-b484-34d8baf05e84</span><br><span class="line">State:          running</span><br><span class="line">Persistent:     yes</span><br><span class="line">Autostart:      yes</span><br><span class="line">Capacity:       1007.80 GiB</span><br><span class="line">Allocation:     70.07 GiB</span><br><span class="line">Available:      937.73 GiB</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">這些資訊會以 XML 的形式存在於 KVM host 上，以 `default` 為例，其 XML 定義檔的位置為 `/etc/libvirt/storage/default.xml`。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">--------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">管理 Storage Pool</span><br><span class="line">=================</span><br><span class="line"></span><br><span class="line">以下使用 filesystem &amp; LVM 作為建立 storage pool 的範例：</span><br><span class="line"></span><br><span class="line"><span class="comment">## (1) 建立 fife system directory backed storage pool</span></span><br><span class="line"></span><br><span class="line">這是上述 `defautl` pool 的方式，使用的是 KVM host 上的 `/var/lib/libvirt/images` 資料夾作為儲存 volume 的位置</span><br><span class="line"></span><br><span class="line">以下是建立一個名稱為 **dedicated_storage** 的簡單方式：</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line"><span class="comment"># 定義 storage pool (會產生 XML 定義檔案在 /etc/libvirt/storage 目錄中)</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-define-as dedicated_storage dir - - - - <span class="string">&quot;/vms&quot;</span></span><br><span class="line">Pool dedicated_storage defined</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 storage pool (建立指定目錄 &amp; 設定 SELinux 相關權限)</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-build dedicated_storage</span><br><span class="line">Pool dedicated_storage built</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 storage pool</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-start dedicated_storage</span><br><span class="line">Pool dedicated_storage started</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 libvirtd 啟動時，同時啟動此 storage pool</span></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-autostart dedicated_storage</span><br><span class="line">Pool dedicated_storage marked as autostarted</span><br></pre></td></tr></table></figure><h2 id="2-建立-LVM-volume-Group-backed-storage-pool"><a href="#2-建立-LVM-volume-Group-backed-storage-pool" class="headerlink" title="(2) 建立 LVM volume Group backed storage pool"></a>(2) 建立 LVM volume Group backed storage pool</h2><p>使用 LVM 的優點就會有以下優點啦：</p><ol><li><p>彈性伸縮磁碟容量</p></li><li><p>整合不同的磁碟</p></li><li><p>volume snapshots</p></li><li><p>自定義的裝置名稱</p></li><li><p>data striping 提昇 I/O throughput</p></li><li><p>Mirror volumes</p></li></ol><p>所以使用 LVM 作為 storage pool 也是個相當不錯的選項。</p><p>假設目前在 KVM host 中有 <strong>/dev/sdb</strong> &amp; <strong>/dev/sdc</strong> 兩個硬碟可拿來作為 LVM volume，可用以下指令建立 LVM storage pool：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ bash -c <span class="string">&#x27;cat &lt;&lt;EOF &gt; /tmp/storage-lvm.xml</span></span><br><span class="line"><span class="string">&lt;pool type=&quot;logical&quot;&gt;</span></span><br><span class="line"><span class="string">  &lt;name&gt;HostVG&lt;/name&gt;</span></span><br><span class="line"><span class="string">  &lt;source&gt;</span></span><br><span class="line"><span class="string">    &lt;device path=&quot;/dev/sdb&quot;/&gt;</span></span><br><span class="line"><span class="string">    &lt;device path=&quot;/dev/sdc&quot;/&gt;</span></span><br><span class="line"><span class="string">  &lt;/source&gt;</span></span><br><span class="line"><span class="string">  &lt;target&gt;</span></span><br><span class="line"><span class="string">    &lt;path&gt;/dev/HostVG&lt;/path&gt;</span></span><br><span class="line"><span class="string">  &lt;/target&gt;</span></span><br><span class="line"><span class="string">&lt;/pool&gt;</span></span><br><span class="line"><span class="string">EOF&#x27;</span></span><br><span class="line"></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-define /tmp/storage-lvm.xml </span><br><span class="line">Pool HostVG defined from /tmp/storage-lvm.xml</span><br><span class="line"></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-build HostVG</span><br><span class="line">Pool HostVG built</span><br><span class="line"></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-start HostVG</span><br><span class="line">Pool HostVG started</span><br><span class="line"></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-autostart HostVG</span><br><span class="line">Pool HostVG marked as autostarted</span><br></pre></td></tr></table></figure><h2 id="3-刪除-storage-pool"><a href="#3-刪除-storage-pool" class="headerlink" title="(3) 刪除 storage pool"></a>(3) 刪除 storage pool</h2><p>刪除 storage pool 就相對簡單，假設要刪除上面的 <strong>HostVG</strong> pool，只要透過以下指令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-destroy HostVG</span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system pool-undefine HostVG</span><br></pre></td></tr></table></figure><hr><h1 id="Storage-Volume-的管理"><a href="#Storage-Volume-的管理" class="headerlink" title="Storage Volume 的管理"></a>Storage Volume 的管理</h1><p>透過 virsh 建立 storage volume 的語法類似如下：</p><blockquote><p>virsh vol-create-as –pool POOL_NAME VOL_NAME VOL_SIZE –format raw|qcow2|qed </p></blockquote><p>因此假設我們要在 pool <strong>dedicated_storage</strong> 中建立一個格式為 <strong>qcow2</strong>，大小為 <strong>10G</strong> 的 volume，可用下列語法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立一個名稱為 vm_vol1.qcow2 的 storage volume</span></span><br><span class="line">$ $ virsh -c qemu+ssh://root@10.20.190.2/system vol-create-as --pool dedicated_storage vm_vol1.qcow2 10G --format qcow2</span><br><span class="line">Vol vm_vol1.qcow2 created</span><br><span class="line"></span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system vol-info --pool dedicated_storage vm_vol1.qcow2</span><br><span class="line">Name:           vm_vol1.qcow2</span><br><span class="line">Type:           file</span><br><span class="line">Capacity:       10.00 GiB</span><br><span class="line">Allocation:     196.00 KiB</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">若要刪除 storage volume，則可用以下指令</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">$ virsh -c qemu+ssh://root@10.20.190.2/system vol-delete --pool dedicated_storage vm_vol1.qcow2</span><br><span class="line">Vol vm_vol1.qcow2 deleted</span><br></pre></td></tr></table></figure><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://beakdoosan.blogspot.tw/2011/01/lvm.html">Beakdoosan’s Weblog: LVM 筆記 - 觀念篇</a></p></li><li><p><a href="https://libvirt.org/storage.html">libvirt: Storage Management</a></p></li><li><p><a href="https://www.suse.com/documentation/sles11/book_kvm/data/sec_libvirt_storage_virsh.html">Suse Doc: Virtualization with KVM - Managing Storage with virsh</a></p></li><li><p><a href="http://docs.ceph.com/docs/hammer/rbd/libvirt/">Using libvirt with Ceph RBD — Ceph Documentation</a></p></li><li><p><a href="http://www.tfcis.org/~lantw44/download/notes/cs/libvirt.domain(5)">KVM XML 設定檔基本內容</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
            <tag> libvirt </tag>
            
            <tag> Storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] libvirt &amp; network</title>
      <link href="/blog/KVM/KVM-libvirt-network/"/>
      <url>/blog/KVM/KVM-libvirt-network/</url>
      
        <content type="html"><![CDATA[<p>介紹如何使用 libvirt 來管理 KVM virtual network</p><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><p>以下圖片來說明 libvirt 在整個虛擬化架構中所扮演的角色：</p><p><img src="http://smilejay.com/wp-content/uploads/2013/03/libvirt-manage-hypervisors.jpg" alt="libvirt 1"></p><blockquote><p>其中 other tools 的部份，甚至可以是 <strong>OpenStack</strong>、<strong>oVirt</strong> … 等工具</p></blockquote><p>libvirt 的管理功能一共包含五個部分：</p><ol><li><p>Virtual Machine</p><blockquote><p>包含 VM lifecycle(啟動/停止/暫停/保存/恢復/Live Migration …. 等等) 的管理，也支援對各種設備的熱插拔(不同的 hypervisor 對熱插拔的支援程度不一)</p></blockquote></li><li><p>Remote Node</p><blockquote><p>只要 remote node 上執行了 libvirtd 服務，libvirt 就可以用遠端的方式進行管理，支援 SSH / TCP socket … 等不同的連線方式，以 SSH 為例，可用像是 <code>virsh -c qemu+ssh://root@remotehost.com/system</code> 的命令進行連線</p></blockquote></li><li><p>Storage</p><blockquote><p>與上面相同，只要 remote node 上執行了 libvirtd 服務，就可以透過 libvirt 來管理不同類型的 storage，例如：建立不同格式的 virtual machine image(qcow2 / raw / vmdk … 等等)、掛載 remote NFS/iSCSI share、磁碟分割 …. 等等</p></blockquote></li><li><p>Network</p><blockquote><p>與上面相同，只要 remote node 上執行了 libvirtd 服務，就可以透過 libvirt 進行像是配置 tap device、建立 virtual network tap、bridge device 設定、VLAN/NAT 管理…等功能</p></blockquote></li><li><p>提供穩定、高效 API interface</p></li></ol><p>更清楚一點解釋 libvirt 在整個 QEMU/KVM 虛擬環境中所扮演的角色，可參考下圖：</p><p><img src="https://github.com/godleon/godleon.github.io/blob/master/_posts/images/2016/KVM-libvirt/libvirt.png?raw=true" alt="libvirt"></p><hr><h1 id="遠端操作-libvirtd"><a href="#遠端操作-libvirtd" class="headerlink" title="遠端操作 libvirtd"></a>遠端操作 libvirtd</h1><p>以下用個很簡單的範例，透過 virsh 命令連線到遠端主機的 libvirtd 並執行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 遠端主機為 10.20.190.3</span></span><br><span class="line"><span class="comment"># 執行命令 &quot;list --all&quot;</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system list --all</span><br><span class="line"> Id    Name                           State</span><br><span class="line">----------------------------------------------------</span><br><span class="line"> 3     centos7                        running</span><br></pre></td></tr></table></figure><p>用圖來表示的話，遠端操作 libvirtd 的流程如下：</p><p><img src="https://github.com/godleon/godleon.github.io/blob/master/_posts/images/2016/KVM-libvirt/remote_access_libvirtd.png?raw=true" alt="Remote Accrss libvirtd"></p><hr><h1 id="Linux-Virtual-Networking"><a href="#Linux-Virtual-Networking" class="headerlink" title="Linux Virtual Networking"></a>Linux Virtual Networking</h1><p>libvirt networking 的主要元件是 virtual network switch，也就是 Linux 中的 **<font color='red'>bridge devices</font>**，而連接在 bridge 上的 interface，我們稱為 **<font color='red'>TAP devices</font>**。</p><blockquote><p>TAP device 是由 Linux 的 TUN/TAP 模組所實作出來，其中 TUN(tunnel) 用來模擬 layer 3 的設備，而 TAP(network tap) 則是用來模擬 layer 2 設備</p></blockquote><p>以下示範如何建立 bridge device &amp; TAP device：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢是否已經掛載 bridge 模組</span></span><br><span class="line">$ lsmod | grep bridge</span><br><span class="line">bridge                119562  0</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增 bridge device</span></span><br><span class="line">$ brctl addbr tester</span><br><span class="line">$ brctl show</span><br><span class="line">bridge name      bridge id               STP enabled    interfaces</span><br><span class="line">.......</span><br><span class="line">tester            8000.000000000000     no        </span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢查 TUN/TAP 模組是否已經載入到 kernel 中</span></span><br><span class="line">$ lsmod | grep tun</span><br><span class="line">tun                    27141  4 vhost_net</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 TAP device(vm-vnic)</span></span><br><span class="line">$ ip tuntap add dev vm-vnic mode tap</span><br><span class="line">$ ip addr show vm-vnic</span><br><span class="line">21: vm-vnic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN qlen 500</span><br><span class="line">    link/ether 92:ee:17:b7:2b:c4 brd ff:ff:ff:ff:ff:ff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 TAP device 與 bridge 相連</span></span><br><span class="line">$ brctl addif tester vm-vnic</span><br><span class="line">$ brctl show</span><br><span class="line">bridge name    bridge id               STP enabled     interfaces</span><br><span class="line">.......</span><br><span class="line">tester          8000.92ee17b72bc4     no                 vm-vnic</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前連接到 bridge 的 TAP devices</span></span><br><span class="line">$ brctl showmacs tester</span><br><span class="line">port no    mac addr        is <span class="built_in">local</span>?    ageing timer</span><br><span class="line">  1    92:ee:17:b7:2b:c4    yes           0.00</span><br><span class="line">  1    92:ee:17:b7:2b:c4    yes           0.00</span><br></pre></td></tr></table></figure><p>若以上的 vm-vnic 連接到 VM 後，整個 network topology 會變成類似如下圖：</p><p><img src="https://github.com/godleon/godleon.github.io/blob/master/_posts/images/2016/KVM-Virtual-Networking/Linux_bridge_taps.png?raw=true" alt="Linux Bridge &amp; TAP devices"></p><p>測試完畢後，我們可以用以下指令移除上面建立的 TAP device &amp; bridge：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ brctl delif tester vm-vnic</span><br><span class="line"></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name    bridge id               STP enabled    interfaces</span><br><span class="line">........</span><br><span class="line">tester          8000.000000000000     no        </span><br><span class="line"></span><br><span class="line">$ ip tuntap del dev vm-vnic mode tap</span><br><span class="line">$ brctl delbr tester</span><br></pre></td></tr></table></figure><hr><h1 id="Virtual-Network-Types"><a href="#Virtual-Network-Types" class="headerlink" title="Virtual Network Types"></a>Virtual Network Types</h1><p>在 KVM 中，virtual network 分為以下幾種類型：</p><ul><li><p><strong>NATed</strong>：以 host 作為 NAT server 的方式提供網路</p></li><li><p><strong>Routed</strong>：透過設定在 hypervisor 上的 routing rules，允許 VM 與實體網路卡相連進行資料傳輸</p></li><li><p><strong>Isolated</strong>：與外界完全區隔的內部私有網路，只有在裡面的 VM 可以互相通訊</p></li></ul><p>透過以下指令可以查詢 virtual network 相關的資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ virsh -c qemu+ssh://root@10.20.190.3/system net-list --all</span><br><span class="line"> Name                 State      Autostart     Persistent</span><br><span class="line">----------------------------------------------------------</span><br><span class="line"> default              active     yes           yes</span><br><span class="line"></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system net-info default</span><br><span class="line">Name:           default  <span class="comment"># virtual network 名稱</span></span><br><span class="line">UUID:           83bea138-4c65-4ca2-8199-db3400d87fa5</span><br><span class="line">Active:         yes      <span class="comment"># 目前的啟動狀態</span></span><br><span class="line">Persistent:     yes      </span><br><span class="line">Autostart:      yes      <span class="comment"># 是否隨著 libvird 一起啟動</span></span><br><span class="line">Bridge:         virbr0   <span class="comment"># 使用那個 device 進行對外通訊 </span></span><br><span class="line"></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system net-dumpxml default</span><br><span class="line">&lt;network&gt;</span><br><span class="line">  &lt;name&gt;default&lt;/name&gt;</span><br><span class="line">  &lt;uuid&gt;83bea138-4c65-4ca2-8199-db3400d87fa5&lt;/uuid&gt;</span><br><span class="line">  &lt;forward mode=<span class="string">&#x27;nat&#x27;</span>&gt;</span><br><span class="line">    &lt;nat&gt;</span><br><span class="line">      &lt;port start=<span class="string">&#x27;1024&#x27;</span> end=<span class="string">&#x27;65535&#x27;</span>/&gt;</span><br><span class="line">    &lt;/nat&gt;</span><br><span class="line">  &lt;/forward&gt;</span><br><span class="line">  &lt;bridge name=<span class="string">&#x27;virbr0&#x27;</span> stp=<span class="string">&#x27;on&#x27;</span> delay=<span class="string">&#x27;0&#x27;</span>/&gt;</span><br><span class="line">  &lt;mac address=<span class="string">&#x27;52:54:00:f8:50:f2&#x27;</span>/&gt;</span><br><span class="line">  &lt;ip address=<span class="string">&#x27;192.168.122.1&#x27;</span> netmask=<span class="string">&#x27;255.255.255.0&#x27;</span>&gt;</span><br><span class="line">    &lt;dhcp&gt;</span><br><span class="line">      &lt;range start=<span class="string">&#x27;192.168.122.2&#x27;</span> end=<span class="string">&#x27;192.168.122.254&#x27;</span>/&gt;</span><br><span class="line">    &lt;/dhcp&gt;</span><br><span class="line">  &lt;/ip&gt;</span><br><span class="line">&lt;/network&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 default virtual network</span></span><br><span class="line">$ virsh net-start default</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刪除 default virtual network</span></span><br><span class="line">$ virsh net-destroy default</span><br></pre></td></tr></table></figure><p>系統預設的 <strong><font color='red'>default</font></strong> virtual network 屬於 NATed，host 會自動派發 IP 給此網路中 VM，並設定相關的 firewall rule 讓 VM 可以透過 host 對外通訊。</p><hr><h1 id="使用-libvirt-管理-virtual-network-Isolated"><a href="#使用-libvirt-管理-virtual-network-Isolated" class="headerlink" title="使用 libvirt 管理 virtual network - Isolated"></a>使用 libvirt 管理 virtual network - Isolated</h1><p>顧名思義，isolated network 表示外面是無法與在 isolated network 中的 VM 進行通訊的，只有加入到此 network 的 VM 才可以相互通訊，而在 isolated network 內部的 VM 也無法對外連線。</p><p><img src="https://github.com/godleon/godleon.github.io/blob/master/_posts/images/2016/KVM-libvirt/kvm_isolated-network.jpg?raw=true" alt="KVM isolated network"></p><p>建立 isolated network 的方式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ bash -c <span class="string">&#x27;cat &lt;&lt;EOF &gt; /tmp/isolated.xml</span></span><br><span class="line"><span class="string">&lt;network&gt; </span></span><br><span class="line"><span class="string">  &lt;name&gt;isolated&lt;/name&gt;</span></span><br><span class="line"><span class="string">&lt;/network&gt;</span></span><br><span class="line"><span class="string">EOF&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義 virtual network </span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system net-define /tmp/isolated.xml </span><br><span class="line">Network isolated defined from /tmp/isolated.xml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前的 virtual network 清單 (isolated 已經加入，但尚未啟動)</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system net-list --all</span><br><span class="line"> Name                 State      Autostart     Persistent</span><br><span class="line">----------------------------------------------------------</span><br><span class="line"> default              active     yes           yes</span><br><span class="line"> isolated             inactive   no            yes</span><br><span class="line"></span><br><span class="line"><span class="comment"># libvirt 會自動指派 UUID, virtual bridge ...等資訊</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system net-dumpxml isolated</span><br><span class="line">&lt;network&gt;</span><br><span class="line">  &lt;name&gt;isolated&lt;/name&gt;</span><br><span class="line">  &lt;uuid&gt;765e746b-334d-4acf-8b9c-cb32be5b77f5&lt;/uuid&gt;</span><br><span class="line">  &lt;bridge name=<span class="string">&#x27;virbr1&#x27;</span> stp=<span class="string">&#x27;on&#x27;</span> delay=<span class="string">&#x27;0&#x27;</span>/&gt;</span><br><span class="line">  &lt;mac address=<span class="string">&#x27;52:54:00:70:8b:87&#x27;</span>/&gt;</span><br><span class="line">&lt;/network&gt;</span><br></pre></td></tr></table></figure><blockquote><p>定義一個新的 virtual network 後，會在 KVM host 上的 <code>/etc/libvirt/qemu/networks</code> 目錄產生對應的 XML 設定檔，以上面為例，XML 設定檔完整名稱是 <code>/etc/libvirt/qemu/networks/isolated.xml</code></p></blockquote><p>完成 virtual network define 之後，接著要啟動 virtual network：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system net-start isolated</span><br><span class="line">Network isolated started</span><br><span class="line"></span><br><span class="line"><span class="comment"># network isolated 已經啟動</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system net-list --all</span><br><span class="line"> Name                 State      Autostart     Persistent</span><br><span class="line">----------------------------------------------------------</span><br><span class="line"> default              active     yes           yes</span><br><span class="line"> isolated             active     no            yes</span><br></pre></td></tr></table></figure><p>完成了 isolated virtual network 的配置後，以下是動態產生 &amp; 刪除 NIC 並與 VM 相連的相關操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 domain interface list (目前只有連接 default network)</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system domiflist centos7</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet0      network    default    virtio      52:54:00:7e:f8:5c</span><br><span class="line"></span><br><span class="line"><span class="comment"># attach-interface =&gt; 產生一個 interface 並連接到後續指定的 virtual network</span></span><br><span class="line"><span class="comment"># --domain =&gt; 指定要變更設定的 VM (稱為 domain)</span></span><br><span class="line"><span class="comment"># --source isolated --type network =&gt; 使用 virtual network isolated</span></span><br><span class="line"><span class="comment"># --model virtio =&gt; 使用 virtio (效能較好)</span></span><br><span class="line"><span class="comment"># --config =&gt; 變更設定 &amp; 儲存 (若沒使用此參數，NIC 僅會暫時出現)</span></span><br><span class="line"><span class="comment"># --live =&gt; 在 VM 執行狀態下動態新增 (若要針對停止中的 VM 新增 NIC 不需要使用此參數)</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system attach-interface --domain centos7 --<span class="built_in">source</span> isolated --<span class="built_in">type</span> network --model virtio --config --live</span><br><span class="line">Interface attached successfully</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新檢視 VM 的 interface list，發現新的 NIC 已經安裝上去了</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system domiflist centos7</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet0      network    default    virtio      52:54:00:7e:f8:5c</span><br><span class="line">vnet1      network    isolated   virtio      52:54:00:07:b8:72</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除指定的 NIC</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system detach-interface --domain centos7 --<span class="built_in">type</span> network --mac 52:54:00:07:b8:72 --config --live</span><br><span class="line">Interface detached successfully</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可看出其中一的 NIC 已經被移除</span></span><br><span class="line">$ virsh --connect qemu+ssh://root@10.20.190.3/system domiflist centos7</span><br><span class="line">Interface  Type       Source     Model       MAC</span><br><span class="line">-------------------------------------------------------</span><br><span class="line">vnet0      network    default    virtio      52:54:00:7e:f8:5c</span><br></pre></td></tr></table></figure><p>當透過 virsh 新增一個 NIC 與 VM 相連後，直接到 KVM host 上使用 <code>brctl show</code> 指令檢視目前與 bridge device 相連的 interface：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ brctl show</span><br><span class="line">bridge name    bridge id              STP enabled    interfaces</span><br><span class="line">......</span><br><span class="line">virbr1          8000.525400708b87    yes              virbr1-nic</span><br><span class="line">                                                        vnet1</span><br></pre></td></tr></table></figure><p>可以看出 interface 已經被 libvirt 自動建立且與 bridge 相連。(其中的 <strong>virbr1-nic</strong> 是在 isolated virtual network 建立時，libvirt 一併同時產生的)</p><hr><h1 id="virtual-network-NATed-amp-Routed"><a href="#virtual-network-NATed-amp-Routed" class="headerlink" title="virtual network - NATed &amp; Routed"></a>virtual network - NATed &amp; Routed</h1><h2 id="NATed-virtual-network"><a href="#NATed-virtual-network" class="headerlink" title="NATed virtual network"></a>NATed virtual network</h2><p>安裝好 KVM 後，預設就會附上一個名稱為 <code>default</code> 的 NATed virtual network，所有使用 default virtual network 的 VM，都可以透過 KVM host 中新增的 bridge device  <code>virbr0</code>，以 NAT 的方式連到外部網路。 </p><p>NATed virtual network 的設定步驟如下：</p><ol><li><p>建立 bridge device，並與特定的 nic 連結</p></li><li><p>設定防火牆規則 for NAT masquerade</p></li><li><p>virtual network 中 XML 定義的 <code>forward mode</code> 要設為 yes </p></li></ol><p>接著就是 <code>virsh net-&#123;define,start,autostart&#125;</code> 的工作了，這與上面雷同，就不再贅述</p><h2 id="Routed-virtual-network"><a href="#Routed-virtual-network" class="headerlink" title="Routed virtual network"></a>Routed virtual network</h2><p>這在實際應用中很少遇到，因此就略過先不測試，有機會試過之後再來補!</p><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="">Networking - KVM</a><a href="http://www.linux-kvm.org/page/Networking">http://www.linux-kvm.org/page/Networking</a></p></li><li><p><a href="https://libvirt.org/formatnetwork.html">libvirt: Network XML format</a></p></li><li><p><a href="http://www.tfcis.org/~lantw44/download/notes/cs/libvirt.domain(5)">KVM XML 設定檔基本內容</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
            <tag> libvirt </tag>
            
            <tag> Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] 半虛擬化驅動(Paravirtualization Driver)</title>
      <link href="/blog/KVM/KVM-Paravirtualization-Drivers/"/>
      <url>/blog/KVM/KVM-Paravirtualization-Drivers/</url>
      
        <content type="html"><![CDATA[<p>介紹如何使用 QEMU/KVM 中的半虛擬化，並了解此技術如何帶來效能上的提升</p><h1 id="QEMU-I-O-Overview"><a href="#QEMU-I-O-Overview" class="headerlink" title="QEMU I/O Overview"></a>QEMU I/O Overview</h1><p>QEMU/KVM 是屬於全虛擬化的解決方案，若沒有硬體加速輔助的情況下，所有的工作都必須透過軟體模擬，其實效率是很不好的，特別是 device I/O 的部分。</p><p>以下的圖可以說明純 QEMU 模擬下的 device I/O 狀況：</p><p><img src="http://smilejay.b0.upaiyun.com/wp-content/uploads/2012/11/qemu-emulated-io.jpg" alt="KVM virtio"></p><p>一個 virtual machine 的 I/O request 會透過以下流程完成：</p><ol><li><p>被 KVM module 中的 I/O trap 捕捉到 &amp; 處理</p></li><li><p>將處理結果放到 I/O sharing page 中</p></li><li><p>通知 QEMU process 來取得 I/O 資訊，並交由 QEMU I/O Emulation Code 來模擬 I/O request</p></li><li><p>完成後將結果放回 I/O sharing page</p></li><li><p>通知 KVM module 中的 I/O trap 將處理結果取回並回傳給 virtual machine</p></li></ol><p>透過 QEMU 可以模擬出各式各樣的 I/O device，甚至很老舊的設備都沒有問題；但從上面複雜的步驟不難看出為何使用 QEMU 模擬 device I/O 會效率不彰，除了每次 I/O request 處理的流程繁複之外，過多的 VMEntry, VMExit, context switch，也都是拖垮 QEMU 效能的原因。</p><hr><h1 id="virtio-Overview"><a href="#virtio-Overview" class="headerlink" title="virtio Overview"></a>virtio Overview</h1><p>有鑑於此，<a href="http://www.linux-kvm.org/page/Virtio">virtio</a> 被提出來，作為運行在 Hypervisor 上的一組 API interface，讓 virtual machine 知道自己運行在虛擬環境中，並根據 virtio 標準與 hypervisor 互動，藉此達到更好的運作效能(I/O 效能提升最為明顯)。</p><p>以下是純 QEMU 模擬 &amp; virtio 的架構比較，可以看出 virtio 省略了 I/O trap，讓 virtual machine 可以直接與 QEMU 的 I/O 模組通訊：</p><p><img src="http://images0.cnblogs.com/blog2015/697113/201506/011807259737673.jpg" alt="QEMU v.s. virtio"></p><p>更細部一點檢視 virtio 的架構：</p><p><img src="http://smilejay.b0.upaiyun.com/wp-content/uploads/2012/11/qemu-kvm-virtio.jpg" alt="virtio architecture"></p><ol><li><p>第一層 <strong>virtio_blk</strong>、<strong>virtio_net</strong>、<strong>virtio_scsi</strong> …. 等等屬於 <strong><font color='red'>virtio Frontend，存在於</font></strong> virtual machine OS kernel module 中</p></li><li><p>最下面一層稱為 **<font color='red'>virtio Backend</font>**，是在 QEMU 中實作，讓 I/O request 可以透過 QEMU 直接送給 host machine 中的 device driver，減少整體 I/O 的 overhead</p></li><li><p><strong><font color='red'>virtio</font></strong> 屬於虛擬佇列，目的是將 Frontend 的驅動程序附加到 Backend 的處理程序 (一個 Frontend 的驅動程序可以根據需求使用 0 個或多個佇列，例如 virtio_net 同時需要傳送 &amp; 接收用的兩個虛擬佇列，virtio_blk 則僅需要一個)</p></li><li><p><strong><font color='red'>virtio-ring</font></strong> 透過實作 ring buffer 的機制，讓 I/O request 得以批次處理，藉以提升 virtual machine 與 hypervisor 之間訊息交換的效率</p></li></ol><p>雖然 virtio 可以大幅的提升 I/O 性能，但不僅需要 host machine 的 OS kernel 有支援，連 virtual machine 也要同時有安裝 virtio driver，不過目前較新的 Linux 都已經支援 virtio 了! 所以若使用的是最近幾年的 Linux 版本，在 I/O 裝置的部分應該都可以選擇以 virtio 的方式運行。</p><blockquote><p>Windows 也都有對應的 virtio driver 可對應下載</p></blockquote><hr><h1 id="檢查-virtio-環境"><a href="#檢查-virtio-環境" class="headerlink" title="檢查 virtio 環境"></a>檢查 virtio 環境</h1><h2 id="1-1-virtio-Backend"><a href="#1-1-virtio-Backend" class="headerlink" title="1.1 virtio Backend"></a>1.1 virtio Backend</h2><p>由於 host machine 安裝的是 CentOS 7，因此都已經預設安裝 virtio driver 了! 可以用以下指令查詢：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ find / -name <span class="string">&#x27;virtio*.ko&#x27;</span> | grep $(uname -r)</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/virtio/virtio_pci.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/virtio/virtio.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/virtio/virtio_ring.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/virtio/virtio_input.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/virtio/virtio_balloon.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/char/virtio_console.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/char/hw_random/virtio-rng.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/scsi/virtio_scsi.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/block/virtio_blk.ko</span><br><span class="line">/usr/lib/modules/3.10.0-327.28.2.el7.x86_64/kernel/drivers/net/virtio_net.ko</span><br></pre></td></tr></table></figure><h2 id="1-2-virtio-Frontend"><a href="#1-2-virtio-Frontend" class="headerlink" title="1.2 virtio Frontend"></a>1.2 virtio Frontend</h2><h3 id="1-2-1-Linux"><a href="#1-2-1-Linux" class="headerlink" title="1.2.1 Linux"></a>1.2.1 Linux</h3><p>目前新版本的 Linux(Ubuntu / CentOS / …. 等等)都已經內建 virtio driver 了，不需要再額外安裝囉!</p><h3 id="1-2-2-Windows"><a href="#1-2-2-Windows" class="headerlink" title="1.2.2 Windows"></a>1.2.2 Windows</h3><p>若是要安裝 Windows 的 virtio driver，可以使用以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://fedorapeople.org/groups/virt/virtio-win/virtio-win.repo -O /etc/yum.repos.d/virtio-win.repo</span><br><span class="line"></span><br><span class="line">$ yum info virtio-win</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Loading mirror speeds from cached hostfile</span><br><span class="line"> * base: ftp.yzu.edu.tw</span><br><span class="line"> * epel: mirror01.idc.hinet.net</span><br><span class="line"> * extras: ftp.yzu.edu.tw</span><br><span class="line"> * updates: ftp.yzu.edu.tw</span><br><span class="line">Available Packages</span><br><span class="line">Name        : virtio-win</span><br><span class="line">Arch        : noarch</span><br><span class="line">Version     : 0.1.102</span><br><span class="line">Release     : 1</span><br><span class="line">Size        : 75 M</span><br><span class="line">Repo        : virtio-win-stable</span><br><span class="line">Summary     : VirtIO para-virtualized drivers <span class="keyword">for</span> Windows(R)</span><br><span class="line">URL         : http://www.redhat.com/</span><br><span class="line">License     : GPLv2</span><br><span class="line">Description : VirtIO para-virtualized Windows(R) drivers <span class="keyword">for</span> 32-bit and 64-bit</span><br><span class="line">            : Windows(R) guests.</span><br><span class="line"></span><br><span class="line">$ yum -y install virtio-win</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢目前 virtio-win 的支援程度</span></span><br><span class="line">$ ls -R /usr/share/virtio-win</span><br><span class="line">/usr/share/virtio-win:</span><br><span class="line">drivers  guest-agent  virtio-win-0.1.102_amd64.vfd  virtio-win-0.1.102.iso  virtio-win-0.1.102_x86.vfd  virtio-win_amd64.vfd  virtio-win.iso  virtio-win_x86.vfd</span><br><span class="line"></span><br><span class="line">/usr/share/virtio-win/drivers:</span><br><span class="line">amd64  i386</span><br><span class="line"></span><br><span class="line">/usr/share/virtio-win/drivers/amd64:</span><br><span class="line">Win2003  Win2008  Win2008R2  Win2012  Win2012R2  Win7  Win8  Win8.1</span><br><span class="line">.......</span><br><span class="line">/usr/share/virtio-win/drivers/amd64/Win2012R2:</span><br><span class="line">netkvm.cat  netkvm.inf  netkvm.sys  vioscsi.cat  vioscsi.inf  vioscsi.sys  viostor.cat  viostor.inf  viostor.sys</span><br><span class="line">.......</span><br><span class="line">/usr/share/virtio-win/drivers/amd64/Win7:</span><br><span class="line">netkvm.cat  netkvm.inf  netkvm.sys  qxl.cat  qxldd.dll  qxl.inf  qxl.sys  vioscsi.cat  vioscsi.inf  vioscsi.sys  viostor.cat  viostor.inf  viostor.sys</span><br><span class="line">.......</span><br></pre></td></tr></table></figure><blockquote><p>從上面的檔案列表得知，目前 stable 的 virtio-win 支援到 Windows 2012R2 &amp; Windows 8.1，若要更新的 Windows OS 需求，可能要使用 latest 版本</p></blockquote><p>若希望可以在最新版的 Windows 上安裝 virtio driver，那就要升級到最新版的 virtio-win，可以透過以下的指令來啟用最新的 repository &amp; 升級：(目前最新版已經支援 Windows 10)</p><blockquote><p>yum –enablerepo=virtio-win-latest update virtio-win</p></blockquote><p>使用方式很容易，只要在啟動 windows virtual machine 時，指定 CDRom 裝置掛載 <font color='red'><strong>/usr/share/virtio-win/virtio-win-0.1.102.iso</strong></font> 後，進入 Windows 把相關的 virtio 裝置驅動即可，詳細的使用方式可以參考 =&gt; <a href="http://www.lijyyh.com/2015/12/linux-kvm-set-up-linux-kvm.html">傲笑紅塵路: 架設 Linux KVM 虛擬化主機 (Set up Linux KVM virtualization host)</a></p><hr><h1 id="使用-virtio-balloon"><a href="#使用-virtio-balloon" class="headerlink" title="使用 virtio_balloon"></a>使用 virtio_balloon</h1><h2 id="balloonning-Overview"><a href="#balloonning-Overview" class="headerlink" title="balloonning Overview"></a>balloonning Overview</h2><p><img src="http://smilejay.com/wp-content/uploads/2012/11/linux-ballooning-demo.jpg" alt="KVM ballooning"></p><p>透過 balloon 的技術，可以如上圖所示，在 virtual machine 運行時動態調整記憶體的配置，而不需要 virtual machine 關機。</p><p>情境如下：</p><ol><li><p>當 host machine 記憶體空間不足時，在 virtual machine 中的記憶體 balloon 會膨脹(inflate)，讓 virtual machine 實際上無法使用到太多的記憶體，進而讓記憶體空間可以讓 host machine 暫時利用</p></li><li><p>當 virtual machine 記憶體空間不足時，在 virtual machine 中的記憶體 balloon 則會壓縮(deflate)，讓 host machine 可以分配閒置的記憶體給 virtual machine</p></li></ol><blockquote><p>以上功能必須透過 <code>virtio-balloon driver</code> 來達成</p></blockquote><h2 id="使用-balloonning-技術的優缺點"><a href="#使用-balloonning-技術的優缺點" class="headerlink" title="使用 balloonning 技術的優缺點"></a>使用 balloonning 技術的優缺點</h2><p>使用 balloon 的技術肯定也是有優缺點的，管理者可以根據實際需求評估使用：</p><p>優點如下：</p><ul><li><p>由於 balloonning 是能夠被監控 &amp; 控制的(不同於不可控制的 KSM 技術)，因此能夠有效率的節省記憶體的實際耗用</p></li><li><p>balloonning 對於記憶體的調度是很靈活的</p></li><li><p>hypervisor 透過 balloonning 從 virtual machine 中取得歸還的部分記憶體空間，不一定一定要用在其他地方，可自己保留住，端看管理者想要如何管理記憶體空間</p></li></ul><p>但 balloonning 同樣也是有些缺點存在的的：</p><ul><li><p>virtual machine 必須安裝 virtio_balloon driver 才可使用此功能(新版的 Linux 有內建，但 Windows 就必須要另外安裝了)</p></li><li><p>若透過 balloonning 從 virtual machine 中取回大量的記憶體空間，可能會提升 virtual machine 對於 swap 的 I/O 存取而導致效能降低；也有可能會造成 virtual machine 中的某些 process 運作時發生記憶體不足的況狀而失敗</p></li><li><p>雖然 balloonning 可被監控 &amp; 控制，但目前尚缺乏有效的自動化機制，對於大規模佈署使用上是不方便的</p></li><li><p>記憶體動態調整的過於頻繁，可能會使記憶體空間配置上過於零散而不連續，如此一來記憶體的使用效率就會降低</p></li></ul><h2 id="在-QEMU-KVM-中使用-balloonning"><a href="#在-QEMU-KVM-中使用-balloonning" class="headerlink" title="在 QEMU/KVM 中使用 balloonning"></a>在 QEMU/KVM 中使用 balloonning</h2><p>首先要先檢查 host machine 是否支援 balloonning：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cat /boot/config-`uname -r` | grep VIRTIO</span><br><span class="line">CONFIG_VIRTIO_BLK=m</span><br><span class="line">CONFIG_SCSI_VIRTIO=m</span><br><span class="line">CONFIG_VIRTIO_NET=m</span><br><span class="line">CONFIG_VIRTIO_CONSOLE=m</span><br><span class="line">CONFIG_HW_RANDOM_VIRTIO=m</span><br><span class="line">CONFIG_VIRTIO=m</span><br><span class="line">CONFIG_VIRTIO_PCI=m</span><br><span class="line">CONFIG_VIRTIO_PCI_LEGACY=y</span><br><span class="line">CONFIG_VIRTIO_BALLOON=m</span><br><span class="line">CONFIG_VIRTIO_INPUT=m</span><br><span class="line"><span class="comment"># CONFIG_VIRTIO_MMIO is not set</span></span><br></pre></td></tr></table></figure><blockquote><p>CONFIG_VIRTIO_BALLOON=m 表示已經支援(以 module 方式載入)</p></blockquote><p>接著要在 virtual machine 中啟用 balloonning，則必須在啟動 virtual machine 的指令中加上以下參數：(預設為 <code>-balloon none</code>)</p><blockquote><p>-balloon virtio[,addr=addr]</p></blockquote><p>參數中的 addr 是可用來指定 virtual machine 中 balloon device 的 PCI address</p><p>也可以使用 <font color='red'><strong>-device</strong></font> 參數來統一設定不同的 device，使用方法如下：</p><blockquote><p>-device driver[,prop[=value][,…]]</p></blockquote><p>了解啟用 balloon 的參數後，可以用以下指令啟動搭載 balloonning 功能的 virtual machine：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kvm -vnc 0.0.0.0:1 -m 2048 /kvm/storage/vm_disks/ubnutu1604.img \</span><br><span class="line">  -net nic -net tap,script=/etc/qemu-ifup \</span><br><span class="line">  -device virtio-balloon-pci --daemonize</span><br></pre></td></tr></table></figure><p>接著連線到 virtual machine 中，查詢 balloonning device 的狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># balloon 功能已開啟</span></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ cat /boot/config-`uname -r` | grep VIRTIO</span><br><span class="line">.....</span><br><span class="line">CONFIG_VIRTIO_BALLOON=y</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視目前 pci device 狀態</span></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ lspci</span><br><span class="line">00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)</span><br><span class="line">00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]</span><br><span class="line">00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]</span><br><span class="line">00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)</span><br><span class="line">00:02.0 VGA compatible controller: Cirrus Logic GD 5446</span><br><span class="line">00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 03)</span><br><span class="line">00:04.0 Unclassified device [00ff]: Red Hat, Inc Virtio memory balloon</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 balloonning device 的詳細資訊</span></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ lspci -s 00:04.0 -v</span><br><span class="line">00:04.0 Unclassified device [00ff]: Red Hat, Inc Virtio memory balloon</span><br><span class="line">        Subsystem: Red Hat, Inc Virtio memory balloon</span><br><span class="line">        Physical Slot: 4</span><br><span class="line">        Flags: bus master, fast devsel, latency 0, IRQ 11</span><br><span class="line">        I/O ports at c040 [size=32]</span><br><span class="line">        Kernel driver <span class="keyword">in</span> use: virtio-pci</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 virtual machine 記憶體狀態</span></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ free -m</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           2000          40        1851           3         108        1823</span><br><span class="line">Swap:          2045           0        2045</span><br></pre></td></tr></table></figure><p>接著在 QEMU monitor 中透過 <code>balloon 512</code> 把 virtual machine 的記憶體限縮到僅能使用 512 MB：</p><p><img src="https://github.com/godleon/godleon.github.io/blob/master/_posts/images/2016/KVM-Paravirtualization-Drivers/kvm_virtio_balloon.PNG?raw=true" alt="balloon"></p><p>以下是 virtual machine 中目前的記憶體狀況：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 經過 balloon 縮小可用記憶體空間後，檢視記憶體狀態</span></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ free -m</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:            464          39         316           3         108         288</span><br><span class="line">Swap:          2045           0        2045</span><br></pre></td></tr></table></figure><blockquote><p>可以看出 total memory 已經變少，可見 balloon 的確是起了作用</p></blockquote><p>另外一個比較需要注意的是，balloon 沒辦法用於增加 virtual machine 的記憶體配置；例如，啟用 virtual machine 時配置了 2048 MB 的記憶體，即使在 QEMU monitor 中使用 <code>ballon 4096</code>，也無法讓 virtual machine 的記憶體變成 4096 MB。</p><hr><h1 id="使用-virtio-net"><a href="#使用-virtio-net" class="headerlink" title="使用 virtio_net"></a>使用 virtio_net</h1><p>使用 virtio network device，有提高 throughput &amp; 降低 latency 的兩項優點，可以達到接近原生網卡的效能，因此通常是配置網路時的優先選擇。</p><p>以下指令可以查詢是否支援 virtio_net：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有出現 virtio 表示支援 virtio network device</span></span><br><span class="line">$ kvm -net nic,model=?</span><br><span class="line">qemu: Supported NIC models: ne2k_pci,i82551,i82557b,i82559er,rtl8139,e1000,pcnet,virtio</span><br></pre></td></tr></table></figure><h2 id="1、啟用支援-virtio-net-的-vitual-machine"><a href="#1、啟用支援-virtio-net-的-vitual-machine" class="headerlink" title="1、啟用支援 virtio_net 的 vitual machine"></a>1、啟用支援 virtio_net 的 vitual machine</h2><p>接著可以用以下指令啟動一個使用 virtio_net 的 virtual machine：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kvm -vnc 0.0.0.0:1 -m 2048 --daemonize \</span><br><span class="line">  -drive format=raw,file=/kvm/storage/vm_disks/ubnutu1604.img \</span><br><span class="line">  -device virtio-balloon-pci \</span><br><span class="line">  -net nic,model=virtio -net tap</span><br></pre></td></tr></table></figure><p>當 virtual machine 啟動後，登入檢查 virtio_net 的狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ubuntu 16.04 原生已經搭載 virtio driver</span></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ cat /boot/config-`uname -r` | grep VIRTIO</span><br><span class="line">.....</span><br><span class="line">CONFIG_VIRTIO_NET=y</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可看出目前 virtual machine 的網卡已經以 virtio 模式運作</span></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ lspci</span><br><span class="line">....</span><br><span class="line">00:03.0 Ethernet controller: Red Hat, Inc Virtio network device</span><br><span class="line">00:04.0 Unclassified device [00ff]: Red Hat, Inc Virtio memory balloon</span><br><span class="line"></span><br><span class="line"><span class="comment"># virtio_net 詳細資訊</span></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ lspci -vv -s 00:03.0</span><br><span class="line">00:03.0 Ethernet controller: Red Hat, Inc Virtio network device</span><br><span class="line">        Subsystem: Red Hat, Inc Virtio network device</span><br><span class="line">        Physical Slot: 3</span><br><span class="line">        Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx+</span><br><span class="line">        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</span><br><span class="line">        Latency: 0</span><br><span class="line">        Interrupt: pin A routed to IRQ 11</span><br><span class="line">        Region 0: I/O ports at c000 [size=32]</span><br><span class="line">        Region 1: Memory at febd1000 (32-bit, non-prefetchable) [size=4K]</span><br><span class="line">        Expansion ROM at feb80000 [disabled] [size=256K]</span><br><span class="line">        Capabilities: &lt;access denied&gt;</span><br><span class="line">        Kernel driver <span class="keyword">in</span> use: virtio-pci</span><br></pre></td></tr></table></figure><h2 id="2、進一步提升半虛擬化網卡效能"><a href="#2、進一步提升半虛擬化網卡效能" class="headerlink" title="2、進一步提升半虛擬化網卡效能"></a>2、進一步提升半虛擬化網卡效能</h2><h3 id="1-關閉-TSO-amp-GSO-以提升-virtio-net-的效能"><a href="#1-關閉-TSO-amp-GSO-以提升-virtio-net-的效能" class="headerlink" title="(1) 關閉 TSO &amp; GSO 以提升 virtio_net 的效能"></a>(1) 關閉 TSO &amp; GSO 以提升 virtio_net 的效能</h3><p>為了提升 virtual machine 網卡的效能，除了使用 virtio 半虛擬化的技術外，還可以透過關閉 host machine 的 TSO &amp; GSO 功能來更進一步提升</p><p>以下可以檢查 host machine 的網卡是否支援 TSO &amp; GSO：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -k ens1f1</span><br><span class="line">Features <span class="keyword">for</span> ens1f1:</span><br><span class="line">.....</span><br><span class="line">tcp-segmentation-offload: on</span><br><span class="line">        tx-tcp-segmentation: on</span><br><span class="line">        tx-tcp-ecn-segmentation: off [fixed]</span><br><span class="line">        tx-tcp6-segmentation: on</span><br><span class="line">.....</span><br><span class="line">generic-segmentation-offload: on</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><p>從上面可以看出目前 TSO &amp; GSO 的功能都是開啟的，可以透過以下方式關閉：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -K ens1f1 tso off</span><br><span class="line">$ ethtool -K ens1f1 gso off</span><br></pre></td></tr></table></figure><p>如此一來 virtio_net 的效能就可以進一步的提升。</p><h3 id="2-使用-vhost-net-後端驅動"><a href="#2-使用-vhost-net-後端驅動" class="headerlink" title="(2) 使用 vhost-net 後端驅動"></a>(2) 使用 vhost-net 後端驅動</h3><p>一般來說，virtio 在 host machine 是由每個 user space 中的 QEMU 來進行後端處理的，但若是可以將 network I/O 的部分移到 kernel space 來處理，不僅可以提高 network throughput，還可以降低 latency，進而提升網路的效率。</p><p>目前比較新的 Linux kernel 中都有搭載稱為 <strong><font color='red'>vhost-net</font></strong> 的 module，可用來將 virtio_net 的後端處理移到 Linux kernel 中進行來提升 network I/O 的效率。</p><p>首先先來檢查 host machine 是否支援 <strong>vhost-net</strong> 的功能：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat /boot/config-`uname -r` | grep VHOST</span><br><span class="line">CONFIG_VHOST_NET=m</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">$ lsmod | grep vhost</span><br><span class="line">vhost_net              18152  1</span><br><span class="line">....</span><br></pre></td></tr></table></figure><p>從上面可以看出 host machine 有支援 vhost-net，接著就可以使用以下指令啟動支援 vhost-net 的 virtual machine：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kvm -vnc 0.0.0.0:1 -m 2048 --daemonize \</span><br><span class="line">  -drive format=raw,file=/kvm/storage/vm_disks/ubnutu1604.img,<span class="keyword">if</span>=virtio \</span><br><span class="line">  -device virtio-balloon-pci \</span><br><span class="line">  -netdev tap,id=net0,vhost=on -device virtio-net-pci,netdev=net0</span><br></pre></td></tr></table></figure><hr><h1 id="使用-virtio-blk"><a href="#使用-virtio-blk" class="headerlink" title="使用 virtio_blk"></a>使用 virtio_blk</h1><p>virtio_blk 提供了 virtual machine 可以透過 virtio API 進行 block device I/O 的相關驅動程式，藉以提升存取 block device 的效能。</p><p>同樣的，要使用 virtio_blk，host machine &amp; virtual machine 都必須要同時支援才行，目前新版的 Linux 都已經預設搭載 virtio driver 了，我們用以下的指令啟動支援 virtio_blk 的 virtual machine：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kvm -vnc 0.0.0.0:1 -m 2048 --daemonize \</span><br><span class="line">  -drive format=raw,file=/kvm/storage/vm_disks/ubnutu1604.img \</span><br><span class="line">  -device virtio-balloon-pci \</span><br><span class="line">  -netdev tap,id=net0,vhost=on -device virtio-net-pci,netdev=net0</span><br></pre></td></tr></table></figure><p>進入到 virtual machine 之後，可以透過以下指令確認 block device 的確是以 virtio driver 所驅動：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@vm-ubuntu1604:~$ cat /boot/config-`uname -r` | grep VIRTIO_BLK</span><br><span class="line">CONFIG_VIRTIO_BLK=y</span><br><span class="line"></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ lspci | grep -i virtio</span><br><span class="line">00:03.0 Unclassified device [00ff]: Red Hat, Inc Virtio memory balloon</span><br><span class="line">00:04.0 Ethernet controller: Red Hat, Inc Virtio network device</span><br><span class="line">00:05.0 SCSI storage controller: Red Hat, Inc Virtio block devicei</span><br><span class="line"></span><br><span class="line">ubuntu@vm-ubuntu1604:~$ lspci -vv -s 00:05.0</span><br><span class="line">00:05.0 SCSI storage controller: Red Hat, Inc Virtio block device</span><br><span class="line">        Subsystem: Red Hat, Inc Virtio block device</span><br><span class="line">        Physical Slot: 5</span><br><span class="line">        Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR+ FastB2B- DisINTx+</span><br><span class="line">        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx-</span><br><span class="line">        Latency: 0</span><br><span class="line">        Interrupt: pin A routed to IRQ 10</span><br><span class="line">        Region 0: I/O ports at c000 [size=64]</span><br><span class="line">        Region 1: Memory at febd2000 (32-bit, non-prefetchable) [size=4K]</span><br><span class="line">        Capabilities: &lt;access denied&gt;</span><br><span class="line">        Kernel driver <span class="keyword">in</span> use: virtio-pci</span><br></pre></td></tr></table></figure><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://read01.com/4aJdOL.html">Virtio 基本概念和設備操作 - 壹讀</a></p></li><li><p><a href="http://www.cnblogs.com/sammyliu/p/4543657.html">KVM 介绍（3）：I/O 全虚拟化和准虚拟化 [KVM I/O QEMU Full-Virtualizaiton Para-virtualization] - SammyLiu - 博客园</a></p></li><li><p><a href="http://www.lijyyh.com/2015/12/linux-kvm-set-up-linux-kvm.html">傲笑紅塵路: 架設 Linux KVM 虛擬化主機 (Set up Linux KVM virtualization host)</a></p></li><li><p><a href="https://fedoraproject.org/wiki/Windows_Virtio_Drivers">Windows Virtio Drivers - FedoraProject</a></p></li><li><p><a href="http://www.linux-kvm.org/page/Virtio">Virtio - KVM</a></p></li><li><p><a href="http://seitran.com/2015/04/13/01-gso-gro-lro/">网卡TSO/GSO/LRO/GRO简要介绍 | Chenny的部落格</a></p></li><li><p><a href="http://lirobo.blogspot.tw/2014/12/tso-gso-lro-gro.html">小蘿蔔工作室 Little Robot Studio: TSO, GSO, LRO, GRO</a></p></li><li><p><a href="https://kris.io/2015/10/01/kvm-network-performance-tso-and-gso-turn-it-off/">KVM Network Performance : TSO and GSO - Turn it off - kris.io : virtualization &amp; cloud</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] Linux KVM concept - Networking</title>
      <link href="/blog/KVM/KVM-Basic-Concept-Networking/"/>
      <url>/blog/KVM/KVM-Basic-Concept-Networking/</url>
      
        <content type="html"><![CDATA[<p>此篇文章介紹學習 Linux KVM 時所需要了解的 Networking 相關知識</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>現今的虛擬化技術中，網路的部分永遠都是比重相當高的一部分，許多大型的 service provider 透過網路提供各種不同的服務給使用者，讓使用者在 IT 科技的幫助下，生活更加便利。</p><p>也因為這樣，當網路發生故障時，這些公司的損失常常難以估計，因此網路架構的設計上，保有彈性 &amp; 穩定性是相當重要的。</p><p>而現在 server 普遍都擁有多張網路卡，為了提高網路使用效率 &amp; 增加備援功能，建議 **<font color='red'>透過 Linux bonding 的技術將所有的網路卡設定成為 single virtual channel</font>**。</p><blockquote><p>bonding mode 有分為 Mode 1(active-backup)、Mode 2(balance-xor)、Mode 4(802.3ad/LACP)、Mode 5(balance-tlb)，其中建議使用 <code>Mode 1(active-backup)</code> &amp; <code>Mode 5(balance-tlb)</code></p></blockquote><hr><h1 id="KVM-QEMU-支援的-Network-Type"><a href="#KVM-QEMU-支援的-Network-Type" class="headerlink" title="KVM/QEMU 支援的 Network Type"></a>KVM/QEMU 支援的 Network Type</h1><p>網路絕對是使用 vitual machine 時最不可或缺的一部分，而 QEMU 提供了 virtual machine 一共四種不同的 network type：</p><ol><li><p>bridge</p></li><li><p>NAT</p></li><li><p>user mode networking</p></li><li><p>VT-d &amp; SR-IOV (直接分配網路設備)</p></li></ol><p>要透過 QEMU 配置 virtual machine 的網路，必須使用 <code>-net</code> 參數，若是完全沒使用任何的網路相關參數，會系統自動帶上 <code>-net nic -net user</code> 做為網路預設值。</p><p>而 QEMU 可以模擬那些網路卡呢? 肯定必須是主流且被廣泛支援的，可以用以下指令查詢：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kvm -net nic,model=?</span><br><span class="line">qemu: Supported NIC models: ne2k_pci,i82551,i82557b,i82559er,rtl8139,e1000,pcnet,virtio</span><br></pre></td></tr></table></figure><blockquote><p>若沒指定任何網路相關參數，則預設會使用 RTL8139 網卡</p></blockquote><hr><h1 id="設定第一張虛擬網卡"><a href="#設定第一張虛擬網卡" class="headerlink" title="設定第一張虛擬網卡"></a>設定第一張虛擬網卡</h1><p>為了要設定虛擬網卡，必須先了解 <code>-net nic</code> 相關參數：</p><blockquote><p>-net nic[,vlan=n][,macaddr=mac][,model=type] [,name=name][,addr=addr][,vectors=v]</p></blockquote><ul><li><p><code>-net nic</code>：表示這是一個網卡的設定 (<strong><font color='red'>必要!</font></strong>)</p></li><li><p><code>vlan=n</code>：將網卡連結到 ID 為 <strong><font color='red'>n</font></strong> 的 VLAN</p></li><li><p><code>macaddr=mac</code>：指定網卡 MAC address</p></li><li><p><code>model=type</code>：設定網卡的種類，預設為 <strong><font color='red'>rtl8139</font></strong></p></li><li><p><code>name=name</code>：為網卡命名(但僅有在 QEMU monitor 看的到)</p></li><li><p><code>addr=addr</code>：指定網卡在 virtual machine 中的 PCI device address</p></li><li><p><code>vectors=v</code>：設定網卡設備的 MSI-X 向量的數量，用於 virtio 驅動的網卡上</p></li></ul><p>簡單做個測試，我們透過以下指令指定網卡：</p><blockquote><p>kvm -vnc 0.0.0.0:1 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img -net nic,vlan=0,macaddr=52:53:00:11:12:13,model=e1000,addr=08 -net user</p></blockquote><p>於是在系統中可以得到以下訊息：</p><p><img src="https://lh3.googleusercontent.com/abxoQOVs8BrZGNtB7rvarf28_a0AomNu_UBJvLgblFG6uoAj2lWJnlMGMuujMu_V3ldlY2p2ZPYTQJa84180vlnBkGp4bkJmidQN29QBEO9zb9cWOOQOK5HTTOwglaXnEJ_WXdSnKO1xGFmKkdHZji1tulTDI_mihaOaghtA3YlT_I5ss4-OQbsdK2lIdLz_deaKnb5AcAQA9cmAt6rPaW0pocvL4THwvZY3d4QXOOMso96DPPTWntwl0qsHEd19BMg4B0uzuAbNlIVglYMX8k80YOTZxTQDsAFi16q5MDlgrTPDa0VY7UOoNFvI-7PWmktPf3RYSpXa1QQ6kmASVax3x3NCYC3pZW_HA27dxjwyA3GRM4IYtsjKA9EBjMkhpVZmHeoxAeCk9_gSXKMJUqEdg8u63JRm9xsJobEcOy_g_1YXmMPCc1v-8Mx1QFpwctMr5C7DXq8DI-C_Voz2Y8DGpz1p7prWgDb2j1v6daMfvJwJvtKAyw2p2HyulbsMuSZE3JNB6bsZLmJgi4WjvXT3rr32l1mJQcE64B4MK-ZLQMZjTA0g18g5w5wk5d83XtQiSahUc-5wGhudJ6l5sqqRwDQ1Nec=w771-h384-no" alt="System NIC Information"></p><p>在 QEMU monitor 則可以看到以下資訊：</p><p><img src="https://lh3.googleusercontent.com/RPzHnMisxvsHZx5xpTctONaavpAXkcjUxfqHdaiNfwYL-RyspfM2dGW4rffxz7Z-NyzLHH1ChC0zGwjNes5AclVYiW4-8Bb8YOgg61jHs58jmMchhv_ICNNvwhm-vkyHmkffIukmKxX7BqufthPly1VgVMNI1J2x17l66DD2LicTEbGPQB-414rpc8tTR1l3pBPnoNPZuaxTNCXNYlkfmvQUdx1ltYfWusoEUsxhLckpakEJ_F8qCEGrJ-dmLgh6BbLjDNlDfqBix-bMJaBdGlyngC8XvWR-f45eTKUyPCukFb-oSFVFH1VspTRqsBgccbSH06fKlq12CDu0pNhBNbte6zFPt7oJMAFn_Yf2ENBWXQbFjRs2H0x8eaCcQ0MiXY01wUmsNYoGJsgmZgKJZ2jQdhLDXu4bjtoH-0DuZw1NSfKPuIkDks9FdQR0j-adxK_rOg9oqpTdNOBOkU_QPfqQJUejhwlTr5OwjLjGkD5XIYHKbXU72D66eDCZNTMVUGxWo_EkhVluszNlN1PiCzOdqlRK4StC0LVWUFscHLHwyEjHuwTwg0BEIRs91_-nD0DrJT5mZF7emdCRPStDDdWGdP9ftVY=w541-h153-no" alt="Network Info in QEMU monitor"></p><hr><h1 id="設定-Bridge-Mode"><a href="#設定-Bridge-Mode" class="headerlink" title="設定 Bridge Mode"></a>設定 Bridge Mode</h1><h2 id="設定參數說明"><a href="#設定參數說明" class="headerlink" title="設定參數說明"></a>設定參數說明</h2><p>在 bridge mode 下，virtual machine 的網路可視為與 host machine 是相同的，可以有自己的獨立 ip，外部網路可以直接存取 virtual machine。</p><p>以下是建立 bridge mode NIC 的參數：</p><blockquote><p>-net tap[,vlan=n][,name=name][,fd=h][,ifname=name][,script=file][,downscript=dfile][,helper=helper]</p></blockquote><ul><li><p><code>tap</code>：表示使用 tap device(屬於 layer 2 的虛擬網路設備；附帶一提，<code>tun</code> 則是屬於 layer 3 的虛擬網路設備)</p></li><li><p><code>vlan=n</code>：指定使用的 VLAN ID</p></li><li><p><code>name=name</code>：設定網卡名稱(可從 QEMU monitor 中檢視，但大多數情況下可忽略)</p></li><li><p><code>fd=h</code>：連接到已經開啟 tap 的 file descriptor (<strong>一般交給 QEMU 自行建立</strong>)</p></li><li><p><code>ifname=name</code>：設定 tap device 名稱(未設定的話會由 QEMU 自行產生)</p></li><li><p><code>script=file</code>：指定啟動 virtual machine 時自動執行的 script</p></li><li><p><code>downscript=dfile</code>：指定關閉 virtual machine 時自動執行的 script</p></li><li><p><code>helper=helper</code>：指定啟動 virtual machine 時會在 host machine 上執行的 helper 程式(例如：建立 tap device)，一般可以忽略使用預設值即可</p></li></ul><h2 id="建立支援-bridge-mode-的環境"><a href="#建立支援-bridge-mode-的環境" class="headerlink" title="建立支援 bridge mode 的環境"></a>建立支援 bridge mode 的環境</h2><h3 id="1、檢查環境支援程度"><a href="#1、檢查環境支援程度" class="headerlink" title="1、檢查環境支援程度"></a>1、檢查環境支援程度</h3><p>在建立 bridge 模式的網路配置之前，我們必須先確定 host machine 已經掛載相關模組：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢查是否有掛載 tun 模組</span></span><br><span class="line">$ lsmod | grep tun</span><br><span class="line">tun                    27141  1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認目前的 user 有 /dev/net/tun 的讀寫權限</span></span><br><span class="line">$ ls -al /dev/net/tun</span><br><span class="line">crw-rw-rw-. 1 root root 10, 200 Aug  3 20:14 /dev/net/tun</span><br></pre></td></tr></table></figure><h3 id="2、在-host-machine-上可建立-bridge-device-的環境"><a href="#2、在-host-machine-上可建立-bridge-device-的環境" class="headerlink" title="2、在 host machine 上可建立 bridge device 的環境"></a>2、在 host machine 上可建立 bridge device 的環境</h3><p>環境說明：</p><ul><li><p>bridge device：<code>br0</code></p></li><li><p>physical device：<code>ens1f0</code></p></li><li><p>ip address：<code>10.20.190.2/24</code></p></li><li><p>gateway：<code>10.20.190.1</code></p></li></ul><p>以下要在 host machine 上建立一個 bridge device，並綁定到要連外的實體網卡，並讓 bridge device 成為連接 host machine 與外網的設備：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]<span class="comment"># nmcli connection add type bridge ifname br0</span></span><br><span class="line">[root@localhost ~]<span class="comment"># nmcli connection modify bridge-br0 bridge.stp yes</span></span><br><span class="line"><span class="comment"># 以下請根據自身的 lab 環境做調整</span></span><br><span class="line">[root@localhost ~]<span class="comment"># nmcli connection modify bridge-br0 ipv4.method manual ipv4.address &quot;10.20.190.2/24&quot; ipv4.gateway &quot;10.20.190.1&quot; ipv4.dns 8.8.8.8</span></span><br><span class="line">[root@localhost ~]<span class="comment"># nmcli connection add type bridge-slave ifname ens1f0 master br0</span></span><br><span class="line">[root@localhost ~]<span class="comment"># nmcli connection delete ens1f0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># bridge device 設定完成後，可以檢查是否有與實體網卡連動</span></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name     bridge id               STP enabled     interfaces</span><br><span class="line">br0             8000.2c600cb163d5       yes             ens1f0</span><br><span class="line">virbr0          8000.5254001001dc       yes             virbr0-nic</span><br></pre></td></tr></table></figure><p>設定完成後，實體網路卡會進入到 <strong><font color='red'>promiscuous mode</font>**，而 bridge device 則會進入到 **<font color='red'>forwarding state</font></strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ dmesg | grep ens1f0</span><br><span class="line">.....</span><br><span class="line">[807464.149861] device ens1f0 entered promiscuous mode</span><br><span class="line">.....</span><br><span class="line">[807494.252510] br0: port 1(ens1f0) entered forwarding state</span><br></pre></td></tr></table></figure><p>接著建立要進行 bridge 網路配置用的 script(<strong><font color='red'>/etc/qemu-if{up,down}</font></strong>) 並設定可執行權限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 script (/etc/qemu-ifup)</span></span><br><span class="line">$ cat &lt;&lt;\EOF &gt;/etc/qemu-ifup</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#This is a qemu-ifup script for bridging.</span></span><br><span class="line"><span class="comment">#You can use it when starting a KVM guest with bridge mode network.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#set your bridge name</span></span><br><span class="line">switch=br0</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$1</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="comment">#start up the TAP interface</span></span><br><span class="line">  ip link <span class="built_in">set</span> <span class="variable">$1</span> up</span><br><span class="line">  sleep 1</span><br><span class="line">  <span class="comment">#add TAP interface to the bridge</span></span><br><span class="line">  brctl addif <span class="variable">$&#123;switch&#125;</span> <span class="variable">$1</span></span><br><span class="line">  <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> “Error: no interface specified”</span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ cat &lt;&lt;\EOF &gt;/etc/qemu-ifdown</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#This is a qemu-ifdown script for bridging.</span></span><br><span class="line"><span class="comment">#You can use it when starting a KVM guest with bridge mode network.</span></span><br><span class="line"><span class="comment">#Don’t use this script in most cases; QEMU will handle it automatically.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#set your bridge name</span></span><br><span class="line">switch=br0</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$1</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="comment"># Delete the specified interfacename</span></span><br><span class="line">  tunctl -d <span class="variable">$1</span></span><br><span class="line">  <span class="comment">#release TAP interface from bridge</span></span><br><span class="line">  brctl delif <span class="variable">$&#123;switch&#125;</span> <span class="variable">$1</span></span><br><span class="line">  <span class="comment">#shutdown the TAP interface</span></span><br><span class="line">  ip link <span class="built_in">set</span> <span class="variable">$1</span> down</span><br><span class="line">  <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> “Error: no interface specified”</span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定執行權限</span></span><br><span class="line">$ chmod +x /etc/qemu-if&#123;up,down&#125;</span><br></pre></td></tr></table></figure><blockquote><p>若是目前使用者沒有 script 的 execute 權限，就無法新增 tap interface for bridge mode 喔!</p></blockquote><h3 id="3、啟動-bridge-mode-virtual-machine"><a href="#3、啟動-bridge-mode-virtual-machine" class="headerlink" title="3、啟動 bridge-mode virtual machine"></a>3、啟動 bridge-mode virtual machine</h3><p>當上述環境都準備好後，可使用以下指令啟動 virtual machine：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -net nic -net tap =&gt; 指定 virtual machine 網卡為 bridge mode，建立 tap interface</span></span><br><span class="line"><span class="comment"># script=/etc/qemu-ifup =&gt; 指定 script 配置網卡</span></span><br><span class="line"><span class="comment"># downscript=/etc/qemu-ifdown =&gt; 指定 script 移除網卡 (若設定為 no 則 QEMU 會自動協助處理)</span></span><br><span class="line"><span class="comment"># --daemonize =&gt; QEMU 程序背景執行</span></span><br><span class="line">$ kvm -vnc 0.0.0.0:1 -m 2048 /kvm/storage/vm_disks/ubnutu1604.img -net nic -net tap,script=/etc/qemu-ifup,downscript=/etc/qemu-ifdown --daemonize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢是否有啟動 tap interface 並與 bridge device 相連</span></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name     bridge id               STP enabled     interfaces</span><br><span class="line">br0             8000.26503b3a5e74       yes             ens1f0</span><br><span class="line">                                                        tap0</span><br><span class="line">virbr0          8000.5254001001dc       yes             virbr0-nic</span><br></pre></td></tr></table></figure><p>當 virtual machine 啟動後(會在 DHCP 的地方卡一段時間)，在 virtual machine 中可以透過以下指令設定 IP：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假設新增的 network interface 為 ens3</span></span><br><span class="line">$ sudo ip addr add 10.20.190.3/24 dev ens3</span><br><span class="line">$ sudo ip route add 0.0.0.0/0 via 10.20.190.1 dev ens3</span><br></pre></td></tr></table></figure><p>如此一來在網路的部分就可以等同 host machine 一樣，正常存取 Internet &amp; 提供網路服務了</p><p>詳細的設定操作可以參考下圖：</p><p><img src="https://github.com/godleon/godleon.github.io/blob/master/_posts/images/2016/KVM-Basic-Concept-Networking/kvm_network-bridge-mode-configure-ip.PNG?raw=true" alt="Configure static IP by using ip command"></p><hr><h1 id="設定-NAT-mode"><a href="#設定-NAT-mode" class="headerlink" title="設定 NAT mode"></a>設定 NAT mode</h1><h2 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h2><p>NAT 是為了解決 public ip address 不足而產生的，其詳細的原理這邊就不贅述了，可參考下列網址：</p><ul><li><a href="http://linux.vbird.org/linux_server/0250simple_firewall/0320nat.php">鳥哥的 Linux 私房菜 – Network Address Transfer ( NAT ) 架設</a></li></ul><h2 id="檢查-host-machine-環境"><a href="#檢查-host-machine-環境" class="headerlink" title="檢查 host machine 環境"></a>檢查 host machine 環境</h2><h3 id="1、檢查-bridge-device"><a href="#1、檢查-bridge-device" class="headerlink" title="1、檢查 bridge device"></a>1、檢查 bridge device</h3><p>在 QEMU/KVM 中，使用 NAT mode 的 virtual machine 其實是透過 host machine 中的 <strong><font color='red'>virbr0</font></strong> 這個 bridge device 連外的，可以來檢視一下相關資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ ip addr show</span><br><span class="line">1: lo: .......</span><br><span class="line">2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</span><br><span class="line">    link/ether 2c:60:0c:c1:b9:2d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">3: ens1f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master br0 state UP qlen 1000</span><br><span class="line">    link/ether 2c:60:0c:b1:63:d5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">4: ens1f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</span><br><span class="line">    link/ether 2c:60:0c:b1:63:d6 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">5: virbr0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN</span><br><span class="line">    link/ether 52:54:00:10:01:dc brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: virbr0-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast master virbr0 state DOWN qlen 500</span><br><span class="line">    link/ether 52:54:00:10:01:dc brd ff:ff:ff:ff:ff:ff</span><br><span class="line">7: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP</span><br><span class="line">    link/ether 2c:60:0c:b1:63:d5 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.20.190.2/24 brd 10.20.190.255 scope global br0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::2e60:cff:feb1:63d5/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>從上面可以看出，QEMU/KVM 已經幫我們把 virbr0 設定完成，使用的是 <code>192.168.122.1/24</code> 的網段。</p><h3 id="2、檢查-DHCP-服務"><a href="#2、檢查-DHCP-服務" class="headerlink" title="2、檢查 DHCP 服務"></a>2、檢查 DHCP 服務</h3><p>但關於 DHCP 的部分呢? QEMU/KVM 同樣也設定好 <strong><font color='red'>dnsmasq</font></strong> 幫我們處理好 ip 的分配 &amp; traffic forwarding 的相關問題，若是要進一步的了解詳細的設定細節，可使用下面方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ ps -aux | grep dns</span><br><span class="line">nobody    4322  0.0  0.0  15552   888 ?        S    Jul27   0:01 /sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper</span><br><span class="line">root      4323  0.0  0.0  15524   176 ?        S    Jul27   0:00 /sbin/dnsmasq --conf-file=/var/lib/libvirt/dnsmasq/default.conf --leasefile-ro --dhcp-script=/usr/libexec/libvirt_leaseshelper</span><br><span class="line">root     16414  0.0  0.0 112652   976 pts/2    S+   06:02   0:00 grep --color=auto dns</span><br><span class="line"></span><br><span class="line">[root@ocp-kvm-host ~]<span class="comment"># cat /var/lib/libvirt/dnsmasq/default.conf</span></span><br><span class="line"><span class="comment">##WARNING:  THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE</span></span><br><span class="line"><span class="comment">##OVERWRITTEN AND LOST.  Changes to this configuration should be made using:</span></span><br><span class="line"><span class="comment">##    virsh net-edit default</span></span><br><span class="line"><span class="comment">## or other application using the libvirt API.</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment">## dnsmasq conf file created by libvirt</span></span><br><span class="line">strict-order</span><br><span class="line">pid-file=/var/run/libvirt/network/default.pid</span><br><span class="line">except-interface=lo</span><br><span class="line">bind-dynamic</span><br><span class="line">interface=virbr0</span><br><span class="line">dhcp-range=192.168.122.2,192.168.122.254</span><br><span class="line">dhcp-no-override</span><br><span class="line">dhcp-lease-max=253</span><br><span class="line">dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile</span><br><span class="line">addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts</span><br></pre></td></tr></table></figure><h3 id="3、檢查-traffic-forward-設定"><a href="#3、檢查-traffic-forward-設定" class="headerlink" title="3、檢查 traffic forward 設定"></a>3、檢查 traffic forward 設定</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/net/ipv4/ip_forward</span><br><span class="line">1</span><br></pre></td></tr></table></figure><blockquote><p>‵<code>1</code> 表示 traffic forward 的功能是開啟的</p></blockquote><h3 id="3、檢查-firewall-設定"><a href="#3、檢查-firewall-設定" class="headerlink" title="3、檢查 firewall 設定"></a>3、檢查 firewall 設定</h3><p>最後要確認 host machine 可以將 virtual machine 對外的流量透過 virbr0 送出，因此要確認防火牆的設定：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iptables -t nat -L -vn</span></span><br><span class="line">Chain PREROUTING (policy ACCEPT 29 packets, 4041 bytes)</span><br><span class="line"> pkts bytes target     prot opt <span class="keyword">in</span>     out     <span class="built_in">source</span>               destination</span><br><span class="line"></span><br><span class="line">Chain INPUT (policy ACCEPT 5 packets, 1044 bytes)</span><br><span class="line"> pkts bytes target     prot opt <span class="keyword">in</span>     out     <span class="built_in">source</span>               destination</span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT 39 packets, 2906 bytes)</span><br><span class="line"> pkts bytes target     prot opt <span class="keyword">in</span>     out     <span class="built_in">source</span>               destination</span><br><span class="line"></span><br><span class="line">Chain POSTROUTING (policy ACCEPT 39 packets, 2906 bytes)</span><br><span class="line"> pkts bytes target     prot opt <span class="keyword">in</span>     out     <span class="built_in">source</span>               destination</span><br><span class="line">    0     0 RETURN     all  --  *      *       192.168.122.0/24     224.0.0.0/24</span><br><span class="line">    0     0 RETURN     all  --  *      *       192.168.122.0/24     255.255.255.255</span><br><span class="line">    0     0 MASQUERADE  tcp  --  *      *       192.168.122.0/24    !192.168.122.0/24     masq ports: 1024-65535</span><br><span class="line">    0     0 MASQUERADE  udp  --  *      *       192.168.122.0/24    !192.168.122.0/24     masq ports: 1024-65535</span><br><span class="line">    0     0 MASQUERADE  all  --  *      *       192.168.122.0/24    !192.168.122.0/24</span><br></pre></td></tr></table></figure><blockquote><p>從上面可看出，來自於 192.168.122.0/24 網段且對外的流量，會以 NAT 的形式連外</p></blockquote><h2 id="建立可運行-NAT-mode-的環境"><a href="#建立可運行-NAT-mode-的環境" class="headerlink" title="建立可運行 NAT mode 的環境"></a>建立可運行 NAT mode 的環境</h2><p>確認好目前的環境後，我們便根據環境設定了兩支 script，分別用來產生 &amp; 移除 tap interface for NAT mode：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置 tap interface for NAT mode 的 script</span></span><br><span class="line">$ cat &lt;&lt;\EOF &gt;/etc/qemu-ifup-NAT</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># network information</span></span><br><span class="line">BRIDGE=virbr0</span><br><span class="line">NETWORK=192.168.122.0</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.168.122.1</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$1</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="comment"># 啟用 bridge device for NAT mode</span></span><br><span class="line">  brctl stp <span class="variable">$&#123;BRIDGE&#125;</span> on</span><br><span class="line">  ifconfig <span class="variable">$&#123;BRIDGE&#125;</span> <span class="variable">$&#123;GATEWAY&#125;</span> netmask <span class="variable">$&#123;NETMASK&#125;</span> up</span><br><span class="line"></span><br><span class="line">  ifconfig <span class="string">&quot;<span class="variable">$1</span>&quot;</span> 0.0.0.0 up</span><br><span class="line">  brctl addif <span class="variable">$&#123;BRIDGE&#125;</span> <span class="string">&quot;<span class="variable">$1</span>&quot;</span></span><br><span class="line">  <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;Error: no interface specified.&quot;</span></span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 tap interface for NAT mode 的 script</span></span><br><span class="line">$ cat &lt;&lt;\EOF &gt;/etc/qemu-ifdown-NAT</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># network information</span></span><br><span class="line">BRIDGE=virbr0</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -n <span class="string">&quot;<span class="variable">$1</span>&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;Tearing down network bridge for <span class="variable">$1</span>&quot;</span></span><br><span class="line">  ip link <span class="built_in">set</span> <span class="string">&quot;<span class="variable">$1</span>&quot;</span> down</span><br><span class="line">  brctl delif <span class="variable">$&#123;BRIDGE&#125;</span> <span class="string">&quot;<span class="variable">$1</span>&quot;</span></span><br><span class="line">  <span class="built_in">exit</span> 0</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">&quot;Error: no interface specified.&quot;</span></span><br><span class="line">  <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 為目前的使用者加入</span></span><br><span class="line">$ chmod +x /etc/qemu-if&#123;up,down&#125;-NAT</span><br></pre></td></tr></table></figure><blockquote><p>以上的 script 可能會因為自身的環境不同而有增減，需要特別注意一下</p></blockquote><h2 id="啟動-NAT-mode-virtual-machine"><a href="#啟動-NAT-mode-virtual-machine" class="headerlink" title="啟動 NAT-mode virtual machine"></a>啟動 NAT-mode virtual machine</h2><p>當環境都準備好後，可以使用以下指令啟動 virtual machine：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -net nic -net tap =&gt; 設定 tap interface</span></span><br><span class="line"><span class="comment"># script=/etc/qemu-ifup-NAT,downscript=/etc/qemu-ifdown-NAT =&gt; 使用指定的 script 配置網路</span></span><br><span class="line">$ kvm -vnc 0.0.0.0:1 -m 2048 /kvm/storage/vm_disks/ubnutu1604.img -net nic -net tap,script=/etc/qemu-ifup-NAT,downscript=/etc/qemu-ifdown-NAT --daemonize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 從下面結果可看出 tap0 是與 virbr0 這個 bridge device 相連</span></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name     bridge id               STP enabled     interfaces</span><br><span class="line">br0             8000.2c600cb163d5       yes             ens1f0</span><br><span class="line">virbr0          8000.1a507abb7dff       yes             tap0</span><br><span class="line">                                                        virbr0-nic</span><br></pre></td></tr></table></figure><p>最後確認一下 virtual machine 的狀況：</p><p><img src="https://github.com/godleon/godleon.github.io/blob/master/_posts/images/2016/KVM-Basic-Concept-Networking/kvm_network-NAT-mode-VM.PNG?raw=true" alt="NAT mode - virtual machine"></p><p>從上圖可看出 virtual machine 的確拿到了 192.168.122.0/24 網段中的 ip address，表示 host machine 中的 dnsmasq 運作正常；此外連外也正常，表示防火牆的設定部分也沒有問題</p><hr><h1 id="設定用戶模式-user-mode-網路-內部網路"><a href="#設定用戶模式-user-mode-網路-內部網路" class="headerlink" title="設定用戶模式(user-mode)網路(內部網路)"></a>設定用戶模式(user-mode)網路(內部網路)</h1><p>若在啟動 virtual machine 沒加上 <code>-net</code> 參數，預設 QEMU 就會使用 <code>-net nic -net user</code> 把 virtual machine 設定為 user mode。</p><p>user mode network 完全是由 QEMU 透過 <a href="http://wiki.qemu.org/Documentation/Networking">SLiRP</a> 所實現的一個虛擬 NAT 網路，不用依賴先前所安裝的 <strong>bridge-utils</strong>、<strong>dnsmaq</strong>、<strong>iptables</strong> 等工具，也不需要 root 的權限就可執行。</p><p>以下是 user mode network 的架構圖：</p><p><img src="http://wiki.qemu.org/images/9/93/Slirp_concept.png" alt="SLiRP concept"></p><p>使用參數：<code>-net user[,option][,option][,...]</code></p><p>以下列出比較常用的 option：</p><ul><li><p><code>vlan=n</code>：將 user mode network 連接到 VLAN ID=n 的 VLAN</p></li><li><p><code>net=addr[/mask]</code>：設定 virtual machine 的 ip address (預設為 <strong><font color='red'>10.0.2.0/24</font></strong>)</p></li><li><p><code>host=addr</code>：設定 host machine 的 ip address (預設為 <strong><font color='red'>10.0.2.2</font></strong>)</p></li><li><p><code>dns=addr</code>：設定 virtual DNS 的 ip address (預設為 <strong><font color='red'>10.0.2.3</font></strong>)</p></li><li><p><code>dhcpstart=addr</code>：設定 DHCP 的第一個 ip address (預設為 <strong><font color='red'>10.0.2.15</font></strong>)</p></li><li><p><code>restrict=y|yes|n|no</code>：若設定 yes，virtual machine 將無法與 host machine 通訊，但不會影響 hostfwd 的設定</p></li><li><p><code>hostfwd=[tcp|udp]:[hostaddr]:hostport-[guestaddr]:guestport</code>：將 host machine 中的 hostport 導向 virtual machine 的 guestport (可一次定義多個 hostfwd 設定)</p></li></ul><p>以下是簡單的應用範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -net nic -net user =&gt; user mode network</span></span><br><span class="line"><span class="comment"># hostfwd=tcp::5022-:22 =&gt; 將 host machine 的 tcp port 5022 導向 virtual machine 的 tcp port 22</span></span><br><span class="line">$ kvm -vnc 0.0.0.0:1 -m 2048 /kvm/storage/vm_disks/ubnutu1604.img -net nic -net user,hostfwd=tcp::5022-:22 --daemonize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 tcp port 5022 連線到 virtual machine</span></span><br><span class="line">$ ssh -p 5022 ubuntu@10.20.190.2</span><br><span class="line">ubuntu@10.20.190.2\<span class="string">&#x27;s password:</span></span><br><span class="line"><span class="string">.......</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 查詢 ip address</span></span><br><span class="line"><span class="string">ubuntu@vm-ubuntu1604:~$ ip addr show</span></span><br><span class="line"><span class="string">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span></span><br><span class="line"><span class="string">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span></span><br><span class="line"><span class="string">    inet 127.0.0.1/8 scope host lo</span></span><br><span class="line"><span class="string">       valid_lft forever preferred_lft forever</span></span><br><span class="line"><span class="string">    inet6 ::1/128 scope host</span></span><br><span class="line"><span class="string">       valid_lft forever preferred_lft forever</span></span><br><span class="line"><span class="string">2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span></span><br><span class="line"><span class="string">    link/ether 52:54:00:12:34:56 brd ff:ff:ff:ff:ff:ff</span></span><br><span class="line"><span class="string">    inet 10.0.2.15/24 brd 10.0.2.255 scope global ens3</span></span><br><span class="line"><span class="string">       valid_lft forever preferred_lft forever</span></span><br><span class="line"><span class="string">    inet6 fe80::5054:ff:fe12:3456/64 scope link</span></span><br><span class="line"><span class="string">       valid_lft forever preferred_lft forever</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 實際上是可以上網的</span></span><br><span class="line"><span class="string">ubuntu@vm-ubuntu1604:~$ curl www.google.com.tw</span></span><br><span class="line"><span class="string">..........(省略一大堆內容)</span></span><br></pre></td></tr></table></figure><p>必須一提的是，使用 user mode network 也是有些缺點在的，像是：</p><ol><li><p>由於網路都是由 QEMU 虛擬出來的，因此效能比起其他模式相對差一些</p></li><li><p>不支援部分網路功能(例如：ICMP)，所以也就無法使用 ping command</p></li><li><p>無法從 host machine or 外部直接存取 virtual machine</p></li></ol><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://linux.die.net/man/1/qemu-kvm">qemu-kvm(1): QEMU Emulator User Documentation - Linux man page</a></p></li><li><p><a href="http://wiki.qemu.org/download/qemu-doc.html">QEMU Emulator User Documentation</a></p></li><li><p><a href="http://www.it610.com/article/4332744.htm">桥网络配置 - info5 - IT610.com</a></p></li><li><p><a href="http://smilejay.com/2012/03/kvm_qemu_network_error/">KVM “qemu-ifup: could not configure /dev/net/tun: Operation not permitted”解决方案 – 笑遍世界</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] Linux KVM concept - Storage</title>
      <link href="/blog/KVM/KVM-Basic-Concept-Storage/"/>
      <url>/blog/KVM/KVM-Basic-Concept-Storage/</url>
      
        <content type="html"><![CDATA[<p>此篇文章介紹學習 Linux KVM 時所需要了解的 Storage 相關知識</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在 Linux KVM 中，是由 QEMU 來提供 storage device 的模擬，可以模擬的 storage 類型很多，包含 IDE device / SCSI device / Floopy disk / USB disk / virtio disk …. 等等，可根據使用者需求的不同來提供不同組合以及不同的開機順序。</p><hr><h1 id="Host-Machine-Swap-空間應該要設定多大"><a href="#Host-Machine-Swap-空間應該要設定多大" class="headerlink" title="Host Machine Swap 空間應該要設定多大?"></a>Host Machine Swap 空間應該要設定多大?</h1><p>以前學 Linux 時知道，swap 是用來作為當記憶體不足時的 buffer 用，而現在虛擬化環境中，每台 host machine 的記憶體容量都很大，那 swap 應該如何規劃配置呢? 以下有幾個原則：</p><h3 id="不使用-memory-overcommit"><a href="#不使用-memory-overcommit" class="headerlink" title="不使用 memory overcommit"></a>不使用 memory overcommit</h3><ol><li><p>若實體記憶體 &lt;= 4GB，則 swap space 設定為 2GB</p></li><li><p>若 4GB &lt; 實體記憶體 &lt;= 16GB，則 swap space 設定為 4GB</p></li><li><p>若 16GB &lt; 實體記憶體 &lt;= 64GB，則 swap space 設定為 8GB</p></li><li><p>若 64GB &lt; 實體記憶體 &lt;= 526GB，則 swap space 設定為 16GB</p></li></ol><h3 id="使用-memory-overcommit"><a href="#使用-memory-overcommit" class="headerlink" title="使用 memory overcommit"></a>使用 memory overcommit</h3><p>假設配置 memory overcommit rate 為 0.5 (例如：128 GB 的記憶體卻配置 <code>128 * (1 + 0.5) = 192 GB</code>)，那 swap space 除了上面的大小外，還要額外在加上記憶體容量 x 0.5 的空間。</p><p>亦即設定 memory overcommit，還要多增加 <strong><font color='red'>physical memory x memory overcommit rate</font></strong> 的容量大小給 swap space。</p><p>舉個實際例子，假設 host machine 有 32GB 記憶體，memory overcommit ratio 設定為 0.5，則 swap space 的容量計算如下：</p><blockquote><p>(32 * 0.5) + 8 = 24 GB</p></blockquote><hr><h1 id="Storage-Device-amp-開機順序的配置"><a href="#Storage-Device-amp-開機順序的配置" class="headerlink" title="Storage Device &amp; 開機順序的配置"></a>Storage Device &amp; 開機順序的配置</h1><p>在 <strong>qemu-kvm</strong> 可以直接在命令行中進行 storage device &amp; 開機順序的配置，以下是常用的參數：</p><h2 id="Storage-Devices"><a href="#Storage-Devices" class="headerlink" title="Storage Devices"></a>Storage Devices</h2><ul><li><code>-hd[a:d] file_path</code>：以 file_path 指定的 image 檔案作為系統的 IDE HDD，在 virtual machine 中會以 <strong>/dev/hd[a:d]</strong> or <strong>/dev/sd[a:d]</strong> 呈現</li></ul><blockquote><p>若在 qemu-kvm 中直接指定檔案而沒有設定 -hd[a:d]，則預設為 <strong>-hda</strong></p></blockquote><ul><li><p><code>-fd[a:b] file_path</code>：以 file_path 指定的 image 檔案作為系統的軟碟機，在 virtual machine 中會以 <strong>/dev/fd[a:b]</strong> 呈現</p></li><li><p><code>-cdrom file_path</code>：用來指定 iso 檔案作為 virtual machine 的 <strong>/dev/cdrom</strong> 時使用：不可與 <strong>-hdc</strong> 參數同時使用，會衝突</p></li><li><p><code>-driver option[, option[, option[, option[,....]]]]</code>：透過 <strong>-driver</strong> 參數指定 storage device 的設定細節</p></li></ul><p>由於以上的設定參數太多，所以這邊直接貼 man page 的內容來看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">-drive option[,option[,option[,...]]]</span><br><span class="line">    Define a new drive. Valid options are:</span><br><span class="line">    file=file</span><br><span class="line">      This option defines <span class="built_in">which</span> disk image to use with this drive. If the filename contains comma, you must double it (<span class="keyword">for</span> instance, <span class="string">&quot;file=my,,file&quot;</span> to use file <span class="string">&quot;my,file&quot;</span>).</span><br><span class="line">    <span class="keyword">if</span>=interface</span><br><span class="line">      This option defines on <span class="built_in">which</span> <span class="built_in">type</span> on interface the drive is connected. Available types are: ide, scsi, sd, mtd, floppy, pflash, virtio.</span><br><span class="line">    bus=bus,unit=unit</span><br><span class="line">      These options define <span class="built_in">where</span> is connected the drive by defining the bus number and the unit id.</span><br><span class="line">    index=index</span><br><span class="line">      This option defines <span class="built_in">where</span> is connected the drive by using an index <span class="keyword">in</span> the list of available connectors of a given interface <span class="built_in">type</span>.</span><br><span class="line">    media=media</span><br><span class="line">      This option defines the <span class="built_in">type</span> of the media: disk or cdrom.</span><br><span class="line">    cyls=c,heads=h,secs=s[,trans=t]</span><br><span class="line">      These options have the same definition as they have <span class="keyword">in</span> -hdachs.</span><br><span class="line">    snapshot=snapshot</span><br><span class="line">      snapshot is <span class="string">&quot;on&quot;</span> or <span class="string">&quot;off&quot;</span> and allows to <span class="built_in">enable</span> snapshot <span class="keyword">for</span> given drive (see -snapshot).</span><br><span class="line">    cache=cache</span><br><span class="line">      cache is <span class="string">&quot;none&quot;</span>, <span class="string">&quot;writeback&quot;</span>, <span class="string">&quot;unsafe&quot;</span>, or <span class="string">&quot;writethrough&quot;</span> and controls how the host cache is used to access block data.</span><br><span class="line">    aio=aio</span><br><span class="line">      aio is <span class="string">&quot;threads&quot;</span>, or <span class="string">&quot;native&quot;</span> and selects between pthread based disk I/O and native Linux AIO .</span><br><span class="line">    format=format</span><br><span class="line">      Specify <span class="built_in">which</span> disk format will be used rather than detecting the format. Can be used to specifiy format=raw to avoid interpreting an untrusted format header.</span><br><span class="line">    serial=serial</span><br><span class="line">      This option specifies the serial number to assign to the device.</span><br><span class="line">    addr=addr</span><br><span class="line">      Specify the controller\<span class="string">&#x27;s PCI address (if=virtio only).</span></span><br><span class="line"><span class="string">    copy-on-read=copy-on-read</span></span><br><span class="line"><span class="string">      copy-on-read is &quot;on&quot; or &quot;off&quot; and enables whether to copy read backing file sectors into the image file.</span></span><br></pre></td></tr></table></figure><p>在預設情況下，QEMU 會為所有的 block device 自動帶上 writethrough caching 的功能，這功能是利用 host page cache 所達成的，效率較差，但安全性高。</p><p>而另外一個 Writeback caching 可以提供更好的效率，但安全性較低，因為若是 host machine 突然掛點或發生電力中斷的情況，cache 中的資料很有可能會遺失；若是加上 <code>snapshot</code> 參數，則會預設使用 Writeback caching。</p><p>若是要完全關閉 cache 功能，可以加上參數 <code>cache=none</code>。</p><p>此外，其實每一個 storage device 都可以用 <code>-drive</code> 來設定，舉例來說：</p><ul><li><p><code>-cdrom file_path</code> = <code>-drive file=file,index=2,media=cdrom</code></p></li><li><p><code>-hda file_path</code> = <code>-drive file=file,index=0,media=disk</code></p></li><li><p><code>-fda file_path</code> = <code>-drive file=file,index=0,if=floppy</code></p></li></ul><h2 id="開機順率"><a href="#開機順率" class="headerlink" title="開機順率"></a>開機順率</h2><p>開機順序的參數設定如下：</p><blockquote><p>-boot [order=drives][,once=drives][,menu=on|off][,splash=sp_name][,splash-time=sp_time][,reboot-timeout=rb_timeout][,strict=on|off]</p></blockquote><p>QEMU 的開機順序，是以英文字母表示：</p><ul><li><p><code>a</code>：第一台軟碟機</p></li><li><p><code>b</code>：第二台軟碟機</p></li><li><p><code>c</code>：第一顆硬碟</p></li><li><p><code>d</code>：光碟機</p></li><li><p><code>n</code>：網路啟動</p></li></ul><p>以下是幾個簡單的範例：</p><ul><li><p><code>-boot order=d</code>：光碟開機</p></li><li><p><code>-boot once=d</code>：第一次以光碟開機，系統重開後以第一顆硬碟開機</p></li></ul><p>此外其他參數的說明：</p><ul><li><p><code>menu=on|off</code>：是否顯示開機選單</p></li><li><p><code>splash=sp_name</code> &amp; <code>splash-time=sp_time</code>：在 <code>menu=on</code> 時才有效，<code>splash</code> 設定開機 logo，<code>splash-time</code> 設定開機 logo 時間</p></li><li><p><code>-boot order=dc,menu=on</code>：開機順序為光碟機 &gt;&gt; 硬碟，顯示開機選單</p></li></ul><hr><h1 id="qemu-img"><a href="#qemu-img" class="headerlink" title="qemu-img"></a>qemu-img</h1><p><strong><a href="http://wiki.qemu.org/download/qemu-doc.html#qemu_005fimg_005finvocation">qemu-img</a></strong> 是 QEMU 的磁碟管理工具，安裝好 QEMU 相關套件時就會存在於系統中，基本使用語法如下：</p><blockquote><p>qemu-img command [command options]</p></blockquote><p>以下就常用的相關命令 &amp; 參數進行介紹：</p><ul><li><code>create [-f fmt] [-o options] filename [size]</code>：建立 disk image；其中 <code>options</code> 的部分可以使用 <code>-o ?</code> 來查詢特定格式所支援的選項；<code>size</code> 則是支援 <code>k</code>(KB) / <code>M</code>(MB) / <code>G</code>(GB) / <code>T</code>(TB) 等大小。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示目前 qcow2 格式支援的 options</span></span><br><span class="line">$ qemu-img create -f qcow2 -o ?</span><br><span class="line">Supported options:</span><br><span class="line">size             Virtual disk size</span><br><span class="line">compat           Compatibility level (0.10 or 1.1)</span><br><span class="line">backing_file     File name of a base image</span><br><span class="line">backing_fmt      Image format of the base image</span><br><span class="line">encryption       Encrypt the image</span><br><span class="line">cluster_size     qcow2 cluster size</span><br><span class="line">preallocation    Preallocation mode (allowed values: off, metadata, falloc, full)</span><br><span class="line">lazy_refcounts   Postpone refcount updates</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增檔名為 ubuntu1604.qcow2，格式為 qcow2，大小為 10GB 的 disk image</span></span><br><span class="line">$ qemu-img create -f qcow2 ubuntu1604.qcow2 10G</span><br><span class="line">Formatting <span class="string">&#x27;ubuntu1604.qcow2&#x27;</span>, fmt=qcow2 size=10737418240 encryption=off cluster_size=65536 lazy_refcounts=off</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增有 backing file(centos7.img，原生大小為 10GB) 的 disk image</span></span><br><span class="line"><span class="comment"># disk image 檔名為 centos7.qcow2，格式為 qcow2，大小為 20GB</span></span><br><span class="line">$ qemu-img create -f qcow2 centos7.img 10G</span><br><span class="line">Formatting <span class="string">&#x27;centos7.img&#x27;</span>, fmt=qcow2 size=10737418240 encryption=off cluster_size=65536 lazy_refcounts=off</span><br><span class="line">$ qemu-img create -f qcow2 -o backing_file=centos7.img,size=20G centos7.qcow2</span><br><span class="line">Formatting <span class="string">&#x27;centos7.qcow2&#x27;</span>, fmt=qcow2 size=21474836480 backing_file=<span class="string">&#x27;centos7.img&#x27;</span> encryption=off cluster_size=65536 lazy_refcounts=off</span><br></pre></td></tr></table></figure><ul><li><code>check [-f fmt] filename</code>：對 disk image 進行一致性 &amp; 錯誤的檢查，但目前只有支援 qcow2 / qed / vdi 等格式；若不加上 <code>[-f fmt]</code> 參數則會自動偵測格式</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ qemu-img check centos7.qcow2</span><br><span class="line">No errors were found on the image.</span><br><span class="line">Image end offset: 262144</span><br></pre></td></tr></table></figure><ul><li><p><code>commit [-f fmt] filename</code>：將指定 disk image 的修改 commit 到 backing file 中(例如上面範例中，把 centos7.qcow2 的修改 commit 回 centos7.img 中)</p></li><li><p><code>convert [-c] [-f fmt] [-O output_fmt] [-o options] filename [filename2 [...]] output_filename</code>：轉換 disk image 的格式；input file 的格式可自動偵測，但 output file 則需要指定，不指定則預設為 <code>raw</code>；<code>-c</code> 則是指定對 output file 進行壓縮，但只支援 qcow &amp; qcow2 格式的檔案；<code>-o options</code> 用來指定各種選項，例如 backing file，檔案大小、是否加密…等，附帶一提，使用 backing file 可以使產生的檔案變成 copy-on-write 的增量檔案</p></li></ul><blockquote><p>若是由 raw 轉成 qcow2，一般來說檔案還會有瘦身的效果</p></blockquote><ul><li><code>info [-f fmt] filename</code>：顯示 disk image 的詳細資訊</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ qemu-img info /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">image: /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">file format: raw</span><br><span class="line">virtual size: 8.0G (8589934592 bytes)</span><br><span class="line">disk size: 8.0G</span><br><span class="line"></span><br><span class="line">$  qemu-img info centos7.qcow2</span><br><span class="line">image: centos7.qcow2</span><br><span class="line">file format: qcow2</span><br><span class="line">virtual size: 20G (21474836480 bytes)</span><br><span class="line">disk size: 196K</span><br><span class="line">cluster_size: 65536</span><br><span class="line">backing file: centos7.img</span><br><span class="line">Format specific information:</span><br><span class="line">    compat: 1.1</span><br><span class="line">    lazy refcounts: <span class="literal">false</span></span><br></pre></td></tr></table></figure><ul><li><p><code>snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename</code>：snapshot 的管理，包含查詢 snapshot 列表資訊(<code>-l</code>) / 使用快照作為 disk image(<code>-a snapshot</code>) / 建立 snapshot(<code>-c snapshot</code>) / 刪除 snapshot(<code>-d snapshot</code>)</p></li><li><p><code>resize filename [+ | -]size</code>：改變 disk image 的大小；縮小空間要確保 disk image 有足夠的空間，否則會有資料損毀的風險；qcow2 格式不支援縮小；增加空間後，在 virtual machine 中需要使用 fdisk or parted 才可以使用到增加的空間</p></li></ul><blockquote><p>強烈建議在 resize 之前要做好備份</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@ocp-kvm-host ~]<span class="comment"># qemu-img resize ubuntu1604.qcow2 +1G</span></span><br><span class="line">Image resized.</span><br><span class="line"></span><br><span class="line">[root@ocp-kvm-host ~]<span class="comment"># qemu-img info ubuntu1604.qcow2</span></span><br><span class="line">image: ubuntu1604.qcow2</span><br><span class="line">file format: qcow2</span><br><span class="line">virtual size: 11G (11811160064 bytes)</span><br><span class="line">disk size: 200K</span><br><span class="line">cluster_size: 65536</span><br><span class="line">Format specific information:</span><br><span class="line">    compat: 1.1</span><br><span class="line">    lazy refcounts: <span class="literal">false</span></span><br></pre></td></tr></table></figure><hr><h1 id="Storage-Types"><a href="#Storage-Types" class="headerlink" title="Storage Types"></a>Storage Types</h1><p>在 QEMU/KVM 中，virual machine 的 disk image 其實可以用很多種不同方式來儲存，例如：</p><ol><li><p>Local storage (最常用的方式)</p></li><li><p>physical disk/partition</p></li><li><p>LVM</p></li><li><p>NFS</p></li><li><p>iSCSI</p></li><li><p>在 Local 端 or 透過 Fiber 連接的 LUN</p></li><li><p>GFS2</p></li></ol><blockquote><p>上面的幾個選項中，phtsical partition &amp; LVM 由於無法存放 MBR，因此無法作為 virtual machine 的開機磁碟，只能用來存放資料用。</p></blockquote><p>基本上，以 file-based 的方式管理 disk images 是比較有彈性的，可放在 Local / NFS / iSCSI / LUN / GFS2 … 等，若是 virtual machine 對於 I/O 效能不是非常要求時，選擇 qcow2 則是較為建議的 disk image 格式，其優點如下：</p><ul><li><p>儲存方便 (file-baed)</p></li><li><p>比起其他方式相對容易使用</p></li><li><p>可移動性佳</p></li><li><p>容易複製</p></li><li><p>可透過 sparse 的機制節省硬碟空間(類似 thin provision)</p></li><li><p>可放在透過網路連結的檔案系統中(例如：NFS，可把 backing file 放在 NFS，差異部分放在 local)，因此可以輕鬆達到 live migration 的效果</p></li></ul><p>最後，若需要高效能的 I/O，則可以使用半虛擬化的 <strong><font color='red'>virtio</font></strong> 作為 disk image 的 driver。</p><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://linux.die.net/man/1/qemu-kvm">qemu-kvm(1): QEMU Emulator User Documentation - Linux man page</a></p></li><li><p><a href="http://wiki.qemu.org/download/qemu-doc.html">QEMU Emulator User Documentation</a></p></li><li><p><a href="http://benjr.tw/20361">Cache 的 write back 和 write through – Benjr.tw</a></p></li><li><p><a href="http://www.pcdvd.com.tw/showthread.php?t=285217">請教Cache運作方式的Write Back與Write Through之分別 - PCDVD數位科技討論區</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] Linux KVM concept - Memory</title>
      <link href="/blog/KVM/KVM-Basic-Concept-Memory/"/>
      <url>/blog/KVM/KVM-Basic-Concept-Memory/</url>
      
        <content type="html"><![CDATA[<p>此篇文章介紹學習 Linux KVM 時所需要了解的 Memory 相關知識</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Memory 在作業系統是用來暫時存放 cpu 要執行的指令以及資料，所有 process 都必須先載入到 memory 中才能正確執行。</p><p>而在虛擬化的環境中，virtual machine 中 memory 的使用是需要額外的 mapping 機制對應到 host machine，在這個部分的效率的高低，當然也就會決定 virtual machine 整體的系統性能。</p><hr><h1 id="VM-可以使用多少記憶體"><a href="#VM-可以使用多少記憶體" class="headerlink" title="VM 可以使用多少記憶體?"></a>VM 可以使用多少記憶體?</h1><p>也許這是很多人想要了解的，在 host machine 上安裝了一大堆實體記憶體，究竟可以分配給 VM 的可以有多少呢? 以下有兩個簡單公式可以計算：</p><ol><li><p>實體記憶體 &lt;= 64 GB</p><blockquote><p>RAM - 2 GB = Amount of RAM available to VMs in GBs</p></blockquote></li><li><p>實體記憶體 &gt; 64GB</p><blockquote><p>RAM - (2 GiB + .5* (RAM/64)) = Amount of RAM available to VMs in GBs</p></blockquote></li></ol><p>假設 host machine 記憶體有 32GB，則一共可以配置 <code>32 - 2 = 10</code>GB 的記憶體給 VM。</p><p>假設 host machine 記憶體有 256GB，則一共可以配置 <code>256 - (2 + 0.5 * (256 / 64)) = 252</code>GB 的記憶體給 VM</p><hr><h1 id="配置-VM-記憶體"><a href="#配置-VM-記憶體" class="headerlink" title="配置 VM 記憶體"></a>配置 VM 記憶體</h1><p>幫 virtual machine 配置記憶體是很容易的，只要使用 <strong><font color='red'>-m</font></strong> 參數即可，預設的格式是：</p><blockquote><p>-m megs</p></blockquote><p>預設是以 <strong><font color='red'>MB</font></strong> 作為預設的單位，也可以使用 <strong><font color='red'>G</font></strong> 表示要使用 GB 為單位，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 記憶體大小為 2048 MB</span></span><br><span class="line">$ kvm -vnc 0.0.0.0:1 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line"></span><br><span class="line"><span class="comment"># 記憶體大小為 4 GB</span></span><br><span class="line">$ kvm -vnc 0.0.0.0:1 -m 4G -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br></pre></td></tr></table></figure><hr><h1 id="EPT"><a href="#EPT" class="headerlink" title="EPT"></a>EPT</h1><p>傳統要把 virtual machine 中運行的應用程式所使用的 memory 對應到 host machine 的 memory，一共是有三層關係的，下圖可做個簡單說明：</p><p><img src="http://m.eet.com/media/1200411/CloudFig4a.jpg" alt="VM memory mapping"></p><blockquote><p>Guest Virtual Address(GVA) &lt;–&gt; Guest Physical Address(GPA) &lt;–&gt; Host Physical Address(HPA)</p></blockquote><p>但由於上面三層的轉換效率是很差的，因此後來透過軟體實作了稱為 <strong><font color='red'>Shadow Page Tables</font></strong> 的機制，將三層中的第二層拿掉，直接讓 Guest Virtual Address 與 Host Physical Address 可以有直接對應的機制：</p><p><img src="http://image.slidesharecdn.com/windays-virtualization-090428034228-phpapp01/95/windows-server-virtualization-hyperv-2008-r2-30-728.jpg?cb=1240890219" alt="Shadow Page Tables"></p><p>此時 hypervisor 就可以把 shadow page tables 載入到 MMU(Memory Management Unit) 中進行 address translation 的工作。</p><p>但 shadow page tables 實作起來不僅複雜，也會 memory 的額外消耗(每一個 virtual machine 都需要一個 shadow page table)；因此 Intel 提出了 EPT(Extended Page Tables)，AMD 提出了 NPT(Nested Page Tables)，在硬體層直接提供了 <code>GVA &lt;--&gt; GPA &lt;--&gt; HPA</code> 的轉換，不僅提升了 memory 虛擬化的效能，也降低了 memory 虛擬化的複雜度。</p><p>以下是 Intel EPT 技術的概觀：</p><p><img src="http://virtualization.info/images/EPT-716833.png" alt="Intel EPT"></p><p>其中可以看到 Intel 在硬體中增加了 CR3(Control Registor 3) 來處理 GVA &lt;–&gt; GPA 的轉換，以及 EPT 來處理 GPA &lt;–&gt; HPA 的轉換。</p><p>由於所有的轉換都在硬體層級完成，因此速度很快；而且由於整個轉換過程只需要一個 EPT Page Table，因此在 memory 的消耗上也相對的低。</p><hr><h1 id="VPID"><a href="#VPID" class="headerlink" title="VPID"></a>VPID</h1><p>要了解 VPID，首先要知道 TLB(Translation Lookaside Buffer) 是什麼? 可以參考以下連結：</p><ul><li><a href="http://www.wikiwand.com/zh-hk/%E8%BD%89%E8%AD%AF%E5%BE%8C%E5%82%99%E7%B7%A9%E8%A1%9D%E5%8D%80">TLB 轉譯後備緩衝區 - Wikiwand</a></li></ul><blockquote><p>分配給 virtual machine 的每一個 vCPU 都會有一個 TLB</p></blockquote><p>而 VPID 則是在硬體層級對 TLB 資源管理的優化，在 virtual machine 進行 migration / VM Entry / VM Exit 時，避免對 TLB 進行轉存 &amp; 清除，進而降低 memory 的額外消耗，對於 live migration 有顯著的效能提升。</p><hr><h1 id="查詢-EPT-amp-VPID-的支援度"><a href="#查詢-EPT-amp-VPID-的支援度" class="headerlink" title="查詢 EPT &amp; VPID 的支援度"></a>查詢 EPT &amp; VPID 的支援度</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢 CPU 是否支援 EPT &amp; VPID</span></span><br><span class="line">$  grep -E <span class="string">&quot;\sept\s|\svpid\s&quot;</span> /proc/cpuinfo | uniq</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 kvm_intel 模組是否有開啟 EPT &amp; VPID 的功能</span></span><br><span class="line">$ cat /sys/module/kvm_intel/parameters/ept</span><br><span class="line">Y</span><br><span class="line">$ cat /sys/module/kvm_intel/parameters/vpid</span><br><span class="line">Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 開啟 kvm_intel 模組 EPT &amp; VPID 的功能</span></span><br><span class="line">$ modprobe kvm_intel ept=1,vpid=1</span><br></pre></td></tr></table></figure><hr><h1 id="Huge-Page"><a href="#Huge-Page" class="headerlink" title="Huge Page"></a>Huge Page</h1><p>x86 架構的 CPU 預設的 memory page table 大小為 4KB，而 x86-64 則可以支援到 2MB 大小的 memory page table(亦稱為 <strong><font color='red'>Huge Page</font></strong>)，在 Linux 2.6 以上的 kernel 都支援這個特性。</p><p>而使用 Huge Page 有何優缺點呢?</p><h3 id="優點："><a href="#優點：" class="headerlink" title="優點："></a>優點：</h3><ul><li><p>page table 數量減少，更節省 memory</p></li><li><p>由於 memory address translation 的工作減少了，因此 page fault 機率降低為 <strong><font color='red'>Huge Page Size / 4KB</font></strong> 分之一</p></li><li><p>提升了 memory 存取的效能</p></li><li><p>提高 TLB 命中率，因而減少 CPU cache 的使用，最後提升了系統整體效能</p></li><li><p>適合用在 memory 存取密集的 virtual machine 上</p></li></ul><h3 id="缺點："><a href="#缺點：" class="headerlink" title="缺點："></a>缺點：</h3><ul><li><p>Huge Page 無法被 swap out 到硬碟上</p></li><li><p>無法使用 Ballooning 的方式自動增長</p></li><li><p>並非適合所有不同工作類型的 virtual machine</p></li></ul><h2 id="在-KVM-中使用-Huge-Page"><a href="#在-KVM-中使用-Huge-Page" class="headerlink" title="在 KVM 中使用 Huge Page"></a>在 KVM 中使用 Huge Page</h2><h3 id="1、檢查-host-machine-中-Huge-Page-的設定資訊："><a href="#1、檢查-host-machine-中-Huge-Page-的設定資訊：" class="headerlink" title="1、檢查 host machine 中 Huge Page 的設定資訊："></a>1、檢查 host machine 中 Huge Page 的設定資訊：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目前預設的 page size</span></span><br><span class="line">$ getconf PAGESIZE</span><br><span class="line">4096</span><br><span class="line"></span><br><span class="line"><span class="comment"># 記憶體資訊中 Huge Page 的相關資訊 (size = 2048 KB)</span></span><br><span class="line">$ cat /proc/meminfo | grep Huge</span><br><span class="line">AnonHugePages:     14336 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br></pre></td></tr></table></figure><h3 id="2、掛載-hugetlbfs-檔案系統"><a href="#2、掛載-hugetlbfs-檔案系統" class="headerlink" title="2、掛載 hugetlbfs 檔案系統"></a>2、掛載 hugetlbfs 檔案系統</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 掛載 hugetlbfs 檔案系統</span></span><br><span class="line">$ mount -t hugetlbfs hugetlbfs /dev/hugepages</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢檔案系統資訊</span></span><br><span class="line">$ mount | grep huge</span><br><span class="line">cgroup on /sys/fs/cgroup/hugetlb <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)</span><br><span class="line">hugetlbfs on /dev/hugepages <span class="built_in">type</span> hugetlbfs (rw,relatime,seclabel)</span><br><span class="line">hugetlbfs on /dev/hugepages <span class="built_in">type</span> hugetlbfs (rw,relatime,seclabel)</span><br></pre></td></tr></table></figure><h3 id="3、設定-Huge-Page-的數量"><a href="#3、設定-Huge-Page-的數量" class="headerlink" title="3、設定 Huge Page 的數量"></a>3、設定 Huge Page 的數量</h3><p>假設要啟動一個 memory 為 2048 MB 的 virtual machine，可以算出 Huge Page(2048 KB) 的數量為：</p><blockquote><p>2048 * 1024 / 2048 = 1024</p></blockquote><p>因此這邊設定 Huge Page 的數量為 1024 個：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定 Huge Page 的數量為 1024 個</span></span><br><span class="line">$ sysctl vm.nr_hugepages=1024</span><br><span class="line">vm.nr_hugepages = 1024</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此時 host machine 中的記憶體資訊已經有 Huge Page 的數量資訊</span></span><br><span class="line">$ cat /proc/meminfo | grep Huge</span><br><span class="line">AnonHugePages:     14336 kB</span><br><span class="line">HugePages_Total:    1024</span><br><span class="line">HugePages_Free:     1024</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br></pre></td></tr></table></figure><h3 id="4、啟動-virtual-machine-使用-Huge-Page"><a href="#4、啟動-virtual-machine-使用-Huge-Page" class="headerlink" title="4、啟動 virtual machine 使用 Huge Page"></a>4、啟動 virtual machine 使用 Huge Page</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kvm -vnc 0.0.0.0:1 -smp 4 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img -mem-path /dev/hugepages</span><br><span class="line"></span><br><span class="line">$ cat /proc/meminfo | grep Huge</span><br><span class="line">AnonHugePages:     26624 kB</span><br><span class="line">HugePages_Total:    1024</span><br><span class="line">HugePages_Free:      738</span><br><span class="line">HugePages_Rsvd:      738</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br></pre></td></tr></table></figure><p>可以看出 virtual machine 的確消耗了一些 huge page，但其實並不是全部，因為 virtual machine 實際上並沒有完整分配到 2048 MB 的 memory；若要完整分配指定的 memory，則需要加上 <code>-mem-prealloc</code> 參數。</p><hr><h1 id="Memory-Overcommit"><a href="#Memory-Overcommit" class="headerlink" title="Memory Overcommit"></a>Memory Overcommit</h1><p>除了之前介紹 CPU 可以 overcommit 之外，memory 也可以設定一定程度的 overcommit，原因是因為每台電腦在運作時一般都不會耗盡記憶體。對 host machine 來說，virtual machine 也只是一個 QEMU process，在啟動的當下是不會分配完整記憶體的，而是隨著 virtual machine 的更多要求下逐步分配到位，因此可以在此行為的前提下設定 memory overcommit。</p><p>在 KVM 中有三種方式可以達到 memory overcommit：</p><ol><li><p>**<font color='red'>Swapping</font>**：透過 system swap(一般為硬碟空間) 來彌補 memory 不足的問題</p></li><li><p>**<font color='red'>Ballooning</font>**：透過 <code>virtio_balloon</code> driver 來達成</p></li><li><p>**<font color='red'>Page Sharing</font>**：使用 <code>KSM(Kernel Samepage Merging)</code> 合併多台 virtual machine 中相同的 memory page</p></li></ol><p>關於第一個方式的 swap，根據 <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch-swapspace.html#tb-recommended-system-swap-space">RedHat RHEL 7 官方文件</a>所提供的建議，swap 大小的設定建議如下：</p><table><thead><tr><th>系統記憶體</th><th>建議 swap 大小</th></tr></thead><tbody><tr><td>⩽ 2 GB</td><td>2 x memory size</td></tr><tr><td>&gt; 2 GB – 8 GB</td><td>等同 memory size</td></tr><tr><td>&gt; 8 GB – 64 GB</td><td>至少 4 GB</td></tr><tr><td>&gt; 64 GB</td><td>至少 4 GB</td></tr></tbody></table><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://linux.die.net/man/1/qemu-kvm">qemu-kvm(1): QEMU Emulator User Documentation - Linux man page</a></p></li><li><p><a href="http://www.gegugu.com/2016/03/22/9088.html">精品：KVM學習筆記 : 歌穀穀</a></p></li><li><p><a href="http://tc.wangchao.net.cn/it/detail_128378.html">Huge Page 是否是拯救性能的萬能良藥？ - 王朝網路 - wangchao.net.cn</a></p></li><li><p><a href="http://brandon-hy-lin.blogspot.tw/2016/04/compound-page-huge-page-transparent.html">隨意寫寫: Compound Page, Huge Page, 和Transparent Huge Page(THP)</a></p></li><li><p><a href="http://jaychu649.blogspot.tw/2014/09/linux-huge-memory-greater-then-hundred.html">IT 研究室 ( 前IT DBA’s 資訊站): 如何在Linux 使用大記憶體(huge memory greater then hundred of GB)</a></p></li><li><p><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch-swapspace.html#tb-recommended-system-swap-space">RedHat RHEL 7 Support &gt; Product Documentation &gt; Storage &gt; Administration Guide</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] Linux KVM concept - CPU</title>
      <link href="/blog/KVM/KVM-Basic-Concept-CPU/"/>
      <url>/blog/KVM/KVM-Basic-Concept-CPU/</url>
      
        <content type="html"><![CDATA[<p>此篇文章介紹學習 Linux KVM 時所需要了解的 CPU 相關知識</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>KVM 在 Linux x86 硬體平台上提供了全虛擬化(Full Virtualization)的 solution，透過 QEMU 的模擬，顯示特定數量的 CPU &amp; 相關 feature 給使用者，而在支援 KVM 的前提下，virtual machine 的 CPU 指令則是直接由 CPU 來輔助執行，藉此大幅提升運作效率。</p><hr><h1 id="vCPU"><a href="#vCPU" class="headerlink" title="vCPU"></a>vCPU</h1><p>QEMU/KVM 提供每一台 virtual machine 有一個模擬的完整硬體環境，在 virual machine 中所看到的 CPU，即是 host machine 上的 vCPU。</p><p>在 KVM 環境中，每一台 virtual machine 都是一個的 QEMU userspace process，而 vCPU 則是 QEMU process 中的 thread。</p><p><img src="http://image.slidesharecdn.com/els305-100323102407-phpapp02/95/virtualization-with-kvm-kernelbased-virtual-machine-4-728.jpg" alt="KVM environment"></p><p>vCPU 一共有以下三種執行模式：</p><ol><li><p>User Mode</p></li><li><p>Kernel Mode</p></li><li><p>Guest Mode</p></li></ol><p>其中前兩個執行模式(<strong>User Mode</strong> &amp; <strong>Kernel Mode</strong>)是一般的 process 所擁有的執行模式，詳細的說明可以參考下面連結：</p><ul><li><p><a href="https://www.ptt.cc/bbs/b97902HW/M.1267018497.A.3B1.html">[系程] 教學: 簡介 Kernel/User Mode 的概念 - 看板 b97902HW - 批踢踢實業坊</a></p></li><li><p><a href="http://peachwaneversay.blogspot.tw/2007/05/user-mode-vs-kernel-mode.html">企鵝幫魚，魚幫兔: User mode vs. Kernel mode</a></p></li></ul><p>而 KVM 多了一個 <strong>Guest Mode</strong>，功能是用來<strong>執行關於 virtual machine 中的相關 I/O request</strong>，無法直接；所有 Memory &amp; CPU 的 I/O request，會透過 <strong>/dev/kvm(QUME)</strong> 來模擬完成，並可透過 QEMU 執行一些特權指令來存取 host machine 的資源。</p><p><img src="http://benjr.tw/wp-content/uploads/2013/11/kvm_qemu01.png" alt="KVM Guest Mode"></p><hr><h1 id="SMP-Symmetric-Multi-Processor"><a href="#SMP-Symmetric-Multi-Processor" class="headerlink" title="SMP(Symmetric Multi-Processor)"></a>SMP(Symmetric Multi-Processor)</h1><p>由於現在 multiple core、hyper threading 等相關技術已經很普遍，這讓作業系統可以進行真正的平行處理；而現在較新的作業系統都已經有對 SMP 的支援(Linux kernel 2.6 以上)，這對虛擬化的推展有相當大的助益。</p><p>透過以下指令，可以用來檢查目前 host machine 對 SMP 的支援程度：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢 logic cpu 數量</span></span><br><span class="line">$ cat /proc/cpuinfo | grep <span class="string">&quot;processor&quot;</span> | wc -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 實體 CPU 數量</span></span><br><span class="line">$ cat /proc/cpuinfo | grep <span class="string">&quot;physical id&quot;</span> | sort | uniq | wc -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每個 CPU 上的 core 數量</span></span><br><span class="line">$ cat /proc/cpuinfo | grep <span class="string">&quot;core id&quot;</span> | sort | uniq | wc -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每個 physical cpu 上分配的 logic cpu 的數量</span></span><br><span class="line">$ cat /proc/cpuinfo | grep <span class="string">&quot;siblings&quot;</span> | sort | uniq | awk -F: <span class="string">&#x27;&#123;print $2&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>以上一篇文章中的範例來說明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kvm -smp 4 -m 2048 \</span><br><span class="line">  -vnc 0.0.0.0:5 -boot order=<span class="built_in">cd</span> \</span><br><span class="line">  -hda /kvm/storage/vm_disks/ubnutu1604.img \</span><br><span class="line">  -cdrom /kvm/os_images/ubuntu-16.04.1-server-amd64.iso</span><br></pre></td></tr></table></figure><p>其中 <code>-smp</code> 參數就是指定要使用多少的 vCPU 支援，完整的使用設定如下：(上例為使用 4 個 vCPU)</p><blockquote><p>-smp n[,**cores=**cores][,**threads=**threads][,**sockets=**sockets][,**maxcpus=**maxcpus]</p></blockquote><h2 id="範例-1-僅使用-smp"><a href="#範例-1-僅使用-smp" class="headerlink" title="範例 1 (僅使用 -smp)"></a>範例 1 (僅使用 -smp)</h2><p>而在 Linux 系統中，每一個 vCPU 的分配都會成為一個 process 運行在 host machine 中，以下開啟一個 vCPU=8 的 virtual machine：</p><blockquote><p>kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</p></blockquote><p>可以從 QEMU monitor 中觀察 vCPU 對應到 kvm 的 process：</p><p><img src="https://lh3.googleusercontent.com/hHP9T9hWydFXixyMUCuCVFDkl7hlZ6Yv_lbljFDdv_azyt_rh01M6X8bHI1fRMV82a2UObBH6wQgAElavdVxByZ99u0cDBToT-t9OIqgWpSY6ZGreqooEuin8WvQqXpQ9g84uwed3-qO2akJatJTCJqpY0Xt_xU9J1ak4702nyJicXQ7h6HqppYXz0G_86NhyQMd6tv2w5venHGgaBoOF46L8UcYYTskX5rPptWqlmhJtNQSkFGj8F6t3DVVGpOJQfgErjdrfFm162spjhQGwZJ6OWiKTsdBAuXbeUscC-NUTPtGbVjSvAHXa-MVo3-h6jjWZ-TlNkjSmQYCICiRiTGIcFsLAkbIVMGw7-PXkPdqXhJyaC9bEWplxhPzhgGptGUeJCAOrdIhvF_l8lXzaSqeWGL-BSnBbA5qCuV-9UUZVGrcHgiQo2p5_Q_iUBBazlkqnRuM69ULxGEeM-q82gIztEmQiqqOrynKeAf9w5MZnJeb_w29u6qu6B-6d81hRGXwc68A1YHKH256EWxgtrd7_aVfsu08fLEIsPwo5qEuDnkef9G_hGNUHccLcWMSIOIr4J1ViP1vOrDsLpAA_qW-08QfZ4Q=w463-h210-no" alt="QEMU monitor - cpu infos"></p><p>以下則可以看出在 host machine 有相對應的 process 存在：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ps -efL | grep kvm</span><br><span class="line">root      1122     2  1122  0    1 Jul27 ?        00:00:00 [kvm-irqfd-clean]</span><br><span class="line">root     14606 11601 14606  3   11 17:46 pts/0    00:00:02 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14610 10   11 17:46 pts/0    00:00:06 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14611  2   11 17:46 pts/0    00:00:01 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14612  1   11 17:46 pts/0    00:00:01 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14613  1   11 17:46 pts/0    00:00:00 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14614  1   11 17:46 pts/0    00:00:00 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14615  1   11 17:46 pts/0    00:00:00 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14616  1   11 17:46 pts/0    00:00:00 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14617  1   11 17:46 pts/0    00:00:01 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14619  1   11 17:46 pts/0    00:00:00 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14606 11601 14631  0   11 17:46 pts/0    00:00:00 kvm -vnc 0.0.0.0:1 -smp 8 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br><span class="line">root     14618     2 14618  0    1 17:46 ?        00:00:00 [kvm-pit/14606]</span><br><span class="line">root     14633 12908 14633  0    1 17:47 pts/1    00:00:00 grep --color=auto kvm</span><br></pre></td></tr></table></figure><p>最後可以用上面的 grep 查詢 physical / cores / threads 等資訊：</p><p><img src="https://lh3.googleusercontent.com/mD_mnwyxWHinCr-2GSCfaMs-kyhi7p5c9cspDW5go_wM_76oHUj0Phj4n5dAahx39fazYXbMYa5xknxe1oRcoErP0cG84fMRu0WRwm9A1-_p4_bH0XediFmabBPzQQ6omUqJeBgJkwuyj0YRQ8-5kQ6zrE6elocc5sAt4q4tVKOXISDO0CZzSiYU9jHr6HeQVWO4xyLcBBkEUTZEeCfeR_tIrJzZF23nMGWDLI9CSu84ADbuL6KilIp4zDwQPNGrZmxNEwi2kUWyKcxC9-iZBm1W3wNdDYdIESmnVkTQuMe19XnTYsGdPiyRiOtMYv01WQ_KmBoCdRRZ1vfn0syyYdkt9PagLY3TjJ75dt57p918PI4z04YUZomaZ0lZ6Pqb8bt1Iaz1oljrslZpyveJ2AtAGWIdO9hFvE_9r3uvzK-ND77ZL1nAMbvuBPcD3rxuAGGr9ZqZTH_RACGOKtUN44cffI9VMvDp261xOj2NvYUDAVKxLogUNUj_ltOSyLGZ3uSffQtBkybY8OBAR3o3ycSsVy6Riwk8x2n6buCDR05dA4oTlaUqmR_o39TyTKsF6c1hzgoiJN9d_tc4cLS2c1PINXUh7W8=w785-h194-no" alt="SMP only"></p><ul><li>logic cpu = 8</li><li>physical cpu = 8</li><li>每個 CPU 上的 core 數量 = 1</li><li>每個 physical cpu 上分配的 logic cpu 的數量 = 1</li></ul><h2 id="範例-2-使用-smp，搭配-sockets-2-cores-2-threads-2"><a href="#範例-2-使用-smp，搭配-sockets-2-cores-2-threads-2" class="headerlink" title="範例 2 (使用 -smp，搭配 sockets=2,cores=2,threads=2)"></a>範例 2 (使用 -smp，搭配 sockets=2,cores=2,threads=2)</h2><p>此範例搭配 socket, cores, threads 等 smp 相關參數</p><blockquote><p> kvm -vnc 0.0.0.0:1 -smp 8,sockets=2,cores=2,threads=2 -m 2048 -hda /kvm/storage/vm_disks/ubnutu1604.img</p></blockquote><p>用 grep 查詢 physical / cores / threads 等資訊：</p><p><img src="https://lh3.googleusercontent.com/JmmR_lsIlsIsr66SlLZXaBJfVKdo6gim3rLxj7oMrVrTbDEYnluhtz-uj5FdlYwUAN4_W-FZDL2a6ykDgFBdwIlYNVrorpLPc7olTSl7oOsHlEoUVT1U9o9yDroSYZFNEGoJVJaMcKoIrtRr9HKB7R9uKrz5VNLR6iX_n2OZs5gL_UxPrxL5UOeYeqyyuZwswNDTyqh61qmqlJ2i0KBAHkSauTcUbASZScuk1rCL_wlp0B0VguRIhd2WZtq2oEe_G9nL6zRMclrXsOqhWsVBy0DyBibyGtqsUPIG7-7rYHcSGs1ao8j_fuTELJVA0P4nWoPpAX5sRI9p84M32vGuppudVSkFDdzACIo43FdB6ZkFftc4sPfRLk-qLXldYJ5ss_waqmqcIA2Qr-be1M443d0QiqYV6BF4XkIn6cKud7gCzy15I30CdM31gVZwK7rkQ6O_eWSCLYhiQoBR1y-B9jXV5qnN7axQfTC8hL46gRlQSyozEG4HXp-X2UmU12rfMWv8rZop2alI6pLa29CE5oywHl40BYI8qKgok52V0znrX1klcmgiCn5iYYdEFPYELqHwDW_EQv0-Z8oAXxmyIpk8tKnSR0E=w777-h190-no" alt="SMP with sockets, cores, and threads"></p><ul><li>logic cpu = 8</li><li>physical cpu = 2</li><li>每個 CPU 上的 core 數量 = 2</li><li>每個 physical cpu 上分配的 logic cpu 的數量 = 4</li></ul><hr><h1 id="Over-Commit"><a href="#Over-Commit" class="headerlink" title="Over-Commit"></a>Over-Commit</h1><p>一般正常使用情況下，每一台 virtual machine 不會總是在高負載狀況，很多時間會是閒置的；此時透過 over-commit 的方式，可以分配比 host machine 中所有的 vCPU 給 virtual machine。</p><p>但不建議分配給單一 virtual machine 超過 host machine 所有 vCPU 的數量，因為這會大大降低 virtual machine 的效能，例如：host machine 總共有 4 個 vCPU，但分配 8 個 vCPU 給 virtual machine。</p><p>若是 4 個 vCPU，分配 1 個 vCPU 給 virtual machine，但分配到 8 台 virtual machine，這樣的效能會比上面的配置更有效率。</p><blockquote><p>若是在 production 的環境，建議還是不要 over-commit</p></blockquote><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://linux.die.net/man/1/qemu-kvm">qemu-kvm(1): QEMU Emulator User Documentation - Linux man page</a></p></li><li><p><a href="https://ncucsie.hackpad.com/ep/pad/static/QrwxkWD88gd">虛擬化技術與應用</a></p></li><li><p><a href="https://www.ptt.cc/bbs/b97902HW/M.1267018497.A.3B1.html">[系程] 教學: 簡介 Kernel/User Mode 的概念 - 看板 b97902HW - 批踢踢實業坊</a></p></li><li><p><a href="http://peachwaneversay.blogspot.tw/2007/05/user-mode-vs-kernel-mode.html">企鵝幫魚，魚幫兔: User mode vs. Kernel mode</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux KVM] 使用 Linux KVM 啟用第一個 virtual machine</title>
      <link href="/blog/KVM/QEMU-KVM-In-CentOS7-GettingStart/"/>
      <url>/blog/KVM/QEMU-KVM-In-CentOS7-GettingStart/</url>
      
        <content type="html"><![CDATA[<p>介紹如何使用 QEMU/KVM 在 Linux 上啟用第一個 virtual machine</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="KVM"><a href="#KVM" class="headerlink" title="KVM"></a>KVM</h2><p>KVM 屬於全虛擬化(Full Virtualization) 的技術，因此在上面運行的 OS 不需要經過任何修改。</p><p>原本因為 Full Virtualization 效能應該是很差的，但因為硬體虛擬化的支援(例如：<a href="http://stenlyho.blogspot.tw/2009/01/vt-xvt-d-intel.html">Intel VT-d</a>，因此大幅提升了 KVM 的效能；此外 KVM 與 QEMU(負責周邊設備的模擬) 的搭配，提供了使用者在 CPU、Memory、Storage、Network、Display 上都有完全相同的虛擬化體驗。</p><h2 id="Linux-kernel"><a href="#Linux-kernel" class="headerlink" title="Linux kernel"></a>Linux kernel</h2><p>關於 Linux kernel 上，建議使用目前較新的 kernel，在以下的環境中，會選用安裝 CentOS 7 作為 host machine，搭配 3.10 版的 Linux kernel 做為測試環境之用。</p><h2 id="環境說明"><a href="#環境說明" class="headerlink" title="環境說明"></a>環境說明</h2><h3 id="硬體"><a href="#硬體" class="headerlink" title="硬體"></a>硬體</h3><p>CPU：Intel(R) Xeon(R) CPU E5-2660 v3 @ 2.60GHz</p><h3 id="軟體"><a href="#軟體" class="headerlink" title="軟體"></a>軟體</h3><ul><li><p>OS: CentOS 7</p></li><li><p>Linux Kernel: 3.10</p></li><li><p>qemu-kvm: 1.5.3 (預設開啟 KVM 加速)</p></li><li><p>qemu-img: 1.5.3</p></li></ul><hr><h1 id="前置環境設定"><a href="#前置環境設定" class="headerlink" title="前置環境設定"></a>前置環境設定</h1><h2 id="安裝套件"><a href="#安裝套件" class="headerlink" title="安裝套件"></a>安裝套件</h2><p>首先要安裝 KVM、QEMU、libvirtd 相關套件 &amp; 啟動 libvirtd service：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># KVM v2.3 需要使用此 repository</span></span><br><span class="line">$ bash -c <span class="string">&quot;echo &#x27;[kvm-common]</span></span><br><span class="line"><span class="string">name=Latest KVM rpms</span></span><br><span class="line"><span class="string">baseurl=http://mirror.centos.org/centos-7/7/virt/x86_64/kvm-common/</span></span><br><span class="line"><span class="string">enabled=1</span></span><br><span class="line"><span class="string">gpgcheck=0&#x27; &gt; /etc/yum.repos.d/kvm.repo&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安裝 KVM 1.5</span></span><br><span class="line">$ yum install -y qemu-kvm \</span><br><span class="line">    qemu-img \</span><br><span class="line">    qemu-system-x86 \</span><br><span class="line">    libvirt \</span><br><span class="line">    virt-install \</span><br><span class="line">    libvirt-python \</span><br><span class="line">    virt-manager \</span><br><span class="line">    python-virtinst \</span><br><span class="line">    libvirt-client \</span><br><span class="line">    bridge-utils \</span><br><span class="line">    net-tools \</span><br><span class="line">    libguestfs-tools-c \</span><br><span class="line">    iptables-services</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可使用 groupinstall 來批次安裝</span></span><br><span class="line">$ yum groupinstall <span class="string">&quot;virtualization&quot;</span> -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若要安裝 KVM 2.3 可使用下面指令</span></span><br><span class="line">$ yum install -y qemu-kvm-ev \</span><br><span class="line">    qemu-img-ev \</span><br><span class="line">    qemu-system-x86 \</span><br><span class="line">    libvirt \</span><br><span class="line">    virt-install \</span><br><span class="line">    libvirt-python \</span><br><span class="line">    virt-manager \</span><br><span class="line">    python-virtinst \</span><br><span class="line">    libvirt-client \</span><br><span class="line">    bridge-utils \</span><br><span class="line">    net-tools \</span><br><span class="line">    libguestfs-tools-c \</span><br><span class="line">    iptables-services</span><br></pre></td></tr></table></figure><p>也可以順便設定 <a href="http://www.linux-kvm.org/images/3/33/02x03-NestedVirtualization.pdf">nested virtualization</a>，可以在虛擬環境中再虛擬一層而不會有太多的 performance lose：(此步驟可以略過)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目前 nested virtualization 是關閉的 </span></span><br><span class="line">$ cat /sys/module/kvm_intel/parameters/nested</span><br><span class="line">N</span><br><span class="line"></span><br><span class="line"><span class="comment"># 開啟 nested virtualization</span></span><br><span class="line">$ sudo rmmod kvm-intel</span><br><span class="line">$ sudo sh -c <span class="string">&quot;echo &#x27;options kvm-intel nested=y&#x27; &gt;&gt; /etc/modprobe.d/dist.conf&quot;</span></span><br><span class="line">$ sudo modprobe kvm-intel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 開啟 nested virtualization 成功</span></span><br><span class="line">$ cat /sys/module/kvm_intel/parameters/nested</span><br><span class="line">Y</span><br></pre></td></tr></table></figure><h2 id="防火牆設定"><a href="#防火牆設定" class="headerlink" title="防火牆設定"></a>防火牆設定</h2><p>停用預設的 <strong>firewalld.service</strong>，並啟用 <strong>iptables.service</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 關閉 firewalld</span></span><br><span class="line">$ systemctl stop firewalld.service</span><br><span class="line">$ systemctl <span class="built_in">disable</span> firewalld.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用 iptables</span></span><br><span class="line">$ systemctl stop iptables.service</span><br><span class="line">$ systemctl <span class="built_in">disable</span> iptables.service</span><br></pre></td></tr></table></figure><p>改用傳統的 itpables 來進行防火牆設定，並使用以下 script 建立防火牆：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># global variables</span></span><br><span class="line">IIF=<span class="string">&quot;ens1f0&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 防止 sync flooding 攻擊(開啟 tcp sync cookie)</span></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/tcp_syncookies</span><br><span class="line"></span><br><span class="line">iptables -t filter -F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 connection track</span></span><br><span class="line">iptables -t filter -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line"><span class="comment"># 避免 INVALID 封包被其他服務所接收</span></span><br><span class="line">iptables -t filter -A INPUT -m state --state INVALID -j DROP</span><br><span class="line"></span><br><span class="line">iptables -t filter -A INPUT -p tcp --dport 22 -j ACCEPT</span><br><span class="line"><span class="comment"># 提供 vnc access</span></span><br><span class="line">iptables -t filter -A INPUT -p tcp --dport 5900:5910 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用來取代 chain default policy</span></span><br><span class="line">iptables -t filter -A INPUT -i <span class="variable">$&#123;IIF&#125;</span> -j DROP</span><br></pre></td></tr></table></figure><blockquote><p>上面的套件安裝 &amp; 防火牆設定完成後，要將 KVM host 重新開啟，並啟動 <strong><font color='red'>libvirtd.service</font></strong></p></blockquote><hr><h1 id="驗證環境"><a href="#驗證環境" class="headerlink" title="驗證環境"></a>驗證環境</h1><p>安裝好 QEMU/KVM 相關套件後，我們可以來檢查目前的環境是否可以正確的運行虛擬化功能：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ virt-host-validate</span><br><span class="line">  QEMU: Checking <span class="keyword">for</span> hardware virtualization                                 : PASS</span><br><span class="line">  QEMU: Checking <span class="keyword">for</span> device /dev/kvm                                         : PASS</span><br><span class="line">  QEMU: Checking <span class="keyword">for</span> device /dev/vhost-net                                   : PASS</span><br><span class="line">  QEMU: Checking <span class="keyword">for</span> device /dev/net/tun                                     : PASS</span><br><span class="line">   LXC: Checking <span class="keyword">for</span> Linux &gt;= 2.6.26                                         : PASS</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">$ virsh nodeinfo</span><br><span class="line">CPU model:           x86_64</span><br><span class="line">CPU(s):              48</span><br><span class="line">CPU frequency:       1200 MHz</span><br><span class="line">CPU socket(s):       1</span><br><span class="line">Core(s) per socket:  12</span><br><span class="line">Thread(s) per core:  2</span><br><span class="line">NUMA cell(s):        2</span><br><span class="line">Memory size:         263930636 KiB</span><br><span class="line"></span><br><span class="line">$ virsh domcapabilities</span><br><span class="line">&lt;domainCapabilities&gt;</span><br><span class="line">  &lt;path&gt;/usr/bin/qemu-system-x86_64&lt;/path&gt;</span><br><span class="line">  &lt;domain&gt;qemu&lt;/domain&gt;</span><br><span class="line">  &lt;machine&gt;pc-i440fx-2.0&lt;/machine&gt;</span><br><span class="line">  &lt;arch&gt;x86_64&lt;/arch&gt;</span><br><span class="line">  &lt;vcpu max=<span class="string">&#x27;255&#x27;</span>/&gt;</span><br><span class="line">  &lt;os supported=<span class="string">&#x27;yes&#x27;</span>&gt;</span><br><span class="line">    &lt;loader supported=<span class="string">&#x27;yes&#x27;</span>&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;type&#x27;</span>&gt;</span><br><span class="line">        &lt;value&gt;rom&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;pflash&lt;/value&gt;</span><br><span class="line">      &lt;/enum&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;readonly&#x27;</span>&gt;</span><br><span class="line">        &lt;value&gt;yes&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;no&lt;/value&gt;</span><br><span class="line">      &lt;/enum&gt;</span><br><span class="line">    &lt;/loader&gt;</span><br><span class="line">  &lt;/os&gt;</span><br><span class="line">  &lt;devices&gt;</span><br><span class="line">    &lt;disk supported=<span class="string">&#x27;yes&#x27;</span>&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;diskDevice&#x27;</span>&gt;</span><br><span class="line">        &lt;value&gt;disk&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;cdrom&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;floppy&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;lun&lt;/value&gt;</span><br><span class="line">      &lt;/enum&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;bus&#x27;</span>&gt;</span><br><span class="line">        &lt;value&gt;ide&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;fdc&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;scsi&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;virtio&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;usb&lt;/value&gt;</span><br><span class="line">      &lt;/enum&gt;</span><br><span class="line">    &lt;/disk&gt;</span><br><span class="line">    &lt;hostdev supported=<span class="string">&#x27;yes&#x27;</span>&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;mode&#x27;</span>&gt;</span><br><span class="line">        &lt;value&gt;subsystem&lt;/value&gt;</span><br><span class="line">      &lt;/enum&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;startupPolicy&#x27;</span>&gt;</span><br><span class="line">        &lt;value&gt;default&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;mandatory&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;requisite&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;optional&lt;/value&gt;</span><br><span class="line">      &lt;/enum&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;subsysType&#x27;</span>&gt;</span><br><span class="line">        &lt;value&gt;usb&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;pci&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;scsi&lt;/value&gt;</span><br><span class="line">      &lt;/enum&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;capsType&#x27;</span>/&gt;</span><br><span class="line">      &lt;enum name=<span class="string">&#x27;pciBackend&#x27;</span>&gt;</span><br><span class="line">        &lt;value&gt;default&lt;/value&gt;</span><br><span class="line">        &lt;value&gt;vfio&lt;/value&gt;</span><br><span class="line">      &lt;/enum&gt;</span><br><span class="line">    &lt;/hostdev&gt;</span><br><span class="line">  &lt;/devices&gt;</span><br><span class="line">&lt;/domainCapabilities&gt;</span><br></pre></td></tr></table></figure><hr><h1 id="啟動第一個-virtual-machine"><a href="#啟動第一個-virtual-machine" class="headerlink" title="啟動第一個 virtual machine"></a>啟動第一個 virtual machine</h1><p>由於 RedHat 建議使用 virsh，因此 <strong><font color='red'>qemu-kvm</font></strong> 就不存在於預設路徑中，用以下指令把它找出來：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將 qemu-kvm 以 symlink 的形式複製到 $PATH</span></span><br><span class="line"><span class="comment"># qemu-kvm 指令已經預設啟用 KVM 支援</span></span><br><span class="line">$ ln -sf /usr/libexec/qemu-kvm /usr/bin/kvm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 製作一個 size = 8GB 的 raw image 作為 virtual machine disk</span></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero of=/kvm/storage/vm_disks/ubnutu1604.img bs=1M count=8192</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目前系統中存在的 iso</span></span><br><span class="line">$ ls /kvm/os_images</span><br><span class="line">ubuntu-16.04.1-server-amd64.iso</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 VM (以光碟開機，安裝作業系統)</span></span><br><span class="line"><span class="comment"># -smp 4 =&gt; vCPU=4</span></span><br><span class="line"><span class="comment"># -m 2048 =&gt; RAM=2048MB</span></span><br><span class="line"><span class="comment"># -vnc 0.0.0.0:5 =&gt; 將 vnc 開在第五個 console，因此透過 vnc viewer 連線要使用 &quot;ip:5&quot; 來進行連線</span></span><br><span class="line"><span class="comment"># -boot order=cd =&gt; 開機順序為 cdrom &gt; hdd</span></span><br><span class="line"><span class="comment"># -hda /kvm/storage/vm_disks/ubnutu1604.img =&gt; 指定 hdd raw image</span></span><br><span class="line"><span class="comment"># -cdrom /kvm/os_images/ubuntu-16.04.1-server-amd64.iso =&gt; 指定開機光碟 iso</span></span><br><span class="line">$ kvm -smp 4 -m 2048 \</span><br><span class="line">  -vnc 0.0.0.0:5 -boot order=<span class="built_in">cd</span> \</span><br><span class="line">  -hda /kvm/storage/vm_disks/ubnutu1604.img \</span><br><span class="line">  -cdrom /kvm/os_images/ubuntu-16.04.1-server-amd64.iso</span><br></pre></td></tr></table></figure><p>接著可以使用 <a href="http://tigervnc.org/">TigerVNC</a> or <a href="https://chrome.google.com/webstore/detail/vnc%C2%AE-viewer-for-google-ch/iabmpiboiopbgfabjmgeedhcmjenhbla">VNC® Viewer for Google Chrome</a> 來進行連線，使用的的連線位置為 <code>server_ip:5</code>；連線進入後，就可以按照一般程序進行 OS 的安裝。</p><p>安裝完成後，<code>/kvm/storage/vm_disks/ubnutu1604.img</code> 將會是一個已經安裝好 OS 的硬碟 image 檔案，我們可以透過以下指令使用此 image 檔案來啟動系統：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -smp 4 =&gt; vCPU=4</span></span><br><span class="line"><span class="comment"># -m 2048 =&gt; RAM=2048MB</span></span><br><span class="line"><span class="comment"># -vnc 0.0.0.0:5 =&gt; 將 vnc 開在第五個 console，因此透過 vnc viewer 連線要使用 &quot;ip:5&quot; 來進行連線</span></span><br><span class="line">$ kvm -smp 4 -m 2048 -vnc 0.0.0.0:5 -hda /kvm/storage/vm_disks/ubnutu1604.img</span><br></pre></td></tr></table></figure><p>使用 VNC viewer 連到 virtual machine 之後，可使用 <code>Ctrl + Alt + 2</code> 切到 QEMU monitor，輸入 <code>kvm info</code> 就可以檢視目前 KVM 是否被使用，或是完全在 QEMU 模擬下產生：</p><p><img src="https://lh3.googleusercontent.com/Or8AH3hxAJbdXsIMADxmVkvUKlFYe_-DwTpSuw878fpP3bDAqyLv_ql_7W_HIrLYGHqc1hha7aecKMM6lytj2Wkv-NEyoXsZPuHE5RKa9mp_6apKEoPn7h-tp2DSjLcHaj72ByMefPKXRFKFYCLmYdbhsax0Ro8A-UCjSweOuB03zEL44VM7YbkxNE85vTmFv-JMUUERlX3CAdDotSWpvzl-ztgHzoU2E0iqwgLawhHAhn1JOX19Mn0ib3J0vxqZyLI6CNqaXEIc-7v5QNhAmAysEdWd3AMVKqkPVI41v8FDiUam2G_MDUkWOsAW4aQjnwrAbiw3-Ljpi72gjsM_iJWnsLF5nCuREGQxWC_LOrhRo7-AKLgU_XmzJuobqLRi6LoMzy_BKva5jI7nSgmgAfXDDTWpsXyGoKSgexpg_J7SRuLVZR1iZZ8HCZ9FZpre9qNIdr6xsvnH1NoUobJ73IKMbKn4YmGD0Z9Odi51bpWSh9eV1kOyCMFnF7Q5KZoPk-9lZwChK0gVUj6eObbJDKhlrFTpSvO7vYGJOmIBtfgCfKB9tAZ1-fpDSWxv8tGP38OQmKirl5oh0ozkGGQ-Z9QAr0mC31U=w642-h143-no" alt="QEMU Monitor"></p><blockquote><p>從上圖來看，可看出關鍵字 <code>kvm support: enabled</code>，表示 KVM 加速是開啟的</p></blockquote><blockquote><p>由於我們在上面所產生的 kvm 指令是來自於 qemu-kvm，因此已經預設啟用 KVM 加速 (若單純使用 QEMU，可搭配 <code>--enable-kvm</code> 參數來啟用 KVM 加速)</p></blockquote><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><ul><li><p><a href="http://newtoypia.blogspot.tw/2015/02/qemu-kvm.html">玩具烏托邦: 五分鐘開始玩 qemu-kvm 虛擬機</a></p></li><li><p><a href="http://www.vpsee.com/2012/04/install-kvm-on-centos-6-2/">在 CentOS 6.2 上安装和配置 KVM @vpsee.com</a></p></li><li><p><a href="http://ot-note.logdown.com/posts/64644/launch-a-vm-with-qemu-kvm">使用 libvirt 與 qemu-kvm 開啓 VM (內含 libvirt sample XML for KVM ) « OT Coding Note</a></p></li></ul><h2 id="VNC"><a href="#VNC" class="headerlink" title="VNC"></a>VNC</h2><ul><li><a href="http://jamyy.us.to/blog/2011/10/3365.html">KVM/QEMU 設置 VNC 連線密碼 « Jamyy’s Weblog</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux] PXE Booting</title>
      <link href="/blog/Linux/Linux-PXE-Booting/"/>
      <url>/blog/Linux/Linux-PXE-Booting/</url>
      
        <content type="html"><![CDATA[<p>此篇文章介紹 PXE &amp; iPXE 的開機流程</p><h1 id="PXE"><a href="#PXE" class="headerlink" title="PXE"></a>PXE</h1><p>首先以下圖說明一下傳統 PXE 的流程：</p><p><img src="https://github.com/coreos/coreos-baremetal/raw/master/Documentation/img/pxelinux.png" alt="PXE"></p><p>PXE 的開機流程，簡單來說就是幾個步驟：</p><ol><li><p>network client 從 DHCP service 取得 IP &amp; 其他 metadata(例如：TFTP 資訊 &amp; NBP 檔名)</p></li><li><p>network client 從 TFTP 取得 NBP(Network Boot Program，上面中的 <code>pxelinux.0</code>)，並啟動</p></li><li><p>network client 使用 NBP 載入 configs, scripts, 以及執行 OS 需要的 kernel(範例中的 <code>kernel.vmlinuz</code>) &amp; ramfs image(上圖中的 <code>initrd.cpio.gz</code>)</p></li></ol><hr><h1 id="Network-Boot-Program-NBP-也稱為-bootloader"><a href="#Network-Boot-Program-NBP-也稱為-bootloader" class="headerlink" title="Network Boot Program (NBP, 也稱為 bootloader)"></a>Network Boot Program (NBP, 也稱為 bootloader)</h1><p>CoreOS 可用多種不同的 bootloader 開機 &amp; 設定，如果是一個全新的設定環境，<a href="http://ipxe.org/">iPXE</a> 是個不錯的選擇。</p><h2 id="PXELINUX"><a href="#PXELINUX" class="headerlink" title="PXELINUX"></a>PXELINUX</h2><p><a href="http://www.syslinux.org/wiki/index.php?title=PXELINUX">PXELINUX</a> 是個相當普遍被使用的 bootloader(檔名為 <code>pxelinux.0</code>)，會自動從 <code>/tftp_bootdir/pxelinux.cfg</code> 目錄中載入設定檔。</p><p>若要控制特定機器使用特定的設定檔，可以利用 network client 的 UUID、MAC address、IP or default 來進行設定，因此設定檔就會變成如下面的範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/tftp_bootdir/pxelinux.cfg/b8945908-d6a6-41a9-611d-74a6ab80b83d</span><br><span class="line">/tftp_bootdir/pxelinux.cfg/01-88-99-aa-bb-cc-dd</span><br><span class="line">/tftp_bootdir/pxelinux.cfg/default</span><br></pre></td></tr></table></figure><p>按照上圖的流程，可以很清楚知道，設定檔的內容肯定就是會包含了像是 config / script / kernel / ramfs image …. 等檔案的位置資訊(可能還會包含<strong>開機選單</strong>)，以下是個簡單範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">default coreos</span><br><span class="line">prompt 1</span><br><span class="line">timeout 15</span><br><span class="line"></span><br><span class="line">display boot.msg</span><br><span class="line"></span><br><span class="line">label coreos</span><br><span class="line">  menu default</span><br><span class="line">  kernel coreos_production_pxe.vmlinuz</span><br><span class="line">  append initrd=coreos_production_pxe_image.cpio.gz cloud-config-url=http://example.com/pxe-cloud-config.yml</span><br></pre></td></tr></table></figure><blockquote><p>上面範例中的檔案若沒有指定目錄，就表示應該將其放在 <code>/tftp_bootdir</code> 目錄中<br>其中設定了一個簡單的 menu、顯示訊息、kernel(coreos_production_pxe.vmlinuz)、ramfs(coreos_production_pxe_image.cpio.gz) &amp; 設定 CoreOS 用的 cloud-config.yaml</p></blockquote><p>PXE 雖然普遍使用，但的確是有些缺點存在的，例如：</p><ol><li><p>TFTP 速度慢</p></li><li><p>若有許多針對不同機器的客製化設定檔需求，會需要撰寫很多份 pxelinux config</p></li></ol><h2 id="iPXE"><a href="#iPXE" class="headerlink" title="iPXE"></a>iPXE</h2><p><a href="http://ipxe.org/">iPXE</a> 可是視為加強版的 PXE bootloader，使用的並非是設定檔，而是 <strong>iPXE script</strong>，而且 iPXE script &amp; image 都可以透過 HTTP 下載</p><p>以下是 iPXE 開機流程示意圖：</p><p><img src="https://github.com/coreos/coreos-baremetal/raw/master/Documentation/img/ipxe.png" alt="iPXE"></p><p>從上圖可以看出，為了可以運作在原有的環境中，使用了一個名稱為 <a href="http://boot.ipxe.org/undionly.kpxe">undionly.kpxe</a> 的 bootloader 來協助開機，接著會發生以下的事情：</p><ol><li><p>bootloader undionly.kpxe 會提供給 machine 上網 &amp; 處理後續 iPXE script 的能力</p></li><li><p>透過網路取得 iPXE script <code>boot.ipxes</code></p></li><li><p>並會使用檔名為 <code>boot.ipxe</code>(來源可以是 HTTP) 的 iPXE script 來繼續執行後續的開機流程</p></li></ol><p>以下是個簡單的 iPXE script 範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!ipxe</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> base-url http://stable.release.core-os.net/amd64-usr/current</span><br><span class="line">kernel <span class="variable">$&#123;base-url&#125;</span>/coreos_production_pxe.vmlinuz cloud-config-url=http://provisioner.example.net/cloud-config.yml</span><br><span class="line">initrd <span class="variable">$&#123;base-url&#125;</span>/coreos_production_pxe_image.cpio.gz</span><br><span class="line">boot</span><br></pre></td></tr></table></figure><blockquote><p>透過 iPXE script，可以用程式化的方式進行更多動態的開機設定</p></blockquote><p>在 iPXE 開機環境 for CoreOS 的架構中，有一些事情是值得注意一下的：</p><ol><li><p>TFTP 只用來提供 <a href="http://boot.ipxe.org/undionly.kpxe">undionly.kpxe</a> bootloader，目的是為了讓老舊的 PXE firmware client 也可以使用 iPXE</p></li><li><p>CoreOS 提供了 [bootcfg](coreos-baremetal/bootcfg.md at master · coreos/coreos-baremetal) 工具，可根據硬體的屬性，用來產生相對應的 iPXE script (把 iPXE script 的來源指向 bootcfg iPXE endpoint)</p></li></ol><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="http://www.syslinux.org/wiki/index.php?title=PXELINUX">PXELINUX - Syslinux Wiki</a></p></li><li><p><a href="http://ipxe.org/howto/chainloading">iPXE - open source boot firmware [howto:chainloading]</a></p></li><li><p><a href="https://github.com/coreos/coreos-baremetal/blob/master/Documentation/network-booting.md">coreos-baremetal/network-booting.md at master · coreos/coreos-baremetal</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Linux] 解決使用帶有密碼的 SSH keypair 時需要重複輸入密碼的問題</title>
      <link href="/blog/Linux/Auto-Authenticate-with-Password-Protected-SSHKeypair/"/>
      <url>/blog/Linux/Auto-Authenticate-with-Password-Protected-SSHKeypair/</url>
      
        <content type="html"><![CDATA[<blockquote><p>use ssh agent and keychain to input the secret of password-protected ssh key</p></blockquote><p>最近被 GitHub 騙了去產生個帶有密碼的 SSH keypair 來用</p><p>發現怎麼每次使用都要我輸入密碼呢….? 於是上網找了一下答案……</p><p>要解決這方式，需要 <strong>ssh agent</strong> 搭配 <strong>keychain</strong> 來將密碼安全的儲存起來</p><p>假設以下情況已經完成：</p><ol><li><p>SSK keypair 已經存在於 <code>~/.ssh/id_rsa*</code></p></li><li><p>keychain 套件已經安裝</p></li></ol><p>接著只要執行以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ tee --append ~/.bash_profile &lt;&lt;-<span class="string">&#x27;EOF&#x27;</span></span><br><span class="line"><span class="comment">### START-Keychain ###</span></span><br><span class="line"><span class="comment"># Let  re-use ssh-agent and/or gpg-agent between logins</span></span><br><span class="line">/usr/bin/keychain <span class="variable">$HOME</span>/.ssh/id_rsa</span><br><span class="line"><span class="built_in">source</span> <span class="variable">$HOME</span>/.keychain/<span class="variable">$HOSTNAME</span>-sh</span><br><span class="line"><span class="comment">### End-Keychain ###</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>接著再重新登入，輸入一次密碼後，後續使用到 SSH keypair 時，就不用一直重複輸入了!</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://help.github.com/articles/generating-an-ssh-key/">Generating an SSH key - User Documentation</a></p></li><li><p><a href="http://unix.stackexchange.com/questions/83608/ssh-agent-how-to-set-it-up-so-my-centos-server-will-only-ask-for-passphrase-onc">ssh-agent: How to set it up so my CentOS server will only ask for passphrase once? - Unix &amp; Linux Stack Exchange</a></p></li><li><p><a href="http://www.cyberciti.biz/faq/ssh-passwordless-login-with-keychain-for-scripts/">keychain: Set Up Secure Passwordless SSH Access For Backup Scripts</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[CoreOS] etcd、fleet &amp; flannel 簡介</title>
      <link href="/blog/CoreOS/CoreOS-Introduction-Of-etcd-fleet-and-flannel/"/>
      <url>/blog/CoreOS/CoreOS-Introduction-Of-etcd-fleet-and-flannel/</url>
      
        <content type="html"><![CDATA[<h1 id="環境說明"><a href="#環境說明" class="headerlink" title="環境說明"></a>環境說明</h1><p>測試環境建立於現有的 OpenStack infrastructure 上，使用以下設定：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#cloud-config</span></span><br><span class="line"></span><br><span class="line"><span class="attr">ssh_authorized_keys:</span></span><br><span class="line">  <span class="bullet">-</span> [<span class="string">YOUR</span> <span class="string">SSH</span> <span class="string">PUBLIC</span> <span class="string">KEY</span>]</span><br><span class="line"></span><br><span class="line"><span class="attr">coreos:</span></span><br><span class="line">  <span class="attr">etcd2:</span></span><br><span class="line">    <span class="attr">discovery:</span> <span class="string">https://discovery.etcd.io/ea524a83ed3f84e550417f0fb89e91c8</span></span><br><span class="line">    <span class="attr">advertise-client-urls:</span> <span class="string">http://$private_ipv4:2379,http://$private_ipv4:4001</span></span><br><span class="line">    <span class="attr">initial-advertise-peer-urls:</span> <span class="string">http://$private_ipv4:2380</span></span><br><span class="line">    <span class="attr">listen-client-urls:</span> <span class="string">http://0.0.0.0:2379,http://0.0.0.0:4001</span></span><br><span class="line">    <span class="attr">listen-peer-urls:</span> <span class="string">http://$private_ipv4:2380</span></span><br><span class="line">  <span class="attr">flannel:</span></span><br><span class="line">    <span class="attr">interface:</span> <span class="string">$public_ipv4</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">units:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">etcd2.service</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">start</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">fleet.service</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">start</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">flanneld.service</span></span><br><span class="line">      <span class="attr">drop-ins:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="number">50</span><span class="string">-network-config.conf</span></span><br><span class="line">          <span class="attr">content:</span> <span class="string">|</span></span><br><span class="line">            [<span class="string">Service</span>]</span><br><span class="line">            <span class="string">ExecStartPre=/usr/bin/etcdctl</span> <span class="string">set</span> <span class="string">/coreos.com/network/config</span> <span class="string">&#x27;&#123; &quot;Network&quot;: &quot;10.1.0.0/16&quot; &#125;&#x27;</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">start</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis.service</span></span><br><span class="line">      <span class="attr">content:</span> <span class="string">|</span></span><br><span class="line">        [<span class="string">Unit</span>]</span><br><span class="line">        <span class="string">Requires=flanneld.service</span></span><br><span class="line">        <span class="string">After=flanneld.service</span></span><br><span class="line">        [<span class="string">Service</span>]</span><br><span class="line">        <span class="string">ExecStart=/usr/bin/docker</span> <span class="string">run</span> <span class="string">redis</span></span><br><span class="line">        <span class="string">Restart=always</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">start</span></span><br></pre></td></tr></table></figure><p>以上設定會把 <strong>etcd</strong>、<strong>fleet</strong>、<strong>flannel</strong>、<strong>docker</strong> 都啟動，還包含了一個執行 <strong>redis</strong> 服務的 container</p><hr><h1 id="驗證步驟"><a href="#驗證步驟" class="headerlink" title="驗證步驟"></a>驗證步驟</h1><p>當 CoreOS 安裝完成之後，首要之務就是驗證 CoreOS 是否安裝成功，以下測試三個主要的 service 分別是 <code>etcd</code>、<code>docker</code>、<code>fleet</code>：</p><h2 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h2><p>在 Machine A 新增資訊到 etcd service：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl <span class="built_in">set</span> /message <span class="string">&quot;Hello CoreOS&quot;</span></span><br><span class="line">Hello CoreOS</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ curl -L http://127.0.0.1:2379/v2/keys/message -XPUT -d value=<span class="string">&quot;Hello CoreOS&quot;</span></span><br><span class="line">&#123;<span class="string">&quot;action&quot;</span>:<span class="string">&quot;set&quot;</span>,<span class="string">&quot;node&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;Hello CoreOS&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:1641,<span class="string">&quot;createdIndex&quot;</span>:1641&#125;,<span class="string">&quot;prevNode&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;Hello CoreOS&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:1455,<span class="string">&quot;createdIndex&quot;</span>:1455&#125;&#125;</span><br></pre></td></tr></table></figure><p>可以從 Machine B 取得：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl get /message   </span><br><span class="line">Hello CoreOS</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ curl -L http://127.0.0.1:2379/v2/keys/message</span><br><span class="line">&#123;<span class="string">&quot;action&quot;</span>:<span class="string">&quot;get&quot;</span>,<span class="string">&quot;node&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;Hello CoreOS&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:1641,<span class="string">&quot;createdIndex&quot;</span>:1641&#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h2><p>由於 CoreOS 使用了 systemd port activate 的機制，因此一開始 docker service 是不會啟動的，直到有 container 啟動為止：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一開始 docker service 不會啟動</span></span><br><span class="line">core@coreos ~ $ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib64/systemd/system/docker.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line">     Docs: http://docs.docker.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 container busybox</span></span><br><span class="line">core@coreos ~ $ docker run busybox /bin/<span class="built_in">echo</span> Hello CoreOS</span><br><span class="line">Unable to find image <span class="string">&#x27;busybox:latest&#x27;</span> locally</span><br><span class="line">latest: Pulling from library/busybox</span><br><span class="line">385e281300cc: Pull complete</span><br><span class="line">a3ed95caeb02: Pull complete</span><br><span class="line">Digest: sha256:4a731fb46adc5cefe3ae374a8b6020fc1b6ad667a279647766e9a3cd89f6fa92</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> busybox:latest</span><br><span class="line">Hello CoreOS</span><br><span class="line"></span><br><span class="line"><span class="comment"># docker service 目前為啟動狀態</span></span><br><span class="line">core@coreos ~ $ systemctl status docker</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/usr/lib64/systemd/system/docker.service; disabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon 2016-06-20 07:58:52 UTC; 53s ago</span><br><span class="line">     Docs: http://docs.docker.com</span><br><span class="line"> Main PID: 1082 (docker)</span><br><span class="line">   Memory: 41.7M</span><br><span class="line">      CPU: 18.356s</span><br><span class="line">   CGroup: /system.slice/docker.service</span><br><span class="line">           └─1082 docker daemon --host=fd:// --exec-opt native.cgroupdriver=systemd --selinux-enabled</span><br><span class="line">.........</span><br></pre></td></tr></table></figure><h2 id="fleet"><a href="#fleet" class="headerlink" title="fleet"></a>fleet</h2><p>CoreOS 會使用 fleet 用來管理 container 的生命週期。</p><p>fleet 是透過接收 <a href="https://coreos.com/os/docs/latest/getting-started-with-systemd.html">systemd unit files</a> 為資訊來源進行工作，將 workload 分配到 cluster 中不同的機器執行；管理者可以透過 <code>fleetctl</code> 工具執行像是查詢 unit 的狀態、log 資訊等工作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ cat &lt;&lt;EOF &gt;hello.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=My Service</span><br><span class="line">After=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">TimeoutStartSec=0</span><br><span class="line">ExecStartPre=-/usr/bin/docker <span class="built_in">kill</span> hello</span><br><span class="line">ExecStartPre=-/usr/bin/docker rm hello</span><br><span class="line">ExecStartPre=/usr/bin/docker pull busybox</span><br><span class="line">ExecStart=/usr/bin/docker run --name hello busybox /bin/sh -c <span class="string">&quot;trap &#x27;exit 0&#x27; INT TERM; while true; do echo Hello World; sleep 1; done&quot;</span></span><br><span class="line">ExecStop=/usr/bin/docker stop hello</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ fleetctl load hello.service</span><br><span class="line">Unit hello.service inactive</span><br><span class="line">Unit hello.service loaded on 0682a8f2.../192.168.50.14</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ fleetctl start hello.service</span><br><span class="line">Unit hello.service launched on 0682a8f2.../192.168.50.14</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ fleetctl status hello.service</span><br><span class="line">● hello.service - My Service</span><br><span class="line">   Loaded: loaded (/run/fleet/units/hello.service; linked-runtime; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Mon 2016-06-20 08:52:43 UTC; 9s ago</span><br><span class="line">...........</span><br><span class="line">Jun 20 08:52:43 coreos docker[1355]: Status: Image is up to date <span class="keyword">for</span> busybox:latest</span><br><span class="line">Jun 20 08:52:43 coreos systemd[1]: Started My Service.</span><br><span class="line">Jun 20 08:52:50 coreos docker[1367]: Hello World</span><br><span class="line">...........</span><br><span class="line">Jun 20 08:52:52 coreos docker[1367]: Hello World</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ fleetctl destroy hello.service</span><br><span class="line">Destroyed hello.service</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ fleetctl status hello.service</span><br><span class="line">Unit hello.service does not exist.</span><br></pre></td></tr></table></figure><h2 id="flannel"><a href="#flannel" class="headerlink" title="flannel"></a>flannel</h2><p>flannel 是用來提供 cross hosts 的 container 互相連結之用，在上面的 cloud-config 中，每個 host 預設會從 10.1.0.0/16 的區段中取得自己的 class C subnet。</p><p>驗證 flannel 是否運作正常的的步驟如下：</p><ol><li><p>登入 machine A，並進入 redis container A 中，查詢到 IP A</p></li><li><p>登入 machine B，並進入 redis container B 中，查詢到 IP B</p></li><li><p>確認可以從 container A ping 到 container B</p></li></ol><p>若是兩個位於不同機器的 container 可以互通，就表示 flannel 目前的運作是正常的!</p><hr><h1 id="etcd-1"><a href="#etcd-1" class="headerlink" title="etcd"></a>etcd</h1><p><strong>etcd</strong> 是種分散式 key/value 儲存服務，存在於每一台 CoreOS 機器中，並在 CoreOS cluster 中負責 shared configuration &amp; service discovery 的工作。</p><p>而執行在 cluster 環境中的 application container 同樣可以使用 etcd 所提供的服務，可進行像是儲存資料庫連線設定、cache 設定、[feature flag](Feature Flag 功能發布控制 - 壹讀) … 等不同的資訊。</p><h2 id="在-etcd-讀寫資料"><a href="#在-etcd-讀寫資料" class="headerlink" title="在 etcd 讀寫資料"></a>在 etcd 讀寫資料</h2><p>當對 etcd service 寫入資料後，cluster 內的機器都可以讀取的到(所以說是分散式的)</p><h3 id="寫入資料"><a href="#寫入資料" class="headerlink" title="寫入資料"></a>寫入資料</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl <span class="built_in">set</span> /message Hello</span><br><span class="line">Hello</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ curl -L -X PUT http://localhost:2379/v2/keys/message -d value=<span class="string">&quot;Hello&quot;</span></span><br><span class="line">&#123;<span class="string">&quot;action&quot;</span>:<span class="string">&quot;set&quot;</span>,<span class="string">&quot;node&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;Hello&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:45040,<span class="string">&quot;createdIndex&quot;</span>:45040&#125;,<span class="string">&quot;prevNode&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;CoreOS&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:44979,<span class="string">&quot;createdIndex&quot;</span>:44979&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="讀取資料"><a href="#讀取資料" class="headerlink" title="讀取資料"></a>讀取資料</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl get /message</span><br><span class="line">Hello</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ curl -L http://localhost:2379/v2/keys/message</span><br><span class="line">&#123;<span class="string">&quot;action&quot;</span>:<span class="string">&quot;get&quot;</span>,<span class="string">&quot;node&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;Hello&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:45040,<span class="string">&quot;createdIndex&quot;</span>:45040&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="刪除資料"><a href="#刪除資料" class="headerlink" title="刪除資料"></a>刪除資料</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl rm /message</span><br><span class="line">PrevNode.Value: Hello</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ etcdctl <span class="built_in">set</span> /message Hello</span><br><span class="line">Hello</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ curl -L -X DELETE http://localhost:2379/v2/keys/message</span><br><span class="line">&#123;<span class="string">&quot;action&quot;</span>:<span class="string">&quot;delete&quot;</span>,<span class="string">&quot;node&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:45482,<span class="string">&quot;createdIndex&quot;</span>:45467&#125;,<span class="string">&quot;prevNode&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;Hello&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:45467,<span class="string">&quot;createdIndex&quot;</span>:45467&#125;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="從-container-中讀寫-etcd-中的資料"><a href="#從-container-中讀寫-etcd-中的資料" class="headerlink" title="從 container 中讀寫 etcd 中的資料"></a>從 container 中讀寫 etcd 中的資料</h2><p>要從 docker container 中對 etcd 讀寫資料，必須將 ip 指向 <code>docker0</code> interface，預設情況下 ip 為 <code>172.17.0.1</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@cb8e1cb26a7e:/<span class="comment"># curl -L http://172.17.0.1:2379/v2/keys</span></span><br><span class="line">&#123;<span class="string">&quot;action&quot;</span>:<span class="string">&quot;get&quot;</span>,<span class="string">&quot;node&quot;</span>:&#123;<span class="string">&quot;dir&quot;</span>:<span class="literal">true</span>,<span class="string">&quot;nodes&quot;</span>:[&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/coreos.com&quot;</span>,<span class="string">&quot;dir&quot;</span>:<span class="literal">true</span>,<span class="string">&quot;modifiedIndex&quot;</span>:5,<span class="string">&quot;createdIndex&quot;</span>:5&#125;]&#125;&#125;</span><br><span class="line"></span><br><span class="line">root@cb8e1cb26a7e:/<span class="comment"># curl -L -X PUT http://172.17.0.1:2379/v2/keys/message -d value=&quot;I am container&quot;</span></span><br><span class="line">&#123;<span class="string">&quot;action&quot;</span>:<span class="string">&quot;set&quot;</span>,<span class="string">&quot;node&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/message&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;I am container&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:47075,<span class="string">&quot;createdIndex&quot;</span>:47075&#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="監控-etcd-directory-的變化"><a href="#監控-etcd-directory-的變化" class="headerlink" title="監控 etcd directory 的變化"></a>監控 etcd directory 的變化</h2><h3 id="1、新增-etcd-directory"><a href="#1、新增-etcd-directory" class="headerlink" title="1、新增 etcd directory"></a>1、新增 etcd directory</h3><p>除了 <code>/</code> 之外，我們可以在 etcd 中自訂所需要的 directory 用來存放自訂的訊息，以下建立名稱為 <strong>foo-service</strong> 的 directory：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl mkdir /foo-service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 etcd directory 中新增訊息</span></span><br><span class="line">core@coreos ~ $ etcdctl <span class="built_in">set</span> /foo-service/container1 localhost:1111</span><br><span class="line">localhost:1111</span><br></pre></td></tr></table></figure><h3 id="2、監控-etcd-directory"><a href="#2、監控-etcd-directory" class="headerlink" title="2、監控 etcd directory"></a>2、監控 etcd directory</h3><p>當其他 CoreOS host 執行 <code>etcdctl set /foo-service/container2 localhost:2222</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl watch --recursive /foo-service</span><br><span class="line">[<span class="built_in">set</span>] /foo-service/container2</span><br><span class="line">localhost:2222</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ curl -L http://localhost:2379/v2/keys/foo-service?<span class="built_in">wait</span>=<span class="literal">true</span>\&amp;recursive=<span class="literal">true</span></span><br><span class="line">&#123;<span class="string">&quot;action&quot;</span>:<span class="string">&quot;set&quot;</span>,<span class="string">&quot;node&quot;</span>:&#123;<span class="string">&quot;key&quot;</span>:<span class="string">&quot;/foo-service/container2&quot;</span>,<span class="string">&quot;value&quot;</span>:<span class="string">&quot;localhost:2222&quot;</span>,<span class="string">&quot;modifiedIndex&quot;</span>:48468,<span class="string">&quot;createdIndex&quot;</span>:48468&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="3、監控-etcd-directory-並觸發執行特定任務"><a href="#3、監控-etcd-directory-並觸發執行特定任務" class="headerlink" title="3、監控 etcd directory 並觸發執行特定任務"></a>3、監控 etcd directory 並觸發執行特定任務</h3><p>當其他 CoreOS host 執行執行以下指令：</p><ol><li><p><code>etcdctl set /foo-service/container2 localhost:2222</code></p></li><li><p><code>etcdctl set /foo-service/container3 localhost:3333</code></p></li><li><p><code>etcdctl rm /foo-service/container2</code></p></li><li><p><code>etcdctl rm /foo-service/container3</code></p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl exec-watch --recursive /foo-service -- sh -c <span class="string">&#x27;echo &quot;\&quot;$ETCD_WATCH_KEY\&quot; key was updated to \&quot;$ETCD_WATCH_VALUE\&quot; value by \&quot;$ETCD_WATCH_ACTION\&quot; action&quot;&#x27;</span></span><br><span class="line"><span class="string">&quot;/foo-service/container2&quot;</span> key was updated to <span class="string">&quot;localhost:2222&quot;</span> value by <span class="string">&quot;set&quot;</span> action</span><br><span class="line"><span class="string">&quot;/foo-service/container3&quot;</span> key was updated to <span class="string">&quot;localhost:3333&quot;</span> value by <span class="string">&quot;set&quot;</span> action</span><br><span class="line"><span class="string">&quot;/foo-service/container2&quot;</span> key was updated to <span class="string">&quot;&quot;</span> value by <span class="string">&quot;delete&quot;</span> action</span><br><span class="line"><span class="string">&quot;/foo-service/container3&quot;</span> key was updated to <span class="string">&quot;&quot;</span> value by <span class="string">&quot;delete&quot;</span> action</span><br></pre></td></tr></table></figure><h2 id="Test-and-Set"><a href="#Test-and-Set" class="headerlink" title="Test and Set"></a>Test and Set</h2><p>etcd 也可作為中央協調服務，提供 <strong>TestAndSet</strong> 功能；設定資料的人必須提供先前的值，有符合才可以變更值的內容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定值為 Hello</span></span><br><span class="line">core@coreos ~ $ etcdctl <span class="built_in">set</span> /message <span class="string">&quot;Hello&quot;</span></span><br><span class="line">Hello</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若是先前的值為 123，將值換為 Hi (操作失敗)</span></span><br><span class="line">core@coreos ~ $ etcdctl <span class="built_in">set</span> /message <span class="string">&quot;Hi&quot;</span> --swap-with-value=<span class="string">&quot;123&quot;</span></span><br><span class="line">Error:  101: Compare failed ([123 != Hello]) [48752]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若是先前的值為 Hello，將值換為 Hi (操作成功)</span></span><br><span class="line">core@coreos ~ $ etcdctl <span class="built_in">set</span> /message <span class="string">&quot;Hi&quot;</span> --swap-with-value=<span class="string">&quot;Hello&quot;</span></span><br><span class="line">Hi</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新值已經變成 Hi</span></span><br><span class="line">core@coreos ~ $ etcdctl get /message</span><br><span class="line">Hi</span><br></pre></td></tr></table></figure><h2 id="TTL"><a href="#TTL" class="headerlink" title="TTL"></a>TTL</h2><p>將資料寫入 etcd 時，還可以設定 TTL，讓資料在一定的時間之後失效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ etcdctl <span class="built_in">set</span> /foo <span class="string">&quot;Expiring Soon&quot;</span> --ttl 10</span><br><span class="line">Expiring Soon</span><br><span class="line">core@coreos ~ $ etcdctl get /foo</span><br><span class="line">Expiring Soon</span><br><span class="line">core@coreos ~ $ etcdctl get /foo</span><br><span class="line">Expiring Soon</span><br><span class="line">.........</span><br><span class="line">core@coreos ~ $ etcdctl get /foo</span><br><span class="line">Error:  100: Key not found (/foo) [49020]</span><br></pre></td></tr></table></figure><hr><h1 id="systemd"><a href="#systemd" class="headerlink" title="systemd"></a>systemd</h1><p>systemd 提供了許多強大的功能作為啟動、停止、管理 process 之用，在 CoreOS 中，幾乎所有的 Docker container 的生命週期都是由 systemd 在管理的。</p><p>CoreOS 預設所使用的 systemd target 為 <code>multi-user.target</code>，裡面包含了所有常用來管理 container 的 <code>systemd unit</code>。(<strong>target 由多個 unit symbolic link 所集合而成</strong>)</p><blockquote><p>systemd unit 是一個描述管理者所要執行的 process 的設定檔</p></blockquote><p>以下用一個簡單範例 <code>/etc/systemd/system/myweb.service</code> 來說明：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[Unit]</span></span><br><span class="line"><span class="attr">Description</span>=My Web Service</span><br><span class="line"><span class="attr">After</span>=etcd2.service</span><br><span class="line"><span class="attr">After</span>=docker.service</span><br><span class="line"></span><br><span class="line"><span class="section">[Service]</span></span><br><span class="line"><span class="attr">TimeoutStartSec</span>=<span class="number">0</span></span><br><span class="line"><span class="comment">; 在 ExecStart 所要執行的命令，&quot;=-&quot; 表示請 systemd 忽略執行命令時所發生的錯誤</span></span><br><span class="line"><span class="attr">ExecStartPre</span>=-/usr/bin/docker kill apache1</span><br><span class="line"><span class="attr">ExecStartPre</span>=-/usr/bin/docker rm apache1</span><br><span class="line"><span class="attr">ExecStartPre</span>=/usr/bin/docker pull coreos/apache</span><br><span class="line"><span class="comment">; service 啟動時所要執行的命令</span></span><br><span class="line"><span class="attr">ExecStart</span>=/usr/bin/docker run --name apache1 -p <span class="number">8081</span>:<span class="number">80</span> coreos/apache /usr/sbin/apache2ctl -D FOREGROUND</span><br><span class="line"><span class="comment">; service 啟動後要執行的指令 (以 machine ID 為 key，寫入訊息到 etcd /domains/example)</span></span><br><span class="line"><span class="attr">ExecStartPost</span>=/usr/bin/etcdctl set /domains/example.com/%m running</span><br><span class="line"><span class="comment">; service 停止時所要執行的指令 (停止 container apache1)</span></span><br><span class="line"><span class="attr">ExecStop</span>=/usr/bin/docker stop apache1</span><br><span class="line"><span class="comment">; service 停止後所要執行的步驟 (以 machine ID 為 key，移除 etcd /domains/example 上的訊息)</span></span><br><span class="line"><span class="attr">ExecStopPost</span>=/usr/bin/etcdctl rm /domains/example.com/%m</span><br><span class="line"></span><br><span class="line"><span class="section">[Install]</span></span><br><span class="line"><span class="attr">WantedBy</span>=multi-user.target</span><br></pre></td></tr></table></figure><blockquote><p>完整的設定參數可參考 =&gt; <a href="https://www.freedesktop.org/software/systemd/man/systemd.service.html">systemd.service</a></p></blockquote><p>另外還可以在 system unit configuration 檔案中指定 <strong>specifiers</strong>，例如：</p><ul><li><p><code>%n</code>：Full Unit Name</p></li><li><p><code>%i</code>：Instance Name</p></li><li><p><code>%m</code>：Machine ID</p></li><li><p><code>%H</code>：Host Name</p></li></ul><p>詳細的資料可以參考 =&gt; <a href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html#Specifiers">systemd.unit - Specifiers</a></p><p>上述的 systemd service unit configuration 執行的結果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ sudo systemctl <span class="built_in">enable</span> myweb.service</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/myweb.service to /etc/systemd/system/myweb.service.</span><br><span class="line">core@coreos ~ $ sudo systemctl start myweb.service</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ sudo systemctl status myweb.service</span><br><span class="line">● myweb.service - My Web Service</span><br><span class="line">   Loaded: loaded (/etc/systemd/system/myweb.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Tue 2016-06-21 07:36:22 UTC; 9s ago</span><br><span class="line">  Process: 2403 ExecStartPost=/usr/bin/etcdctl <span class="built_in">set</span> /domains/example.com/%m running (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 2391 ExecStartPre=/usr/bin/docker pull coreos/apache (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 2379 ExecStartPre=/usr/bin/docker rm apache1 (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 2369 ExecStartPre=/usr/bin/docker <span class="built_in">kill</span> apache1 (code=exited, status=1/FAILURE)</span><br><span class="line"> Main PID: 2402 (docker)</span><br><span class="line">   Memory: 11.5M</span><br><span class="line">      CPU: 5.695s</span><br><span class="line">   CGroup: /system.slice/myweb.service</span><br><span class="line">           └─2402 /usr/bin/docker run --name apache1 -p 8081:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND</span><br><span class="line">......</span><br><span class="line">Jun 21 07:36:19 coreos docker[2391]: Status: Image is up to date <span class="keyword">for</span> coreos/apache:latest</span><br><span class="line">Jun 21 07:36:22 coreos etcdctl[2403]: running</span><br><span class="line">Jun 21 07:36:22 coreos systemd[1]: Started My Web Service.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 container 運作狀態</span></span><br><span class="line">core@coreos ~ $ docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                  NAMES</span><br><span class="line">441cf4a6bf54        coreos/apache       <span class="string">&quot;/usr/sbin/apache2ctl&quot;</span>   35 seconds ago      Up 26 seconds       0.0.0.0:8081-&gt;80/tcp   apache1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認 web server 正常服務</span></span><br><span class="line">core@coreos ~ $ curl http://localhost:8081</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;This is the default web page <span class="keyword">for</span> this server.&lt;/p&gt;</span><br><span class="line">&lt;p&gt;The web server software is running but no content has been added, yet.&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 systemd 寫入 etcd 中的資料</span></span><br><span class="line">core@coreos ~ $ etcdctl ls /domains/example.com</span><br><span class="line">/domains/example.com/72bda15890c14ee89b15214c1b87d71f</span><br><span class="line">core@coreos ~ $ etcdctl get /domains/example.com/72bda15890c14ee89b15214c1b87d71f</span><br><span class="line">running</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止 service</span></span><br><span class="line">core@coreos ~ $ sudo systemctl stop myweb.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 container 狀態</span></span><br><span class="line">core@coreos ~ $ docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS                        PORTS               NAMES</span><br><span class="line">441cf4a6bf54        coreos/apache       <span class="string">&quot;/usr/sbin/apache2ctl&quot;</span>   About a minute ago   Exited (137) 13 seconds ago                       apache1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 systemd 寫入 etcd 中的資料 (已經移除)</span></span><br><span class="line">core@coreos ~ $ etcdctl ls /domains/example.com</span><br></pre></td></tr></table></figure><hr><h1 id="fleet-1"><a href="#fleet-1" class="headerlink" title="fleet"></a>fleet</h1><p>fleet 是個 cluster manager，用途是在 cluster level 上管理 systemd，因此要在 CoreOS cluster 上運行相關服務，正確的方式就是”<strong>使用標準的 systemd unit 搭配 fleet</strong>“來達成。</p><h2 id="fleet-unit-type"><a href="#fleet-unit-type" class="headerlink" title="fleet unit type"></a>fleet unit type</h2><p>可以運行在 cluster 上的 unit 有兩種，分別是：</p><ol><li><p><strong>standard unit</strong>：會被分派到獨立的單一主機上長時間運行的 process，若是機器掛點了，standard unit 就會被轉移到其他正常的主機上重新啟動並繼續執行</p></li><li><p><strong>global unit</strong>：global unit 將會運行在 cluster 中所有的機器上，這種 unit 適合用在像是 monitoring agent，甚至像是 high level 的 orchestration tool，例如：Kubernetes、Mesos、OpenStack …. 等等。</p></li></ol><h2 id="在-cluster-中啟動-container"><a href="#在-cluster-中啟動-container" class="headerlink" title="在 cluster 中啟動 container"></a>在 cluster 中啟動 container</h2><p>在這邊我們利用之前寫好的 <code>/etc/systemd/system/myweb.service</code> 作示範：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~ $ fleetctl start /etc/systemd/system/myweb.service</span><br><span class="line">Unit myweb.service inactive</span><br><span class="line">Unit myweb.service launched on 0682a8f2.../192.168.50.14</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ fleetctl list-units   </span><br><span class="line">UNIT        MACHINE                ACTIVE        SUB</span><br><span class="line">myweb.service    0682a8f2.../192.168.50.14    activating    start-pre</span><br><span class="line"></span><br><span class="line">(...... it takes some time to <span class="built_in">wait</span> .......)</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ fleetctl list-units</span><br><span class="line">UNIT        MACHINE                ACTIVE    SUB</span><br><span class="line">myweb.service    0682a8f2.../192.168.50.14    active    running</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ curl http://192.168.50.14:8081</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;This is the default web page <span class="keyword">for</span> this server.&lt;/p&gt;</span><br><span class="line">&lt;p&gt;The web server software is running but no content has been added, yet.&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br><span class="line"></span><br><span class="line">core@coreos ~ $ fleetctl list-machines</span><br><span class="line">MACHINE        IP        METADATA</span><br><span class="line">0682a8f2...    192.168.50.14    -</span><br><span class="line">72bda158...    192.168.50.16    -</span><br><span class="line">90c1e40c...    192.168.50.15    -</span><br></pre></td></tr></table></figure><h2 id="運行-High-Availability-Service"><a href="#運行-High-Availability-Service" class="headerlink" title="運行 High Availability Service"></a>運行 High Availability Service</h2><p>關於 fleet unit 的詳細設定方式，可以參考 =&gt; <a href="https://coreos.com/fleet/docs/latest/unit-files-and-scheduling.html">fleet - Overview of Unit Files and Scheduling</a></p><p>以下將會設計一個具有 High Availability 的 web service，其中較為重要的設定就是 <code>[X-Fleet]</code> 區塊部份的設定：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~/demo $ cat &lt;&lt;EOF &gt;apache@.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=My Apache Frontend</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">TimeoutStartSec=0</span><br><span class="line">ExecStartPre=-/usr/bin/docker <span class="built_in">kill</span> apache1</span><br><span class="line">ExecStartPre=-/usr/bin/docker rm apache1</span><br><span class="line">ExecStartPre=/usr/bin/docker pull coreos/apache</span><br><span class="line">ExecStart=/usr/bin/docker run --rm --name apache1 -p 80:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND</span><br><span class="line">ExecStop=/usr/bin/docker stop apache1</span><br><span class="line"></span><br><span class="line">[X-Fleet]</span><br><span class="line">; 告知 fleet 不要把 apache@* 的 service 放在同一台機器上</span><br><span class="line">Conflicts=apache@*.service</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 複製兩份 unit configuration file</span></span><br><span class="line">core@coreos ~/demo $ cp apache\@.service apache\@1.service</span><br><span class="line">core@coreos ~/demo $ cp apache\@.service apache\@2.service</span><br><span class="line">core@coreos ~/demo $ ls  </span><br><span class="line">apache@.service  apache@1.service  apache@2.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動第 1 個 apache container</span></span><br><span class="line">core@coreos ~/demo $ fleetctl start apache@1</span><br><span class="line">Unit apache@1.service inactive</span><br><span class="line">Unit apache@1.service launched on 0682a8f2.../192.168.50.14</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動第 2 個 apache container</span></span><br><span class="line">core@coreos ~/demo $ fleetctl start apache@2</span><br><span class="line">Unit apache@2.service inactive</span><br><span class="line">Unit apache@2.service launched on 72bda158.../192.168.50.16</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 units</span></span><br><span class="line">core@coreos ~/demo $ fleetctl list-units</span><br><span class="line">UNIT            MACHINE                ACTIVE    SUB</span><br><span class="line">apache@1.service    0682a8f2.../192.168.50.14    active    running</span><br><span class="line">apache@2.service    72bda158.../192.168.50.16    active    running</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 驗證 web 服務有正確啟動</span></span><br><span class="line">core@coreos ~/demo $ curl http://192.168.50.14</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;This is the default web page <span class="keyword">for</span> this server.&lt;/p&gt;</span><br><span class="line">&lt;p&gt;The web server software is running but no content has been added, yet.&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br><span class="line">core@coreos ~/demo $ curl http://192.168.50.16</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;This is the default web page <span class="keyword">for</span> this server.&lt;/p&gt;</span><br><span class="line">&lt;p&gt;The web server software is running but no content has been added, yet.&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>接著把 <code>192.168.50.14</code> 這台機器重新開機，此時 service 將會被 fleet 轉移到正常的機器上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~/demo $ fleetctl list-units</span><br><span class="line">UNIT            MACHINE                ACTIVE        SUB</span><br><span class="line">apache@1.service    90c1e40c.../192.168.50.15    activating    start-pre</span><br><span class="line">apache@2.service    72bda158.../192.168.50.16    active        running</span><br><span class="line"></span><br><span class="line">core@coreos ~/demo $ fleetctl list-units</span><br><span class="line">UNIT            MACHINE                ACTIVE        SUB</span><br><span class="line">apache@1.service    90c1e40c.../192.168.50.15    activating    start-pre</span><br><span class="line">apache@2.service    72bda158.../192.168.50.16    active        running</span><br><span class="line"></span><br><span class="line">core@coreos ~/demo $ fleetctl list-units</span><br><span class="line">UNIT            MACHINE                ACTIVE    SUB</span><br><span class="line">apache@1.service    90c1e40c.../192.168.50.15    active    running</span><br><span class="line">apache@2.service    72bda158.../192.168.50.16    active    running</span><br></pre></td></tr></table></figure><blockquote><p>存取這些 HA service 比較好的方式是透過 <strong>sidekick container</strong>，這類的 container 是用來提供像是 service discovery、load balancer、DNS 等工作，不建議直接存取 service container</p></blockquote><h2 id="運行-sidekick"><a href="#運行-sidekick" class="headerlink" title="運行 sidekick"></a>運行 sidekick</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">core@coreos ~/demo $ cat &lt;&lt;EOF &gt;apache_discovery@.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=Announce Apache1</span><br><span class="line">BindsTo=apache@%i.service</span><br><span class="line">After=apache@%i.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/bin/sh -c <span class="string">&quot;while true; do etcdctl set /services/website/apache@%i &#x27;&#123; \&quot;machine\&quot;: \&quot;%m\&quot;, \&quot;port\&quot;: 80, \&quot;version\&quot;: \&quot;52c7248a14\&quot; &#125;&#x27; --ttl 60;sleep 45;done&quot;</span></span><br><span class="line">ExecStop=/usr/bin/etcdctl rm /services/website/apache@%i</span><br><span class="line"></span><br><span class="line">[X-Fleet]</span><br><span class="line">MachineOf=apache@%i.service</span><br><span class="line">EOF</span><br><span class="line">core@coreos ~/demo $ cp apache_discovery\@.service apache_discovery\@1.service</span><br><span class="line">core@coreos ~/demo $ cp apache_discovery\@.service apache_discovery\@2.service</span><br><span class="line">core@coreos ~/demo $ ls</span><br><span class="line">apache@.service  apache@1.service  apache@2.service  apache_discovery@.service  apache_discovery@1.service  apache_discovery@2.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 service discovery</span></span><br><span class="line">core@coreos ~/demo $ fleetctl start apache_discovery@1         </span><br><span class="line">Unit apache_discovery@1.service inactive</span><br><span class="line">Unit apache_discovery@1.service launched on 90c1e40c.../192.168.50.15</span><br><span class="line"><span class="comment"># 啟動 service discovery</span></span><br><span class="line">core@coreos ~/demo $ fleetctl start apache_discovery@2</span><br><span class="line">Unit apache_discovery@2.service inactive</span><br><span class="line">Unit apache_discovery@2.service launched on 72bda158.../192.168.50.16</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">core@coreos ~/demo $ fleetctl list-units</span><br><span class="line">UNIT                MACHINE                ACTIVE    SUB</span><br><span class="line">apache@1.service        90c1e40c.../192.168.50.15    active    running</span><br><span class="line">apache@2.service        72bda158.../192.168.50.16    active    running</span><br><span class="line">apache_discovery@1.service    90c1e40c.../192.168.50.15    active    running</span><br><span class="line">apache_discovery@2.service    72bda158.../192.168.50.16    active    running</span><br><span class="line"></span><br><span class="line">core@coreos ~/demo $ etcdctl ls /services/ --recursive</span><br><span class="line">/services/website</span><br><span class="line">/services/website/apache@1</span><br><span class="line">/services/website/apache@2</span><br><span class="line"></span><br><span class="line">core@coreos ~/demo $ etcdctl get /services/website/apache@1</span><br><span class="line">&#123; <span class="string">&quot;machine&quot;</span>: <span class="string">&quot;90c1e40ca1e749abbc23fa8b2391ce27&quot;</span>, <span class="string">&quot;port&quot;</span>: 80, <span class="string">&quot;version&quot;</span>: <span class="string">&quot;52c7248a14&quot;</span> &#125;</span><br></pre></td></tr></table></figure><h2 id="Global-Unit"><a href="#Global-Unit" class="headerlink" title="Global Unit"></a>Global Unit</h2><p>Global Unit 與其他一般的 unit 沒什麼太大差別，只有在 <code>[X-Fleet]</code>區段中多加了 <code>Global=true</code> 設定而已，如此一來這個 system unit 就會在所有機器上執行</p><blockquote><p>若要把 Global Unit 執行在特定幾台機器上，可搭配 Machine Metadata 使用，就可以達到此目的</p></blockquote><h2 id="Machine-Metadata"><a href="#Machine-Metadata" class="headerlink" title="Machine Metadata"></a>Machine Metadata</h2><p>Mechine Metadata 的來源是從 cloud-config 所設定的，設定的語法如下：</p><blockquote><p>metadata=”platform=metal,provider=rackspace,region=east,disk=ssd”</p></blockquote><p>接著 Machine Metadata 會設定在 unit configuration 中，以下是幾個範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[X-Fleet]</span><br><span class="line">MachineMetadata=disk=ssd</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[X-Fleet]</span><br><span class="line">Conflicts=webapp*</span><br><span class="line">MachineMetadata=provider=rackspace</span><br><span class="line">MachineMetadata=platform=metal</span><br><span class="line">MachineMetadata=region=east</span><br></pre></td></tr></table></figure><hr><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><p><a href="https://coreos.com/os/docs/latest/quickstart.html">CoreOS Quick Start</a></p></li><li><p><a href="https://coreos.com/etcd/docs/latest/getting-started-with-etcd.html">Getting Started with etcd on CoreOS</a></p></li><li><p><a href="https://coreos.com/os/docs/latest/getting-started-with-docker.html">Getting Started with docker</a></p></li><li><p><a href="https://coreos.com/os/docs/latest/getting-started-with-systemd.html">Getting Started with systemd</a></p></li><li><p><a href="https://coreos.com/fleet/docs/latest/launching-containers-fleet.html">Launching Containers with fleet</a></p></li><li><p><a href="https://github.com/coreos/unit-examples/tree/master/simple-fleet">unit-examples/simple-fleet at master · coreos/unit-examples</a></p></li><li><p><a href="https://coreos.com/fleet/docs/latest/unit-files-and-scheduling.html">fleet - Overview of Unit Files and Scheduling</a></p></li><li><p><a href="https://coreos.com/os/docs/latest/cloud-config.html">Customize with Cloud-Config</a></p></li><li><p><a href="https://coreos.com/flannel/docs/latest/flannel-config.html">Configuring flannel Networking for CoreOS</a></p></li><li><p><a href="http://dockone.io/article/618">DockOne技术分享（十八）：一篇文章带你了解Flannel - DockOne.io</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> CoreOS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> CoreOS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH254 Chapter 6 Configuring Email Transmission Learning Notes</title>
      <link href="/blog/RHCE/RHCE7-RH254-LearningNotes-CH06_ConfiguringEmailTransmission/"/>
      <url>/blog/RHCE/RHCE7-RH254-LearningNotes-CH06_ConfiguringEmailTransmission/</url>
      
        <content type="html"><![CDATA[<h1 id="老師補充"><a href="#老師補充" class="headerlink" title="老師補充"></a>老師補充</h1><h2 id="postfix"><a href="#postfix" class="headerlink" title="postfix"></a>postfix</h2><ul><li><p>把功能獨立寫成一個個執行檔(例如：smtpd 收信、smtp 送信)</p></li><li><p>平時這些功能不會啟動，唯有真的要運作時，postfix(稱為 <strong>master</strong>) 才會把該功能叫起來執行後結束</p></li><li><p>可以加入 or 移除 third party 的模組(例如：防毒、防垃圾郵件)</p></li></ul><h2 id="郵件遞送流程"><a href="#郵件遞送流程" class="headerlink" title="郵件遞送流程"></a>郵件遞送流程</h2><ol><li><p>Kevin 寫了一封信給 David，使用 <strong>SMTP</strong> 協定把信送給自己公司的 mail server <strong>Mail_K</strong></p></li><li><p><strong>Mail_K</strong> 會暫時把 mail 放到系統的佇列中(<strong>/var/spool/postfix/*</strong>)</p><blockquote><p>透過 <code>postqueue -p</code> 可以搜尋目前佇列中待處理的 mail</p></blockquote></li><li><p>Kevin 的 mail 會被從佇列取出，並透過 SMTP 協定送給 David 公司的 mail server <strong>Mail_D</strong>，並存於 <strong>/var/spool/postfix/*</strong> 中</p></li><li><p>Mail_D 收完信後，會呼叫程式 <strong>local</strong>，把信件搬移到 <strong>/var/spool/mail</strong> 目錄中，並存成檔名 <strong>david</strong></p></li><li><p>David 可以使用 mail client，透過 <strong>POP3(tcp 110)</strong> or <strong>IMAP(tcp 43)</strong> 協定，把信件取回</p></li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol><li><p>POP3 會把 mail server 收回來，mail 就不會存在 mail sever 上了</p></li><li><p>IMAP 只用來讀取標題，可以指定讀取特定的信中的內容，從 mail server 複製回來</p></li></ol><h2 id="設定-mail-server"><a href="#設定-mail-server" class="headerlink" title="設定 mail server"></a>設定 mail server</h2><h3 id="架構"><a href="#架構" class="headerlink" title="架構"></a>架構</h3><ol><li><p>serverX 收信</p></li><li><p>desktopX 收信 &amp; 寄信</p></li><li><p>foundation 使用 mail function</p></li></ol><ul><li>模組設定檔：**/etc/postfix/master.cf**</li></ul><h3 id="設定-desktopX-mail-sender-smtp0-example-com"><a href="#設定-desktopX-mail-sender-smtp0-example-com" class="headerlink" title="設定 desktopX (mail sender) (smtp0.example.com)"></a>設定 desktopX (mail sender) (smtp0.example.com)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果有多個 ip，表示只會在這兩個 ip 上開啟 tcp port 25(全開寫 all)</span></span><br><span class="line"><span class="comment">#$ sudo postconf -e &quot;inet_interface = 127.0.0.1, 172.25.0.1&quot;</span></span><br><span class="line">$ sudo postconf -e <span class="string">&#x27;inet_interface = all&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 郵件地址偽裝</span></span><br><span class="line">$ sudo postconf -e <span class="string">&#x27;myorigin = example.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ACL (relay controll)</span></span><br><span class="line">$ sudo postconf -e <span class="string">&#x27;mynetworks = 127.0.0.0/8, 172.25.0.0/24&#x27;</span></span><br><span class="line"></span><br><span class="line">$ sudo systemctl restart postfix.service</span><br></pre></td></tr></table></figure><h3 id="設定-serverX-mail-sender-amp-receiver-imap0-example-com"><a href="#設定-serverX-mail-sender-amp-receiver-imap0-example-com" class="headerlink" title="設定 serverX (mail sender &amp; receiver) (imap0.example.com)"></a>設定 serverX (mail sender &amp; receiver) (imap0.example.com)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo postconf -e <span class="string">&#x27;inet_interface = all&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只接收 &quot;@&quot; 之後指定的 domain 的信，例如：只收寄到 xxxx@imap0.example.com 的信</span></span><br><span class="line">$ sudo postconf -e <span class="string">&#x27;mydestination = $myhostname, localhost.$mydomain, localhost, server0.example.com&#x27;</span></span><br><span class="line"></span><br><span class="line">$ sudo systemctl restart postfix.service</span><br></pre></td></tr></table></figure><h3 id="foundation-發信"><a href="#foundation-發信" class="headerlink" title="foundation 發信"></a>foundation 發信</h3><p>發信流程：<code>user</code> -&gt; <code>sendmail</code> -&gt; <code>Queue</code> -&gt; <code>postfix</code> -&gt; 送出去</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將信交給 smtp0.example.com 寄送</span></span><br><span class="line">$ sudo postconf -e <span class="string">&#x27;relay_host = [smtp0.example.com]&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 避免寄信時 domain 變成 foundation0.example.com</span></span><br><span class="line">$ sudo postconf -e <span class="string">&#x27;myorigin = example.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 別人寄信過來，信會被退回，後面加上自己設定的字串(local delivery disabled)</span></span><br><span class="line">$ sudo postconf -e <span class="string">&#x27;local_transport = error: local delivery disabled&#x27;</span></span><br><span class="line"></span><br><span class="line">$ sudo systemctl restart postfix.service</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;THIS IS MAIL TEST&quot;</span> | mail -s <span class="string">&quot;mail test&quot;</span> server0.example.com</span><br></pre></td></tr></table></figure><h2 id="如何在-postfix-上設定-POP3"><a href="#如何在-postfix-上設定-POP3" class="headerlink" title="如何在 postfix 上設定 POP3"></a>如何在 postfix 上設定 POP3</h2><h3 id="設定-POP3-server"><a href="#設定-POP3-server" class="headerlink" title="設定 POP3 server"></a>設定 POP3 server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@server0 ~]<span class="comment"># yum -y install dovecot</span></span><br><span class="line"></span><br><span class="line">[root@server0 ~]<span class="comment"># vim /etc/dovecot/dovecot.conf</span></span><br><span class="line"><span class="comment"># 同時提供 imap &amp; POP3</span></span><br><span class="line">protocols = imap pop3</span><br><span class="line"></span><br><span class="line">[root@server0 ~]<span class="comment"># vim /etc/dovecot/conf.d/10-auth.conf</span></span><br><span class="line"><span class="comment"># 不允許明碼驗證 (建議實務上設定為 yes)</span></span><br><span class="line">disable_plaintext_auth = no</span><br><span class="line"></span><br><span class="line">[root@server0 ~]<span class="comment"># vim /etc/dovecot/conf.d/10-ssl.conf</span></span><br><span class="line"><span class="comment"># 不支援 SSL 加密(實務上建議設定為 required)</span></span><br><span class="line">ssl = no</span><br><span class="line"></span><br><span class="line">[root@server0 ~]<span class="comment"># vim /etc/dovecot/conf.d/10-mail.conf</span></span><br><span class="line"><span class="comment"># 告訴 POP3 server，使用者郵件信箱的路徑</span></span><br><span class="line">mail_localtion = mbox:~/mail:INBOX=/var/mail%u</span><br><span class="line"><span class="comment"># 指定用哪個群組的身份讀取 mail</span></span><br><span class="line">mail_access_groups = mail</span><br><span class="line"></span><br><span class="line">[root@server0 ~]<span class="comment"># systemctl restart dovecot</span></span><br></pre></td></tr></table></figure><h3 id="驗證-POP3"><a href="#驗證-POP3" class="headerlink" title="驗證 POP3"></a>驗證 POP3</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@server0 ~]<span class="comment"># telnet server0 110</span></span><br><span class="line">user student</span><br><span class="line">pass student</span><br><span class="line"><span class="comment"># 列出信件</span></span><br><span class="line">list</span><br><span class="line"><span class="comment"># 取得第一封信件</span></span><br><span class="line">retr 1</span><br><span class="line">quit</span><br></pre></td></tr></table></figure><hr><h1 id="6-1-Configuring-a-Send-only-Email-Configuration"><a href="#6-1-Configuring-a-Send-only-Email-Configuration" class="headerlink" title="6.1 Configuring a Send-only Email Configuration"></a>6.1 Configuring a Send-only Email Configuration</h1><h2 id="6-1-1-Email-architecture-and-null-clients"><a href="#6-1-1-Email-architecture-and-null-clients" class="headerlink" title="6.1.1 Email architecture and null clients"></a>6.1.1 Email architecture and null clients</h2><p>RHEL7 Postfix 提供了 <code>/usr/sbin/sendmail</code> 作為內部發送通知訊息之用。</p><p><code>null client</code> 的工作僅將所有 email 轉送到其他的 mail relay，本章重點在於如何把機器設定為 Postfix null client，藉此透過 <code>sendmail</code> &amp; <code>SMTP</code> 協定將訊息送到外面的 mail server</p><h2 id="6-2-2-Transmission-of-an-email-message"><a href="#6-2-2-Transmission-of-an-email-message" class="headerlink" title="6.2.2 Transmission of an email message"></a>6.2.2 Transmission of an email message</h2><ol><li><p>mail client 透過 <code>SMTP</code> 傳送郵件</p></li><li><p>內部的寄信需求可能不需要認證就可以被接受(relay 的部分則大多會有一些規則上 &amp; 防火牆上的限制)</p></li><li><p>外部的 relay server 會根據目的地 domain 的 <code>MX</code> 紀錄，將信轉過去</p></li><li><p>收信者的 mail server 可能會支援 <code>POP3</code> or <code>IMAP</code> 等協定來將信件取回，也可能提供 web interface 供使用</p></li></ol><h2 id="6-2-3-Postfix"><a href="#6-2-3-Postfix" class="headerlink" title="6.2.3 Postfix"></a>6.2.3 Postfix</h2><p>Postfix 是 RHEL7 預設的 mail server，模組化設計，主要設定檔位於 <code>/etc/postfix/main.cf</code> 中 (設定檔都集中在 <code>/etc/postfix</code> 目錄下)</p><p>以下是比較重要的 Postfix 設定：</p><table><thead><tr><th>Setting</th><th>Purpose</th></tr></thead><tbody><tr><td><code>inet_interfaces</code></td><td>設定監聽流入 &amp; 流出訊息的網路介面。<br /><code>loopback-only</code>: 監聽 127.0.0.1 &amp; ::1<br /><code>all</code>: 監聽所有網路介面<br />Default: <code>inet_interfaces = localhost</code><br /><strong>【註】</strong>若有多個 hostname or ip 需要監聽，則用空白隔開</td></tr><tr><td><code>myorigin</code></td><td>將本地端發出去的 mail domain 改為此台主機<br />Default:<code>myorigin = $myhostname</code></td></tr><tr><td><code>relayhost</code></td><td>將訊息轉到指定的外部 mail relay<br />Default:<code>relayhost = </code></td></tr><tr><td><code>mydestination</code></td><td>設定 mail 的終點，到指定的地方後就會直接進入 local mailbox<br />Default:<code>mydestination = $myhostname, localhost.$mydomain, localhost</code></td></tr><tr><td><code>local_transport</code></td><td>定義到 <strong>mydestination</strong> 的 mail 要怎麼處理<br /><code>local:$myhostname</code>：使用 local mail agent 把 mail 存放到 /var/spool/mail 中</td></tr><tr><td><code>mynetworks</code></td><td>允許從這台 mail server 進行 relay 的來源清單(用空白隔開)<br />Default:<code>mynetworks = 127.0.0.0/8 [::1]/128</code></td></tr></tbody></table><p>要變更 Postfix 的行為，使用者可以透過手動修改 <code>/etc/postfix/main.cf</code> 或是透過 <code>postconf</code> 指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示所有設定</span></span><br><span class="line">$ postconf</span><br><span class="line">.....</span><br><span class="line">unverified_sender_reject_reason =</span><br><span class="line">unverified_sender_tempfail_action = <span class="variable">$reject_tempfail_action</span></span><br><span class="line">verp_delimiter_filter = -=+</span><br><span class="line">virtual_alias_domains = <span class="variable">$virtual_alias_maps</span></span><br><span class="line">virtual_alias_expansion_limit = 1000</span><br><span class="line">virtual_alias_maps = <span class="variable">$virtual_maps</span></span><br><span class="line">virtual_alias_recursion_limit = 1000</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line"><span class="comment"># 僅查詢指定的設定</span></span><br><span class="line">$ postconf inet_interfaces myorigin</span><br><span class="line">inet_interfaces = localhost</span><br><span class="line">myorigin = <span class="variable">$myhostname</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 &quot;-e&quot; 參數修改設定</span></span><br><span class="line">$ sudo postconf -e <span class="string">&#x27;myorigin = example.com&#x27;</span></span><br><span class="line">$ postconf myorigin</span><br><span class="line">myorigin = example.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要 reload or restart postfix.service 才會生效</span></span><br><span class="line">$ sudo systemctl reload postfix.service</span><br></pre></td></tr></table></figure><blockquote><p>建議直接編輯 <code>/etc/postfix/main.cf</code>，因為參數選項太多不容易記憶，詳細參數說明可參考 <code>postconf(5)</code></p></blockquote><h2 id="6-2-4-Postfix-null-client-configuration"><a href="#6-2-4-Postfix-null-client-configuration" class="headerlink" title="6.2.4 Postfix null client configuration"></a>6.2.4 Postfix null client configuration</h2><p>要把 Postfix 設定成一個標準的 null client，要達到下面幾個要求：</p><ol><li><p>sendmail 是用來將所有 email 轉送到外部的 mail relay 之用</p></li><li><p>local Postfix 不接受處理本地端傳送任何郵件訊息，只會負責 relay 出去</p></li><li><p>使用者可以透過 null client 收發信件</p></li></ol><p>統整之後，一共需要以下設定：</p><table><thead><tr><th>Directive</th><th>Null Client(serverX.example.com)</th></tr></thead><tbody><tr><td><code>inet_interfaces</code></td><td>inet_interfaces=loopback-only</td></tr><tr><td><code>myorigin</code></td><td>myorigin=desktopX.example.com</td></tr><tr><td><code>relayhost</code></td><td>relayhost=[smtpX.example.com]</td></tr><tr><td><code>mydestination</code></td><td>mydestination=</td></tr><tr><td><code>local_transport</code></td><td>local_transport=error: local delivery disabled</td></tr><tr><td><code>mynetworks</code></td><td>mynetworks=127.0.0.0/8, [::1]/128</td></tr></tbody></table><p>因此我們可以透過以下指令設定 null client：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定要 relay mail 的主機資訊</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;relayhost=[smtpX.example.com]&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定監聽介面</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;inet_interfaces = loopback-only&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定來自 localhsot 的郵件訊息都由 relay 處理</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;mynetworks=127.0.0.0/8 [::1]/128&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改寄信出去所表示的 domain name</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;myorigin=desktopX.example.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 null client 不是郵件的終點位置，避免郵件訊息傳到本機帳號</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;mydestination=&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止 local mail delivery</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;local_transport=error: local delivery disabled&quot;</span></span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Configuring-Send-only-Email-Service"><a href="#Practice-Configuring-Send-only-Email-Service" class="headerlink" title="Practice: Configuring Send-only Email Service"></a>Practice: Configuring Send-only Email Service</h1><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><ol><li><p>將 serverX 設定成 null client</p></li><li><p>將所有的 mail relay 到 <code>smtpX.example.com</code></p></li></ol><blockquote><p>desktopX &amp; serverX 都已經執行 lab 相關指令</p></blockquote><h2 id="實作過程"><a href="#實作過程" class="headerlink" title="實作過程"></a>實作過程</h2><p>1、設定 serverX</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo postconf -e <span class="string">&quot;relayhost=[smtp0.example.com]&quot;</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;inet_interfaces=loopback-only&quot;</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;mynetworks=127.0.0.0/8 [::1]/128&quot;</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;myorigin=desktop0.example.com&quot;</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;mydestination=&quot;</span></span><br><span class="line">$ sudo postconf -e <span class="string">&quot;local_transport=error: local delivery disabled&quot;</span></span><br><span class="line"></span><br><span class="line">[student@server0 ~]$ sudo systemctl restart postfix.service</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH254 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH254 Chapter 4 Network Port Security Learning Notes</title>
      <link href="/blog/RHCE/RHCE7-RH254-LearningNotes-CH04_NetworkPortSecurity/"/>
      <url>/blog/RHCE/RHCE7-RH254-LearningNotes-CH04_NetworkPortSecurity/</url>
      
        <content type="html"><![CDATA[<h1 id="4-1-Managing-Firewalld"><a href="#4-1-Managing-Firewalld" class="headerlink" title="4.1 Managing Firewalld"></a>4.1 Managing Firewalld</h1><h2 id="4-1-1-Firewalld-overview"><a href="#4-1-1-Firewalld-overview" class="headerlink" title="4.1.1 Firewalld overview"></a>4.1.1 Firewalld overview</h2><p><code>firewalld.service</code> 在 RHEL7 中目前是預設的防火牆管理用服務，因此也管理了 Linux kernel netfilter。</p><p>但因為 <code>firewalld.service</code> 與舊有的 <code>iptables.service</code>/<code>ip6tables.service</code>/<code>ebtables.service</code> 會衝突，因此兩個陣營建議選一個使用就好，假設若要使用 firewalld，那就使用 <code>systemctl mask</code> 指令把其他的 system unit 停止掉：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl mask iptables.service</span><br><span class="line">ln -s <span class="string">&#x27;/dev/null&#x27;</span> <span class="string">&#x27;/etc/systemd/system/iptables.service&#x27;</span></span><br><span class="line"></span><br><span class="line">$ sudo systemctl mask ip6tables.service</span><br><span class="line">ln -s <span class="string">&#x27;/dev/null&#x27;</span> <span class="string">&#x27;/etc/systemd/system/ip6tables.service&#x27;</span></span><br><span class="line"></span><br><span class="line">$ sudo systemctl mask ebtables.service</span><br><span class="line">ln -s <span class="string">&#x27;/dev/null&#x27;</span> <span class="string">&#x27;/etc/systemd/system/ebtables.service&#x27;</span></span><br></pre></td></tr></table></figure><p>firewalld 將所有進入的流量分為不同的 <code>zone</code> 來看待，每個 zone 都會擁有相對應的一組規則，處理原則就是<strong>先比對到的規則就先處理</strong></p><p>預設的 firewalld zone 如下：</p><table><thead><tr><th>Zone name</th><th>Default configuration</th></tr></thead><tbody><tr><td><code>trusted</code></td><td>允許全部進入的流量</td></tr><tr><td><code>home</code></td><td>1. 允許 <code>ssh</code>, <code>mdns</code>, <code>ipp-client</code>, <code>samba-client</code>, <code>dhcpv6-client</code> 等服務的流量<br />2. 允許與連外相關的進入流量<br/>3. 其他所有進入的流量皆拒絕</td></tr><tr><td><code>internal</code></td><td>與 zone <code>home</code> 相同</td></tr><tr><td><code>work</code></td><td>1. 允許 <code>ssh</code>, <code>ipp-client</code>, <code>dhcpv6-client</code> 等服務的流量<br />2. 允許與連外相關的進入流量<br/>3. 其他所有進入的流量皆拒絕</td></tr><tr><td><code>public</code></td><td>1. 允許 <code>ssh</code>, <code>dhcpv6-client</code> 等服務的流量<br />2. 允許與連外相關的進入流量<br/>3. 其他所有進入的流量皆拒絕<br /><strong>4. 是新增 network interface 的 default zone</strong></td></tr><tr><td><code>external</code></td><td>1. 僅允許 <code>ssh</code> 服務的流量<br />2. 允許與連外相關的進入流量<br/>3. 其他所有進入的流量皆拒絕<br />4. 所有連外的流量都會偽裝來源(<strong>NAT</strong>)</td></tr><tr><td><code>dmz</code></td><td>1. 僅允許 <code>ssh</code> 服務的流量<br />2. 允許與連外相關的進入流量<br/>3. 其他所有進入的流量皆拒絕</td></tr><tr><td><code>block</code></td><td>僅允許與連外相關的進入流量</td></tr><tr><td><code>drop</code></td><td>允許與連外相關的進入流量(<strong>連 ICMP error 都不會回應</strong>)</td></tr></tbody></table><h2 id="4-1-2-Managing-firewalld"><a href="#4-1-2-Managing-firewalld" class="headerlink" title="4.1.2 Managing firewalld"></a>4.1.2 Managing firewalld</h2><h3 id="Configure-firewall-settings-with-firewall-cmd"><a href="#Configure-firewall-settings-with-firewall-cmd" class="headerlink" title="Configure firewall settings with firewall-cmd"></a>Configure firewall settings with firewall-cmd</h3><p>RHEL7 提供 <code>firewall-cmd</code>(console) &amp; <code>firewall-config</code>(GUI) 作為管理防火牆之用。</p><p>透過 firewall-cmd 設定規則，有幾個重點需要注意：</p><ul><li><p>若沒特別指定都只是會只有在 runtime 才有效，重開機就無效了，除非加上 <code>--permanent</code> 選項。</p></li><li><p>指令中都會加上 <code>--zone=&lt;ZONE&gt;</code>，若沒加就會預設為 default zone</p></li><li><p>設定規則一般都會加上 <code>--permanent</code> 選項，並使用 <code>firewall-cmd --reload</code> 來進行永久性的變更套用</p></li><li><p>若只是暫時的測試，可以使用 <code>timeout=&lt;TIMEINSECONDS&gt;</code> 來讓規則短暫在 runtime 時有效</p></li></ul><table><thead><tr><th>firewall-cmd commands</th><th>Explanation</th></tr></thead><tbody><tr><td><code>--get-default-zone</code></td><td>查詢 default zone</td></tr><tr><td><code>--set-default-zone=&lt;ZONE&gt;</code></td><td>設定 dfault zone</td></tr><tr><td><code>--get-zones</code></td><td>列出所有支援的 zone</td></tr><tr><td><code>--get-services</code></td><td>列出所有支援的 service</td></tr><tr><td><code>--add-source=&lt;CIDR&gt;</code> [–zone=<ZONE>]</td><td>允許來自指定 source 的進入流量</td></tr><tr><td><code>--remove-source=&lt;CIDR&gt;</code> [–zone=<ZONE>]</td><td>禁止來自特定 source 的進入流量</td></tr><tr><td><code>--add-interface=&lt;INTERFACE&gt; [--zone=&lt;ZONE&gt;]</code></td><td>允許從特定裝置進入的流量</td></tr><tr><td><code>--change-interface=&lt;INTERFACE&gt; [--zone=&lt;ZONE&gt;]</code></td><td>設定指定的 interface 與 zone 關聯</td></tr><tr><td><code>--list-all [--zone=&lt;ZONE&gt;]</code></td><td>列出指定 zone 的所有 interface, source, service, port 等規則</td></tr><tr><td><code>--list-all-zones</code></td><td>取得所有 zone 的設定規則資訊</td></tr><tr><td><code>--add-service=&lt;SERVICE&gt; [--zone=&lt;ZONE&gt;]</code></td><td>允許進入流量到指定的 service</td></tr><tr><td><code>--add-port=&lt;PORT/PROTOCOL&gt; [--zone=&lt;ZONE&gt;]</code></td><td>允許進入流量到指定的 port or protocol</td></tr><tr><td><code>--remove-service=&lt;SERVICE&gt; [--zone=&lt;ZONE&gt;]</code></td><td>禁止進入流量到指定的 service</td></tr><tr><td><code>--remove-port=&lt;PORT/PROTOCOL&gt;</code> [–zone=<ZONE>]</td><td>禁止進入流量到指定的 port or protocol</td></tr><tr><td><code>--reload</code></td><td>將 runtime 設定變成永久設定<br />(<strong>沒有加上 –permanent 選項的規則會失效</strong>)</td></tr></tbody></table><h3 id="firewall-cmd-example"><a href="#firewall-cmd-example" class="headerlink" title="firewall-cmd example"></a>firewall-cmd example</h3><p>以下用幾個簡單範例，示範使用 firewall-cmd 達成以下設定：(使用 <code>dmz</code> zone)</p><ol><li><p>將 default zone 設定為 <code>dmz</code></p></li><li><p>zone <code>internal</code> 允許來自 192.168.0.0/24 的流量</p></li><li><p>zone <code>internal</code> 允許存取 mysql 服務</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo firewall-cmd --set-default-zone=dmz</span><br><span class="line"></span><br><span class="line">$ sudo firewall-cmd --permanent --zone=internal --add-source=192.168.0.0/24</span><br><span class="line"></span><br><span class="line">$ sudo firewall-cmd --perminant --zone=internal --add-service=mysql</span><br><span class="line"></span><br><span class="line">$ sudo firewall-cmd --reload</span><br></pre></td></tr></table></figure><h3 id="Firewall-configuration-files"><a href="#Firewall-configuration-files" class="headerlink" title="Firewall configuration files"></a>Firewall configuration files</h3><p>firewalld 的設定檔放在 <code>/etc/firewalld</code> &amp; <code>/usr/lib/firewalld</code> 目錄下，同樣的設定，放在 <strong>/etc/firewalld</strong> 目錄下的會優先被採用，這是讓管理者可以用來修改原有的預設值之用</p><hr><h1 id="Practice-Configuring-a-Firewall"><a href="#Practice-Configuring-a-Firewall" class="headerlink" title="Practice: Configuring a Firewall"></a>Practice: Configuring a Firewall</h1><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><ol><li><p>在 server 上安裝 <code>httpd</code> &amp; <code>mod_ssl</code> 套件，並確認 <code>httpd.service</code> 有啟用且運行中</p></li><li><p>web 首頁顯示 <code>COFFEE</code> 字眼</p></li><li><p>server 上必須啟動 <code>firewalld.service</code></p></li><li><p>在 server 上進行 firewalld 設定，使用 <code>dmz</code> zone 的設定處理未指定的連線</p></li><li><p>來自 <code>172.25.X.0/24</code> 的流量必須導引到 <code>work</code> zone 來處理</p></li><li><p><code>work</code> zone 必須開啟 <code>https</code> 服務所需要的所有 port，而 <code>http</code> 的流量則必須被過濾</p></li></ol><h2 id="實作過程"><a href="#實作過程" class="headerlink" title="實作過程"></a>實作過程</h2><p>安裝 &amp; 設定 httpd.service，並修改首頁：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum -y install httpd mod_ssl</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 httpd.service</span></span><br><span class="line">$ sudo systemctl <span class="built_in">enable</span> httpd.service</span><br><span class="line">ln -s <span class="string">&#x27;/usr/lib/systemd/system/httpd.service&#x27;</span> <span class="string">&#x27;/etc/systemd/system/multi-user.target.wants/httpd.service&#x27;</span></span><br><span class="line">$ sudo systemctl start httpd.service</span><br><span class="line"></span><br><span class="line">$ sudo systemctl status httpd.service</span><br><span class="line">httpd.service - The Apache HTTP Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled)</span><br><span class="line">   Active: active (running) since Thu 2016-05-26 10:25:01 JST; 5s ago</span><br><span class="line"> Main PID: 29559 (httpd)</span><br><span class="line">   Status: <span class="string">&quot;Processing requests...&quot;</span></span><br><span class="line">   CGroup: /system.slice/httpd.service</span><br><span class="line">           ├─29559 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           ├─29560 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           ├─29561 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           ├─29562 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           ├─29563 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           └─29564 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line"></span><br><span class="line">May 26 10:25:01 server0.example.com systemd[1]: Started The Apache HTTP Server.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改首頁</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;COFFEE&quot;</span> | sudo tee --append /var/www/html/index.html</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認網頁回傳的內容為 COFFEE</span></span><br><span class="line">$ curl http://localhost</span><br><span class="line">COFFEE</span><br></pre></td></tr></table></figure><p>確認 firewalld.service 狀態為啟動中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># firewalld.service 狀態</span></span><br><span class="line">$ sudo systemctl status firewalld.service</span><br><span class="line">firewalld.service - firewalld - dynamic firewall daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled)</span><br><span class="line">   Active: active (running) since Thu 2016-05-26 09:59:39 JST; 28min ago</span><br><span class="line"> Main PID: 475 (firewalld)</span><br><span class="line">   CGroup: /system.slice/firewalld.service</span><br><span class="line">           └─475 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid</span><br><span class="line"></span><br><span class="line">May 26 09:59:39 localhost systemd[1]: Started firewalld - dynamic firewall daemon.</span><br><span class="line"></span><br><span class="line"><span class="comment"># iptables.service 狀態</span></span><br><span class="line">$ sudo systemctl status iptables.service</span><br><span class="line">iptables.service - IPv4 firewall with iptables</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/iptables.service; disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ip6tables.service 狀態</span></span><br><span class="line">$ sudo systemctl status ip6tables.service</span><br><span class="line">ip6tables.service - IPv6 firewall with ip6tables</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ip6tables.service; disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ebtables.service 狀態</span></span><br><span class="line">$ sudo systemctl status ebtables.service</span><br><span class="line">ebtables.service - Ethernet Bridge Filtering tables</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ebtables.service; disabled)</span><br><span class="line">   Active: inactive (dead)</span><br></pre></td></tr></table></figure><p>設定 <code>dmz</code> zone 處理為指定連線：(即是設定為預設的 zone)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo firewall-cmd --set-default-zone=dmz</span><br><span class="line">success</span><br></pre></td></tr></table></figure><p>來自 <code>172.25.X.0/24</code> 的流量必須導引到 <code>work</code> zone 來處理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo firewall-cmd --permanent --zone=work --add-source=172.25.X.0/24</span><br></pre></td></tr></table></figure><p><code>work</code> zone 必須開啟 <code>https</code> 服務所需要的所有 port，而 <code>http</code> 的流量則必須被過濾：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo firewall-cmd --permanent --zone=work --add-service=https</span><br></pre></td></tr></table></figure><p>套用 &amp; 檢查設定：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ sudo firewall-cmd --reload</span><br><span class="line">success</span><br><span class="line"></span><br><span class="line">$ sudo firewall-cmd --get-default-zone</span><br><span class="line">dmz</span><br><span class="line"></span><br><span class="line">$ sudo firewall-cmd --get-active-zones</span><br><span class="line">dmz</span><br><span class="line">  interfaces: eth0</span><br><span class="line">work</span><br><span class="line">  sources: 172.25.X.0/24</span><br><span class="line">ROL</span><br><span class="line">  sources: 172.25.0.252/32</span><br><span class="line"></span><br><span class="line">$ sudo firewall-cmd --zone=work --list-all</span><br><span class="line">work</span><br><span class="line">  interfaces:</span><br><span class="line">  sources: 172.25.X.0/24</span><br><span class="line">  services: dhcpv6-client https ipp-client ssh</span><br><span class="line">  ports:</span><br><span class="line">  masquerade: no</span><br><span class="line">  forward-ports:</span><br><span class="line">  icmp-blocks:</span><br><span class="line">  rich rules:</span><br></pre></td></tr></table></figure><p>最後，從 desktop 查詢：<code>curl -k https://serverX</code></p><hr><h1 id="4-2-Managing-Rich-Rules"><a href="#4-2-Managing-Rich-Rules" class="headerlink" title="4.2 Managing Rich Rules"></a>4.2 Managing Rich Rules</h1><h2 id="4-2-1-Rich-rules-concepts"><a href="#4-2-1-Rich-rules-concepts" class="headerlink" title="4.2.1 Rich rules concepts"></a>4.2.1 Rich rules concepts</h2><p>除了 firewalld 原生支援的 zone &amp; service 之外，使用者還可以透過 <code>direct rules</code> &amp; <code>rich rules</code> 兩種機制來自訂規則</p><h3 id="Direct-rules"><a href="#Direct-rules" class="headerlink" title="Direct rules"></a>Direct rules</h3><p>顧名思義，這種機制就是為了相容 {ip,ip6,eb}tables rule 所產生出來的，若已經有既有的 rules，可透過此機制加入到 firewalld 的管理下，詳細的設定可以參考 <code>firewall-cmd(1)</code> &amp; <code>firewalld.direct(5)</code></p><h3 id="Rich-rules"><a href="#Rich-rules" class="headerlink" title="Rich rules"></a>Rich rules</h3><p>這就是全新給 firewalld 使用的客製化 rule 機制，除了規則之外，還可以設定 logging 的機制(使用 <code>syslog</code> &amp; <code>auditd</code>)，甚至連 port forwards, masquerading, rate limiting 等功能都可以設定。</p><p>rich rule 的設定基本語法架構如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rule</span><br><span class="line">    [<span class="built_in">source</span>]</span><br><span class="line">    [destination]</span><br><span class="line">    service|port|protocol|icmp-block|masquerade|forward-port</span><br><span class="line">    [<span class="built_in">log</span>]</span><br><span class="line">    [audit]</span><br><span class="line">    [accept|reject|drop]</span><br></pre></td></tr></table></figure><blockquote><p>完整的 rich rule 設定語法可以參考 <code>firewalld.richlanguage(5)</code></p></blockquote><h3 id="Rule-ordering"><a href="#Rule-ordering" class="headerlink" title="Rule ordering"></a>Rule ordering</h3><p>當規則多起來時，順序的問題就很重要了，順序如下：</p><ol><li><p>任何與 port forwarding &amp; masquerading 相關的規則</p></li><li><p>任何 logging 相關的規則</p></li><li><p>任何放行的規則</p></li><li><p>任何禁止的規則</p></li></ol><p>基本上，先比對到的規則就會先使用，都沒相符的規則就使用預設的規則，而每個 zone 都會有不同的預設規則。</p><p>不過 direct rule 會是例外，direct rule 會在 firewalld 開始過濾前就會先執行。</p><h3 id="Test-and-debugging"><a href="#Test-and-debugging" class="headerlink" title="Test and debugging"></a>Test and debugging</h3><p>為了測試與 debug 的方便，幾乎所有加入到 runtime 設定的規則都可以額外加上 <code>timeout</code> 的設定，時間到就會自動移除，之後確認沒問題就可以改用 <code>--permanent</code> 讓設定永久生效</p><h2 id="4-2-2-Working-with-rich-rules"><a href="#4-2-2-Working-with-rich-rules" class="headerlink" title="4.2.2 Working with rich rules"></a>4.2.2 Working with rich rules</h2><p>firewall-cmd 有 4 個選項用來處理 rich rule 之用，這些選項都可以搭配 <code>--permanent</code> &amp; <code>--zone</code> 使用：</p><ul><li><p><code>--add-rich-rule=&#39;&lt;RULE&gt;&#39;</code>新增規則</p></li><li><p><code>--remove-rich-rule=&#39;&lt;RULE&gt;&#39;</code>：移除規則</p></li><li><p><code>--query-rich-rule-&#39;&lt;RULE&gt;&#39;</code>：查詢規則是否有被加入到指定的 zone(or default zone)，回傳 0 表示有，1 則是沒有</p></li><li><p><code>--list-rich-rules</code>：顯示所有規則 (也可以用 <code>--list-all</code> or <code>--list-all-zones</code> 檢視所有規則)</p></li></ul><p>以下為一些簡單範例：(以下權限以 root 身份執行)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default zone = public</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --get-default-zone</span></span><br><span class="line">public</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 zone &quot;work&quot; 中設定拒絕來自 192.168.0.11 的流量，永久生效</span></span><br><span class="line"><span class="comment"># family 可以省略</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --zone=work --add-rich-rule=&#x27;rule family=ipv4 source address=192.168.0.11/32 reject&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 default zone 中，允許每分鐘能有兩個 ftp 連線產生</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --add-rich-rule=&#x27;rule service name=ftp limit value=2/m accept&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 default zone 中，丟棄 IPSec esp 協定的所有封包</span></span><br><span class="line"><span class="comment"># reject 會送 ICMP 回 client，但 drop 則丟棄後完全不回應(因此 drop 會常用在已知的惡意網路段上)</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --add-rich-rule=&#x27;rule protocol value=esp drop&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 zone &quot;work&quot; 中，設定允許來自 192.168.1.0/24 網段的流量，進入 tcp port 7900-7905</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --zone=work --add-rich-rule=&#x27;rule family=ipv4 source address=192.168.1.0/24 port port=7900-7905 protocol=tcp accept&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="4-2-3-Loggind-with-rich-rules"><a href="#4-2-3-Loggind-with-rich-rules" class="headerlink" title="4.2.3 Loggind with rich rules"></a>4.2.3 Loggind with rich rules</h2><p>logging 的功能很適合用在 debug or monitor 的需求上，firewalld 可以使用 <code>syslog</code>，也可以將訊息送給 <code>auditd</code>；此外，還可以設定 rate limit，避免 log 爆量產生塞滿硬碟</p><p>若要搭配 <code>syslog</code>，使用下面語法：(其中 Log Level 有 <code>emerg</code>, <code>alert</code>, <code>crit</code>, <code>error</code>, <code>warning</code>, <code>notice</code>, <code>info</code>, <code>debug</code> 幾種可用)</p><blockquote><p>log [prefix=”<PREFIX TEXT>“] [level=<LOG LEVEL>] [limit value=”&lt;RATE/DURATION&gt;”]</p></blockquote><p>若搭配 <code>auditd</code>，則使用下列語法：</p><blockquote><p>audit [limit value=”&lt;RATE/DURATION&gt;”]</p></blockquote><p>以下是則是設定 logging 功能相關的範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 zone &quot;work&quot; 中，設定允許 ssh 的流量</span></span><br><span class="line"><span class="comment"># 搭配 syslog 記錄(prefix=&quot;ssh &quot;, level=&quot;notice&quot;, 一分鐘最多三筆資料)</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --zone=work --add-rich-rule &#x27;rule service name=&quot;ssh&quot; log prefix=&quot;ssh &quot; level=&quot;notice&quot; limit value=&quot;3/m&quot; accept&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 default zone 中，拒絕來自 ipv6 &quot;2001:db8::/64&quot; 網段到達 dns 服務的流量</span></span><br><span class="line"><span class="comment"># 搭配 auditd，每個小時最多一筆紀錄</span></span><br><span class="line"><span class="comment"># 此規則在 300 秒後會自動失效</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --add-rich-rule=&#x27;rule family=ipv6 source address=&quot;2001:db8::/64&quot; service name=&quot;dns&quot; audit limit value=&quot;1/h&quot; reject&#x27; --timeout=300</span></span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Writing-Custom-Rules"><a href="#Practice-Writing-Custom-Rules" class="headerlink" title="Practice: Writing Custom Rules"></a>Practice: Writing Custom Rules</h1><h2 id="目標-1"><a href="#目標-1" class="headerlink" title="目標"></a>目標</h2><ol><li><p>server 安裝 <code>web server</code> 要讓 desktop 可以連</p></li><li><p>限定<code>最多每秒三筆 log 記錄</code>，且 log 記錄開頭必須為 <code>NEW HTTP </code></p></li></ol><h2 id="實作過程-1"><a href="#實作過程-1" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum -y install httpd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定首頁</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;NEW HTTP&quot;</span> | sudo tee /var/www/html/index.html</span><br><span class="line">NEW HTTP</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 web server</span></span><br><span class="line">$ sudo systemctl <span class="built_in">enable</span> httpd.service</span><br><span class="line">ln -s <span class="string">&#x27;/usr/lib/systemd/system/httpd.service&#x27;</span> <span class="string">&#x27;/etc/systemd/system/multi-user.target.wants/httpd.service&#x27;</span></span><br><span class="line">$ sudo systemctl start httpd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 確認本機服務正常 (此時 desktop 無法連入)</span></span><br><span class="line">$ curl http://localhost</span><br><span class="line">NEW HTTP</span><br><span class="line"></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --add-rich-rule=&#x27;rule family=ipv4 source address=172.25.0.10/32 service name=&quot;http&quot; log level=notice prefix=&quot;NEW HTTP &quot; limit value=&quot;3/s&quot; accept&#x27;</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --reload</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###### dekstop 連入一次 ######</span></span><br><span class="line">[root@server ~]<span class="comment"># tail /var/log/messages</span></span><br><span class="line">.......</span><br><span class="line">May 27 16:41:23 localhost kernel: NEW HTTP IN=eth0 OUT= MAC=52:54:00:00:00:0b:52:54:00:00:00:0a:08:00 SRC=172.25.0.10 DST=172.25.0.11 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=55651 DF PROTO=TCP SPT=49712 DPT=80 WINDOW=14600 RES=0x00 SYN URGP=0</span><br></pre></td></tr></table></figure><hr><h1 id="4-3-Masquerading-and-Port-Forwarding"><a href="#4-3-Masquerading-and-Port-Forwarding" class="headerlink" title="4.3 Masquerading and Port Forwarding"></a>4.3 Masquerading and Port Forwarding</h1><p>firewalld 也支援了 <code>masquerading</code>(<strong>SNAT</strong>, 稍有差異) &amp; <code>port forwarding</code>(<strong>DNAT</strong>) 兩種 NAT 模式</p><h2 id="4-3-1-Masquerading"><a href="#4-3-1-Masquerading" class="headerlink" title="4.3.1 Masquerading"></a>4.3.1 Masquerading</h2><p>設定 masquerading 的方式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將此機器設定為 zone &quot;work&quot; 的 NAT</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --zone=work --add-masquerade</span></span><br></pre></td></tr></table></figure><p>也可以使用 rich rule 設定較為複雜的規則：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 zone &quot;work&quot; 中，將此機器設定為來源為 192.168.0.0/24 的 NAT</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --zone=work --add-rich-rule &#x27;rule family=ipv4 source address=192.168.0.0/24 masquerade&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="4-3-2-Port-forwarding"><a href="#4-3-2-Port-forwarding" class="headerlink" title="4.3.2 Port forwarding"></a>4.3.2 Port forwarding</h2><p>設定 port forwarding 的語法如下：</p><blockquote><p>firewall-cmd –permanent –zone=&lt;ZONE&gt; –add-forward-port=port=&lt;PORT NUMBER&gt;:proto=&lt;PROTOCOL&gt;[:toport=&lt;PORTNUMBER&gt;] [:toaddr=&lt;IPADDR&gt;]</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 zone &quot;public&quot; 中，將到達 tcp:513 的流量導向 192.168.0.254:132</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --zone=public --add-forward-port=port=513:proto=tcp:toport=132:toaddr=192.168.0.254</span></span><br></pre></td></tr></table></figure><p>也可以使用 rich rule 設定較為複雜的規則：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 zone &quot;work&quot; 中，將來自 192.168.0.0/26，且到 tcp:80 的流量，導向 tcp:8080</span></span><br><span class="line">[root@server ~]<span class="comment">#  firewall-cmd --permanent --zone=work --add-rich-rule=&#x27;rule family=ipv4 source address=192.168.0.0/26 forward-port port=80 protocol=tcp to-port=8080&#x27;</span></span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Forwarding-a-port"><a href="#Practice-Forwarding-a-port" class="headerlink" title="Practice: Forwarding a port"></a>Practice: Forwarding a port</h1><h2 id="目標-2"><a href="#目標-2" class="headerlink" title="目標"></a>目標</h2><p>將來自 172.25.0.10/32　的 <code>tcp port 443</code> <code>ssh</code> 連線流量轉到 <code>tcp port 22</code></p><h2 id="實作過程-2"><a href="#實作過程-2" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --add-rich-rule=&#x27;rule family=ipv4 source address=172.25.0.10/32 forward-port port=443 protocol=tcp to-port=22&#x27;</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --reload</span></span><br></pre></td></tr></table></figure><hr><h1 id="4-4-Managing-SELinux-Port-Labeling"><a href="#4-4-Managing-SELinux-Port-Labeling" class="headerlink" title="4.4 Managing SELinux Port Labeling"></a>4.4 Managing SELinux Port Labeling</h1><p>要確保服務可以正常運行，就要確定 network port 擁有正確的 SELinux type　來與服務搭配才可以</p><h2 id="4-4-1-SELinux-port-labeling"><a href="#4-4-1-SELinux-port-labeling" class="headerlink" title="4.4.1 SELinux port labeling"></a>4.4.1 SELinux port labeling</h2><p>不僅是 file &amp; process，連 network traffic 都屬於 SELinux 的管理範圍內，舉例來說：<code>22/tcp</code> 就有一個 <code>ssh_port_t</code> 的 label 與其相關</p><p>因此當一個 process 要監聽某一個 port 時，SELinux 會去檢查 port 有沒有相對應的 label 可供 process 進行 binding，若沒有則 process 會無法正常監聽指定的 port；這功能可以防範不正常的 process 使用到一般服務的 port</p><h2 id="4-4-2-Managing-SELinux-port-labeling"><a href="#4-4-2-Managing-SELinux-port-labeling" class="headerlink" title="4.4.2 Managing SELinux port labeling"></a>4.4.2 Managing SELinux port labeling</h2><h3 id="Listing-port-labels"><a href="#Listing-port-labels" class="headerlink" title="Listing port labels"></a>Listing port labels</h3><p>需要調整 label 通常是因為管理者要讓服務監聽一個非標準的 port 上，以下指令可以列出目前系統中已經存在的 SELinux port type：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo semanage port -l</span><br><span class="line"></span><br><span class="line">SELinux Port Type              Proto    Port Number</span><br><span class="line">.........</span><br><span class="line">amavisd_send_port_t            tcp      10025</span><br><span class="line">amqp_port_t                    tcp      5671-5672</span><br><span class="line">amqp_port_t                    udp      5671-5672</span><br><span class="line">aol_port_t                     tcp      5190-5193</span><br><span class="line">.........</span><br></pre></td></tr></table></figure><h3 id="Managing-port-labels"><a href="#Managing-port-labels" class="headerlink" title="Managing port labels"></a>Managing port labels</h3><p>RHEL7 中提供 <code>semanage</code> 指令可用來修改 SELinux port type，指令類似如下：</p><blockquote><p>sudo semanage port -a -t PORT_LABEL -p tcp|udp PORT_NUMBER</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 允許 gopher service 監聽 tcp port 71</span></span><br><span class="line">$ sudo semanage port -a -t gopher_port_t -p tcp 71</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 port lebel 為 http_port_t</span></span><br><span class="line">$ sudo semanage port -m -t http_port_t -p tcp 71</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 port label</span></span><br><span class="line">$ sudo semanage port -d -t http_port_t -p tcp 71</span><br></pre></td></tr></table></figure><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>詳細的 SELinux 參考文件可透過安裝 <code>selinux-policy-devel</code> 套件取得：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum -y install selinux-policy-devel</span><br><span class="line">$ sudo mandb</span><br><span class="line">$ man -k _selinux</span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Managing-SELinux-Port-Labeling"><a href="#Practice-Managing-SELinux-Port-Labeling" class="headerlink" title="Practice: Managing SELinux Port Labeling"></a>Practice: Managing SELinux Port Labeling</h1><h2 id="目標-3"><a href="#目標-3" class="headerlink" title="目標"></a>目標</h2><ol><li><p>確保 <code>httpd.service</code> 啟用且執行</p></li><li><p>web server 監聽 <code>82/tcp</code></p></li></ol><h2 id="實作過程-3"><a href="#實作過程-3" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新增 port type label</span></span><br><span class="line">$ sudo semanage port -a -t http_port_t -p tcp 82</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新啟動 httpd.service</span></span><br><span class="line">[student@server0 ~]$ sudo systemctl <span class="built_in">enable</span> httpd.service</span><br><span class="line">[student@server0 ~]$ sudo systemctl restart httpd.service</span><br><span class="line">[student@server0 ~]$ sudo systemctl status httpd.service</span><br><span class="line">httpd.service - The Apache HTTP Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled)</span><br><span class="line">   Active: active (running) since Sat 2016-05-28 10:43:14 JST; 2s ago</span><br><span class="line"> Main PID: 2080 (httpd)</span><br><span class="line">   Status: <span class="string">&quot;Processing requests...&quot;</span></span><br><span class="line">   CGroup: /system.slice/httpd.service</span><br><span class="line">           ├─2080 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           ├─2081 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           ├─2082 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           ├─2083 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           ├─2084 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">           └─2085 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line"></span><br><span class="line">May 28 10:43:14 server0.example.com systemd[1]: Started The Apache HTTP Server.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 root 身分新增 firewall 設定</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --add-port=82/tcp</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --reload</span></span><br></pre></td></tr></table></figure><hr><h1 id="Lab-Network-Port-Security"><a href="#Lab-Network-Port-Security" class="headerlink" title="Lab: Network Port Security"></a>Lab: Network Port Security</h1><h2 id="目標-4"><a href="#目標-4" class="headerlink" title="目標"></a>目標</h2><ol><li><p>讓 <code>sshd.service</code> 同時監聽兩個 port(<code>22/tcp</code> &amp; <code>999/tcp</code>)</p></li><li><p>來自 <code>172.25.0.0/24</code> 的流量可以允許進入 zone <code>work</code></p></li><li><p><code>22/tcp</code> &amp; <code>999/tcp</code> 在 zone <code>work</code> 中必須都要能夠使用</p></li></ol><h2 id="實作過程-4"><a href="#實作過程-4" class="headerlink" title="實作過程"></a>實作過程</h2><p>檢查 sshd.service 狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl status sshd.service</span><br><span class="line">sshd.service - OpenSSH server daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/sshd.service; enabled)</span><br><span class="line">   Active: active (running) since Sat 2016-05-28 10:48:41 JST; 2min 6s ago</span><br><span class="line">  Process: 1742 ExecStartPre=/usr/sbin/sshd-keygen (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 1743 (sshd)</span><br><span class="line">   CGroup: /system.slice/sshd.service</span><br><span class="line">           └─1743 /usr/sbin/sshd -D</span><br><span class="line"></span><br><span class="line">May 28 10:48:41 server0.example.com systemd[1]: Starting OpenSSH server daemon...</span><br><span class="line">May 28 10:48:41 server0.example.com systemd[1]: Started OpenSSH server daemon.</span><br><span class="line">May 28 10:48:41 server0.example.com sshd[1743]: error: Bind to port 999 on 0.0.0.0 failed: Permission denied.</span><br><span class="line">May 28 10:48:41 server0.example.com sshd[1743]: error: Bind to port 999 on :: failed: Permission denied.</span><br><span class="line">May 28 10:48:41 server0.example.com sshd[1743]: Server listening on 0.0.0.0 port 22.</span><br><span class="line">May 28 10:48:41 server0.example.com sshd[1743]: Server listening on :: port 22.</span><br><span class="line">May 28 10:48:42 server0.example.com python[1747]: SELinux is preventing /usr/sbin/sshd from name_bind access on the tcp_socket .</span><br><span class="line"></span><br><span class="line">                                                  *****  Plugin bind_ports (92.2 confidence) suggests   ************************...</span><br><span class="line">May 28 10:48:42 server0.example.com python[1747]: SELinux is preventing /usr/sbin/sshd from name_bind access on the tcp_socket .</span><br><span class="line"></span><br><span class="line">                                                  *****  Plugin bind_ports (92.2 confidence) suggests   ************************...</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>加入 port label for sshd.service：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@server ~]<span class="comment"># semanage port -a -t ssh_port_t -p tcp 999</span></span><br><span class="line">[root@server ~]<span class="comment"># systemctl reload sshd.service</span></span><br><span class="line">[root@server ~]<span class="comment"># systemctl status sshd.service</span></span><br><span class="line">sshd.service - OpenSSH server daemon</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/sshd.service; enabled)</span><br><span class="line">   Active: active (running) since Sat 2016-05-28 11:10:17 JST; 2min 51s ago</span><br><span class="line">  Process: 2225 ExecReload=/bin/<span class="built_in">kill</span> -HUP <span class="variable">$MAINPID</span> (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 1708 ExecStartPre=/usr/sbin/sshd-keygen (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 1710 (sshd)</span><br><span class="line">   CGroup: /system.slice/sshd.service</span><br><span class="line">           └─1710 /usr/sbin/sshd -D</span><br><span class="line"></span><br><span class="line">May 28 11:10:17 server0.example.com sshd[1710]: Server listening on :: port 22.</span><br><span class="line">May 28 11:10:18 server0.example.com python[1713]: SELinux is preventing /usr/sbin/sshd from name_bind access on the tcp_socket .</span><br><span class="line"></span><br><span class="line">                                                  *****  Plugin bind_ports (92.2 confidence) suggests   ************************...</span><br><span class="line">May 28 11:10:18 server0.example.com python[1713]: SELinux is preventing /usr/sbin/sshd from name_bind access on the tcp_socket .</span><br><span class="line"></span><br><span class="line">                                                  *****  Plugin bind_ports (92.2 confidence) suggests   ************************...</span><br><span class="line">May 28 11:13:03 server0.example.com systemd[1]: Reloading OpenSSH server daemon.</span><br><span class="line">May 28 11:13:04 server0.example.com sshd[1710]: Received SIGHUP; restarting.</span><br><span class="line">May 28 11:13:04 server0.example.com systemd[1]: Reloaded OpenSSH server daemon.</span><br><span class="line">May 28 11:13:04 server0.example.com sshd[1710]: Server listening on 0.0.0.0 port 999.</span><br><span class="line">May 28 11:13:04 server0.example.com sshd[1710]: Server listening on :: port 999.</span><br><span class="line">May 28 11:13:04 server0.example.com sshd[1710]: Server listening on 0.0.0.0 port 22.</span><br><span class="line">May 28 11:13:04 server0.example.com sshd[1710]: Server listening on :: port 22.</span><br><span class="line">Hint: Some lines were ellipsized, use -l to show <span class="keyword">in</span> full.</span><br></pre></td></tr></table></figure><p>放行相關的防火牆：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --zone=work --add-source=172.25.0.0/24</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --permanent --zone=work --add-port=999/tcp</span></span><br><span class="line">[root@server ~]<span class="comment"># firewall-cmd --reload</span></span><br></pre></td></tr></table></figure><hr><h1 id="老師補充-iptables"><a href="#老師補充-iptables" class="headerlink" title="老師補充(iptables)"></a>老師補充(iptables)</h1><ul><li><p>Linux 防火牆叫做 <strong>netfilter</strong></p></li><li><p>Linux 把很多功能獨立出來成一個一個小檔案，可以到 <strong>/lib/modules/<kernel version>/kernel/</strong> 目錄下找 <code>*.ko</code> 的檔案</p></li></ul><blockquote><p>在上面的目錄的 <code>netfilter</code> / <code>ipv4</code> / <code>ipv6</code> 等資料夾下，都是跟網路有關的模組，可參考 <a href="http://www.netfilter.org/">http://www.netfilter.org</a> 尋找 <code>extention howto</code> 文件，查詢 ko 的使用方式</p></blockquote><h2 id="1、netfilter-tables"><a href="#1、netfilter-tables" class="headerlink" title="1、netfilter tables"></a>1、netfilter tables</h2><p>主要有五個 table：</p><ol><li><p><strong>filter</strong>：主要防火牆的功能</p></li><li><p><strong>nat</strong>：IP 分享器的功能進到主機之後，就直接轉出去的封包，歸類為 FORWARD (例如：本機為 router)</p></li><li><p><strong>mangle</strong>：可對封包的內容小幅度的修改(例如：TTL，可偽裝成不同的作業系統)</p></li><li><p><strong>raw</strong>：調整連線追蹤的功能，可以指定某些電腦不做連線追蹤</p></li><li><p><strong>security</strong></p></li></ol><h3 id="1-1-filter-chains"><a href="#1-1-filter-chains" class="headerlink" title="1.1 filter chains"></a>1.1 filter chains</h3><ul><li><p><strong>INPUT</strong>：從外面進來，目的地為本機應用程式的封包，歸類為 INPUT</p></li><li><p><strong>FORWARD</strong>：進到主機之後，就直接轉出去的封包，歸類為 FORWARD (例如：本機為 router)</p></li><li><p><strong>OUTPUT</strong>：本機的應用程式，目的地是外面主機的封包，歸類為 OUTPUT</p></li></ul><blockquote><p>Linux netfilter 架構：<code>table</code> -&gt; <code>chain</code> -&gt; <code>rule</code></p></blockquote><blockquote><p>chain default policy：只有 <code>ACCEPT</code> &amp; <code>DROP</code> 兩種，完全沒動過就會是 ACCEPT</p></blockquote><h3 id="1-2-連線追蹤-使用-state-模組"><a href="#1-2-連線追蹤-使用-state-模組" class="headerlink" title="1.2 連線追蹤(使用 state 模組)"></a>1.2 連線追蹤(使用 state 模組)</h3><p>狀態分為四種：</p><ol><li><p><strong>NEW</strong>：所有連線的第一個封包狀態，皆為 NEW</p></li><li><p><strong>ESTABLISHED</strong>：當封包穿越了 FW，這條連線的後續封包狀態都會變成 ESTABLISHED</p></li><li><p><strong>RELATED</strong>：因為主動產生的連線而發生的其他類型封包</p></li><li><p><strong>INVALID</strong>：狀態不明的封包，建議一律丟棄</p></li></ol><h3 id="1-3-iptables-他使用上的注意事項"><a href="#1-3-iptables-他使用上的注意事項" class="headerlink" title="1.3 iptables 他使用上的注意事項"></a>1.3 iptables 他使用上的注意事項</h3><ul><li><p>若要阻擋特定其他使用上的注意事項</p></li><li><p>若要阻擋特定網路流量，對內使用 <strong>REJECT</strong>、對外使用 <strong>DROP</strong></p></li><li><p><code>--dport 80</code>：destination port = 80</p></li><li><p><code>-i eth0</code>：incoming interface = eth0</p></li><li><p><code>-o eth1</code>：outgoing interface = eth1</p></li><li><p>為了避免影響 loopback interface，不建議設定 chain default policy，而是加上 <code>iptables -t filter -A INPUT -i eth0 -j DROP</code> 到最後一條規則</p></li><li><p><code>iptables -L -v -n</code>：可看出那一條規則比較熱門</p></li></ul><h2 id="2、NAT-種類"><a href="#2、NAT-種類" class="headerlink" title="2、NAT 種類"></a>2、NAT 種類</h2><ul><li><p>把來源端 ip 換掉的 NAT，稱為 <strong>SNAT</strong> (POSTROUTING chain 的任務)</p></li><li><p>把目的端 ip 換掉的 NAT，稱為 <strong>DNAT</strong> (PREROUTING chain 的任務)</p></li><li><p>本機產生的封包，原本僅能有 SNAT，而 nat OUTPUT chain 是要補足 DNAT 的功能</p></li><li><p>要作 NAT 必須開啟 port forward 功能：<code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward</code></p></li></ul><h2 id="3、iptables-使用方式"><a href="#3、iptables-使用方式" class="headerlink" title="3、iptables 使用方式"></a>3、iptables 使用方式</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@server0 /]<span class="comment"># systemctl disable firewalld</span></span><br><span class="line">[root@server0 /]<span class="comment"># systemctl stop firewalld</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># List chains of filter table (可以看 chain default rule)</span></span><br><span class="line">[root@server0 /]<span class="comment"># iptables -t filter -L</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Append rule to chain FORWARD</span></span><br><span class="line"><span class="comment"># -p：指定 protocol，可參考 /etc/protocols</span></span><br><span class="line"><span class="comment"># -s：source</span></span><br><span class="line">[root@server0 /]<span class="comment"># iptables -t filter -A FORWARD -p tcp -s 172.25.254.250 -j ACCEPT</span></span><br><span class="line">[root@server0 /]<span class="comment"># iptables -t filter -A FORWARD -p udp -s 172.25.254.250 -j ACCEPT</span></span><br><span class="line">[root@server0 /]<span class="comment"># iptables -t filter -A FORWARD -p icmp -s 172.25.254.250 -j ACCEPT</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -n：以數字方式表示</span></span><br><span class="line"><span class="comment"># --line-number：顯示第幾條規則</span></span><br><span class="line">[root@server0 /]<span class="comment"># iptables -t filter -L -n --line-number</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 chain default rule</span></span><br><span class="line">[root@server0 /]<span class="comment"># iptables -t filter -P INPUT DROP</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除 chain rules</span></span><br><span class="line">[root@server0 /]<span class="comment"># iptables -t filter -F</span></span><br></pre></td></tr></table></figure><h2 id="4、基本操作練習"><a href="#4、基本操作練習" class="headerlink" title="4、基本操作練習"></a>4、基本操作練習</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -L</span></span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line"></span><br><span class="line">Chain FORWARD (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line"></span><br><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -A INPUT -p tcp -j ACCEPT</span></span><br><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -A INPUT -p udp -j ACCEPT</span></span><br><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -A INPUT -p icmp -j ACCEPT</span></span><br><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -L</span></span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line">ACCEPT     tcp  --  anywhere             anywhere            </span><br><span class="line">ACCEPT     udp  --  anywhere             anywhere            </span><br><span class="line">ACCEPT     icmp --  anywhere             anywhere            </span><br><span class="line"></span><br><span class="line">Chain FORWARD (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 chain default policy  </span></span><br><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -P INPUT DROP</span></span><br><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -L</span></span><br><span class="line">Chain INPUT (policy DROP)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line">ACCEPT     tcp  --  anywhere             anywhere            </span><br><span class="line">ACCEPT     udp  --  anywhere             anywhere            </span><br><span class="line">ACCEPT     icmp --  anywhere             anywhere            </span><br><span class="line"></span><br><span class="line">Chain FORWARD (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line"></span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination         </span><br><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -F</span></span><br><span class="line">FORWARD  INPUT    OUTPUT   </span><br><span class="line"></span><br><span class="line"><span class="comment"># remove all chain rules</span></span><br><span class="line">[root@server0 ~]<span class="comment"># iptables -t filter -F INPUT</span></span><br></pre></td></tr></table></figure><h2 id="5、使用-shell-script-建立-FW"><a href="#5、使用-shell-script-建立-FW" class="headerlink" title="5、使用 shell script 建立 FW"></a>5、使用 shell script 建立 FW</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 防止 sync flooding 攻擊(開啟 tcp sync cookie)</span></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/net/ipv4/tcp_syncookies</span><br><span class="line"></span><br><span class="line">SRV=172.25.0.11</span><br><span class="line"></span><br><span class="line">iptables -t filter -F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 connection track</span></span><br><span class="line">iptables -t filter -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line"><span class="comment"># 避免 INVALID 封包被其他服務所接收</span></span><br><span class="line">iptables -t filter -A INPUT -m state --state INVALID -j DROP</span><br><span class="line"></span><br><span class="line">iptables -t filter -A INPUT -d <span class="variable">$SRV</span> -p tcp --dport 22 -j ACCEPT</span><br><span class="line">iptables -t filter -A INPUT -d <span class="variable">$SRV</span> -p tcp --dport 25 -j ACCEPT</span><br><span class="line">iptables -t filter -A INPUT -d <span class="variable">$SRV</span> -p tcp --dport 110 -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -t nat -A PREROUTING -i eth0 --dport 443 -j DNAT --to 127.0.0.1:22</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用來取代 chain default policy</span></span><br><span class="line">iptables -t filter -A INPUT -i eth0 -j DROP</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH254 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH254 Chapter 3 Configuring Link Aggregation And Bridging Learning Notes</title>
      <link href="/blog/RHCE/RHCE7-RH254-LearningNotes-CH03_ConfiguringLinkAggregationAndBridging/"/>
      <url>/blog/RHCE/RHCE7-RH254-LearningNotes-CH03_ConfiguringLinkAggregationAndBridging/</url>
      
        <content type="html"><![CDATA[<h1 id="老師補充"><a href="#老師補充" class="headerlink" title="老師補充"></a>老師補充</h1><h3 id="runner"><a href="#runner" class="headerlink" title="runner"></a>runner</h3><ul><li><p><strong>activebackup</strong>：同時間只有一張網卡提供服務</p></li><li><p><strong>roundrobin</strong>：outbound 頻寬可以合併，但 inbound 無法</p></li><li><p><strong>broadcast</strong>：封包往外送時，會往每張網卡都各送一份</p></li><li><p><strong>loadbalance</strong>：以 client ip 作 hash 後決定用哪個網卡提供網路服務</p></li><li><p><strong>lacp</strong>：進出都有 load balance</p></li></ul><h3 id="teaming"><a href="#teaming" class="headerlink" title="teaming"></a>teaming</h3><p>透過 nmcli 修改 runner：<code>nmcli connection modify team0 team.config &#39;&#123;&quot;runner&quot;: &#123;&quot;name&quot;: &quot;activebackup&quot;&#125;&#125;&#39;</code></p><hr><h1 id="3-1-Configuring-Network-Teaming"><a href="#3-1-Configuring-Network-Teaming" class="headerlink" title="3.1 Configuring Network Teaming"></a>3.1 Configuring Network Teaming</h1><h2 id="3-1-1-Network-teaming"><a href="#3-1-1-Network-teaming" class="headerlink" title="3.1.1 Network teaming"></a>3.1.1 Network teaming</h2><p>Network Teaming 的用途是將兩個實體的網路卡結合成一張邏輯上的虛擬網卡，用來提供 failover &amp; 更高的 throughput 目的之用，目前 RHEL7 中提供的 network teaming 比起以前的 bonding 有較好的效能以及擴充性(模組式設計)。</p><p>RHEL 7 中有一個 kernel driver(負責有效率的處理網路封包) 以及稱為 <code>teamd</code>(負責管理 interface) 的 user-space daemon 來負責實現 network teaming 的功能；其中還有稱為 <code>runner</code> 的軟體用來處理網路封包在多條實體網路間配送的問題。</p><p>RHEL 7 中以下支援五種 runner，分別是 <code>broadcast</code>、<code>roundrobin</code>、<code>activebackup</code>、<code>loadbalance</code>、<code>lacp</code>，其中以 <strong>lacp</strong> 的效率最好，但網路卡所連接的 switch 也要做相對應的設定才可以。</p><blockquote><p>bonding 消耗的資源比 teaming 還多，因此建議使用 teaming</p></blockquote><h2 id="3-1-2-Configuring-network-teams"><a href="#3-1-2-Configuring-network-teams" class="headerlink" title="3.1.2 Configuring network teams"></a>3.1.2 Configuring network teams</h2><p>設定 network teaming 共分為四個步驟：(以下直接用範例說明)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 team interface (runner 會使用 json 格式字串來指定)</span></span><br><span class="line">$ sudo nmcli connection add con-name team0 <span class="built_in">type</span> team ifname team0 config <span class="string">&#x27;&#123;&quot;runner&quot;: &#123;&quot;name&quot;: &quot;loadbalance&quot;&#125;&#125;&#x27;</span></span><br><span class="line">Connection <span class="string">&#x27;team0&#x27;</span> (5d8173d9-fc07-4b11-a72c-5be10d1bea68) successfully added.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 馬上就會出現 teaming interface(team0) 了</span></span><br><span class="line">$ ip addr show team0</span><br><span class="line">5: team0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN</span><br><span class="line">    link/ether 6e:68:9a:d9:75:e7 brd ff:ff:ff:ff:ff:ff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 IPv4 &amp; IPv6 相關資訊，以及此 teaming interface 的其他屬性</span></span><br><span class="line">$ sudo nmcli connection modify team0 ipv4.method manual ipv4.addresses 192.168.1.10/24</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定要做 network teaming 的網路卡</span></span><br><span class="line">[student@server0 ~]$ sudo nmcli connection add con-name team0-eth1 <span class="built_in">type</span> team-slave ifname eth1 master team0</span><br><span class="line">Connection <span class="string">&#x27;team0-eth1&#x27;</span> (07fc837c-81d8-4f4e-b4c1-a66151128e17) successfully added.</span><br><span class="line">[student@server0 ~]$ sudo nmcli connection add con-name team0-eth2 <span class="built_in">type</span> team-slave ifname eth2 master team0</span><br><span class="line">Connection <span class="string">&#x27;team0-eth2&#x27;</span> (70589bef-cd0d-4e6a-8ccc-fdba85733d1d) successfully added.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 teaming 介面</span></span><br><span class="line">$ sudo nmcli connection up team0</span><br><span class="line">Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/4)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視 teaming 介面狀態</span></span><br><span class="line">$ sudo teamdctl team0 state</span><br><span class="line">setup:</span><br><span class="line">  runner: loadbalance</span><br><span class="line">ports:</span><br><span class="line">  eth1</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line">  eth2</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line"></span><br><span class="line"><span class="comment"># 若因特殊需求需要斷掉某個實體網路卡，可使用以下指令</span></span><br><span class="line">$ sudo nmcli device connect eth1</span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Configuring-Network-Teaming"><a href="#Practice-Configuring-Network-Teaming" class="headerlink" title="Practice: Configuring Network Teaming"></a>Practice: Configuring Network Teaming</h1><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><ol><li><p>建立一個名稱為 <code>team0</code> 的 network team interface，使用 <code>eno1</code> &amp; <code>eno2</code> 兩張網卡</p></li><li><p>runner 為 <code>activebackup</code></p></li><li><p>IP 為 <code>192.168.0.100/24</code></p></li></ol><h2 id="實作過程"><a href="#實作過程" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ sudo nmcli connection add con-name team0 <span class="built_in">type</span> team ifname team0 config <span class="string">&#x27;&#123;&quot;runner&quot;: &#123;&quot;name&quot;: &quot;activebackup&quot;&#125;&#125;&#x27;</span></span><br><span class="line">Connection <span class="string">&#x27;team0&#x27;</span> (fa5de783-c31d-4239-ab4c-613a0ed0e9c2) successfully added.</span><br><span class="line"></span><br><span class="line">$ sudo nmcli connection modify team0 ipv4.method manual ipv4.addresses 192.168.0.100/24</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ sudo nmcli connection add con-name team0-eno1 <span class="built_in">type</span> team-slave ifname eno1 master team0</span><br><span class="line">Connection <span class="string">&#x27;team0-eno1&#x27;</span> (62f7a5d8-171d-457a-b113-af714042463e) successfully added.</span><br><span class="line">$ sudo nmcli connection add con-name team0-eno2 <span class="built_in">type</span> team-slave ifname eno2 master team0</span><br><span class="line">Connection <span class="string">&#x27;team0-eno2&#x27;</span> (565abdf0-0131-40e5-b492-281a27baf540) successfully added.</span><br><span class="line"></span><br><span class="line">$ sudo nmcli connection up team0</span><br><span class="line">Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/4)</span><br><span class="line">[student@server0 ~]$ sudo teamdctl team0 state</span><br><span class="line">setup:</span><br><span class="line">  runner: activebackup</span><br><span class="line">ports:</span><br><span class="line">  eno1</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">  instance[link_watch_0]:        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line">  eno2</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line">runner:</span><br><span class="line">  active port: eno2</span><br></pre></td></tr></table></figure><hr><h1 id="3-2-Managing-Network-Teaming"><a href="#3-2-Managing-Network-Teaming" class="headerlink" title="3.2 Managing Network Teaming"></a>3.2 Managing Network Teaming</h1><h2 id="3-2-2-Setting-and-adjusting-team-configuration"><a href="#3-2-2-Setting-and-adjusting-team-configuration" class="headerlink" title="3.2.2 Setting and adjusting team configuration"></a>3.2.2 Setting and adjusting team configuration</h2><p>當 network team interface 設定完成後，後續還是可以進行修改調整，大部分與原有 nmcli connection modify 沒甚麼差別，主要是要更改 runner 的設定，需要加上 <code>team.config</code> 關鍵字。</p><p>一開始設定時使用一個簡單的 json 字串指定 runner 的設定，也可以透過一個較為複雜的 json 檔案來進行更細緻的 network teaming 設定，以下是個範例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;device&quot;</span>: <span class="string">&quot;team0&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;mcast_rejoin&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;count&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;notify_peers&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;count&quot;</span>: <span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;ports&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;eno1&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;prio&quot;</span>: <span class="number">-10</span>,    <span class="comment">/* -32729~32727，數字愈小優先權愈高 */</span></span><br><span class="line">        <span class="attr">&quot;sticky&quot;</span>: <span class="literal">true</span>, <span class="comment">/* eno1 掛了會切到 eno2, eno1 回復後會再度主動切回 eno1(以 eno1 為優先使用的 NIC) */</span></span><br><span class="line">            <span class="attr">&quot;link_watch&quot;</span>: &#123;</span><br><span class="line">                <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;arp_ping&quot;</span>, <span class="comment">/* 比 ethtool 準確 */</span></span><br><span class="line">                <span class="attr">&quot;interval&quot;</span>: <span class="number">100</span>, <span class="comment">/* 100ms ping 一次 */</span></span><br><span class="line">                <span class="attr">&quot;missed_max&quot;</span>: <span class="number">30</span>, <span class="comment">/* 掉封包超過 30 個以上 */</span></span><br><span class="line">                <span class="attr">&quot;source_host&quot;</span>: <span class="string">&quot;192.168.23.2&quot;</span>,  <span class="comment">/* 需要給 ip 做檢測 */</span></span><br><span class="line">                <span class="attr">&quot;target_host&quot;</span>: <span class="string">&quot;192.168.23.1&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;eno2&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;link_watch&quot;</span>: &#123;</span><br><span class="line">                <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;ethtool&quot;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;runner&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;loadbalance&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>假設檔案名稱為 <code>/tmp/team.conf</code>，接著依序執行以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmcli connection modify team0 team.config /tmp/team.conf</span><br><span class="line">$ sudo nmcli connection down team0</span><br><span class="line">$ sudo nmcli connection up team0</span><br><span class="line">$ sudo nmcli connection up team0-eno1</span><br><span class="line">$ sudo nmcli connection up team0-eno2</span><br></pre></td></tr></table></figure><blockquote><p>重新啟動 network teaming interface，它所包含的 port 也要一起重新啟動</p></blockquote><h2 id="3-2-3-Troubleshooting-network-teams"><a href="#3-2-3-Troubleshooting-network-teams" class="headerlink" title="3.2.3 Troubleshooting network teams"></a>3.2.3 Troubleshooting network teams</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢指定 network teaming interface 所相關連的 port$ sudo teamnl team0 ports</span></span><br><span class="line"> 4: eno1: up 10000Mbit FD</span><br><span class="line"> 6: eno2: up 10000Mbit FD</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 active port</span></span><br><span class="line">$ sudo teamnl team0 getoption activeport</span><br><span class="line">6</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 network teaming interface 所有相關資訊</span></span><br><span class="line">$ sudo teamnl team0 options</span><br><span class="line"> queue_id (port:eno2) 0</span><br><span class="line"> priority (port:eno2) 0</span><br><span class="line"> user_linkup_enabled (port:eno2) <span class="literal">false</span></span><br><span class="line"> user_linkup (port:eno2) <span class="literal">true</span></span><br><span class="line"> enabled (port:eno2) <span class="literal">true</span></span><br><span class="line"> queue_id (port:eno1) 0</span><br><span class="line"> priority (port:eno1) 0</span><br><span class="line"> user_linkup_enabled (port:eno1) <span class="literal">false</span></span><br><span class="line"> user_linkup (port:eno1) <span class="literal">true</span></span><br><span class="line"> enabled (port:eno1) <span class="literal">true</span></span><br><span class="line"> mcast_rejoin_interval 0</span><br><span class="line"> mcast_rejoin_count 1</span><br><span class="line"> notify_peers_interval 0</span><br><span class="line"> notify_peers_count 1</span><br><span class="line"> mode roundrobin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 network team interface 的狀態</span></span><br><span class="line">$ sudo teamdctl team0 state</span><br><span class="line">setup:</span><br><span class="line">  runner: activebackup</span><br><span class="line">ports:</span><br><span class="line">  eno1</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line">  eno2</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line">runner:</span><br><span class="line">  active port: eno2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 json 格式匯出 network teaming interface 的設定</span></span><br><span class="line">$ sudo teamdctl team0 config dump &gt; /tmp/team0.json</span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Managing-Network-Teaming"><a href="#Practice-Managing-Network-Teaming" class="headerlink" title="Practice: Managing Network Teaming"></a>Practice: Managing Network Teaming</h1><h2 id="目標-1"><a href="#目標-1" class="headerlink" title="目標"></a>目標</h2><ol><li><p>現有一個 network team interface 為 <code>team0</code>，結合了 <code>eno1</code> &amp; <code>eno2</code> 兩個 interface，並設定 IP 為 <code>192.168.0.100/24</code>，runner 為 <code>activebackup</code></p></li><li><p>將 runner 從 <code>activebackup</code> 改為 <code>roundrobin</code></p></li></ol><h2 id="實作過程-1"><a href="#實作過程-1" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 匯出 network teaming interface 設定檔</span></span><br><span class="line">$ sudo teamdctl team0 config dump &gt;&gt; /tmp/team.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 runner 從 activebackup 改為 roundrobin，並套用設定</span></span><br><span class="line">$ sudo sed -i <span class="string">&#x27;s/activebackup/roundrobin/g&#x27;</span> /tmp/team.conf</span><br><span class="line">$ sudo nmcli connection modify team0 team.config /tmp/team.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停用 connection</span></span><br><span class="line">$ sudo nmcli connection down team0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用 connection</span></span><br><span class="line">$ sudo nmcli connection up team0</span><br><span class="line">Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/7)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查雲 network teaming interface status (僅有 runner 資訊可看，要另外啟動兩個 port)</span></span><br><span class="line">$ sudo teamdctl team0 state</span><br><span class="line">setup:</span><br><span class="line">  runner: roundrobin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新啟動 participant port</span></span><br><span class="line">$ sudo nmcli connection up team0-eno1</span><br><span class="line">Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/8)</span><br><span class="line">$ sudo nmcli connection up team0-eno2</span><br><span class="line">Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/9)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新檢視 network teaming interface 資訊</span></span><br><span class="line">$ sudo teamdctl team0 state</span><br><span class="line">setup:</span><br><span class="line">  runner: roundrobin</span><br><span class="line">ports:</span><br><span class="line">  eno1</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line">  eno2</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line"></span><br><span class="line"><span class="comment"># 驗證連線</span></span><br><span class="line">$ ping -c 1 -I team0 192.168.0.254</span><br><span class="line">PING 192.168.0.254 (192.168.0.254) from 192.168.0.100 team0: 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.0.254: icmp_seq=1 ttl=64 time=0.142 ms</span><br><span class="line"></span><br><span class="line">--- 192.168.0.254 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.142/0.142/0.142/0.000 ms</span><br></pre></td></tr></table></figure><hr><h1 id="3-3-Configuring-Software-Bridges"><a href="#3-3-Configuring-Software-Bridges" class="headerlink" title="3.3 Configuring Software Bridges"></a>3.3 Configuring Software Bridges</h1><h2 id="3-2-2-Configure-software-bridges"><a href="#3-2-2-Configure-software-bridges" class="headerlink" title="3.2.2 Configure software bridges"></a>3.2.2 Configure software bridges</h2><p>新增一個 bridge <code>br0</code>，並與實體 NIC <code>eno1</code> &amp; <code>eno2</code> 連接：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmcli connection add con-name br0 <span class="built_in">type</span> bridge ifname br0</span><br><span class="line">Connection <span class="string">&#x27;br0&#x27;</span> (8280bb56-62fb-4743-8ca3-86b3ed405c5d) successfully added.</span><br><span class="line"></span><br><span class="line">$ sudo nmcli connection add con-name br0-eno1 <span class="built_in">type</span> bridge-slave ifname eno1 master br0</span><br><span class="line">Connection <span class="string">&#x27;br0-eno1&#x27;</span> (2cc68634-e2ca-4dcf-8485-69f619a77e37) successfully added.</span><br><span class="line"></span><br><span class="line">$ sudo nmcli connection add con-name br0-eno2 <span class="built_in">type</span> bridge-slave ifname eno2 master br0</span><br><span class="line">Connection <span class="string">&#x27;br0-eno2&#x27;</span> (02df2464-0daf-4bc6-ad0f-c979323a5ef8) successfully added.</span><br><span class="line"></span><br><span class="line">[student@server0 ~]$ sudo brctl show</span><br><span class="line">bridge name        bridge id             STP enabled     interfaces</span><br><span class="line">br0                8000.9e32e837e9d6     yes             eno1</span><br><span class="line">                                                     eno2</span><br></pre></td></tr></table></figure><blockquote><p>詳細的使用方式可參考 <code>nmcli-examples(5)</code> &amp; <code>brctl(8)</code></p></blockquote><p>建立完 bridge 後，會產生以下鄉對應的設定檔：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/sysconfig/network-scripts/ifcfg-br0</span><br><span class="line">DEVICE=br0</span><br><span class="line">STP=yes</span><br><span class="line">BRIDGING_OPTS=priority=32768</span><br><span class="line">TYPE=Bridge</span><br><span class="line">BOOTPROTO=dhcp</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">PEERDNS=yes</span><br><span class="line">PEERROUTES=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_PEERDNS=yes</span><br><span class="line">IPV6_PEERROUTES=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">NAME=br0</span><br><span class="line">UUID=8280bb56-62fb-4743-8ca3-86b3ed405c5d</span><br><span class="line">ONBOOT=yes</span><br><span class="line"></span><br><span class="line">$ cat /etc/sysconfig/network-scripts/ifcfg-br0-eno1</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">NAME=br0-eno1</span><br><span class="line">UUID=2cc68634-e2ca-4dcf-8485-69f619a77e37</span><br><span class="line">DEVICE=eno1</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BRIDGE=br0</span><br><span class="line"></span><br><span class="line">$ cat /etc/sysconfig/network-scripts/ifcfg-br0-eno2</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">NAME=br0-eno2</span><br><span class="line">UUID=02df2464-0daf-4bc6-ad0f-c979323a5ef8</span><br><span class="line">DEVICE=eno2</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BRIDGE=br0</span><br></pre></td></tr></table></figure><blockquote><p>Network Manager 僅支援將實體網卡連接到 bridge，不支援將 aggregate interface 加入到 bridge</p></blockquote><hr><h1 id="Practice-Configuring-Software-Bridges"><a href="#Practice-Configuring-Software-Bridges" class="headerlink" title="Practice: Configuring Software Bridges"></a>Practice: Configuring Software Bridges</h1><h2 id="目標-2"><a href="#目標-2" class="headerlink" title="目標"></a>目標</h2><ol><li><p>建立一個名稱為 <code>br1</code> 的 bridge，並設定實體 NIC <code>eno1</code> 與其連接</p></li><li><p>設定 static ip 為 <code>192.168.0.100/24</code></p></li></ol><h2 id="實作過程-2"><a href="#實作過程-2" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmcli connection add con-name br1 <span class="built_in">type</span> bridge ifname br1 ip4 192.168.0.100/24</span><br><span class="line">Connection <span class="string">&#x27;br1&#x27;</span> (cf675bd7-607e-445e-a952-3dcdab359636) successfully added.</span><br><span class="line"></span><br><span class="line">$ sudo nmcli connection add con-name br1-eno1 <span class="built_in">type</span> bridge-slave ifname eno1 master br1</span><br><span class="line">Connection <span class="string">&#x27;br1-eno1&#x27;</span> (7f204182-dddf-4031-90dc-d8573059199f) successfully added.</span><br><span class="line"></span><br><span class="line">$ ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000</span><br><span class="line">    link/ether 52:54:00:00:00:0b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">4: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master br1 state UP mode DEFAULT qlen 1000</span><br><span class="line">    link/ether 4e:ea:9e:33:ac:ca brd ff:ff:ff:ff:ff:ff</span><br><span class="line">6: eno2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000</span><br><span class="line">    link/ether f6:a5:73:ed:7c:61 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">8: br1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT</span><br><span class="line">    link/ether 4e:ea:9e:33:ac:ca brd ff:ff:ff:ff:ff:ff</span><br><span class="line"></span><br><span class="line">$ brctl show</span><br><span class="line">bridge name        bridge id             STP enabled    interfaces</span><br><span class="line">br1                8000.4eea9e33acca     yes            eno1</span><br><span class="line"></span><br><span class="line">$ ping -I br1 -c 1 192.168.0.254</span><br><span class="line">PING 192.168.0.254 (192.168.0.254) from 192.168.0.100 br1: 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.0.254: icmp_seq=1 ttl=64 time=0.054 ms</span><br><span class="line"></span><br><span class="line">--- 192.168.0.254 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.054/0.054/0.054/0.000 ms</span><br></pre></td></tr></table></figure><hr><h1 id="Lab-Configuration-Link-Aggregation-and-Bridging"><a href="#Lab-Configuration-Link-Aggregation-and-Bridging" class="headerlink" title="Lab: Configuration Link Aggregation and Bridging"></a>Lab: Configuration Link Aggregation and Bridging</h1><h2 id="目標-3"><a href="#目標-3" class="headerlink" title="目標"></a>目標</h2><ol><li><p>建立一個 network teaming interface <code>team0</code>，runner 為 <code>activebackup</code>，使用 <code>eno1</code> &amp; <code>eno2</code> 兩個實體 NIC</p></li><li><p>建立一個 bridge <code>brteam0</code>，連接步驟一設定好的 team0，並設定 IP 為 <code>192.168.0.100/24</code></p></li></ol><h3 id="實作過程-3"><a href="#實作過程-3" class="headerlink" title="實作過程"></a>實作過程</h3><p>設定 network teaming interface：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmcli connection add con-name team0 <span class="built_in">type</span> team ifname team0 config <span class="string">&#x27;&#123;&quot;runner&quot;: &#123;&quot;name&quot;: &quot;activebackup&quot;&#125;&#125;&#x27;</span></span><br><span class="line">Connection <span class="string">&#x27;team0&#x27;</span> (064b65f0-a794-41a1-819f-ead0ba4f03c5) successfully added.</span><br><span class="line"></span><br><span class="line">$ sudo nmcli connection add con-name team0-eno1 <span class="built_in">type</span> team-slave ifname eno1 master team0</span><br><span class="line">Connection <span class="string">&#x27;team0-eno1&#x27;</span> (93004100-b241-4aa4-81d4-393995cb2147) successfully added.</span><br><span class="line">$ sudo nmcli connection add con-name team0-eno2 <span class="built_in">type</span> team-slave ifname eno2 master team0</span><br><span class="line">Connection <span class="string">&#x27;team0-eno2&#x27;</span> (1b171dbd-2de4-4cdb-b7dc-5c597ba59797) successfully added.</span><br><span class="line"></span><br><span class="line">$ sudo teamdctl team0 state</span><br><span class="line">setup:</span><br><span class="line">  runner: activebackup</span><br><span class="line">ports:</span><br><span class="line">  eno1</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line">  eno2</span><br><span class="line">    link watches:</span><br><span class="line">      link summary: up</span><br><span class="line">      instance[link_watch_0]:</span><br><span class="line">        name: ethtool</span><br><span class="line">        link: up</span><br><span class="line">runner:</span><br><span class="line">  active port: eno1</span><br></pre></td></tr></table></figure><p>建立 bridge <code>brteam0</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmcli connection add con-name brteam0 <span class="built_in">type</span> bridge ifname brteam0 ip4 192.168.0.100/24</span><br></pre></td></tr></table></figure><p>接著停止 network teaming interface &amp; NetworkManager 服務：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmcli connection down team0</span><br><span class="line"></span><br><span class="line">$ sudo systemctl stop NetworkManager.service</span><br><span class="line">$ sudo systemctl <span class="built_in">disable</span> NetworkManager.service</span><br><span class="line">rm <span class="string">&#x27;/etc/systemd/system/multi-user.target.wants/NetworkManager.service&#x27;</span></span><br><span class="line">rm <span class="string">&#x27;/etc/systemd/system/dbus-org.freedesktop.NetworkManager.service&#x27;</span></span><br><span class="line">rm <span class="string">&#x27;/etc/systemd/system/dbus-org.freedesktop.nm-dispatcher.service&#x27;</span></span><br></pre></td></tr></table></figure><p>修改 <code>/etc/sysconfig/network-scripts/ifcfg-*</code> 相關設定檔：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;BRIDGE=brteam0&quot;</span> | sudo tee --append /etc/sysconfig/network-scripts/ifcfg-team0</span><br><span class="line"></span><br><span class="line">$ sudo sed -e <span class="string">&#x27;/^BOOTPROTO.*/d&#x27;</span> -e <span class="string">&#x27;/^DEFROUTE.*/d&#x27;</span> -e <span class="string">&#x27;/^PEER.*/d&#x27;</span> -e <span class="string">&#x27;/^IPV.*/d&#x27;</span> -i /etc/sysconfig/network-scripts/ifcfg-team0-eno1</span><br><span class="line"></span><br><span class="line">$ sudo sed -e <span class="string">&#x27;/^BOOTPROTO.*/d&#x27;</span> -e <span class="string">&#x27;/^DEFROUTE.*/d&#x27;</span> -e <span class="string">&#x27;/^PEER.*/d&#x27;</span> -e <span class="string">&#x27;/^IPV.*/d&#x27;</span> -i /etc/sysconfig/network-scripts/ifcfg-team0-eno2</span><br></pre></td></tr></table></figure><p>最後重啟網路並重開機：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl restart network</span><br><span class="line"></span><br><span class="line">$ reboot</span><br><span class="line"></span><br><span class="line"><span class="comment"># 驗證是否成功</span></span><br><span class="line">$ ping -I brteam0 -c 3 192.168.0.254</span><br><span class="line">PING 192.168.0.254 (192.168.0.254) from 192.168.0.100 brteam0: 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.0.254: icmp_seq=1 ttl=64 time=0.146 ms</span><br><span class="line">64 bytes from 192.168.0.254: icmp_seq=2 ttl=64 time=3.44 ms</span><br><span class="line">64 bytes from 192.168.0.254: icmp_seq=3 ttl=64 time=0.148 ms</span><br><span class="line"></span><br><span class="line">--- 192.168.0.254 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2003ms</span><br><span class="line">rtt min/avg/max/mdev = 0.146/1.245/3.442/1.553 ms</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH254 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH254 Chapter 2 Managing IPv6 Networking Learning Notes</title>
      <link href="/blog/RHCE/RHCE7-RH254-LearningNotes-CH02_ManagingIPv6Networking/"/>
      <url>/blog/RHCE/RHCE7-RH254-LearningNotes-CH02_ManagingIPv6Networking/</url>
      
        <content type="html"><![CDATA[<h1 id="老師補充"><a href="#老師補充" class="headerlink" title="老師補充"></a>老師補充</h1><h3 id="設定網路卡以舊的方式呈現-eth0-eth1…"><a href="#設定網路卡以舊的方式呈現-eth0-eth1…" class="headerlink" title="設定網路卡以舊的方式呈現(eth0, eth1…)"></a>設定網路卡以舊的方式呈現(eth0, eth1…)</h3><ol><li><p>編輯 <code>/boot/grub2/grub.cfg</code></p></li><li><p>尋找 kernel 參數列：尋找 <code>linux16</code> 開頭的設定</p></li><li><p>加上參數 <code>biosdevname=0 net.ifnames=0</code>，重開機後即完成</p></li></ol><h3 id="ip-指令可達成的功能"><a href="#ip-指令可達成的功能" class="headerlink" title="ip 指令可達成的功能"></a>ip 指令可達成的功能</h3><ul><li><p>不考慮 network namespace 的前提下，Linux kernel 有 256 個 routing table</p></li><li><p>設定 tc(traffic control) 的功能</p></li><li><p><code>ip route show</code>：可查詢 default gateway</p></li><li><p>查詢 network namespace：<code>ip netns show</code></p></li><li><p>切換到指定的 network namespace(<strong>hidden</strong>)：<code>ip exec netns hidden bash</code></p></li></ul><h3 id="nmcli"><a href="#nmcli" class="headerlink" title="nmcli"></a>nmcli</h3><ul><li><p>沒有加上 ip 資訊則表示設定為 DHCP：<code>nmcli connection add con-name office type ethernet ifname eth1</code></p></li><li><p>把跟 eth1 相關的 connection 全部停掉：<code>nmcli device disconnect eth1</code></p></li><li><p>把 DHCP 改為 static 的示範：<code>nmcli connection modify office ipv4.method manual ipv4.addresses &quot;192.168.0.1/24 192.168.0.254&quot;</code></p></li></ul><h3 id="IPV6"><a href="#IPV6" class="headerlink" title="IPV6"></a>IPV6</h3><ul><li><p><code>::1/128</code>：等同於 ipv4 的 <strong>127.0.0.1/8</strong></p></li><li><p><code>::</code>：等同於 ipv4 的 <strong>0.0.0.0</strong>(for listen port check)</p></li><li><p><code>::/0</code>：default gateway ipv4 的 <strong>0.0.0.0/0</strong></p></li><li><p><code>2000::/3</code>：<code>2000::/16</code> ~ <code>3ffff::/16</code>：為 ipv6 的 public ip</p></li><li><p><code>fd00::/8</code>：ipv6 所使用的 private ip</p></li><li><p><code>fe80::/64</code>：link local address，沒有 DHCP service 的時候會用這一段的 ip(避免 ip 衝突，Linux 會把 MAC address 嵌入到 ip 內)</p></li><li><p><code>ff00::/8</code>：作為 multicast 之用，等同 ipv4 的 <strong>224.0.0.0/4</strong></p></li><li><p>顯示 ipv6 address：<code>ip -6 addr show</code></p></li><li><p>顯示 ipv6 routing table：<code>ip -6 route show</code></p></li><li><p>指定網卡 ping 其他 ipv6 ip(<strong>%</strong> 之後帶網卡名稱)：<code>ping6 ff02:;1%eth0</code></p></li><li><p>持續監控傳遞延遲時間：<code>mtr 8.8.8.8</code></p></li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li><p>參數補齊功能要安裝 <code>bash-completion</code> 套件才會有</p></li><li><p>查詢 hostname 對應的 ip：<code>dig server0.example.com</code></p></li><li><p>查詢 ip 對應的 hostname：<code>dig -x 172.25.0.11</code></p><ul><li>client 向 DHCP 要求 ip</li><li>client 取得 ip</li><li>client 拿 ip 向 DNS server 作反向名稱解析</li><li>DNS server 給 client 對應的 hostname</li><li>client 使用上一個步驟給的 hostname 作為自己的 hostname</li></ul></li></ul><blockquote><p>透過 DHCP 取得 ip 的狀況下不會有 <code>/etc/hostname</code> 這個檔案</p></blockquote><hr><h1 id="2-1-Reviewing-of-IPv4-Networking-Configuration"><a href="#2-1-Reviewing-of-IPv4-Networking-Configuration" class="headerlink" title="2.1 Reviewing of IPv4 Networking Configuration"></a>2.1 Reviewing of IPv4 Networking Configuration</h1><h2 id="關於-autoconnect"><a href="#關於-autoconnect" class="headerlink" title="關於 autoconnect"></a>關於 autoconnect</h2><p>若透過 nmcli 設定的 connection 有加上 <code>autoconnect yes</code> 的屬性，當執行 <code>sudo nmcli connection down [CONNECTION_NAME]</code> 時，nmcli 會自動帶起其他有 autoconnect yes 屬性的 connection。</p><h2 id="關於-nmcli-的設定參數"><a href="#關於-nmcli-的設定參數" class="headerlink" title="關於 nmcli 的設定參數"></a>關於 nmcli 的設定參數</h2><ul><li><p>設定好的 connection 會有相對應的檔案存放在 <code>/etc/sysconfig/network-scripts/ifcfg-[CONNECTION_NAME]</code> 中</p></li><li><p>詳細設定可以參考 man page <code>nm-settings(5)</code></p></li><li><p>若要將原本為 DHCP 設定改成 static IP 指定，要特別加上 <code>ipv4.method manual</code>，並附上 IP 相關資訊，以下為範例：</p></li></ul><blockquote><p>sudo nmcli connection modify “System eth0” ipv4.addresses “192.168.0.10/24 192.168.0.1” ipv4.method manual</p></blockquote><ul><li><code>nm-settings</code> 與 <code>ifcfg-*</code> 的對應：</li></ul><table><thead><tr><th>nmcli con mod</th><th>ifcfg-* file</th><th>效果</th></tr></thead><tbody><tr><td>ipv4.method manual</td><td>BOOTPROTO=none</td><td>IPv4 IP 以固定的方式指定</td></tr><tr><td>ipv4.method auto</td><td>BOOTPROTO=dhcp</td><td>以 DHCP 方式取得 ip</td></tr><tr><td>ipv4.addresses “192.168.0.10/24 192.168.0.1”</td><td>IPADDR0=192.168.0.10<br />PREFIX0=24<br />GATEWAY0=192.168.0.1</td><td>指定 IP, Netmask, and Gateway</td></tr><tr><td>ipv4.dns 8.8.8.8</td><td>DNS0=8.8.8.8</td><td>修改 <code>/etc/resolv.conf</code> 中的設定，加入 <code>nameserver 8.8.8.8</code></td></tr><tr><td>ipv4.dns-search example.com</td><td>DOMAIN=example.com</td><td>修改 <code>/etc/resolv.conf</code> 中的 <code>search</code> 設定</td></tr><tr><td>ipv4.ignore-auto-dns true</td><td>PEERDNS=no</td><td>忽略來自 DHCP 的 DNS server 資訊</td></tr><tr><td>connection.autoconnect yes</td><td>ONBOOT=yes</td><td>開機自動啟動</td></tr><tr><td>connection.id eth0</td><td>NAME=eth0</td><td>指定 connection 名稱</td></tr><tr><td>connection.interface-name eth0</td><td>DEVICE=eth0</td><td>指定 connection 所要綁定的裝置名稱</td></tr><tr><td>802-3-ethernet.mac-address xxxxx</td><td>HWADDR=xxxxx</td><td>指定 connection 所要綁定裝置的 MAC address</td></tr></tbody></table><blockquote><p>因為 nmcli 都會直接去改 <code>/etc/resolv.conf</code> 的設定，因此若是每個 connection 有不同的DNS 設定，建議直接去 <code>/etc/sysconfig/network-scripts/ifcfg-*</code> 檔案中修改 <code>DNSn</code> &amp; <code>DOMAIN</code> 的設定值</p></blockquote><h2 id="2-1-6-Deleting-a-network-connection"><a href="#2-1-6-Deleting-a-network-connection" class="headerlink" title="2.1.6 Deleting a network connection"></a>2.1.6 Deleting a network connection</h2><p>移除 connection 的指令為 <code>sudo nmcli connection delete [CONNECTION_NAME]</code></p><p>同時間也會將此 connection 斷線，並移除 /etc/sysconfig/network-scripts 目錄中相對應的 ifcfg 檔案</p><h2 id="2-1-7-Modifying-the-system-host-name"><a href="#2-1-7-Modifying-the-system-host-name" class="headerlink" title="2.1.7 Modifying the system host name"></a>2.1.7 Modifying the system host name</h2><p>hostname 相關指令：</p><ul><li><p><code>hostnamectl status</code>：查詢 host 狀態</p></li><li><p><code>sudo hostnamectl set-hostname [HOSTNAME]</code>：變更 hostname</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢 host 狀態</span></span><br><span class="line">[student@server0 ~]$ hostnamectl status</span><br><span class="line">   Static hostname: server5.example.com</span><br><span class="line">         Icon name: computer</span><br><span class="line">           Chassis: n/a</span><br><span class="line">        Machine ID: 946cb0e817ea4adb916183df8c4fc817</span><br><span class="line">           Boot ID: 87802c89e9a54f7087857bf5e6de16de</span><br><span class="line">  Operating System: Red Hat Enterprise Linux Server 7.0 (Maipo)</span><br><span class="line">       CPE OS Name: cpe:/o:redhat:enterprise_linux:7.0:GA:server</span><br><span class="line">            Kernel: Linux 3.10.0-123.el7.x86_64</span><br><span class="line">      Architecture: x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 hostname</span></span><br><span class="line">[student@server0 ~]$ sudo hostnamectl set-hostname server1.example.com</span><br><span class="line">[student@server0 ~]$ hostnamectl status</span><br><span class="line">   Static hostname: server1.example.com</span><br><span class="line">         Icon name: computer</span><br><span class="line">           Chassis: n/a</span><br><span class="line">        Machine ID: 946cb0e817ea4adb916183df8c4fc817</span><br><span class="line">           Boot ID: 87802c89e9a54f7087857bf5e6de16de</span><br><span class="line">  Operating System: Red Hat Enterprise Linux Server 7.0 (Maipo)</span><br><span class="line">       CPE OS Name: cpe:/o:redhat:enterprise_linux:7.0:GA:server</span><br><span class="line">            Kernel: Linux 3.10.0-123.el7.x86_64</span><br><span class="line">      Architecture: x86_64</span><br><span class="line"></span><br><span class="line">$ cat /etc/hostname</span><br><span class="line">server1.example.com</span><br></pre></td></tr></table></figure><blockquote><p>透過 hostnamectl 修改 hostname 之後，會反映到 /etc/hostname 的內容上</p></blockquote><hr><h1 id="Practice-Configuring-IPv4-Networking"><a href="#Practice-Configuring-IPv4-Networking" class="headerlink" title="Practice: Configuring IPv4 Networking"></a>Practice: Configuring IPv4 Networking</h1><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><ol><li><p>透過 nmcli 建立一個 connection，名稱為 <code>eno1</code></p></li><li><p>設定 connection 使用的網路介面為 <code>eno1</code>，IP <code>192.168.0.1/24</code>，沒有 Gateway</p></li><li><p>進行 DNS 查詢時，會把 <code>otherhost</code> 解析為 <code>192.168.0.254</code></p></li></ol><h2 id="實作過程"><a href="#實作過程" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmcli connection add con-name eno1 ifname eno1 <span class="built_in">type</span> ethernet autoconnect yes ip4 192.168.0.1/24</span><br><span class="line">Connection <span class="string">&#x27;eno1&#x27;</span> (4b182e51-8798-406c-a07c-c63289733543) successfully added.</span><br><span class="line">$ sudo nmcli connection up eno1</span><br><span class="line"> successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/2)</span><br><span class="line"></span><br><span class="line"> [student@server0 ~]$ ip addr show eno1</span><br><span class="line"> 6: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">     link/ether f6:0f:ec:b7:98:e1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">     inet 192.168.0.1/24 brd 192.168.0.255 scope global eno1</span><br><span class="line">        valid_lft forever preferred_lft forever</span><br><span class="line">     inet6 fe80::f40f:ecff:feb7:98e1/64 scope link</span><br><span class="line">        valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;192.168.0.254 otherhost&quot;</span> | sudo tee --append /etc/hosts</span><br><span class="line">192.168.0.254 otherhost</span><br><span class="line"></span><br><span class="line">$ ping otherhost</span><br><span class="line">PING otherhost (192.168.0.254) 56(84) bytes of data.</span><br><span class="line">64 bytes from otherhost (192.168.0.254): icmp_seq=1 ttl=64 time=0.055 ms</span><br><span class="line">♥</span><br><span class="line">--- otherhost ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.055/0.055/0.055/0.000 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示目前有那些額外的 network namespace</span></span><br><span class="line">$ ip netns show</span><br><span class="line">hidden</span><br><span class="line"><span class="comment"># 切換到指定的 network namespace</span></span><br><span class="line">$ ip <span class="built_in">exec</span> netns hidden bash</span><br></pre></td></tr></table></figure><hr><h1 id="2-2-IPv6-Networking-Concepts"><a href="#2-2-IPv6-Networking-Concepts" class="headerlink" title="2.2 IPv6 Networking Concepts"></a>2.2 IPv6 Networking Concepts</h1><h2 id="2-2-1-IPv6-overview"><a href="#2-2-1-IPv6-overview" class="headerlink" title="2.2.1 IPv6 overview"></a>2.2.1 IPv6 overview</h2><p>IPv6 目的是要解決 IPv4 IP 不足的問題，但因為沒有一個簡單的方式可以直接讓 IPv6 獨立運作，因此目前屬於過渡時期的方式，稱為 <code>dual-stack</code>，即是讓 IPv4 與 IPv6 同時存在</p><h2 id="2-2-2-Interpreting-IPv6-addresses"><a href="#2-2-2-Interpreting-IPv6-addresses" class="headerlink" title="2.2.2 Interpreting IPv6 addresses"></a>2.2.2 Interpreting IPv6 addresses</h2><p>IPv6 address 長度為 128 bits，以 16 進位顯示，每 16 個 bits 會用冒號隔開，格式大概如下：</p><blockquote><p>2001:0DB8:02de:0000:0000:0000:0000:0e13</p></blockquote><p>但這樣太難記了，所以可以用較簡單的表示方式：(<strong>都以小寫表示</strong>)</p><ul><li><p>2001:db8:2de:0000:0000:0000:0000:e13</p></li><li><p>2001:db8:2de:000:000:000:000:e13</p></li><li><p>2001:db8:2de:00:00:00:00:e13</p></li><li><p>2001:db8:2de:0:0:0:0:e13</p></li><li><p>2001:db8:2de::e13</p><p>以上這幾種所表示的 IPv6 address 都是同一個(<strong>每個由冒號區隔的 group，開頭的 0 都可以省略不寫</strong>)</p></li></ul><blockquote><p>雙冒號不能同時出現兩個，這樣會無法推斷出正確的 IPv6 address</p></blockquote><blockquote><p>若要指定特定的 port，則會以類似這樣的形式表示：<code>[2001:DB8:2de::e13]:80</code></p></blockquote><h3 id="IPv6-subnets"><a href="#IPv6-subnets" class="headerlink" title="IPv6 subnets"></a>IPv6 subnets</h3><ul><li><p>IPv6 同樣也有 subnet 的概念，標準的 IPv6 <code>network prefix</code> 長度為 64 bits(<code>/64</code>)，其餘 64 bits 則是屬於 <code>interface ID</code></p></li><li><p>從上面可知，每個 subnet 可包含的網路裝置數量是很驚人的…..(<code>2^64</code>)</p></li><li><p>實際應用上，64 bits 真是太大了，而且一個 subnet 當然也不夠用，因此使用上還是會切成 <code>/48</code> 居多，這樣就可以從只有一個 subnet 變成有 <code>65536(2^16)</code> 個 subnet 可用</p></li></ul><p><img src="https://lh3.googleusercontent.com/M6--ZzzQCdXEj7HPZ2HNgI304o4xkBiUwCq0WAKL8pNQSHyvWs9fRiOgjCV32b2AkoMwNmYXU5umlO9TVWL-CeQ6Kh4tZEXfhYQ8_f60JDc7bHwgt6wFGSoYRyRex7_UmYLL9HAoofc_5wi5P7p3aEhQVJds1ZVEjgdmF-2g6ZTQ2DMloS0R0s3vJelc7QG8j7unlYjRf8QEuzF-mRSwU5xcYtfy3yn66ytEoiE0eIxJDlPrzDKHiLmk1M6-RPlCJJHMmEO9Bd7mAweT4TKMkO6o4lB8QLjwkDSODKf_hOpBls_AvmVWJMrTrFD-9JV-KuYp5o5LSr18Hp5cwm4N4NkUrJgwGpf2bLBdshT0AHgt3k6863PVTSAkbZJyKqKP4_6Ot1rg5RZehlJnRqEEZFnXUaJbTZJZc4G3fd7w5zOjvmYIuYW6wUKz3Pu2-6ttyWFCzGbgqMytn1XOelwbUEkgP4WxV3IpZ-Ou3eQmfZROn1FHh9jZ84UgoYspcHRptpPgoxUP710J3OhVEDFOz825EUL3U4Huhcc_gPqSchZf27Hr1AoCsw48zMmwa66oCyDgBA2Ey2sY4DeDLPQF82l0-vR666c=w650-h232-no" alt="IPv6"></p><h2 id="2-2-3-IPv6-address-allocation"><a href="#2-2-3-IPv6-address-allocation" class="headerlink" title="2.2.3 IPv6 address allocation"></a>2.2.3 IPv6 address allocation</h2><p>IPv6 跟 IPv4 相同也有一些特定的位址，作為特定目的使用，例如：<code>localhost</code>、<code>multicast</code>、<code>unitcast</code>….等等。</p><p>詳細資料可參考<a href="https://zh.wikipedia.org/wiki/IPv6#IPv6.E4.BD.8D.E5.9D.80.E7.9A.84.E5.88.86.E9.A1.9E">IPv6 - 維基百科，自由的百科全書</a></p><h3 id="Link-local-addresses"><a href="#Link-local-addresses" class="headerlink" title="Link-local addresses"></a>Link-local addresses</h3><p>IPv6 中，Link-local address 是用來讓 host 指定的網路中互相交流之用(<strong>僅在內部</strong>)，以 <code>fe80::</code> 為開頭，完整的位址還包含了網路介面卡的名稱，例如：<code>fe80::211:22ff:fwaa:bbcc%eth0</code></p><h3 id="Multicast"><a href="#Multicast" class="headerlink" title="Multicast"></a>Multicast</h3><p>在 IPv6 中已經沒有 broadcast，因此 multicast 扮演了一個更重要的角色，而 multicast 的位址為 <code>ff02::1</code>，後面還會帶上網路介面卡名稱，因此完整位址寫法是：<code>ff02::1%eth0</code></p><h2 id="2-2-4-IPv6-address-configuration"><a href="#2-2-4-IPv6-address-configuration" class="headerlink" title="2.2.4 IPv6 address configuration"></a>2.2.4 IPv6 address configuration</h2><p>IPv6 位址同樣也可以透過靜態指定 or 動態指定(使用 <code>DHCPv6</code> 服務)</p><h3 id="Static-addressing"><a href="#Static-addressing" class="headerlink" title="Static addressing"></a>Static addressing</h3><p>跟 IPv4 相同，每個 IPv6 subnet 會有保留位址供特定目的使用，因此在設定時要避開：</p><ol><li><p><code>0000:0000:0000:0000</code>：這位址是作為 routing 之用，以 <strong>2001:db8::/64</strong> 為例，此位址就是 <code>2001:db8::</code></p></li><li><p>從 <code>fdff:ffff:ffff:ff80</code> ~ <code>fdff:ffff:ffff:ffff</code> 這一段位址</p></li></ol><h3 id="DHCPv6-configuration"><a href="#DHCPv6-configuration" class="headerlink" title="DHCPv6 configuration"></a>DHCPv6 configuration</h3><p>因為 IPv6 沒有 broadcast，因此 DHCPv6 的運作原理跟 DHCPv4 就不太一樣。</p><p>DHCP client 會從自己的 link-local address 送出 DHCP request 到 all-dhcp-servers link-local multicast group 中(<code>ff02::1:2</code> port <code>547/UDP</code>)，而 DHCPv6 server 則會回到 DHCP client 的 link-local address port <code>546/UDP</code></p><h3 id="SLAAC-configuration"><a href="#SLAAC-configuration" class="headerlink" title="SLAAC configuration"></a>SLAAC configuration</h3><p>這是另一種透過 router 協助來完成位址配發的技術，詳細的資訊可以參考下列網址：</p><ul><li><p><a href="http://www.myhome.net.tw/2012_09/p03.htm">2012台網中心電子報─IPv6位址配發技術介紹</a></p></li><li><p><a href="http://www.lijyyh.com/2012/04/ipv6ipv6-auto-configuration.html">傲笑紅塵路: IPv6自動組態配置(IPv6 Auto configuration)</a></p></li></ul><hr><h1 id="2-3-IPv6-Networking-Configuration"><a href="#2-3-IPv6-Networking-Configuration" class="headerlink" title="2.3 IPv6 Networking Configuration"></a>2.3 IPv6 Networking Configuration</h1><h2 id="2-3-2-Adding-an-IPv6-network-connection"><a href="#2-3-2-Adding-an-IPv6-network-connection" class="headerlink" title="2.3.2 Adding an IPv6 network connection"></a>2.3.2 Adding an IPv6 network connection</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmcli connection add con-name <span class="string">&quot;eth2-ipv6&quot;</span> <span class="built_in">type</span> ethernet ifname eth2 ip6 2001:db8:0:1::c000:207/64 gw6 2001:db8:0:1::1 ip4 192.168.2.7/24 gw4 192.168.2.1</span><br><span class="line">Connection <span class="string">&#x27;eth2-ipv6&#x27;</span> (e7c97cdd-efea-43c2-a4d7-52b3bb4be4e2) successfully added.</span><br><span class="line"></span><br><span class="line">$ sudo nmcli connection modify eth2-ipv6 +ipv6.dns 2001:4860:4860::8888</span><br></pre></td></tr></table></figure><h2 id="2-3-3-Modifying-network-connection-settings-for-IPv6"><a href="#2-3-3-Modifying-network-connection-settings-for-IPv6" class="headerlink" title="2.3.3 Modifying network connection settings for IPv6"></a>2.3.3 Modifying network connection settings for IPv6</h2><p>修改的方式跟 IPv4 幾乎一模一樣：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改 IP 資訊</span></span><br><span class="line">$ sudo nmcli connection modify eth2-ipv6 ipv6.addresses <span class="string">&quot;2001:db8:0:1::a00:1/64 2001:db8:0:1::1&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加 DNS 資訊</span></span><br><span class="line">$[student@server0 ~]$ sudo nmcli connection modify eth2-ipv6 +ipv6.dns 2001:4860:4860::8888</span><br></pre></td></tr></table></figure><table><thead><tr><th>nmcli con mod</th><th>ifcfg-* file</th><th>效果</th></tr></thead><tbody><tr><td>ipv6.method manual</td><td>IPV6_AUTOCONF=none</td><td>IPv6 IP 以固定的方式指定</td></tr><tr><td>ipv6.method auto</td><td>IPV6_AUTOCONF=yes</td><td>透過 SLAAC，從 router advertisements 取得 ip</td></tr><tr><td>ipv6.method dhcp</td><td>IPV6_AUTOCONF=no<br />DHCPV6C=yes</td><td>使用 DHCPv6 取得 ip</td></tr><tr><td>ipv6.addresses “2001:db8::a/64 2001:db8::1”</td><td>IPV6ADDR=2001:db8::a/64<br />IPV6_DEFAULTGW=2001:db8::1</td><td>指定 IP, Netmask, and Gateway</td></tr><tr><td>ipv6.dns 8.8.8.8</td><td>DNS0=8.8.8.8</td><td>修改 <code>/etc/resolv.conf</code> 中的設定，加入 <code>nameserver 8.8.8.8</code>(與 IPv4 相同)</td></tr><tr><td>ipv6.dns-search example.com</td><td>DOMAIN=example.com</td><td>修改 <code>/etc/resolv.conf</code> 中的 <code>search</code> 設定(與 IPv4 相同)</td></tr><tr><td>ipv4.ignore-auto-dns true</td><td>IPV6_PEERDNS=no</td><td>忽略來自 DHCP 的 DNS server 資訊</td></tr><tr><td>connection.autoconnect yes</td><td>ONBOOT=yes</td><td>開機自動啟動</td></tr><tr><td>connection.id eth0</td><td>NAME=eth0</td><td>指定 connection 名稱</td></tr><tr><td>connection.interface-name eth0</td><td>DEVICE=eth0</td><td>指定 connection 所要綁定的裝置名稱</td></tr><tr><td>802-3-ethernet.mac-address xxxxx</td><td>HWADDR=xxxxx</td><td>指定 connection 所要綁定裝置的 MAC address</td></tr></tbody></table><h2 id="其他：IPv6-相關指令"><a href="#其他：IPv6-相關指令" class="headerlink" title="其他：IPv6 相關指令"></a>其他：IPv6 相關指令</h2><p>檢視 IPv6 網路資訊：</p><ul><li><p><code>ip addr show eth0</code>：可檢視 eth0 上 IPv6 相關資訊，尋找關鍵字 <code>inet6</code> 即可</p></li><li><p><code>ip -6 route show</code>：顯示 IPv6 部分的 routing table</p></li></ul><p>trouble shooting 相關指令：</p><ul><li><p><code>ping6 ff02::1%eth1</code>：ping <strong>link-local address</strong> &amp; <strong>link-local all-nodes multicast group</strong>，需要帶上 network interface name</p></li><li><p><code>tracepath6 2001:db8:0:2::451</code>：查詢連線到指定 host 所走的路徑</p></li><li><p><code>ss -A inet -n</code> &amp; <code>netstat -46n</code>：查詢目前 network socket 的狀態</p></li></ul><hr><h1 id="Practice-Configuration-IPv6-Networking"><a href="#Practice-Configuration-IPv6-Networking" class="headerlink" title="Practice: Configuration IPv6 Networking"></a>Practice: Configuration IPv6 Networking</h1><h2 id="目標-1"><a href="#目標-1" class="headerlink" title="目標"></a>目標</h2><ul><li><p>建立一個名稱為 <code>eno1</code> 的連線</p></li><li><p>network interface 名稱為 <code>eno1</code>，設定 IP 為 <code>fddb:fe2a:ab1e::c0a8:1/64</code>，Gateway 為 <code>fddb:fe2a:ab1e::c0a8:fe</code></p></li></ul><h2 id="實作過程-1"><a href="#實作過程-1" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新增連線</span></span><br><span class="line">$ sudo nmcli connection add con-name eno1 <span class="built_in">type</span> ethernet ifname eno1 ip6 fddb:fe2a:ab1e::c0a8:1/64 gw6 fddb:fe2a:ab1e::c0a8:fe</span><br><span class="line">Connection <span class="string">&#x27;eno1&#x27;</span> (13505591-aeb6-452d-acaa-8f32810eebb3) successfully added.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改連線設定</span></span><br><span class="line">$ sudo nmcli connection modify eno1 ipv6.method manual</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用連線</span></span><br><span class="line">$ sudo nmcli connection up eno1</span><br><span class="line">Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/3)</span><br></pre></td></tr></table></figure><hr><h1 id="Lab-Managing-IPv6-Networking"><a href="#Lab-Managing-IPv6-Networking" class="headerlink" title="Lab: Managing IPv6 Networking"></a>Lab: Managing IPv6 Networking</h1><h2 id="目標-2"><a href="#目標-2" class="headerlink" title="目標"></a>目標</h2><ul><li><p>建立一個名稱為 <code>eno1</code> 的連線，使用 network interface 為 <code>eno1</code></p></li><li><p>IPv4 的位址為 <code>192.168.0.100/24</code>，IPv6 的位址為 <code>fddb:fe2a:ab1e::c0a8:64/64</code></p></li><li><p>啟動連線是否都有取得正確 IP</p></li><li><p>ping IPv4 gateway(<code>192.168.0.254</code>) &amp; IPv6 gateway(<code>fddb:fe2a:ab1e::c0a8:fe</code>) 確認網路設定正確</p></li></ul><h2 id="實作過程-2"><a href="#實作過程-2" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新增連線</span></span><br><span class="line">$ sudo nmcli connection add con-name eno1 <span class="built_in">type</span> ethernet ifname eno1 ip4 192.168.0.100/24 ip6 fddb:fe2a:ab1e::c0a8:64/64</span><br><span class="line">Connection <span class="string">&#x27;eno1&#x27;</span> (82399d91-7ae1-4544-975c-ae43424de35a) successfully added.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定 ip 設定方式為 manual</span></span><br><span class="line">$ sudo nmcli connection modify eno1 ipv4.method manual ipv6.method manual\</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動連線</span></span><br><span class="line">$ sudo nmcli connection up eno1</span><br><span class="line">Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢 IP 資訊</span></span><br><span class="line">$ ip addr show eno1</span><br><span class="line">6: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether da:f4:04:59:60:2c brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.0.100/24 brd 192.168.0.255 scope global eno1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fddb:fe2a:ab1e::c0a8:64/64 scope global</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::d8f4:4ff:fe59:602c/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping IPv4 address</span></span><br><span class="line">[student@server0 ~]$ ping -c 1 192.168.0.254</span><br><span class="line">PING 192.168.0.254 (192.168.0.254) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.0.254: icmp_seq=1 ttl=64 time=0.085 ms</span><br><span class="line"></span><br><span class="line">--- 192.168.0.254 ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.085/0.085/0.085/0.000 ms</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping IPv6 address</span></span><br><span class="line">$ ping6 -c 1 fddb:fe2a:ab1e::c0a8:fe</span><br><span class="line">PING fddb:fe2a:ab1e::c0a8:fe(fddb:fe2a:ab1e::c0a8:fe) 56 data bytes</span><br><span class="line">64 bytes from fddb:fe2a:ab1e::c0a8:fe: icmp_seq=1 ttl=64 time=0.187 ms</span><br><span class="line"></span><br><span class="line">--- fddb:fe2a:ab1e::c0a8:fe ping statistics ---</span><br><span class="line">1 packets transmitted, 1 received, 0% packet loss, time 0ms</span><br><span class="line">rtt min/avg/max/mdev = 0.187/0.187/0.187/0.000 ms</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH254 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH254 Chapter 1 Controlling Services and Daemons Learning Notes</title>
      <link href="/blog/RHCE/RHCE7-RH254-LearningNotes-CH01_ControllingServicesAndDaemons/"/>
      <url>/blog/RHCE/RHCE7-RH254-LearningNotes-CH01_ControllingServicesAndDaemons/</url>
      
        <content type="html"><![CDATA[<h1 id="1-1-Controlling-Services-with-systemctl"><a href="#1-1-Controlling-Services-with-systemctl" class="headerlink" title="1.1 Controlling Services with systemctl"></a>1.1 Controlling Services with systemctl</h1><h2 id="1-1-1-Introduction-to-systemd"><a href="#1-1-1-Introduction-to-systemd" class="headerlink" title="1.1.1 Introduction to systemd"></a>1.1.1 Introduction to systemd</h2><p>RHEL7 已經使用 <code>systemd</code> 取代原有的 <code>init</code> &amp; <code>xinetd</code>，因此目前 ID=1 的 process 為 systemd，主要加入了下列新的特性：</p><ol><li><p>平行處理，可加速系統開機</p></li><li><p>on-demand 啟動服務的能力</p></li><li><p>service 相依性的自動處理，避免造成系統長時間等待特定服務(例如：網路不通時不會把 network service 啟動)</p></li><li><p>可透過 Linux control group 追蹤相關的 process 的狀態</p></li></ol><blockquote><p>現在大多 service 的相關設定都被移到 <code>/etc/sysconfig</code> 目錄中了，只有某些 legacy services 才依然是以 shell-based 的形式存在</p></blockquote><p>現在 RHEL7 中使用 <code>systemctl</code> 管理各式各樣的 systemd 物件，這些物件統稱為 <code>units</code>，常看到的 units 會有：</p><ol><li><p><code>Service units</code>(<strong>.service</strong>)：這就是系統服務了，通常會以 daemon 的形式存在</p></li><li><p><code>Socket Units</code>(<strong>.socket</strong>)：指的是 IPC socket，通常在有新的連線與 service 產生時，socket unit 會被傳給 daemon 處理</p></li><li><p><code>Path units</code>(<strong>.path</strong>)：用來 delay 特定 service，直到特定的檔案有所變化，例如：列印服務</p></li></ol><blockquote><p>若 system unit 狀態為 <code>static</code>，表示無法被 enable，但可以被 <code>enabled unit</code> 自動啟用</p></blockquote><h2 id="1-1-2-Starting-and-stopping-daemons-on-a-running-system"><a href="#1-1-2-Starting-and-stopping-daemons-on-a-running-system" class="headerlink" title="1.1.2 Starting and stopping daemons on a running system"></a>1.1.2 Starting and stopping daemons on a running system</h2><p><code>restart</code> 會改變 service 的 process ID；但 <code>reload</code> 則會維持 service 原有的 process ID</p><h2 id="其他：systemctl-特別的指令"><a href="#其他：systemctl-特別的指令" class="headerlink" title="其他：systemctl 特別的指令"></a>其他：systemctl 特別的指令</h2><ol><li><p><code>sudo systemctl --failed --type=service</code>：列出狀態為 failed 的 service</p></li><li><p><code>sudo systemctl list-dependencies [UNIT]</code>：列出指定 system unit 的相依關係</p></li><li><p><code>sudo systemctl list-dependencies --reverse [UNIT]</code>：列出指定 system unit 啟動所需要預先啟動的其他 system unit</p></li></ol><hr><h1 id="1-2-Controlling-the-Boot-Process"><a href="#1-2-Controlling-the-Boot-Process" class="headerlink" title="1.2 Controlling the Boot Process"></a>1.2 Controlling the Boot Process</h1><h2 id="1-2-1-Selecting-a-systemd-target"><a href="#1-2-1-Selecting-a-systemd-target" class="headerlink" title="1.2.1 Selecting a systemd target"></a>1.2.1 Selecting a systemd target</h2><p>以下列出幾個重要的 systemd target：</p><ul><li><p><code>graphical.target</code>：就是一般的圖形模式，也包含了文字介面</p></li><li><p><code>multi-user.target</code>：純文字模式</p></li><li><p><code>rescue.target</code>：要求用 root 身分登入，基本系統服務已經初始化完成</p></li><li><p><code>emergency.target</code>：要求用 root 身分登入，initramfs 已經執行完畢，並把 system root 以 read-only 的方式掛載</p></li></ul><p>每個 target 之間會有某種程度的相依關係，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可看出 graphical.target unit 與那些 target unit 有相依</span></span><br><span class="line">$ systemctl list-dependencies graphical.target | grep target</span><br><span class="line">graphical.target</span><br><span class="line">└─multi-user.target</span><br><span class="line">  ├─basic.target</span><br><span class="line">  │ ├─paths.target</span><br><span class="line">  │ ├─slices.target</span><br><span class="line">  │ ├─sockets.target</span><br><span class="line">  │ ├─sysinit.target</span><br><span class="line">  │ │ ├─cryptsetup.target</span><br><span class="line">  │ │ ├─local-fs.target</span><br><span class="line">  │ │ └─swap.target</span><br><span class="line">  │ └─timers.target</span><br><span class="line">  ├─getty.target</span><br><span class="line">  ├─nfs.target</span><br><span class="line">  └─remote-fs.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有 target unit</span></span><br><span class="line">$ systemctl list-units --<span class="built_in">type</span>=target --all</span><br><span class="line">  UNIT                   LOAD   ACTIVE   SUB    DESCRIPTION</span><br><span class="line">  basic.target           loaded active   active Basic System</span><br><span class="line">  cryptsetup.target      loaded active   active Encrypted Volumes</span><br><span class="line">  emergency.target       loaded inactive dead   Emergency Mode</span><br><span class="line">  final.target           loaded inactive dead   Final Step</span><br><span class="line">  getty.target           loaded active   active Login Prompts</span><br><span class="line">  graphical.target       loaded active   active Graphical Interface</span><br><span class="line">  local-fs-pre.target    loaded active   active Local File Systems (Pre)</span><br><span class="line">  local-fs.target        loaded active   active Local File Systems</span><br><span class="line">  multi-user.target      loaded active   active Multi-User System</span><br><span class="line">  network-online.target  loaded inactive dead   Network is Online</span><br><span class="line">  ......</span><br></pre></td></tr></table></figure><p>透過 <code>systemctl isolate [TARGET_UNIT]</code> 可以馬上變換不同的 target unit，假設目前在圖形模式下，執行指令 <code>sudo systemctl isolate multi-user.target</code> 可以切換到文字模式</p><h3 id="Setting-a-default-target"><a href="#Setting-a-default-target" class="headerlink" title="Setting a default target"></a>Setting a default target</h3><p><code>default.target</code> 為開機時所啟動的 target unit，這是個 symbolic link，指向真正的 target unit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 目前指向 graphical.target，因此開機會是圖形介面</span></span><br><span class="line">$ ls -al /etc/systemd/system/default.target</span><br><span class="line">lrwxrwxrwx. 1 root root 40 Jul 11  2014 /etc/systemd/system/default.target -&gt; /usr/lib/systemd/system/graphical.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以用另外一個指令取得 default target unit</span></span><br><span class="line">$ systemctl get-default</span><br><span class="line">graphical.target</span><br></pre></td></tr></table></figure><p>當然也有指令可以設定 default target unit：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl set-default multi-user.target</span><br><span class="line">rm <span class="string">&#x27;/etc/systemd/system/default.target&#x27;</span></span><br><span class="line">ln -s <span class="string">&#x27;/usr/lib/systemd/system/multi-user.target&#x27;</span> <span class="string">&#x27;/etc/systemd/system/default.target&#x27;</span></span><br><span class="line"></span><br><span class="line">$ systemctl get-default</span><br><span class="line">multi-user.target</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line">&gt; 開機時編輯開機設定，在 `linux16` 那一行的最後面加上 `systemd.unit=[TARGET_UNIT]` 也可以直接進入所指定的 target unit</span><br><span class="line"></span><br><span class="line">----------------------------------------------------</span><br><span class="line"></span><br><span class="line">Lab: Controlling Services and Daemons</span><br><span class="line">=====================================</span><br><span class="line"></span><br><span class="line"><span class="comment">## 目標</span></span><br><span class="line"></span><br><span class="line">- 設定開機時同時支援文字介面登入 &amp; 圖形介面登入</span><br><span class="line"></span><br><span class="line">- 設定 `rsyslog` service 開機時啟動</span><br><span class="line"></span><br><span class="line"><span class="comment">## 實作步驟</span></span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line"><span class="comment"># 檢查目前的 target unit</span></span><br><span class="line">$ systemctl get-default</span><br><span class="line">multi-user.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 target unit 為 graphical.target (同時支援文字介面登入 &amp; 圖形介面登入)</span></span><br><span class="line">[student@server0 ~]$ sudo systemctl set-default graphical.target</span><br><span class="line">rm <span class="string">&#x27;/etc/systemd/system/default.target&#x27;</span></span><br><span class="line">ln -s <span class="string">&#x27;/usr/lib/systemd/system/graphical.target&#x27;</span> <span class="string">&#x27;/etc/systemd/system/default.target&#x27;</span></span><br><span class="line"></span><br><span class="line">[student@server0 ~]$ systemctl get-default</span><br><span class="line">graphical.target</span><br><span class="line"></span><br><span class="line">[student@server0 ~]$ systemctl status rsyslog</span><br><span class="line">rsyslog.service - System Logging Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; disabled)</span><br><span class="line">   Active: inactive (dead)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 rsyslog.service 開機啟動</span></span><br><span class="line">[student@server0 ~]$ sudo systemctl <span class="built_in">enable</span> rsyslog.service</span><br><span class="line">ln -s <span class="string">&#x27;/usr/lib/systemd/system/rsyslog.service&#x27;</span> <span class="string">&#x27;/etc/systemd/system/multi-user.target.wants/rsyslog.service&#x27;</span></span><br><span class="line">[student@server0 ~]$ sudo systemctl restart rsyslog.service</span><br><span class="line">[student@server0 ~]$ systemctl status rsyslog</span><br><span class="line">rsyslog.service - System Logging Service</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/rsyslog.service; enabled)</span><br><span class="line">   Active: active (running) since Sat 2016-05-21 17:21:24 JST; 2s ago</span><br><span class="line"> Main PID: 31841 (rsyslogd)</span><br><span class="line">   CGroup: /system.slice/rsyslog.service</span><br><span class="line">           └─31841 /usr/sbin/rsyslogd -n</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH254 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 13. Controlling and Troubleshooting the RHEL Boot Process 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH13_ControllingAndTroubleshootingTheRHELBootProcess/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH13_ControllingAndTroubleshootingTheRHELBootProcess/</url>
      
        <content type="html"><![CDATA[<h1 id="13-1-The-RHEL-Boot-Process"><a href="#13-1-The-RHEL-Boot-Process" class="headerlink" title="13.1 The RHEL Boot Process"></a>13.1 The RHEL Boot Process</h1><h2 id="13-1-1-The-RHCE-7-boot-process"><a href="#13-1-1-The-RHCE-7-boot-process" class="headerlink" title="13.1.1 The RHCE 7 boot process"></a>13.1.1 The RHCE 7 boot process</h2><ol><li><p>電腦開機從 BIOS 開始</p></li><li><p>進行 POST 檢查</p></li><li><p>stage-1：載入 Boot Loader 並開始執行</p></li><li><p>stage-2：讀取 <code>/boot/grub2/grub.cfg</code> 並執行</p></li><li><p><code>grub.cfg</code> 解壓縮 <code>initramfs-xxxx</code>(很小的 gzip + cpio 的 Linux，若要額外加上 HW driver 就要包到這裡)，並載入記憶體中</p></li><li><p><code>grub.cfg</code> 執行 <code>vmlinuz-xxx</code>(可執行的 Linux kernel 映像檔)，將記憶體的內容掛載到 <code>/</code></p></li><li><p>執行 <code>/init</code> (已經變成 soft link) 指向其他地方</p></li><li><p><code>init</code> 會 <code>mount -o ro [HD root partition] /sysroot</code></p></li><li><p><code>chroot /sysroot</code></p></li><li><p>執行 <code>/lib/systemd/systemd</code></p></li></ol><p>boot load 解壓縮 initramfs-xxx 到記憶體中</p><p>第 8 個步驟 <code>-o ro</code> 的原因是開機時會作 fsck，因為無法 unmount 根目錄，因此只能用 read only 的方式</p><h2 id="13-1-3-Selecting-a-systemd-target"><a href="#13-1-3-Selecting-a-systemd-target" class="headerlink" title="13.1.3 Selecting a systemd target"></a>13.1.3 Selecting a systemd target</h2><p><strong>systemd target</strong> 是由一組 systemd unit 所組成，可以讓 user 進入到某一個特定狀態(例如：圖形介面)，比較常用的包含了：</p><ul><li><p>graphical.target：圖形 &amp; 文字介面</p></li><li><p>multi-user.target：文字介面</p></li><li><p>rescue.target：初始化完成，會提示輸入 root 密碼</p></li><li><p>emergency.target：initramfs 掛載 / 完成，只有 read only 權限，且會提示輸入 root 密碼</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 啟動 graphic.target 需要由那些 systemd target unit 所組成</span></span><br><span class="line">$ systemctl list-dependencies graphical.target | grep target</span><br><span class="line">graphical.target</span><br><span class="line">└─multi-user.target</span><br><span class="line">  ├─basic.target</span><br><span class="line">  │ ├─paths.target</span><br><span class="line">  │ ├─slices.target</span><br><span class="line">  │ ├─sockets.target</span><br><span class="line">  │ ├─sysinit.target</span><br><span class="line">  │ │ ├─cryptsetup.target</span><br><span class="line">  │ │ ├─local-fs.target</span><br><span class="line">  │ │ └─swap.target</span><br><span class="line">  │ └─timers.target</span><br><span class="line">  ├─getty.target</span><br><span class="line">  ├─nfs.target</span><br><span class="line">  └─remote-fs.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出系統中所有的 systemd target unit</span></span><br><span class="line">$ systemctl list-units --<span class="built_in">type</span>=target --all</span><br><span class="line">UNIT                   LOAD   ACTIVE   SUB    DESCRIPTION</span><br><span class="line">basic.target           loaded active   active Basic System</span><br><span class="line">cryptsetup.target      loaded active   active Encrypted Volumes</span><br><span class="line">emergency.target       loaded inactive dead   Emergency Mode</span><br><span class="line">.......</span><br><span class="line">umount.target          loaded inactive dead   Unmount All Filesystems</span><br><span class="line"></span><br><span class="line">LOAD   = Reflects whether the unit definition was properly loaded.</span><br><span class="line">ACTIVE = The high-level unit activation state, i.e. generalization of SUB.</span><br><span class="line">SUB    = The low-level unit activation state, values depend on unit <span class="built_in">type</span>.</span><br><span class="line"></span><br><span class="line">27 loaded units listed.</span><br><span class="line">To show all installed unit files use <span class="string">&#x27;systemctl list-unit-files&#x27;</span>.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 線上直接切換不同的 target (但這會停掉指定 target 中不需要的服務)</span></span><br><span class="line">$ sudo systemctl isolate multi-user.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 graphical.target 設定為預設值</span></span><br><span class="line">$ $ sudo systemctl set-default graphical.target</span><br><span class="line">rm <span class="string">&#x27;/etc/systemd/system/default.target&#x27;</span></span><br><span class="line">ln -s <span class="string">&#x27;/usr/lib/systemd/system/graphical.target&#x27;</span> <span class="string">&#x27;/etc/systemd/system/default.target&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>也可以在開機時，修改開機設定，在 <code>linux16</code> 那一行最後面，加上 <code>systemd.unit=graphical.target</code>，也可以進入圖形模式</p></blockquote><hr><h1 id="13-2-Repairing-Common-Boot-Issues"><a href="#13-2-Repairing-Common-Boot-Issues" class="headerlink" title="13.2 Repairing Common Boot Issues"></a>13.2 Repairing Common Boot Issues</h1><h2 id="13-2-1-Recovering-the-root-password"><a href="#13-2-1-Recovering-the-root-password" class="headerlink" title="13.2.1 Recovering the root password"></a>13.2.1 Recovering the root password</h2><blockquote><p>透過修改開機設定，加上 <code>rd.break</code> 進入可救援的模式來修改 root 密碼，但這樣會破壞 SELinux security context</p></blockquote><ol><li><p>修改 linux16 開頭的設定，從最後面移除設定直到 <code>ro</code> 前，並在該行最後加上 <code>rd.break</code>，按下 <code>Ctrl+x</code> 使用此設定開機 (開機完成後會位於 initramfs 的 <strong>/**，並包含了 **real-only</strong> 的 <strong>/sysroot</strong>)</p></li><li><p>重新掛載 /sysroot 目錄為可讀寫：<code>mount -o remount,rw /sysroot</code></p></li><li><p>切換根目錄到 /sysroot 上：<code>chroot /sysroot</code></p></li><li><p>修改 root 密碼：<code>passwd root</code></p></li><li><p>產生 <code>/.autorelabel</code> 檔案，讓開機過程可以重新 relabel：<code>touch /.autorelabel</code></p></li><li><p>連續輸入兩次 <code>exit</code>，回到開機程序</p></li></ol><p><code>/.autorelabel</code>：此檔案用途會讓 SELinux 進行 relabel 的動作，將 security context 恢復到正確的狀態</p><h3 id="Using-journalctl"><a href="#Using-journalctl" class="headerlink" title="Using journalctl"></a>Using journalctl</h3><p>預設 log 儲存在記憶體中，只要 <code>sudo mkdir -p -m2775 /var/log/journal</code> 就可以將 systemd journal 放到硬碟中</p><p>顯示上一次開機的過程中，Level ERROR 的 log：<code>sudo journalctl -b -1 -p err</code></p><h3 id="Diagnose-and-repair-systemd-boot-issues"><a href="#Diagnose-and-repair-systemd-boot-issues" class="headerlink" title="Diagnose and repair systemd boot issues"></a>Diagnose and repair systemd boot issues</h3><p><code>sudo systemctl enable debug-shell.service</code>：重開機後可以透過 Ctrl + Alt + F9 得到一個 root shell</p><p>開機流程中，得到 shell 的順序是：</p><ol><li><p>rd.break</p></li><li><p>emergency.target</p></li><li><p>rescue.target</p></li></ol><blockquote><p>所以其實使用 rd.break 就可以解決所有開機的相關問題</p></blockquote><p><code>systemctl list-job</code>：可用來觀察開機時的 stuck job</p><hr><h1 id="13-3-Repairing-File-System-Issues-at-Boot"><a href="#13-3-Repairing-File-System-Issues-at-Boot" class="headerlink" title="13.3 Repairing File System Issues at Boot"></a>13.3 Repairing File System Issues at Boot</h1><ul><li><p>每次開機 fsck 都會嘗試自動修復，若無法自動處理則會進入 emergency shell</p></li><li><p>設備不存在，systemd 會等一段時間，若依然沒有就會進入 emergency shell 給管理者除錯</p></li><li><p>mount point 不存在，systemd 會嘗試自動建立，否則就進入 emergency shell</p></li><li><p>/etc/fstab 錯誤，直接進入 emergency shell</p></li></ul><hr><h1 id="13-4-Repairing-Boot-Loader-Issues"><a href="#13-4-Repairing-Boot-Loader-Issues" class="headerlink" title="13.4 Repairing Boot Loader Issues"></a>13.4 Repairing Boot Loader Issues</h1><ul><li><p>GRUB2 同時支援 BIOS &amp; UEFI</p></li><li><p>主要設定檔位於 <code>/boot/grub2/grub.cfg</code>；但若是要設定 UEFI 時，grub.conf 就要到 <code>/boot/efi</code> 找</p></li><li><p>在 linux16 那一行，再最後加上 <code>net.ifnames=0</code> 後，網卡就會以傳統的方式命名(eth0, eth1 … etc)</p></li><li><p><code>set root</code> 是描述開機相關的檔案(vmlinuz-xxx / initramfs-xxx … etc)存在於哪個 partition</p></li><li><p>linux16 設定中，<code>root=UUID=xxxx</code> 則是用來指定 root partition</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 11. Accessing Network Storage with Network File System (NFS) 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH11_AccessingNetworkStorageWithNetworkFileSystem(NFS)/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH11_AccessingNetworkStorageWithNetworkFileSystem(NFS)/</url>
      
        <content type="html"><![CDATA[<h1 id="11-1-Mounting-Network-Storage-with-NFS"><a href="#11-1-Mounting-Network-Storage-with-NFS" class="headerlink" title="11.1 Mounting Network Storage with NFS"></a>11.1 Mounting Network Storage with NFS</h1><h2 id="11-1-1-Manualling-mouting-and-unmouting-NFS-shares"><a href="#11-1-1-Manualling-mouting-and-unmouting-NFS-shares" class="headerlink" title="11.1.1 Manualling mouting and unmouting NFS shares"></a>11.1.1 Manualling mouting and unmouting NFS shares</h2><h3 id="NFS-4-0-之前："><a href="#NFS-4-0-之前：" class="headerlink" title="NFS 4.0 之前："></a>NFS 4.0 之前：</h3><ul><li><p>NFS 啟動時會向 rpcbind 註冊</p></li><li><p>NFS 的 port 是 rpcbind(TCP 111) 所配發的</p></li><li><p>rpcbind restart，NFS 也要跟著 restart</p></li><li><p>查詢遠端主機開放的目錄：<code>sudo showmount -e [remote_host_name or IP]</code></p></li><li><p>掛載：<code>sudo mount -t nfs [remote_host_name or IP]:/content /mnt</code></p></li></ul><h3 id="NFS-4-0："><a href="#NFS-4-0：" class="headerlink" title="NFS 4.0："></a>NFS 4.0：</h3><ul><li><p>固定使用 TCP port 2049</p></li><li><p>無法使用 <strong>showmount</strong>，因此必須預先知道開放的目錄</p></li><li><p>同時掛載所有開放的目錄：<code>mount -t nfs [remote_host_name or IP]:/ /mnt</code> (確定路徑也可以 mount 特定目錄)</p></li></ul><blockquote><p>NFSv2, NFSv3, NFSv4 預設是同時開啟的</p></blockquote><p>RHEL 7 預設會使用 NFSv4，若不支援才會往下降；NFSv4 使用 TCP，舊版本則會用 TCP or UDP</p><p>掛載 NFS 方式：</p><ul><li><p>手動下命令</p></li><li><p>加到 <code>/etc/fstab</code> 中：<code>[remote_host_name]:/content  /mnt  nfs   default,sync,sec=xxx   0 0</code></p></li></ul><blockquote><p>網路磁碟機建議使用 sync 選項，確保資料完整性 (另外了解 soft &amp; hard 的差異)</p></blockquote><blockquote><p>使用 fsck 檢查的設備，必須是 umount or readonly 的狀態所，所出來的結果才是正確的</p></blockquote><h2 id="11-1-2-Security-methods"><a href="#11-1-2-Security-methods" class="headerlink" title="11.1.2 Security methods"></a>11.1.2 Security methods</h2><ul><li><p><code>none</code>：client 匿名存取(送來的身份會被忽略)，身份全部都會轉成 <strong>nfsnobody</strong></p></li><li><p><code>sys</code>：client 存取時會把身份送進來(例如：UID 1000)，NFS server 會找到對應的身份 &amp; 權限 (**<font color='red'>預設值</font>**，但其實不安全)</p></li><li><p><code>krb5</code>：使用 kerberos ticket，但通訊時以明碼傳遞</p></li><li><p><code>krb5i</code>：同上，但加上完整性驗證，可判斷傳輸的資料是否被修改</p></li><li><p><code>krb5p</code>：同上，但傳輸的資料會被加密</p></li></ul><blockquote><p>若要讓 NFS 可以向 Kerberos 進行驗證，也同時需要有 <code>nfs-secure</code> 服務(在 <code>nfs-utils</code> 套件中)</p></blockquote><blockquote><p>使用 <code>klist</code> 可以查詢目前所擁有的 kerberos ticket</p></blockquote><hr><h1 id="Practice-Mounting-and-Unmounting-NFS"><a href="#Practice-Mounting-and-Unmounting-NFS" class="headerlink" title="Practice: Mounting and Unmounting NFS"></a>Practice: Mounting and Unmounting NFS</h1><h2 id="目前已存在設定"><a href="#目前已存在設定" class="headerlink" title="目前已存在設定"></a>目前已存在設定</h2><ol><li><p>server1 分享 <code>/shares/manual</code> &amp; <code>/shares/public</code> 兩個目錄</p></li><li><p>desktop1 掛載點為 <code>/mnt/manual</code> &amp; <code>/mnt/public</code></p></li><li><p><code>public</code> 分享目錄需要使用 <code>krb5p</code> 認證，<code>manual</code> 分享目錄則是使用 <code>sys</code> 認證</p></li><li><p><code>krb5.keytab</code> 位置位於 <code>http://classroom.example.com/pub/keytabs/desktop1.keytab</code></p></li></ol><h2 id="實作過程"><a href="#實作過程" class="headerlink" title="實作過程"></a>實作過程</h2><p>1、取得 krb5.keytab</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo wget -O /etc/krb5.keytab http://classroom.example.com/pub/keytabs/desktop1.keytab</span><br><span class="line">$ ls -lZ /etc/krb5.keytab</span><br><span class="line">-rw-r--r--. root root unconfined_u:object_r:krb5_keytab_t:s0 /etc/krb5.keytab</span><br></pre></td></tr></table></figure><p>2、啟動 <code>nfs-secure.service</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl <span class="built_in">enable</span> nfs-secure.service</span><br><span class="line">ln -s <span class="string">&#x27;/usr/lib/systemd/system/nfs-secure.service&#x27;</span> <span class="string">&#x27;/etc/systemd/system/nfs.target.wants/nfs-secure.service&#x27;</span></span><br><span class="line">[student@desktop0 ~]$ sudo systemctl restart nfs-secure.service</span><br></pre></td></tr></table></figure><p>3、建立目錄並設定掛載資訊</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;server1:/shares/public /mnt/public nfs sec=krb5p,sync 0 0&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;server1:/shares/manual /mnt/manual nfs sec=sys,sync 0 0&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line"></span><br><span class="line">$ sudo mount -a</span><br><span class="line">$ $ sudo df -hT</span><br><span class="line">Filesystem             Type      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1              xfs        10G  3.1G  7.0G  31% /</span><br><span class="line">devtmpfs               devtmpfs  906M     0  906M   0% /dev</span><br><span class="line">tmpfs                  tmpfs     921M   80K  921M   1% /dev/shm</span><br><span class="line">tmpfs                  tmpfs     921M   17M  904M   2% /run</span><br><span class="line">tmpfs                  tmpfs     921M     0  921M   0% /sys/fs/cgroup</span><br><span class="line">server1:/shares/public nfs4       10G  3.1G  7.0G  31% /mnt/public</span><br><span class="line">server1:/shares/manual nfs4       10G  3.1G  7.0G  31% /mnt/manual</span><br></pre></td></tr></table></figure><hr><h1 id="11-2-Automounting-Network-Storage-with-NFS"><a href="#11-2-Automounting-Network-Storage-with-NFS" class="headerlink" title="11.2 Automounting Network Storage with NFS"></a>11.2 Automounting Network Storage with NFS</h1><p>需要安裝套件 <code>autofs</code>，會在系統中提供一個 autofs system service</p><p><strong>優點：</strong></p><ul><li><p>使用者不需要 root 權限來執行 mount &amp; unmount 指令</p></li><li><p>automounter 的設定對所有使用者皆有效，主要是著重在存取權限的調整</p></li><li><p>不會與 NFS server 一直持續保持連線，節省網路 &amp; 系統資源</p></li><li><p>與原本 mount 所使用的設定參數相同</p></li><li><p>提供 direct &amp; indirect 兩種目錄 mapping 模式，以提供 mount point 一定程度的彈性</p></li><li><p>indirect 模式下，目錄會被自動生成，降低手動的需求</p></li><li><p>automounter 其實可以用在 NFS 之外的其他檔案系統上</p></li></ul><h2 id="direct-map"><a href="#direct-map" class="headerlink" title="direct-map"></a>direct-map</h2><p>目錄 <code>/etc/auto.master.d</code>，檔名不拘，副檔名必須是 <code>.autofs</code></p><p><code>/etc/sysconfig/autofs</code> 中有 <code>TIMEOUT</code> 參數可以用，表示離開目錄多久會自動 unmount</p><p>自動掛載光碟機：<code>/mnt/cdrom   -ro,iso9660   :/dev/sr0</code></p><h2 id="indirect-map"><a href="#indirect-map" class="headerlink" title="indirect-map"></a>indirect-map</h2><p>掛載的目錄都會在同一個目錄下</p><p>可用萬用字元</p><h2 id="掛載方式"><a href="#掛載方式" class="headerlink" title="掛載方式"></a>掛載方式</h2><h3 id="1、安裝套件"><a href="#1、安裝套件" class="headerlink" title="1、安裝套件"></a>1、安裝套件</h3><p>首先要安裝 <code>autofs</code> 套件</p><h3 id="2、編輯-master-map-檔案"><a href="#2、編輯-master-map-檔案" class="headerlink" title="2、編輯 master-map 檔案"></a>2、編輯 master-map 檔案</h3><p>master-map 檔案名稱必須是 <code>.autofs</code> 結尾，並存放於 <code>/etc/auto.master.d</code> 目錄中，以下舉例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cat /etc/auto.master.d/demo.autofs</span><br><span class="line"><span class="comment"># indirect-map，並以 &quot;/share&quot; 目錄作為掛載目錄的基底，詳細設定檔內容位於 /etc/auto.demo 檔案中</span></span><br><span class="line">/shares   /etc/auto.demo</span><br><span class="line"><span class="comment"># direct-map，以 &quot;/-&quot; 作為掛載基底，詳細設定檔內容位於 /etc/auto.direct 檔案中</span></span><br><span class="line">/-        /etc/auto.direct</span><br></pre></td></tr></table></figure><h3 id="3、設定-mapping-檔案"><a href="#3、設定-mapping-檔案" class="headerlink" title="3、設定 mapping 檔案"></a>3、設定 mapping 檔案</h3><p>mapping 檔案則是要撰寫詳細的設定內容：</p><p><code>/etc/auto.demo</code> indirect-map 設定檔內容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用本地目錄 /shares/work，掛載遠端目錄 serverX:/shares/work</span></span><br><span class="line">work  -rw,sync  serverX:/shares/work</span><br><span class="line"><span class="comment"># 使用本地目錄 /shares，掛載遠端目錄 serverX:/shares 下的所有目錄(會一一取得對應)</span></span><br><span class="line">*     -rw,sync  serverX:/shares/&amp;</span><br></pre></td></tr></table></figure><blockquote><p>indirect-map 設定使用的是相對路徑的設計概念</p></blockquote><p><code>/etc/auto.direct</code> direct-map 設定檔內容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用本地目錄 /mnt/docs，掛載遠端目錄 serverX:/shares/docs</span></span><br><span class="line">/mnt/docs   -rw,sync  serverX:/shares/docs</span><br></pre></td></tr></table></figure><blockquote><p>由此可看出 direct-map 就是很明確的指定完整路徑</p></blockquote><h3 id="4、啟動-autofs-服務"><a href="#4、啟動-autofs-服務" class="headerlink" title="4、啟動 autofs 服務"></a>4、啟動 autofs 服務</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl <span class="built_in">enable</span> autofs</span><br><span class="line"></span><br><span class="line">$ sudo systemctl start autofs</span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Automounting-NFS"><a href="#Practice-Automounting-NFS" class="headerlink" title="Practice: Automounting NFS"></a>Practice: Automounting NFS</h1><h2 id="環境設定"><a href="#環境設定" class="headerlink" title="環境設定"></a>環境設定</h2><ol><li><p>遠端主機 <code>server1</code> 使用 NFS 分享了 <code>/shares/&#123;docs,work,public&#125;</code> 三個目錄</p></li><li><p>存取遠端主機必須使用 Kerberos 協定，使用的是 <code>krb5p</code></p></li><li><p>本地主機 <code>desktop1</code> 使用 <code>/shares/&#123;docs,work&#125;</code> &amp; <code>/mnt/public</code> 目錄進行掛載</p></li><li><p><code>krb5.keytab</code> 檔案可到 <code>http://classroom.example.com/pub/keytabs/desktop1.keytab</code> 下載</p></li><li><p>必須要設定成永久性掛載</p></li></ol><h2 id="設定方式"><a href="#設定方式" class="headerlink" title="設定方式"></a>設定方式</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安裝 autofs 套件</span></span><br><span class="line">$ sudo yum -y install autofs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立掛載用目錄</span></span><br><span class="line">$ sudo mkdir /shares</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得 krb5.keytab</span></span><br><span class="line">$ sudo wget -O /etc/krb5.keytab http://classroom.example.com/pub/keytabs/desktop0.keytab</span><br><span class="line"><span class="comment"># 驗證 SElinux context 是否正確</span></span><br><span class="line">$ ls -lZ /etc/krb5.keytab</span><br><span class="line">-rw-r--r--. root root unconfined_u:object_r:krb5_keytab_t:s0 /etc/krb5.keytab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 編輯 master-map 檔案</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;/shares   /etc/autofs.indirect&quot;</span> | sudo tee --append /etc/auto.master.d/practice.autofs</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;/-   /etc/autofs.direct&quot;</span> | sudo tee --append /etc/auto.master.d/practice.autofs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 編輯 indrect-map 檔案</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;docs  -rx,sync,sec=krb5p  server1:/shares/docs&quot;</span> | sudo tee --append /etc/autofs.indirect</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;work  -rx,sync,sec=krb5p  server1:/shares/work&quot;</span> | sudo tee --append /etc/autofs.indirect</span><br><span class="line"></span><br><span class="line"><span class="comment"># 編輯 direct-map 檔案</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;/mnt/public  -rx,sync,sec=krb5p  server1:/shares/public&quot;</span> | sudo tee --append /etc/autofs.direct</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 nfs-secure 服務(為了進行 Kerberos 認證)</span></span><br><span class="line">$ sudo systemctl start nfs-secure.service</span><br><span class="line">$ sudo systemctl restart autofs.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 autofs 服務</span></span><br><span class="line">$ sudo systemctl <span class="built_in">enable</span> autofs.service</span><br><span class="line">$ sudo systemctl start autofs.service</span><br></pre></td></tr></table></figure><hr><h1 id="Lab-Accessing-Network-Storage-with-Network-File-System-NFS"><a href="#Lab-Accessing-Network-Storage-with-Network-File-System-NFS" class="headerlink" title="Lab: Accessing Network Storage with Network File System(NFS)"></a>Lab: Accessing Network Storage with Network File System(NFS)</h1><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><ol><li><p><code>desktop1</code> 已經設定好 LDAP &amp; Kerberos 認証，遠端主機為 <code>classroom.example.com</code></p></li><li><p>遠端主機分享了 <code>ldapuserX</code> 所有的家目錄，分享路徑為 <code>/home/guests</code> 底下</p></li><li><p>掛載於本地端的 <code>/home/guests</code> 下</p></li><li><p>ldapuserX 登入後會位於 <code>/home/guests/ldapuserX</code> 家目錄中</p></li></ol><h2 id="實作過程-1"><a href="#實作過程-1" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir /home/guests</span><br><span class="line">$ sudo yum -y install autofs</span><br><span class="line"></span><br><span class="line"><span class="comment"># autofs 設定</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;/home/guests    /etc/autofs.indirect&quot;</span> | sudo tee --append /etc/auto.master.d/lab.autofs</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;*   -rw,sync    classroom.example.com:/home/guests/&amp;&quot;</span> | sudo tee --append /etc/autofs.indirect</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用服務</span></span><br><span class="line">$ sudo systemctl <span class="built_in">enable</span> autofs.service</span><br><span class="line">$ sudo systemctl restart autofs.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登入驗證</span></span><br><span class="line">$ ssh ldapuser1@localhost</span><br><span class="line">[ldapuser1@desktop1 ~]$ <span class="built_in">pwd</span></span><br><span class="line">/home/guests/ldapuser1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 12. Accessing Network Storage with SMB 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH12_AccessingNetworkStorageWithSMB/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH12_AccessingNetworkStorageWithSMB/</url>
      
        <content type="html"><![CDATA[<h1 id="12-1-Accessing-Network-Storage-with-SMB"><a href="#12-1-Accessing-Network-Storage-with-SMB" class="headerlink" title="12.1 Accessing Network Storage with SMB"></a>12.1 Accessing Network Storage with SMB</h1><h2 id="Manually-mounting-and-unmouting-SMB-file-systems"><a href="#Manually-mounting-and-unmouting-SMB-file-systems" class="headerlink" title="Manually mounting and unmouting SMB file systems"></a>Manually mounting and unmouting SMB file systems</h2><p>要裝 <code>cifs-utils</code> &amp; <code>samba-client</code> 兩個套件</p><h3 id="查詢"><a href="#查詢" class="headerlink" title="查詢"></a>查詢</h3><p>查詢 server 分享了什麼資源(匿名)：<code>smbclient -L //server0</code></p><p>查詢 server 分享了什麼資源：<code>smbclient -L //server0 -U [UserName] -W [DomainName]</code> (<code>-U</code> 指定使用者，<code>-W</code> 指定網域)</p><h3 id="掛載"><a href="#掛載" class="headerlink" title="掛載"></a>掛載</h3><p>匿名掛載：<code>sudo mount -t cifs -o guest //server0/public /mnt</code></p><p>指定使用者掛載：<code>sudo mount -t cifs -o username=[UserName],password=[Password],workgroup=[DomainName] //server0/public /mnt</code> (若不加 <code>password</code> 會被要求手動輸入密碼)</p><h2 id="透過-etc-fstab-掛載"><a href="#透過-etc-fstab-掛載" class="headerlink" title="透過 /etc/fstab 掛載"></a>透過 /etc/fstab 掛載</h2><p>加入 <code>/etc/fstab</code> 中：<code>//server0/pubilic  /mnt  cifs  defaults,username=[UserName],password=[Password],workgroup=[DomainName] 0 0</code></p><h2 id="指定-credential-掛載"><a href="#指定-credential-掛載" class="headerlink" title="指定 credential 掛載"></a>指定 credential 掛載</h2><p>以 credential 的方式設定：<code>//server0/pubilic  /mnt  cifs  defaults,credentials=/root/smb.txt 0 0</code></p><p><code>/root/smb.txt</code> 的內容如下：(<strong>only root access, chmod 600</strong>)</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">username</span>=[UserName]</span><br><span class="line"><span class="attr">password</span>=[Password]</span><br><span class="line"><span class="attr">domain</span>=[DomainName]</span><br></pre></td></tr></table></figure><h2 id="Mounting-SMB-file-systems-with-the-automounter"><a href="#Mounting-SMB-file-systems-with-the-automounter" class="headerlink" title="Mounting SMB file systems with the automounter"></a>Mounting SMB file systems with the automounter</h2><p>除了 NFS 之外，SMB 也可以使用 automounter 來協助自動掛載，差別只有掛載參數上的不同，以下是設定範例：</p><ul><li><p>本地目錄：<code>/bakerst/cases</code></p></li><li><p>遠端目錄：<code>//serverX/cases</code></p></li></ul><p>1、安裝 <code>autofs</code> 套件</p><p>2、新增檔案 <strong>/etc/auto.master.d/bakerst.autofs</strong>，內容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bakerst  /etc/auto.bakerst</span><br></pre></td></tr></table></figure><p>3、新增檔案 <strong>/etc/auto.bakerst</strong>，內容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cases   -fstype=cifs,credentials=/secure/sherlock   ://serverX/cases</span><br></pre></td></tr></table></figure><p>4、新增檔案 <strong>/secure/sherlock</strong>，內容如下：(<strong>only root access</strong>, permission <strong>600</strong>)</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">username</span>=[UserName]</span><br><span class="line"><span class="attr">password</span>=[Password]</span><br><span class="line"><span class="attr">domain</span>=[DomainName]</span><br></pre></td></tr></table></figure><p>5、啟動 autofs：<code>sudo systemctl enable autofs &amp;&amp; sudo systemctl restart autofs</code></p><hr><h1 id="Practice-Mounting-a-SMB-File-System"><a href="#Practice-Mounting-a-SMB-File-System" class="headerlink" title="Practice: Mounting a SMB File System"></a>Practice: Mounting a SMB File System</h1><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><ol><li><p>掛載遠端目錄 <code>//server1/student</code> 到本地端 <code>~/work</code> 中</p></li><li><p>連線帳號/密碼/Domain = student/student/MYGROUP</p></li><li><p>永久性掛載</p></li></ol><h2 id="實作過程"><a href="#實作過程" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum -y install cifs-utils</span><br><span class="line"></span><br><span class="line">$ sudo bash -c <span class="string">&#x27;cat &lt;&lt; EOF &gt; /root/student.smb</span></span><br><span class="line"><span class="string">username=student</span></span><br><span class="line"><span class="string">password=student</span></span><br><span class="line"><span class="string">domain=MYGROUP</span></span><br><span class="line"><span class="string">EOF&#x27;</span></span><br><span class="line"></span><br><span class="line">$ mkdir ~/work</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;//server0/student  /home/student/work  cifs  credentials=/root/student.smb  0 0&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line">$ sudo mount -a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 驗證連線結果</span></span><br><span class="line">[student@desktop0 ~]$ df -hT | grep work</span><br><span class="line">//server0/student cifs       10G  3.1G  7.0G  31% /home/student/work</span><br></pre></td></tr></table></figure><hr><h1 id="Lab-Accessing-Network-Storage-with-SMB"><a href="#Lab-Accessing-Network-Storage-with-SMB" class="headerlink" title="Lab: Accessing Network Storage with SMB"></a>Lab: Accessing Network Storage with SMB</h1><h2 id="目標-1"><a href="#目標-1" class="headerlink" title="目標"></a>目標</h2><h3 id="環境"><a href="#環境" class="headerlink" title="環境"></a>環境</h3><ol><li><p>遠端主機：<code>server1</code></p></li><li><p>DOMAIN：<code>MYGROUP</code></p></li><li><p>使用者帳號密碼：<code>student</code> / <code>student</code></p></li></ol><h3 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h3><ol><li><p>自動掛載遠端主機的 <code>student</code> 到本地端的家目錄 <code>/shares/work</code></p></li><li><p>自動掛載遠端主機的 <code>public</code> 到本地端目錄 <code>/shares/docs</code> 公開分享的目錄，允許任何人存取，權限為 <code>read-only</code></p></li><li><p>自動掛載遠端主機的 <code>/shares/cases</code> 到本地端目錄 <code>/shares/cases</code>，並限制只有 <code>bakerst</code>(GID=10221) 群組可以存取，權限為 <code>read-write</code></p></li><li><p>要設定為永久性掛載(重開機之後要依然生效)</p></li></ol><h2 id="實作過程-1"><a href="#實作過程-1" class="headerlink" title="實作過程"></a>實作過程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安裝套件</span></span><br><span class="line">$ sudo yum -y install cifs-utils autofs</span><br><span class="line"></span><br><span class="line">$ sudo bash -c <span class="string">&#x27;cat &lt;&lt; EOF &gt; /root/student.smb</span></span><br><span class="line"><span class="string">username=student</span></span><br><span class="line"><span class="string">password=student</span></span><br><span class="line"><span class="string">domain=MYGROUP</span></span><br><span class="line"><span class="string">EOF&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立本地掛載目錄</span></span><br><span class="line">[student@desktop0 ~]$ sudo mkdir -p /shares/&#123;work,docs,cases&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 autofs</span></span><br><span class="line">[student@desktop0 ~]$ <span class="built_in">echo</span> <span class="string">&quot;/shares /etc/autofs.indirect&quot;</span> | sudo tee --append /etc/auto.master.d/smb.autofs</span><br><span class="line">[student@desktop0 ~]$ <span class="built_in">echo</span> <span class="string">&quot;work -fstype=cifs,credentials=/root/student.smb ://server0/student&quot;</span> | sudo tee --append /etc/autofs.indirect</span><br><span class="line">[student@desktop0 ~]$ <span class="built_in">echo</span> <span class="string">&quot;docs -fstype=cifs,guest ://server0/public&quot;</span> | sudo tee --append /etc/autofs.indirect</span><br><span class="line">[student@desktop0 ~]$ <span class="built_in">echo</span> <span class="string">&quot;cases -fstype=cifs,credentials=/root/student.smb ://server0/bakerst&quot;</span> | sudo tee --append /etc/autofs.indirect</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 autofs 服務</span></span><br><span class="line">[student@desktop0 ~]$ sudo systemctl <span class="built_in">enable</span> autofs.service</span><br><span class="line">[student@desktop0 ~]$ sudo systemctl start autofs.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 backerst 群組，並將 student 帳號加入</span></span><br><span class="line">[student@desktop0 ~]$ sudo groupadd -g 10221 bakerst</span><br><span class="line">[student@desktop0 ~]$ sudo usermod -aG bakerst student</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 10. Managing Logical Volume Management(LVM) Storage 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH10_ManagingLogicalVolumeManagement(LVM)Storage/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH10_ManagingLogicalVolumeManagement(LVM)Storage/</url>
      
        <content type="html"><![CDATA[<h1 id="10-1-Logical-Volume-Management-Concepts"><a href="#10-1-Logical-Volume-Management-Concepts" class="headerlink" title="10.1 Logical Volume Management Concepts"></a>10.1 Logical Volume Management Concepts</h1><p>可以從下圖很清楚看出 Physical Drivers / Partitions / PV(Physical Volume) / VG(Volume Group) / LV(Logical Volume) 的相互關係</p><p><img src="http://mo.morsi.org/blog/files/lvm1.png" alt="LVM"></p><p>此外，還有 PE(Physical Extent) &amp; LE(Logical Extent) 與上述其他概念的組成關係：</p><p><img src="https://camo.githubusercontent.com/d8809c16fe3e3f5532e87165dfe4012346b3b13e/687474703a2f2f73332e353163746f2e636f6d2f7779667330322f4d30302f37322f33352f774b696f4c31586574513777746d7a6641414a364d4351464339303134332e6a7067" alt="PE_LE_1"></p><p><img src="http://www.linux-tips-and-tricks.de/images/stories/lvm.jpg" alt="PE_LE_2"></p><hr><h1 id="10-2-Managing-Logical-Volumes"><a href="#10-2-Managing-Logical-Volumes" class="headerlink" title="10.2 Managing Logical Volumes"></a>10.2 Managing Logical Volumes</h1><h2 id="10-2-1-Create-PV"><a href="#10-2-1-Create-PV" class="headerlink" title="10.2.1 Create PV"></a>10.2.1 Create PV</h2><ol><li><p>在 /dev/sda 中建立一個 partition，使用所有空間，partition type = 8e</p></li><li><p>第一個硬碟建立 PV：<code>sudo pvcreate /dev/sda1</code></p></li><li><p>第二個硬碟建立 PV：<code>sudo pvcreate /dev/sdb</code>(沒有 partition 的狀況下)</p></li></ol><h3 id="其他相關指令"><a href="#其他相關指令" class="headerlink" title="其他相關指令"></a>其他相關指令</h3><ul><li><p><code>pvscan</code>：目前有哪些 PV</p></li><li><p><code>pvdisplay [pv_name]</code>：顯示 PV 的詳細資訊</p></li></ul><h2 id="10-2-2-Create-VG"><a href="#10-2-2-Create-VG" class="headerlink" title="10.2.2 Create VG"></a>10.2.2 Create VG</h2><ol><li><p>建立 Volume Group：<code>sudo vgcreate -s 16M vg0 /dev/sda1 /dev/sdb</code> (<strong><font color='red'>設定 PE 大小為 16M</font></strong>)</p><blockquote><p>相同 VG 中的 PE 都一樣大</p></blockquote></li><li><p>檢查 PV：<code>sudo pvscan</code> (顯示 PV 被使用中)</p></li><li><p>檢查 VG：<code>sudo vgscan</code></p></li><li><p>檢視 VG 詳細資料：<code>sudo vgdisplay [vg_name]</code></p></li></ol><h2 id="10-2-3-Create-LV"><a href="#10-2-3-Create-LV" class="headerlink" title="10.2.3 Create LV"></a>10.2.3 Create LV</h2><ol><li><p>建立 LV(從 vg0 切割 200M 空間，給 LV 使用，並命名為 lv-1)：<code>sudo lvcreate -L 200M -n lv-1 vg0</code></p></li><li><p>建立 LV(使用 PE 個數指定大小)：<code>sudo lvcreate -l 13 -n lv-2 vg0</code></p></li><li><p>檢查 LV：<code>sudo lvscan</code></p></li><li><p>檢視 LV 詳細資料：<code>sudo lvdisplay /dev/vg0/lv-1</code> &amp; <code>sudo lvdisplay /dev/vg0/lv-2</code></p></li></ol><blockquote><p>要使用 LV 前要記得進行 format 的動作</p></blockquote><hr><h1 id="Practice-Adding-a-Logical-Volume"><a href="#Practice-Adding-a-Logical-Volume" class="headerlink" title="Practice: Adding a Logical Volume"></a>Practice: Adding a Logical Volume</h1><h2 id="目標"><a href="#目標" class="headerlink" title="目標"></a>目標</h2><ol><li><p>建立一個 volume group，名稱為 <code>shazam</code>，由兩個大小為 <code>256 MB</code> 的 physical partition 組成(來源為 <code>/dev/vdb</code>)</p></li><li><p>從 volume group 中建立一個 <code>400 MB</code> 的 logical volume，名稱為 <code>storage</code></p></li><li><p>將 logical volume 掛載在 <code>/storage</code> 目錄</p></li></ol><h2 id="實作"><a href="#實作" class="headerlink" title="實作"></a>實作</h2><h3 id="1、建立一個-volume-group，名稱為-shazam，由兩個大小為-256-MB-的-physical-partition-組成-來源為-dev-vdb"><a href="#1、建立一個-volume-group，名稱為-shazam，由兩個大小為-256-MB-的-physical-partition-組成-來源為-dev-vdb" class="headerlink" title="1、建立一個 volume group，名稱為 shazam，由兩個大小為 256 MB 的 physical partition 組成(來源為 /dev/vdb)"></a>1、建立一個 volume group，名稱為 <code>shazam</code>，由兩個大小為 <code>256 MB</code> 的 physical partition 組成(來源為 <code>/dev/vdb</code>)</h3><p>1.1 分割硬碟：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk /dev/vdb</span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line"></span><br><span class="line">Changes will remain <span class="keyword">in</span> memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write <span class="built_in">command</span>.</span><br><span class="line"></span><br><span class="line">Device does not contain a recognized partition table</span><br><span class="line">Building a new DOS disklabel with disk identifier 0xa2e37afd.</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): n</span><br><span class="line">Partition <span class="built_in">type</span>:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p):</span><br><span class="line">Partition number (1-4, default 1):</span><br><span class="line">First sector (2048-20971519, default 2048):</span><br><span class="line">Using default value 2048</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (2048-20971519, default 20971519): +256M</span><br><span class="line">Partition 1 of <span class="built_in">type</span> Linux and of size 256 MiB is <span class="built_in">set</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): n</span><br><span class="line">Partition <span class="built_in">type</span>:</span><br><span class="line">   p   primary (1 primary, 0 extended, 3 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p):</span><br><span class="line">Using default response p</span><br><span class="line">Partition number (2-4, default 2):</span><br><span class="line">First sector (526336-20971519, default 526336):</span><br><span class="line">Using default value 526336</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (526336-20971519, default 20971519): +256M</span><br><span class="line">Partition 2 of <span class="built_in">type</span> Linux and of size 256 MiB is <span class="built_in">set</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): t</span><br><span class="line">Partition number (1,2, default 2): 1</span><br><span class="line">Hex code (<span class="built_in">type</span> L to list all codes): 8e</span><br><span class="line">Changed <span class="built_in">type</span> of partition <span class="string">&#x27;Linux&#x27;</span> to <span class="string">&#x27;Linux LVM&#x27;</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): t</span><br><span class="line">Partition number (1,2, default 2): 2</span><br><span class="line">Hex code (<span class="built_in">type</span> L to list all codes): 8e</span><br><span class="line">Changed <span class="built_in">type</span> of partition <span class="string">&#x27;Linux&#x27;</span> to <span class="string">&#x27;Linux LVM&#x27;</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">Syncing disks.</span><br><span class="line"></span><br><span class="line">$ sudo partprobe -s</span><br><span class="line">/dev/vda: msdos partitions 1</span><br><span class="line">/dev/vdb: msdos partitions 1 2</span><br></pre></td></tr></table></figure><p>1.2 建立 physical volume</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pvcreate /dev/vdb1 /dev/vdb2</span><br><span class="line">  Physical volume <span class="string">&quot;/dev/vdb1&quot;</span> successfully created</span><br><span class="line">  Physical volume <span class="string">&quot;/dev/vdb2&quot;</span> successfully created</span><br></pre></td></tr></table></figure><p>1.3 建立 volume group</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vgcreate shazam /dev/vdb1 /dev/vdb2</span><br><span class="line">  Volume group <span class="string">&quot;shazam&quot;</span> successfully created</span><br></pre></td></tr></table></figure><h3 id="2、從-volume-group-中建立一個-400-MB-的-logical-volume，名稱為-storage"><a href="#2、從-volume-group-中建立一個-400-MB-的-logical-volume，名稱為-storage" class="headerlink" title="2、從 volume group 中建立一個 400 MB 的 logical volume，名稱為 storage"></a>2、從 volume group 中建立一個 <code>400 MB</code> 的 logical volume，名稱為 <code>storage</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ sudo lvcreate -n storage -L 400M shazam</span><br><span class="line">  Logical volume <span class="string">&quot;storage&quot;</span> created</span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化 LV</span></span><br><span class="line">$ sudo mkfs.ext4 /dev/shazam/storage</span><br><span class="line">  mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">  Filesystem label=</span><br><span class="line">  OS <span class="built_in">type</span>: Linux</span><br><span class="line">  Block size=1024 (<span class="built_in">log</span>=0)</span><br><span class="line">  Fragment size=1024 (<span class="built_in">log</span>=0)</span><br><span class="line">  Stride=0 blocks, Stripe width=0 blocks</span><br><span class="line">  102400 inodes, 409600 blocks</span><br><span class="line">  20480 blocks (5.00%) reserved <span class="keyword">for</span> the super user</span><br><span class="line">  First data block=1</span><br><span class="line">  Maximum filesystem blocks=34078720</span><br><span class="line">  50 block groups</span><br><span class="line">  8192 blocks per group, 8192 fragments per group</span><br><span class="line">  2048 inodes per group</span><br><span class="line">  Superblock backups stored on blocks:</span><br><span class="line">          8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409</span><br><span class="line"></span><br><span class="line">  Allocating group tables: <span class="keyword">done</span></span><br><span class="line">  Writing inode tables: <span class="keyword">done</span></span><br><span class="line">  Creating journal (8192 blocks): <span class="keyword">done</span></span><br><span class="line">  Writing superblocks and filesystem accounting information: <span class="keyword">done</span></span><br></pre></td></tr></table></figure><h3 id="3、將-logical-volume-掛載在-storage-目錄"><a href="#3、將-logical-volume-掛載在-storage-目錄" class="headerlink" title="3、將 logical volume 掛載在 /storage 目錄"></a>3、將 logical volume 掛載在 <code>/storage</code> 目錄</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo blkid</span><br><span class="line">/dev/vda1: UUID=<span class="string">&quot;9bf6b9f7-92ad-441b-848e-0257cbb883d1&quot;</span> TYPE=<span class="string">&quot;xfs&quot;</span></span><br><span class="line">/dev/vdb1: UUID=<span class="string">&quot;MxwktT-fxCb-KjzV-muCq-Dk7z-IHZU-WdclEG&quot;</span> TYPE=<span class="string">&quot;LVM2_member&quot;</span></span><br><span class="line">/dev/vdb2: UUID=<span class="string">&quot;mQaiiR-3LEA-XhT4-cd36-gT4T-7jjR-HrQreq&quot;</span> TYPE=<span class="string">&quot;LVM2_member&quot;</span></span><br><span class="line">/dev/mapper/shazam-storage: UUID=<span class="string">&quot;4b5e590a-10af-4e44-8619-aba91e436d17&quot;</span> TYPE=<span class="string">&quot;ext4&quot;</span></span><br><span class="line"></span><br><span class="line">$ sudo mkdir /storage</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;UUID=4b5e590a-10af-4e44-8619-aba91e436d17 /storage ext4 defaults 0 2&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line">$ sudo mount -a</span><br><span class="line">$ sudo mount | grep storage</span><br><span class="line">/dev/mapper/shazam-storage on /storage <span class="built_in">type</span> ext4 (rw,relatime,seclabel,data=ordered)</span><br></pre></td></tr></table></figure><hr><h1 id="10-3-Extending-Logical-Volumes"><a href="#10-3-Extending-Logical-Volumes" class="headerlink" title="10.3 Extending Logical Volumes"></a>10.3 Extending Logical Volumes</h1><h2 id="10-3-1-Extending-and-reducing-a-volume-group"><a href="#10-3-1-Extending-and-reducing-a-volume-group" class="headerlink" title="10.3.1 Extending and reducing a volume group"></a>10.3.1 Extending and reducing a volume group</h2><ol><li><p>建立新的 PV：<code>sudo pvcreate /dev/sdc</code></p></li><li><p>將新的 PV(/dev/sdc) 加到 VG 中：<code>sudo vgextend vg0 /dev/sdc</code></p></li></ol><blockquote><p>若要把 PV 移出 VG 可以使用類似的指令：<code>sudo vgreduce vg0 /dev/sda1</code></p></blockquote><h2 id="10-3-2-Extend-a-logical-volume-and-XFS-file-system"><a href="#10-3-2-Extend-a-logical-volume-and-XFS-file-system" class="headerlink" title="10.3.2 Extend a logical volume and XFS file system"></a>10.3.2 Extend a logical volume and XFS file system</h2><p>目標：<strong>將 LV 容量加大到 300M</strong></p><ol><li><p>LV 放大可以 online，縮小需要 offline：<code>sudo lvextend -L +100M /dev/vg0/lv-1</code> or <code>sudo lvextend -L 300M /dev/vg0/lv-1</code></p></li><li><p>也可以用指定 PE 的個數來放大：<code>sudo lvextend -l +7 /dev/vg0/lv-2</code> or <code>sudo lvextend -l 19 /dev/vg0/lv-2</code></p></li><li><p>檢查 LV 狀態：<code>sudo lvscan</code></p></li><li><p>放大 XFS 檔案系統：<code>sudo xfs_growfs /dev/vg0/lv-1</code></p></li><li><p>放大 EXT4 檔案系統：<code>sudo resize2fs /dev/vg0/lv-2</code></p></li></ol><h3 id="如何縮小-LV"><a href="#如何縮小-LV" class="headerlink" title="如何縮小 LV"></a>如何縮小 LV</h3><p><strong><font color='red'>【註】</font></strong> XFS 檔案系統僅能放大，不能縮小(上面的 lv-1 已經無法縮小)</p><p>目標：縮小 EXT4 到 200M</p><ol><li><p>umount EXT4 partition：<code>sudo umount /lv-2</code></p></li><li><p>檢查 LV：<code>sudo fsck -f /dev/vg0/lv-2</code></p></li><li><p>縮小 EXT4 檔案系統：<code>sudo resize2fs /dev/vg0/lv-2 200M</code></p></li><li><p>縮小 LV：<code>sudo lvreduce -L 200M /dev/vg0/lv-2</code></p></li><li><p>重新掛載磁區：<code>sudo mount -a</code></p></li></ol><h3 id="如何縮小-VG-amp-移除-PV-硬碟"><a href="#如何縮小-VG-amp-移除-PV-硬碟" class="headerlink" title="如何縮小 VG &amp; 移除 PV(硬碟)"></a>如何縮小 VG &amp; 移除 PV(硬碟)</h3><p>目標：移除第一個 PV(<code>/dev/sda1</code>)</p><ol><li><p>移動指定 PV 中的檔案到其他 PV 上：<code>pvscan</code> -&gt; <code>sudo pvmove /dev/sda1 /dev/sdc</code>(也可以不指定目的裝置)</p></li><li><p>移除 VG 中的 PV：<code>sudo vgreduce vg0 /dev/sda1</code> -&gt; <code>pvscan</code></p></li><li><p>徹底移除 PV：<code>sudo pvremove /dev/sda1</code></p></li></ol><p>最後就可以把電腦關機並移除硬碟 /dev/sda 囉!</p><h3 id="移除-LV"><a href="#移除-LV" class="headerlink" title="移除 LV"></a>移除 LV</h3><p>目標：<strong>移除 /dev/vg0/lv-1</strong></p><ol><li>卸載檔案系統：<code>sudo umount /lv-1</code></li><li>移除 LV：<code>sudo lvremove /dev/vg0/lv-1</code> -&gt; <code>sudo lvscan</code></li></ol><hr><h1 id="Practice-Extending-a-Logical-Volume"><a href="#Practice-Extending-a-Logical-Volume" class="headerlink" title="Practice: Extending a Logical Volume"></a>Practice: Extending a Logical Volume</h1><h2 id="目標-1"><a href="#目標-1" class="headerlink" title="目標"></a>目標</h2><ol><li><p>以上一個練習為基礎，增加一個容量為 <code>800MB</code> 的 physical volume</p></li><li><p>把原有的 logical volume(<code>storage</code>)大小增加為 1GB</p></li></ol><h3 id="1、以上一個練習為基礎，增加一個容量為-800MB-的-physical-volume"><a href="#1、以上一個練習為基礎，增加一個容量為-800MB-的-physical-volume" class="headerlink" title="1、以上一個練習為基礎，增加一個容量為 800MB 的 physical volume"></a>1、以上一個練習為基礎，增加一個容量為 <code>800MB</code> 的 physical volume</h3><p>1.1 分割硬碟</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk /dev/vdb</span><br><span class="line">[sudo] password <span class="keyword">for</span> student:</span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line"></span><br><span class="line">Changes will remain <span class="keyword">in</span> memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write <span class="built_in">command</span>.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): n</span><br><span class="line">Partition <span class="built_in">type</span>:</span><br><span class="line">   p   primary (2 primary, 0 extended, 2 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p):</span><br><span class="line">Using default response p</span><br><span class="line">Partition number (3,4, default 3):</span><br><span class="line">First sector (1050624-20971519, default 1050624):</span><br><span class="line">Using default value 1050624</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (1050624-20971519, default 20971519): +800M</span><br><span class="line">Partition 3 of <span class="built_in">type</span> Linux and of size 800 MiB is <span class="built_in">set</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): t</span><br><span class="line">Partition number (1-3, default 3):</span><br><span class="line">Hex code (<span class="built_in">type</span> L to list all codes): 8e</span><br><span class="line">Changed <span class="built_in">type</span> of partition <span class="string">&#x27;Linux&#x27;</span> to <span class="string">&#x27;Linux LVM&#x27;</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line"></span><br><span class="line">WARNING: Re-reading the partition table failed with error 16: Device or resource busy.</span><br><span class="line">The kernel still uses the old table. The new table will be used at</span><br><span class="line">the next reboot or after you run partprobe(8) or kpartx(8)</span><br><span class="line">Syncing disks.</span><br><span class="line"></span><br><span class="line">$ sudo partprobe -s</span><br><span class="line">/dev/vda: msdos partitions 1</span><br><span class="line">/dev/vdb: msdos partitions 1 2 3</span><br></pre></td></tr></table></figure><p>1.2 加入 physical volume</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pvcreate /dev/vdb3</span><br><span class="line">  Physical volume <span class="string">&quot;/dev/vdb3&quot;</span> successfully created</span><br></pre></td></tr></table></figure><p>1.3 擴充 volume group 空間</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vgextend shazam /dev/vdb3</span><br><span class="line">  Volume group <span class="string">&quot;shazam&quot;</span> successfully extended</span><br><span class="line">$ sudo vgdisplay shazam</span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               shazam</span><br><span class="line">  System ID</span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        3</span><br><span class="line">  Metadata Sequence No  3</span><br><span class="line">  VG Access             <span class="built_in">read</span>/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                1</span><br><span class="line">  Open LV               1</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                3</span><br><span class="line">  Act PV                3</span><br><span class="line">  VG Size               1.27 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              325</span><br><span class="line">  Alloc PE / Size       100 / 400.00 MiB</span><br><span class="line">  Free  PE / Size       225 / 900.00 MiB</span><br><span class="line">  VG UUID               s4g8vT-JCW9-p84f-wVU1-0dsn-xkrh-vytTVc</span><br></pre></td></tr></table></figure><h3 id="2、把原有的-logical-volume-storage-大小增加為-1GB"><a href="#2、把原有的-logical-volume-storage-大小增加為-1GB" class="headerlink" title="2、把原有的 logical volume(storage)大小增加為 1GB"></a>2、把原有的 logical volume(<code>storage</code>)大小增加為 1GB</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ sudo lvextend -L 1G /dev/shazam/storage</span><br><span class="line">  Extending logical volume storage to 1.00 GiB</span><br><span class="line">  Logical volume storage successfully resized</span><br><span class="line"></span><br><span class="line">$ sudo resize2fs /dev/shazam/storage</span><br><span class="line">resize2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Filesystem at /dev/shazam/storage is mounted on /storage; on-line resizing required</span><br><span class="line">old_desc_blocks = 4, new_desc_blocks = 8</span><br><span class="line">The filesystem on /dev/shazam/storage is now 1048576 blocks long.</span><br><span class="line"></span><br><span class="line">$ df -hT | grep storage</span><br><span class="line">/dev/mapper/shazam-storage ext4      984M  2.8M  932M   1% /storage</span><br></pre></td></tr></table></figure><hr><h1 id="10-4-Snapshot-補充"><a href="#10-4-Snapshot-補充" class="headerlink" title="10.4 Snapshot (補充)"></a>10.4 Snapshot (補充)</h1><p>假設：有個 20T 的 MySQL 資料庫，需要備份</p><ol><li><p>停掉 MySQL 服務</p></li><li><p>建立 Snapshop：<code>sudo lvcreate -L 100M -s -n backup /dev/vg0/lv-2</code> -&gt; ‘sudo lvscan’</p></li><li><p>啟動 MySQL 服務</p></li><li><p>檢查 LV 之間的關聯：<code>sudo lvs</code></p></li></ol><blockquote><p>只有在 PE 中的資料異動前，才會有資料複製的動作發生</p></blockquote><h2 id="老師設計的實驗"><a href="#老師設計的實驗" class="headerlink" title="老師設計的實驗"></a>老師設計的實驗</h2><ol><li><p>reset server</p></li><li><p>幫 server 加上 200M/300M/500M IDE 硬碟共三顆</p></li><li><p>開機，按 F12 選擇 virtio(4) 開機</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 08. Connecting to Network-defined Users and Groups 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH08_ConnectingToNetwork-DefinedUsersAndGroups/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH08_ConnectingToNetwork-DefinedUsersAndGroups/</url>
      
        <content type="html"><![CDATA[<h1 id="老師補充"><a href="#老師補充" class="headerlink" title="老師補充"></a>老師補充</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得使用者資訊(跟 /etc/passwd 沒有絕對關係)</span></span><br><span class="line">[vagrant@server tmp]$ getent passwd user1</span><br><span class="line">user1:x:1001:1001::/home/user1:/bin/bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取得群組資訊</span></span><br><span class="line">[vagrant@server tmp]$ getent group user1</span><br><span class="line">user1:x:1001:</span><br></pre></td></tr></table></figure><p>驗證是否通過：</p><ol><li><p>帳號 &amp; 密碼正確</p></li><li><p>可以正確取得使用者資訊</p></li></ol><p>NIS &amp; LDAP 單獨可做到：</p><ol><li><p>可提供使用者資訊</p></li><li><p>可提供帳號密碼的驗證方法</p></li></ol><blockquote><p>但 Kerberos 僅能提供帳號密碼驗證，無法提供使用者資訊，因此一般會與 LDAP 搭配來解決提供使用者資訊的問題，Windows AD 目前就是依照這種方式達成，安全性提高，且提供了 SSO 的能力)</p></blockquote><blockquote><p>目前使用 LDAP + Kerberos 架構的產品有 Windows AD, SAMBA 4.x, RedHat IDM(IPA)</p></blockquote><hr><h1 id="8-1-Using-Identity-Management-Services"><a href="#8-1-Using-Identity-Management-Services" class="headerlink" title="8.1 Using Identity Management Services"></a>8.1 Using Identity Management Services</h1><h2 id="8-1-1-User-information-and-authentication-services"><a href="#8-1-1-User-information-and-authentication-services" class="headerlink" title="8.1.1 User information and authentication services"></a>8.1.1 User information and authentication services</h2><p>中央認證系統會包含兩個部分：</p><h3 id="1-Account-information"><a href="#1-Account-information" class="headerlink" title="1. Account information"></a>1. Account information</h3><p>此部分用來存放使用者帳號的資訊，以及使用者權限等相關資訊。(例如：**/etc/passwd**)</p><h3 id="2-Authentication-information"><a href="#2-Authentication-information" class="headerlink" title="2. Authentication information"></a>2. Authentication information</h3><p>此部分的目的則是驗證使用者是否為其所宣稱的使用者，因此會包含密碼資訊，但這些密碼資訊通常會透過密碼學的技術輔助來加密，避免以明碼的方式呈現。(例如：**/etc/shadow**)</p><h3 id="其他補充"><a href="#其他補充" class="headerlink" title="其他補充"></a>其他補充</h3><p>Kerberos 僅能提供安全的驗證功能，無法提供使用者資訊；若需要使用者資訊，需要搭配 LDAP/NIS …. 等服務來提供。</p><p>目前 LDAP + Kerberos 的選擇有 Windows AD / SAMBA 4.x / RedHat IDM … 等。</p><p>透過調整 <code>/etc/nsswitch.conf</code>，可以調整驗證的順序</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@localhost ~]$ cat /etc/nsswitch.conf | grep <span class="string">&#x27;^[^#]&#x27;</span></span><br><span class="line">passwd:     files sss</span><br><span class="line">shadow:     files sss</span><br><span class="line">group:      files sss</span><br><span class="line">hosts:      files dns</span><br><span class="line">bootparams: nisplus [NOTFOUND=<span class="built_in">return</span>] files</span><br><span class="line">ethers:     files</span><br><span class="line">netmasks:   files</span><br><span class="line">networks:   files</span><br><span class="line">protocols:  files</span><br><span class="line">rpc:        files</span><br><span class="line">services:   files sss</span><br><span class="line">netgroup:   files sss</span><br><span class="line">publickey:  nisplus</span><br><span class="line">automount:  files</span><br><span class="line">aliases:    files nisplus</span><br></pre></td></tr></table></figure><blockquote><p>NSS 設定中的 sss 表示指向系統中的 sssd(security system daemon, for cache)，即使在沒網路的狀態下也還是可以進行認証</p></blockquote><p>RedHat 設計的工具大多以 <code>system-config</code> 開頭：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@localhost ~]$ system-config-</span><br><span class="line">system-config-authentication  system-config-firewall-tui    system-config-kickstart       system-config-printer-applet  </span><br><span class="line">system-config-date            system-config-kdump           system-config-language        system-config-users           </span><br><span class="line">system-config-firewall        system-config-keyboard        system-config-printer</span><br></pre></td></tr></table></figure><p>安裝 <code>system-config-authentication</code> 相關的工具：<code>sudo yum -y install authconfig-gtk sssd krb5-workstation</code></p><p>若 LDAP 過程中要加密，LDAP server 設定時就不能使用 IP，必須使用 domain name，否則 CA 憑證檢查不會過</p><blockquote><p>設定 Kerneros 要把 <code>Use DNS to resolve hosts to realm</code> &amp; <code>Use DNS to locate KDCs for realm</code> 兩個選項拿掉</p></blockquote><h2 id="8-1-2-Attaching-a-system-to-centralized-LDAP-and-Kerberos-servers"><a href="#8-1-2-Attaching-a-system-to-centralized-LDAP-and-Kerberos-servers" class="headerlink" title="8.1.2 Attaching a system to centralized LDAP and Kerberos servers"></a>8.1.2 Attaching a system to centralized LDAP and Kerberos servers</h2><p>設定時建議安裝套件：<code>authconfig-gtk</code>, <code>sssd</code>, <code>krb5-workstation</code></p><h3 id="Authconfig"><a href="#Authconfig" class="headerlink" title="Authconfig"></a>Authconfig</h3><p>RHCE 中的示範以 LDAP + Kerberos 為組合，有一些設定資訊是必須了解的：</p><ul><li><p><strong>/etc/ldap.conf</strong>：提供 LDAP 服務的 server 的相關設定</p></li><li><p><strong>/etc/krb5.conf</strong>：Kerberos 的相關設定</p></li><li><p><strong>/etc/sssd/sssd.conf</strong>：system security services daemon(sssd) 的設定，負責用來取得 &amp; 快取使用者的認證資訊</p></li><li><p><strong>/etc/nsswitch.conf</strong>：用來設定認證使用者所要使用的服務 or 系統</p></li><li><p><strong>/etc/pam.d</strong>：此目錄包含了提供了不同驗證機制的模組</p></li><li><p>/etc/openldap/cacerts：儲存 certificate 用</p></li></ul><blockquote><p>認證設定的過程建議使用 <code>authconfig-gtk</code>(提供 <strong>system-config-authentication</strong> 指令) or <code>authconfig-tui</code> 套件來協助，有圖形介面較為容易設定；不建議使用 <code>authconfig</code> 套件</p></blockquote><h3 id="LDAP"><a href="#LDAP" class="headerlink" title="LDAP"></a>LDAP</h3><p>LDAP 帳號描述範例：<code>cn=kevin,ou=sales,ou=tp,ou=tw,dc=example,dc=com</code> (DN, Distinguish Name)</p><p>LDAP 認証可用 SSL/TLS 加密</p><p><strong>要使用 LDAP 服務，要提供以下必要資訊</strong>：</p><ol><li><p>LDAP server hostname</p></li><li><p>base DN(即是上面的 <code>ou=tp,ou=tw,dc=example,dc=com</code>，依據要認證到的層級而定)</p></li><li><p>若要加密則必須 CA</p></li></ol><h3 id="Kerberos"><a href="#Kerberos" class="headerlink" title="Kerberos"></a>Kerberos</h3><p>兩大訴求：</p><ol><li><p>SSO (Single Sign On)</p></li><li><p>安全性高(驗證過程中，帳號密碼不會在網路上傳遞)</p></li></ol><p>密碼儲存 &amp; 認証方式：</p><ol><li><p>Server 儲存 hash 過後的密碼</p></li><li><p>client 認証時，會將輸入的密碼 hash 後的結果作為加密帳號資訊的對稱式金鑰，將資訊(user_account,time,….)加密後傳到 server 驗證</p></li><li><p>server 收到加密結果後，會拿資料庫中的 hash value 並進行解密，若是有符合則通過</p></li><li><p>為防止重送攻擊，被加密的資料有包含時間資訊(因此系統時間很重要)</p></li><li><p>假設驗證通過，server 會發送一個 ticket 給 client，之後 client 要存取其他 resource 只要提供 account &amp; ticket 即可，resource owner 會自動去找 KDC(Kerberos Key Distribution Center) 驗證 ticket 是否有效</p></li></ol><p><strong>要使用 Kerberos 服務，要提供以下必要資訊</strong>：</p><ol><li><p>Kerberos realm，類似 domain name 的概念</p></li><li><p>至少一個 key distribution center(KDC)，即為 Kerberos hostname</p></li><li><p>Admin server hostname(用來協助使用者密碼及其他資訊變更之用)，通常與 KDC 會是相同一台機器</p></li></ol><h2 id="8-1-3-Attaching-a-System-to-an-IPA-Server"><a href="#8-1-3-Attaching-a-System-to-an-IPA-Server" class="headerlink" title="8.1.3 Attaching a System to an IPA Server"></a>8.1.3 Attaching a System to an IPA Server</h2><p>在 RHEL 7 中提供了方便工具設定，安裝 <code>ipa-client</code> 套件即可。</p><p>以 non-interactive 的方式設定 IPA server 認証：<code>sudo ipa-client-install --domain=serverX.example.com --no-ntp --mkhomedir -p admin -w redhat123 -U</code></p><hr><h1 id="Practice-Connecting-to-a-Central-LDAP-and-Kerberos-Server"><a href="#Practice-Connecting-to-a-Central-LDAP-and-Kerberos-Server" class="headerlink" title="Practice: Connecting to a Central LDAP and Kerberos Server"></a>Practice: Connecting to a Central LDAP and Kerberos Server</h1><p>設定 LDAP + Kerberos 驗證的步驟：</p><p>1、安裝相關套件：<code>sudo yum -y install sssd authconfig-gtk krb5-workstation</code></p><p>2、使用圖形工具進行設定(設定如下)</p><p><img src="https://lh3.googleusercontent.com/yrIOq6ZotETj8rEMxH3hnoBqJzUmaF4A_aYQQlvMTuqU6u2-xBCA7aUMIkVBUi_uwjiSOcEr5V_TXrlv0jYDX_ikqhPjZVWPiXo5XKIoWUovuYNtbyPu4nibuVU1sZ150Y43kME_1dExiLtYF3cowhoHA_ex4CbHBDGULMGkfPJQnJkoWM_PVlb-xk1ENpKjElCw0If4VUMnyk6QsSjWTeKrUt6gl8A9Szi-DEWgh2FuhnXFFfL2BOlUmcvLeUDDDu7cT1v09QRqQI1usK3bDKIjrmNUaSAYQ8hPoxDqZaT0-KMycPnDwc8SWJXmm4En8uQMHSGMqFKlx1LmJqufLKZMaFZgF3rM1BN8pCYFxkUkws2nNuG-tmzHi0yxVq7T0vYFerWX66XTvhrOHiReMjLKp3IEm7ue4-AUNY9bcwlPgyjoOBJXId1vT4AgyHeR7sivsMFvwfw2jTwZdEEp2723scmSxiYoO0baKp7rrSnls11b9SI_ZIPd7gAD_42AqnstDCEWWsKzXCpGLXuAXFFjk14lO5g0MHtCd73J2aJmsz8QN_rMZK_Hd9YW44VJ6AEQF3rB4Sxq09bT6q8HPF9ht-ak_qg=w474-h710-no" alt="system-config-authentication"></p><p>3、驗證設定結果：<code>sudo getent passwd ldapuser1</code> or <code>ssh ldapuser1@server1</code>(密碼 <code>kerberos</code>)</p><hr><h1 id="Lab-Connecting-to-Network-defined-Users-and-Groups"><a href="#Lab-Connecting-to-Network-defined-Users-and-Groups" class="headerlink" title="Lab: Connecting to Network-defined Users and Groups"></a>Lab: Connecting to Network-defined Users and Groups</h1><p>Lab 設計為連線到 IPA server 進行驗證，步驟如下：</p><p>1、安裝相關套件：<code>sudo yum -y install ipa-client</code></p><p>2、設定連線：<code>sudo ipa-client-install --domain=server1.example.com --no-ntp --mkhomedir</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ipa-client-install --domain=server0.example.com --no-ntp --mkhomedir</span><br><span class="line"></span><br><span class="line">Discovery was successful!</span><br><span class="line">Hostname: desktop1.example.com</span><br><span class="line">Realm: SERVER0.EXAMPLE.COM</span><br><span class="line">DNS Domain: server1.example.com</span><br><span class="line">IPA Server: server1.example.com</span><br><span class="line">BaseDN: dc=server1,dc=example,dc=com</span><br><span class="line"></span><br><span class="line">Continue to configure the system with these values? [no]: yes</span><br><span class="line">User authorized to enroll computers: admin</span><br><span class="line">Synchronizing time with KDC...</span><br><span class="line">Unable to sync time with IPA NTP server, assuming the time is <span class="keyword">in</span> sync. Please check that 123 UDP port is opened.</span><br><span class="line">Password <span class="keyword">for</span> admin@SERVER1.EXAMPLE.COM:</span><br><span class="line">Successfully retrieved CA cert</span><br><span class="line">    Subject:     CN=Certificate Authority,O=SERVER1.EXAMPLE.COM</span><br><span class="line">    Issuer:      CN=Certificate Authority,O=SERVER1.EXAMPLE.COM</span><br><span class="line">    Valid From:  Tue May 17 20:31:07 2016 UTC</span><br><span class="line">    Valid Until: Sat May 17 20:31:07 2036 UTC</span><br><span class="line"></span><br><span class="line">Enrolled <span class="keyword">in</span> IPA realm SERVER1.EXAMPLE.COM</span><br><span class="line">Created /etc/ipa/default.conf</span><br><span class="line">New SSSD config will be created</span><br><span class="line">Configured /etc/sssd/sssd.conf</span><br><span class="line">Configured /etc/krb5.conf <span class="keyword">for</span> IPA realm SERVER1.EXAMPLE.COM</span><br><span class="line">trying https://server1.example.com/ipa/xml</span><br><span class="line">Forwarding <span class="string">&#x27;ping&#x27;</span> to server <span class="string">&#x27;https://server1.example.com/ipa/xml&#x27;</span></span><br><span class="line">Forwarding <span class="string">&#x27;env&#x27;</span> to server <span class="string">&#x27;https://server1.example.com/ipa/xml&#x27;</span></span><br><span class="line">Adding SSH public key from /etc/ssh/ssh_host_rsa_key.pub</span><br><span class="line">Adding SSH public key from /etc/ssh/ssh_host_ecdsa_key.pub</span><br><span class="line">Forwarding <span class="string">&#x27;host_mod&#x27;</span> to server <span class="string">&#x27;https://server1.example.com/ipa/xml&#x27;</span></span><br><span class="line">Could not update DNS SSHFP records.</span><br><span class="line">SSSD enabled</span><br><span class="line">Configured /etc/openldap/ldap.conf</span><br><span class="line">Configured /etc/ssh/ssh_config</span><br><span class="line">Configured /etc/ssh/sshd_config</span><br><span class="line">Client configuration complete.</span><br></pre></td></tr></table></figure><p>3、驗證設定結果：<code>sudo getent passwd ipauser</code> &amp; <code>ssh ipauser@localhost</code>(原本密碼為 <code>password</code>，改為 <code>redhat123</code>)</p><p>4、程式驗證：<code>lab ipaclient grade</code> (@desktop)</p>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 09. Adding Disks, Partitions, and File Systems to a Linux System 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH09_AddingDisksPartitionAndFileSystemsToALinuxSystem/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH09_AddingDisksPartitionAndFileSystemsToALinuxSystem/</url>
      
        <content type="html"><![CDATA[<h1 id="9-1-Adding-Partitions-File-Systems-and-Persistent-Mounts"><a href="#9-1-Adding-Partitions-File-Systems-and-Persistent-Mounts" class="headerlink" title="9.1 Adding Partitions, File Systems, and Persistent Mounts"></a>9.1 Adding Partitions, File Systems, and Persistent Mounts</h1><h2 id="9-1-1-Disk-partitioning"><a href="#9-1-1-Disk-partitioning" class="headerlink" title="9.1.1 Disk partitioning"></a>9.1.1 Disk partitioning</h2><p>Hard Disk 的分割結構：(從頭到尾)</p><ol><li>MBR(512 bytes) = Boot Loader(446 byte) + Partition Table(64 bytes)</li></ol><p>Partition Type：</p><ul><li><p><code>Primary</code>：一個硬碟最多可以切 4 個 primary partition (系統要啟動必須裝載 Primary Partition)</p></li><li><p><code>Extended</code>：若要超過 4 個 partition，則要將 primary partition 來換成 extended partition</p></li><li><p><code>Logical</code>：在 extended partition 中的 partition (Logical Partition Table 存在 Extended Partition 的前面)，且**<font color='red'>代碼從 5 開始(/dev/sda5)</font>**</p></li></ul><p>備份 /dev/sda1：<code>dd if=/dev/sda1 of=/tmp/sda1.dd</code></p><p>還原 /dev/sda1：<code>dd if=/tmp/sda1.dd of=/dev/sda1</code></p><h3 id="MBR-Partition"><a href="#MBR-Partition" class="headerlink" title="MBR Partition"></a>MBR Partition</h3><ul><li><p>支援 2^32 個定址空間</p></li><li><p>傳統硬碟最大容量為 2T：2^32 x 512 bytes / 1024 / 1024 /1024 / 1024 = 2T</p></li><li><p>超過 2T 的硬碟 -&gt; Advanced Format(block size 從 512 bytes 變成 4K) = 最大可支援到 16T (過渡時期的解決方案)</p></li><li><p>備份 MBR：<code>dd if=/dev/sda of=/tmp/mbr.dd bs=512 count=1</code></p></li></ul><h3 id="GPT-Partition"><a href="#GPT-Partition" class="headerlink" title="GPT Partition"></a>GPT Partition</h3><ul><li><p>若 OS 裝在 GPT Partition 上，沒辦法在舊電腦上用(因為傳統的 int 13h 問題)；但開機後，OS level 可以認得超過 2T 的硬碟</p></li><li><p>UEFI BIOS 上的 int 13h 改寫過，因此同時支援 MBR &amp; GPT 兩種 partition</p></li><li><p>最多支援 128 個 partition，支援 2^64 個定址空間，因此最大可以支援到 8ZB(512 bytes block size)，若是使用 4K block size 則可支援到 64ZB</p></li><li><p>GPT 為每一個 partition 提供 128 bits GUID 作為辨識之用</p></li><li><p>GPT partition table 在硬碟的頭尾各有一個，並搭配 CRC checksum，因此有備援的機制</p></li></ul><h2 id="9-1-2-Managing-MBR-partitions-with-fdisk-gdisk"><a href="#9-1-2-Managing-MBR-partitions-with-fdisk-gdisk" class="headerlink" title="9.1.2 Managing MBR partitions with fdisk/gdisk"></a>9.1.2 Managing MBR partitions with fdisk/gdisk</h2><ul><li>若沒有使用 <code>w</code> 儲存 partition 分割設定，就不會實際切割硬碟</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢目前的 partition table</span></span><br><span class="line">[vagrant@localhost ~]$ cat /proc/partitions</span><br><span class="line">major minor  <span class="comment">#blocks  name</span></span><br><span class="line"></span><br><span class="line">   8        0   83886080 sda</span><br><span class="line">   8        1     512000 sda1</span><br><span class="line">   8        2   83373056 sda2</span><br><span class="line"> 253        0   52428800 dm-0</span><br><span class="line"> 253        1    1048576 dm-1</span><br><span class="line"> 253        2   29827072 dm-2</span><br></pre></td></tr></table></figure><p>切割完 partition 後，記得執行 <code>sudo partprobe [device name]</code> or <code>sudo partprobe -s</code> 強制 kernel 重新讀取最新的 partition table</p><h2 id="9-1-4-Creating-file-systems"><a href="#9-1-4-Creating-file-systems" class="headerlink" title="9.1.4 Creating file systems"></a>9.1.4 Creating file systems</h2><p>硬碟分割完後需要進行格式化，一般會使用 <code>mkfs</code> 指令：</p><p><code>sudo mkfs -t xfs /dev/vdb1</code>：格式化成 XFS，也可以寫成 <code>sudo mkfs.xfs /dev/vdb1</code></p><h2 id="9-1-4-Mounting-file-systems"><a href="#9-1-4-Mounting-file-systems" class="headerlink" title="9.1.4 Mounting file systems"></a>9.1.4 Mounting file systems</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir /XFS</span><br><span class="line"></span><br><span class="line">$ sudo mkdir /EXT4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 掛載 XFS partition</span></span><br><span class="line">$ sudo mount /dev/vdb1 /XFS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 掛載 EXT4 partition</span></span><br><span class="line">$ sudo mount /dev/vdb2 /EXT4-t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示 partition type(-T) &amp; human readable(-h)</span></span><br><span class="line">$ df -T -h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢目前在 mount point 的 user 有哪些</span></span><br><span class="line">$ sudo fuser -v /XFS</span><br><span class="line">                     USER        PID ACCESS COMMAND</span><br><span class="line">/mnt:                root     kernel mount /XFS</span><br><span class="line">                     student    1663 ..c.. bash</span><br><span class="line">                     root       1936 ..c.. sudo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 踢掉目前在指定 mount point 的 user</span></span><br><span class="line">$ sudo fuser -km /NFS</span><br></pre></td></tr></table></figure><h3 id="etc-fstab"><a href="#etc-fstab" class="headerlink" title="/etc/fstab"></a>/etc/fstab</h3><p>若要讓 partition 開機後就固定掛載在指定目錄，就要使用 <strong>/etc/fstab</strong> 來達成：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ cat /etc/fstab | grep <span class="string">&#x27;^[^#]&#x27;</span></span><br><span class="line"><span class="comment"># 1. partition = /dev/mapper/centos-root (也可以用 partition UUID)</span></span><br><span class="line"><span class="comment"># 2. mount point = /</span></span><br><span class="line"><span class="comment"># 3. partition type = xfs</span></span><br><span class="line"><span class="comment"># 4. mount options = defaults (rw,suid,dev,exec,auto,nouser,async)</span></span><br><span class="line"><span class="comment"># 5. 指定是否需要 dump = 0 (不需 dump)</span></span><br><span class="line"><span class="comment"># 6. 系統開機時進行 fsck 檢查的順序</span></span><br><span class="line">/dev/mapper/centos-root /                       xfs     defaults        0 0</span><br><span class="line">UUID=cb6b4419-b7cc-4315-b5bd-5926d21e944e   /boot    xfs     defaults        0 0</span><br><span class="line">/dev/mapper/centos-home /home                   xfs     defaults        0 0</span><br><span class="line">/dev/mapper/centos-swap swap                    swap    defaults        0 0</span><br><span class="line"></span><br><span class="line">/dev/vdb1   /XFS    xfs   defaults    1   2</span><br><span class="line">/dev/vdb2   /EXT4   ext4  defaults    1   2</span><br></pre></td></tr></table></figure><p>編輯完 <code>/etc/fstab</code> 後，必須執行 <code>sudo mount -a</code> 檢查是否 partition 可以正確掛載</p><blockquote><p>建議 partition 透過 UUID 指定，可使用 <code>blkid [partition_name]</code> 來取得 partition UUID 的資訊(但似乎只對已經 mount 到特定目錄的 partition 有效)</p></blockquote><p>也可以使用 LABEL 的方式設定在 /etc/fstab 中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 為 XFS partition 命名</span></span><br><span class="line">$ sudo xfs_admin -L XFS /dev/vdb1</span><br><span class="line"><span class="comment"># 為 EXT4 partition 命名</span></span><br><span class="line">$ sudo e2label /dev/vdb2 EXT4</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢結果</span></span><br><span class="line">$ sudo blkid</span><br><span class="line"></span><br><span class="line">$ sudo mount -L XFS /XFS</span><br><span class="line">$ sudo mount -L EXT4 /EXT4</span><br><span class="line"></span><br><span class="line"><span class="comment"># /etc/fstab 的設定可以改成如下</span></span><br><span class="line">LABEL=XFS   /XFS    xfs   defaults    1   2</span><br><span class="line">LABEL=EXT4  /EXT4   ext4  defaults    1   2</span><br></pre></td></tr></table></figure><blockquote><p>Label 還是有可能重複，因此原廠建議用上面的 UUID 掛載，是最保險沒問題的</p></blockquote><hr><h1 id="Practice-Adding-Partitions-File-Systems-and-Persistent-Mounts"><a href="#Practice-Adding-Partitions-File-Systems-and-Persistent-Mounts" class="headerlink" title="Practice: Adding Partitions, File Systems, and Persistent Mounts"></a>Practice: Adding Partitions, File Systems, and Persistent Mounts</h1><h2 id="目標：從-dev-vdb-中切割-1GB-的空間，並掛載在-archive-目錄"><a href="#目標：從-dev-vdb-中切割-1GB-的空間，並掛載在-archive-目錄" class="headerlink" title="目標：從 /dev/vdb 中切割 1GB 的空間，並掛載在 /archive 目錄"></a>目標：從 <code>/dev/vdb</code> 中切割 1GB 的空間，並掛載在 <code>/archive</code> 目錄</h2><h3 id="1、切割-1GB-partition"><a href="#1、切割-1GB-partition" class="headerlink" title="1、切割 1GB partition"></a>1、切割 1GB partition</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 從 /dev/vdb 中切割出 1GB 空間</span></span><br><span class="line">$ sudo fdisk /dev/vdb</span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line"></span><br><span class="line">Changes will remain <span class="keyword">in</span> memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write <span class="built_in">command</span>.</span><br><span class="line"></span><br><span class="line">Device does not contain a recognized partition table</span><br><span class="line">Building a new DOS disklabel with disk identifier 0x1825a331.</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): bn</span><br><span class="line">Partition <span class="built_in">type</span>:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): p</span><br><span class="line">Partition number (1-4, default 1):</span><br><span class="line">First sector (2048-20971519, default 2048):</span><br><span class="line">Using default value 2048</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (2048-20971519, default 20971519): +1G</span><br><span class="line">Partition 1 of <span class="built_in">type</span> Linux and of size 1 GiB is <span class="built_in">set</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">Syncing disks.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 強制 kernel 重新讀取 partition Table</span></span><br><span class="line">$ sudo partprobe -s</span><br><span class="line">/dev/vda: msdos partitions 1</span><br><span class="line">/dev/vdb: msdos partitions 1</span><br><span class="line"></span><br><span class="line">$ cat /proc/partitions</span><br><span class="line">major minor  <span class="comment">#blocks  name</span></span><br><span class="line"></span><br><span class="line"> 253        0   10485760 vda</span><br><span class="line"> 253        1   10484142 vda1</span><br><span class="line"> 253       16   10485760 vdb</span><br><span class="line"> 253       17    1048576 vdb1</span><br></pre></td></tr></table></figure><h3 id="2、格式化-partition"><a href="#2、格式化-partition" class="headerlink" title="2、格式化 partition"></a>2、格式化 partition</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 格式化 partition</span></span><br><span class="line">$ sudo mkfs.ext4 /dev/vdb1</span><br><span class="line">$ sudo blkid /dev/vdb1</span><br><span class="line">/dev/vdb1: UUID=<span class="string">&quot;200ad7d4-aeef-4ffc-ac27-25fbcef5d5b2&quot;</span> TYPE=<span class="string">&quot;ext4&quot;</span></span><br></pre></td></tr></table></figure><h3 id="3、建立目錄並掛載"><a href="#3、建立目錄並掛載" class="headerlink" title="3、建立目錄並掛載"></a>3、建立目錄並掛載</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立掛載目錄</span></span><br><span class="line">$ sudo mkdir /archive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 編輯 /etc/fstab 以達成永久掛載的目的</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;UUID=200ad7d4-aeef-4ffc-ac27-25fbcef5d5b2 /archive ext4 defaults 0 2&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line">$ sudo mount -a</span><br><span class="line">$ df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">......</span><br><span class="line">/dev/vdb1       976M  2.6M  907M   1% /archive</span><br><span class="line">$ mount</span><br><span class="line">.....</span><br><span class="line">/dev/vdb1 on /archive <span class="built_in">type</span> ext4 (rw,relatime,seclabel,data=ordered)</span><br></pre></td></tr></table></figure><hr><h1 id="9-2-Managing-Swap-Space"><a href="#9-2-Managing-Swap-Space" class="headerlink" title="9.2 Managing Swap Space"></a>9.2 Managing Swap Space</h1><p>以下以切割 /dev/vdb 為例：</p><ol><li><p>使用 <code>fdisk</code>(82) or <code>gdisk</code>(8200) 進行硬碟分割</p></li><li><p>將 partition 格式化為 swap：<code>sudo mkswap /dev/vdb1</code></p></li><li><p>掛載 swap partition：<code>sudo swapon /dev/vdb1</code></p></li></ol><p>若是要在 <strong>/etc/fstab</strong> 中掛載 fstab，在 dump &amp; fsck 的欄位都要給 0，標準設定方式如下：</p><p><code>UUID=e02aae2f-70a9-4b1b-a1e1-9016b0acc3e4  swap  swap  defaults  0 0</code></p><h3 id="其他-swap-相關指令"><a href="#其他-swap-相關指令" class="headerlink" title="其他 swap 相關指令"></a>其他 swap 相關指令</h3><ul><li><p><code>sudo swapon -a</code>：檢查 <code>/etc/fstab</code> 中的 swap partition 設定並掛載</p></li><li><p><code>sudo swapon -s</code>：檢視目前系統中 swap partition 的資訊以及使用優先權</p></li></ul><hr><h1 id="Practice-Adding-and-Enabling-Swap-Space"><a href="#Practice-Adding-and-Enabling-Swap-Space" class="headerlink" title="Practice: Adding and Enabling Swap Space"></a>Practice: Adding and Enabling Swap Space</h1><h2 id="目標：在第二個硬碟中切割出-500-MB-的空間作為-swap"><a href="#目標：在第二個硬碟中切割出-500-MB-的空間作為-swap" class="headerlink" title="目標：在第二個硬碟中切割出 500 MB 的空間作為 swap"></a>目標：在第二個硬碟中切割出 500 MB 的空間作為 swap</h2><h3 id="1、切割-500MB-partition"><a href="#1、切割-500MB-partition" class="headerlink" title="1、切割 500MB partition"></a>1、切割 500MB partition</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk /dev/vdb</span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line"></span><br><span class="line">Changes will remain <span class="keyword">in</span> memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write <span class="built_in">command</span>.</span><br><span class="line"></span><br><span class="line">Device does not contain a recognized partition table</span><br><span class="line">Building a new DOS disklabel with disk identifier 0x7db6d48a.</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): n</span><br><span class="line">Partition <span class="built_in">type</span>:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): p</span><br><span class="line">Partition number (1-4, default 1):</span><br><span class="line">First sector (2048-20971519, default 2048):</span><br><span class="line">Using default value 2048</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (2048-20971519, default 20971519): +500MB</span><br><span class="line">Partition 1 of <span class="built_in">type</span> Linux and of size 500 MiB is <span class="built_in">set</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): t</span><br><span class="line">Selected partition 1</span><br><span class="line">Hex code (<span class="built_in">type</span> L to list all codes): 82</span><br><span class="line">Changed <span class="built_in">type</span> of partition <span class="string">&#x27;Linux&#x27;</span> to <span class="string">&#x27;Linux swap / Solaris&#x27;</span></span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): p</span><br><span class="line"></span><br><span class="line">Disk /dev/vdb: 10.7 GB, 10737418240 bytes, 20971520 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk label <span class="built_in">type</span>: dos</span><br><span class="line">Disk identifier: 0x7db6d48a</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vdb1            2048     1026047      512000   82  Linux swap / Solaris</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">Syncing disks.</span><br></pre></td></tr></table></figure><h3 id="2、格式化-partition-1"><a href="#2、格式化-partition-1" class="headerlink" title="2、格式化 partition"></a>2、格式化 partition</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkswap /dev/vdb1</span><br><span class="line">Setting up swapspace version 1, size = 511996 KiB</span><br><span class="line">no label, UUID=ffb162b9-c1e6-4ce9-bde7-964b3ce9c43a</span><br></pre></td></tr></table></figure><h3 id="3、編輯-etc-fstab-並掛載"><a href="#3、編輯-etc-fstab-並掛載" class="headerlink" title="3、編輯 /etc/fstab 並掛載"></a>3、編輯 /etc/fstab 並掛載</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkswap /dev/vdb1</span><br><span class="line">Setting up swapspace version 1, size = 511996 KiB</span><br><span class="line">no label, UUID=ffb162b9-c1e6-4ce9-bde7-964b3ce9c43a</span><br><span class="line"></span><br><span class="line">$ sudo blkid /dev/vdb1</span><br><span class="line">/dev/vdb1: UUID=<span class="string">&quot;ffb162b9-c1e6-4ce9-bde7-964b3ce9c43a&quot;</span> TYPE=<span class="string">&quot;swap&quot;</span></span><br><span class="line"></span><br><span class="line">[student@desktop0 ~]$ <span class="built_in">echo</span> <span class="string">&quot;UUID=ffb162b9-c1e6-4ce9-bde7-964b3ce9c43a swap swap defaults 0 0&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line">UUID=ffb162b9-c1e6-4ce9-bde7-964b3ce9c43a swap swap defaults 0 0</span><br><span class="line"></span><br><span class="line">$ sudo mount -a</span><br><span class="line">$ sudo swapon /dev/vdb1</span><br><span class="line">$ sudo swapon -s</span><br><span class="line">Filename                                Type            Size    Used    Priority</span><br><span class="line">/dev/vdb1                               partition       511996  0       -1</span><br></pre></td></tr></table></figure><hr><h1 id="Lab-Adding-Disks-Partitions-and-File-Systems-to-a-Linux-System"><a href="#Lab-Adding-Disks-Partitions-and-File-Systems-to-a-Linux-System" class="headerlink" title="Lab: Adding Disks, Partitions, and File Systems to a Linux System"></a>Lab: Adding Disks, Partitions, and File Systems to a Linux System</h1><h2 id="目標："><a href="#目標：" class="headerlink" title="目標："></a>目標：</h2><ol><li><p>在第二個硬碟中新增一個 2GB 的 XFS partition，並永久掛載於 /backup 目錄</p></li><li><p>在第二個硬碟中新增 512MB swap，擁有預設的 priority</p></li><li><p>在第二個硬碟中新增 512MB swap，priority 為 1</p></li></ol><h3 id="1、建立三個-partition"><a href="#1、建立三個-partition" class="headerlink" title="1、建立三個 partition"></a>1、建立三個 partition</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ sudo gdisk /dev/vdb</span><br><span class="line">Command (? <span class="keyword">for</span> <span class="built_in">help</span>): n</span><br><span class="line">Partition number (1-128, default 1):</span><br><span class="line">First sector (34-20971486, default = 2048) or &#123;+-&#125;size&#123;KMGTP&#125;:</span><br><span class="line">Last sector (2048-20971486, default = 20971486) or &#123;+-&#125;size&#123;KMGTP&#125;: +2G</span><br><span class="line">Current <span class="built_in">type</span> is <span class="string">&#x27;Linux filesystem&#x27;</span></span><br><span class="line">Hex code or GUID (L to show codes, Enter = 8300):</span><br><span class="line">Changed <span class="built_in">type</span> of partition to <span class="string">&#x27;Linux filesystem&#x27;</span></span><br><span class="line"></span><br><span class="line">Command (? <span class="keyword">for</span> <span class="built_in">help</span>): n</span><br><span class="line">Partition number (2-128, default 2):</span><br><span class="line">First sector (34-20971486, default = 4196352) or &#123;+-&#125;size&#123;KMGTP&#125;:</span><br><span class="line">Last sector (4196352-20971486, default = 20971486) or &#123;+-&#125;size&#123;KMGTP&#125;: +512M</span><br><span class="line">Current <span class="built_in">type</span> is <span class="string">&#x27;Linux filesystem&#x27;</span></span><br><span class="line">Hex code or GUID (L to show codes, Enter = 8300): 8200</span><br><span class="line">Changed <span class="built_in">type</span> of partition to <span class="string">&#x27;Linux swap&#x27;</span></span><br><span class="line"></span><br><span class="line">Command (? <span class="keyword">for</span> <span class="built_in">help</span>): n</span><br><span class="line">Partition number (3-128, default 3):</span><br><span class="line">First sector (34-20971486, default = 5244928) or &#123;+-&#125;size&#123;KMGTP&#125;:</span><br><span class="line">Last sector (5244928-20971486, default = 20971486) or &#123;+-&#125;size&#123;KMGTP&#125;: +512M</span><br><span class="line">Current <span class="built_in">type</span> is <span class="string">&#x27;Linux filesystem&#x27;</span></span><br><span class="line">Hex code or GUID (L to show codes, Enter = 8300): 8200</span><br><span class="line">Changed <span class="built_in">type</span> of partition to <span class="string">&#x27;Linux swap&#x27;</span></span><br><span class="line"></span><br><span class="line">Command (? <span class="keyword">for</span> <span class="built_in">help</span>): w</span><br><span class="line"></span><br><span class="line">Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTING</span><br><span class="line">PARTITIONS!!</span><br><span class="line"></span><br><span class="line">Do you want to proceed? (Y/N): y</span><br><span class="line">OK; writing new GUID partition table (GPT) to /dev/vdb.</span><br><span class="line">The operation has completed successfully.</span><br></pre></td></tr></table></figure><h3 id="2、格式化硬碟"><a href="#2、格式化硬碟" class="headerlink" title="2、格式化硬碟"></a>2、格式化硬碟</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">$ sudo partprobe -s</span><br><span class="line">/dev/vda: msdos partitions 1</span><br><span class="line">/dev/vdb: gpt partitions 1 2 3</span><br><span class="line"></span><br><span class="line">$ cat /proc/partitions</span><br><span class="line">major minor  <span class="comment">#blocks  name</span></span><br><span class="line"></span><br><span class="line"> 253        0   10485760 vda</span><br><span class="line"> 253        1   10484142 vda1</span><br><span class="line"> 253       16   10485760 vdb</span><br><span class="line"> 253       17    2097152 vdb1</span><br><span class="line"> 253       18     524288 vdb2</span><br><span class="line"> 253       19     524288 vdb3</span><br><span class="line"></span><br><span class="line"> $ sudo mkfs.xfs /dev/vdb1</span><br><span class="line">meta-data=/dev/vdb1              isize=256    agcount=4, agsize=131072 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=0</span><br><span class="line">data     =                       bsize=4096   blocks=524288, imaxpct=25</span><br><span class="line">         =                       sunit=0      swidth=0 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=0</span><br><span class="line"><span class="built_in">log</span>      =internal <span class="built_in">log</span>           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br><span class="line"></span><br><span class="line">$ sudo mkswap /dev/vdb2</span><br><span class="line">Setting up swapspace version 1, size = 524284 KiB</span><br><span class="line">no label, UUID=a875704a-d0ba-4542-98a6-f201f1b3a539</span><br><span class="line"></span><br><span class="line">$ sudo mkswap /dev/vdb3</span><br><span class="line">Setting up swapspace version 1, size = 524284 KiB</span><br><span class="line">no label, UUID=9a906cce-e50e-44ee-a880-b3811252e5e1</span><br><span class="line"></span><br><span class="line">$ sudo blkid</span><br><span class="line">/dev/vda1: UUID=<span class="string">&quot;9bf6b9f7-92ad-441b-848e-0257cbb883d1&quot;</span> TYPE=<span class="string">&quot;xfs&quot;</span></span><br><span class="line">/dev/vdb1: UUID=<span class="string">&quot;796676d5-0e5c-4023-82e3-3417e8d00952&quot;</span> TYPE=<span class="string">&quot;xfs&quot;</span> PARTLABEL=<span class="string">&quot;Linux filesystem&quot;</span> PARTUUID=<span class="string">&quot;bf36a837-22e7-44c5-8c74-9d9fe8161aa3&quot;</span></span><br><span class="line">/dev/vdb2: UUID=<span class="string">&quot;a875704a-d0ba-4542-98a6-f201f1b3a539&quot;</span> TYPE=<span class="string">&quot;swap&quot;</span> PARTLABEL=<span class="string">&quot;Linux swap&quot;</span> PARTUUID=<span class="string">&quot;794f5ef2-2fec-410d-a72d-9195ded90386&quot;</span></span><br><span class="line">/dev/vdb3: UUID=<span class="string">&quot;9a906cce-e50e-44ee-a880-b3811252e5e1&quot;</span> TYPE=<span class="string">&quot;swap&quot;</span> PARTLABEL=<span class="string">&quot;Linux swap&quot;</span> PARTUUID=<span class="string">&quot;ff0fe2fa-a946-4d83-9290-4a0629767548&quot;</span></span><br></pre></td></tr></table></figure><h3 id="3、建立目錄並掛載-partition"><a href="#3、建立目錄並掛載-partition" class="headerlink" title="3、建立目錄並掛載 partition"></a>3、建立目錄並掛載 partition</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir /backup</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;UUID=796676d5-0e5c-4023-82e3-3417e8d00952 /backup xfs defaults 0 2&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;UUID=a875704a-d0ba-4542-98a6-f201f1b3a539 swap swap defaults 0 0&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;UUID=9a906cce-e50e-44ee-a880-b3811252e5e1 swap swap pri=1 0 0&quot;</span> | sudo tee --append /etc/fstab</span><br><span class="line"></span><br><span class="line">$ sudo mount -a</span><br><span class="line">$ sudo swapon -a</span><br></pre></td></tr></table></figure><h3 id="4、驗證是否設定完成"><a href="#4、驗證是否設定完成" class="headerlink" title="4、驗證是否設定完成"></a>4、驗證是否設定完成</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo swapon -s</span><br><span class="line">Filename                                Type            Size    Used    Priority</span><br><span class="line">/dev/vdb2                               partition       524284  0       -1</span><br><span class="line">/dev/vdb3                               partition       524284  0       1</span><br><span class="line"></span><br><span class="line">$ sudo mount</span><br><span class="line">...</span><br><span class="line">/dev/vdb1 on /backup <span class="built_in">type</span> xfs (rw,relatime,seclabel,attr2,inode64,noquota)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[KVM] Linux KVM 基本觀念</title>
      <link href="/blog/KVM/Linux-KVM-GettingStart/"/>
      <url>/blog/KVM/Linux-KVM-GettingStart/</url>
      
        <content type="html"><![CDATA[<h1 id="1、libvirt-and-libvirt-tools"><a href="#1、libvirt-and-libvirt-tools" class="headerlink" title="1、libvirt and libvirt tools"></a>1、libvirt and libvirt tools</h1><h2 id="libvirt"><a href="#libvirt" class="headerlink" title="libvirt"></a>libvirt</h2><p><code>libvirt</code> 是一組獨立於 hypervisor 之外的 API：</p><ol><li><p>提供一個標準，一般化且穩定的虛擬層，且可安全的管理主機上的虛擬機器</p></li><li><p>可用來管理本地系統以及透過網路相連主機的標準介面</p></li><li><p>提供的 API 包含了 provision, create, modify, monitor, control, migrate, stop 虛擬主機等相關功能，但也必須要 hypervisor 支援的前提下才可使用，但這些 API 緊限定於單一主機上的操作</p></li></ol><h2 id="virsh"><a href="#virsh" class="headerlink" title="virsh"></a>virsh</h2><p>virsh 是一個基於 libvirt management API 所打造出來的 CLI，提供非常多管理 hypervisor &amp; guest VM 的指令，且相當方便搭配 script 一起使用，同時也是 <code>virt-manager</code> GUI tool 的基礎。</p><blockquote><p>若不想用 virsh，也可以直接使用 <code>qemu-kvm</code> 指令</p></blockquote><h2 id="virt-manager"><a href="#virt-manager" class="headerlink" title="virt-manager"></a>virt-manager</h2><p>圖形介面的管理工具，也是提供了很多管理 VM 的功能(基於 virsh 為基礎所開發出來的)，包含了 provision VM, 管理 virtual network, 存取 VM console, 檢視效能統計資訊….等等。</p><h2 id="virt-install"><a href="#virt-install" class="headerlink" title="virt-install"></a>virt-install</h2><p>virt-install 是專門用來協助 provision VM 的 CLI，支援純文字 &amp; 圖形安裝(可透過 serial, SPICE or VNC 等不同協定)，且可指定 local or remote(NFS, HTTP or FTP) 的安裝媒體，搭配 Kickstart 作大量自動化安裝很好用。</p><hr><h1 id="2、Virtualized-hardware-devices"><a href="#2、Virtualized-hardware-devices" class="headerlink" title="2、Virtualized hardware devices"></a>2、Virtualized hardware devices</h1><h2 id="Para-virtualized-devices"><a href="#Para-virtualized-devices" class="headerlink" title="Para-virtualized devices"></a>Para-virtualized devices</h2><p>para-virtualized 技術提供了 VM 一個更快速且有效率的方式跟 host 主機溝通；而 KVM 透過 <code>virtio</code> API 作為中介層，提供 para-virtualized devices 給 VM 使用，</p><p>對於 I/O 工作頻繁的 VM 來說，建議使用 para-virtualized devices 來取代 emulated devices。</p><p>所有的 virtio device 都包含兩個部份：<code>host device</code> &amp; <code>guest driver</code>，而 Para-virtualized device drivers 的目的是在於協助 guest VM 可以直接存取 host 主機上的實體裝置而不需要再經過模擬轉換。</p><p>目前有 virtio-net, virtio-blk(block device), virtio-scsi(效能比 virtio-blk 好很多), virtio-balloon ….. 等等。</p><h2 id="VFIO"><a href="#VFIO" class="headerlink" title="VFIO"></a>VFIO</h2><p>Virtual Function I/O (VFIO) 是個 kernel driver，讓 guest VM 可直接高效率的存取 host 主機上的硬體裝置；VFIO 將 device assignment 的工作移出 KVM hypervisor 中，並將實體裝置在 kernel level 中獨立出來以達成直接被 VM 存取的目的。</p><h2 id="SR-IOV"><a href="#SR-IOV" class="headerlink" title="SR-IOV"></a>SR-IOV</h2><p>SR-IOV (Single Root I/O Virtualization) 是用在 PCI-e 的裝置上，讓裝置可以分享自身的 virtual fcuntion(VF) 出來直接給 guest VM 使用的一種技術。</p><h2 id="NPIV"><a href="#NPIV" class="headerlink" title="NPIV"></a>NPIV</h2><p>N_Port ID Virtualization (NPIV) 是種應用在高速企業級儲存裝置的功能(例如：SAN)，功能類似 SR-IOV，都是讓 VM 可以直接存取硬體支援的技術。</p><hr><h1 id="3、Storage"><a href="#3、Storage" class="headerlink" title="3、Storage"></a>3、Storage</h1><h2 id="Disk-Images-的存在型式"><a href="#Disk-Images-的存在型式" class="headerlink" title="Disk Images 的存在型式"></a>Disk Images 的存在型式</h2><p>Disk Image 會根據在本地 or 遠端存放的不同，而已不同的型式存在：</p><ol><li>Image files</li></ol><p>直接以 image 檔案的方式存在，這又可分為 raw &amp; qcow2 兩種格式，其中 raw 格式速度快，但功能少；而 qcow2 則提供很多其他功能(例如：snapshot, compression, encryption, 從 template image 啟動 VM …. 等等)。</p><ol start="2"><li>LVM volumes</li></ol><p>Logical volume 可以直接作為 disk image 使用，同時也提供了 thin provision, snapshot 等功能。</p><ol start="3"><li>Host devices</li></ol><p>可以直接使用 CD-ROM, 或是透過 SAN or iSCSI 掛載的 LUN 作為 image</p><ol start="4"><li>Distributed storage systems</li></ol><p>在 RHEL7 中甚至支援把 disk image 放在 GlusterFS 上</p><hr><h1 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h1><ul><li><a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Virtualization_Getting_Started_Guide/index.html">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Virtualization_Getting_Started_Guide/index.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> KVM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> KVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 7. Managing SELinux Security 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH07_ManagingSELinuxSecurity/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH07_ManagingSELinuxSecurity/</url>
      
        <content type="html"><![CDATA[<a name="ch7.1" />7.1 Enabling and Monitoring Security Enhanced Linux (SELinux)=============================================================<h2 id="7-1-1-Basic-SELinux-security-concepts"><a href="#7-1-1-Basic-SELinux-security-concepts" class="headerlink" title="7.1.1 Basic SELinux security concepts"></a>7.1.1 Basic SELinux security concepts</h2><p>SELinux 為系統安全提供了多一層的保護，目的在於當系統服務被攻陷後，還有另外一層機制(object-based)可以保護使用者的資料。</p><p><img src="https://lh3.googleusercontent.com/bR6HlmRdHb9TosMXahqB3BxjdVdlFnfgpHh3bYbHeOmr2xGY_tvnAd7YBE4S4UWVzO_DX56XWwh6ytRqH1V27HhfcgRVlSHI5il3oCrJDaq0CRj_SBKElh8LLtCi4oDh3X1YXTK0gRX3p39i0soYp_ILkDBUku85ao5bYgFwmADGGN7S4Hd2FCQaREgbk9SFKgXQRJgMsvpfwmCg3KBKR6yEhG9T9iiQZg2cxZdXxELuzOFLx0jjrbgn84jxtXNnkNJJlWaOKQKtq8qGReElZkvZy2F1ZaJTcJtO208xPgEtoHHbI4N9GJyGUIcQzDhQ3rlXBuPbcuo0TT_W1yULLmLxA-uGR_JnA4cm9wv16tJZs3zMdeUKsFNxALBFZptxtOkOcpi1jopCJUEvCw4NAHhExPVHVHkVdFS2a-KUjRMBRyrFX-sa-xC5Ey3l7dbgKgjHjYta11VozADz7WIUV1FUVkR2tUmdXv6_XJVwVpclaD2PXnd7tHbOmM8Mnf4mwMP4E6yjjWiuTk9YVpoGx_2_8Nyw3xzzJ9CHYJy-WVPgSI1Q-m0Co6ZcR2bK_6p02bE8h9nft0354PiugpLIqV6dKQ0SW6g=w888-h225-no" alt="SELinux"></p><p>在傳統架構下，當 web server 被攻陷後，駭客取得 <strong>apache</strong> 使用者的權限，就可以自由地存取 <strong>/var/www/html</strong>, <strong>/tmp</strong>, <strong>/var/tmp</strong> 等目錄；但若是在 SELinux 的限制下，就僅有 <strong>/var/www/html</strong> 可以存取。</p><p>從上面就不難看出，SELinux 定義了特定服務可以存取的特定 file、directory、port …. 等資訊，用以限縮 service owner 可以存取檔案的範圍；而方法就在於每一個 file, directory, process, port 都有所謂的 <code>SELinux context</code>。</p><p>而 context 大概長這樣 =&gt; <code>system_u:object_r:httpd_sys_content_t:s0</code></p><p>以 <strong>:</strong> 作為分隔，分別是 user(system_u), role(object_r), type(httpd_sys_content_t), sensitvity(s0)；而 SELinux rule 的制定則是以 type 為主來設計的。</p><p>舉例來說，與 Apache 相關的檔案位於 <strong>/var/www/html</strong> 中，而這裡檔案的 context type 則為 <code>httpd_sys_content_t</code> or <code>httpd_t</code>，與 Apache 服務運行相關的檔案則為 <code>http_port_t</code>，這些 context type　的檔案都可以讓 Apache service 存取；但若是 <strong>/tmp</strong> 與 <strong>/var/tmp</strong> 中的檔案，其 context type 則為 <code>tmp_t</code>，而 Apache service 要嘗試存取時就會被拒絕。</p><p>許多指令都可以透過 <code>-Z</code> 參數取得 SELinux context 資訊，例如：<code>ps axZ</code>, <code>ps -ZC httpd</code>, <code>ls -Z /var/www</code> … 等等。</p><h2 id="7-1-2-SELinux-modes"><a href="#7-1-2-SELinux-modes" class="headerlink" title="7.1.2 SELinux modes"></a>7.1.2 SELinux modes</h2><p>預設為 <strong>enforcing</strong> mode，但若是基於臨時性的需求而需要關掉 SELinux，可轉換成 <strong>permissive</strong> mode，此時只會有警告 &amp; Log，但不會被安全機制阻擋，且可以 online 切換，不須 reboot；但如果要完全 disable SELinux ，就需要重開機。</p><blockquote><p>使用 <code>getenforce</code> 指令就可以知道目前的 SELinux mode</p></blockquote><h2 id="7-1-3-SELinux-Booleans"><a href="#7-1-3-SELinux-Booleans" class="headerlink" title="7.1.3 SELinux Booleans"></a>7.1.3 SELinux Booleans</h2><p><strong>SELinux Booleans</strong> 是用來設定 SELinux Rules 是否啟用，可以用來調整 SELinux 的原始設定；若要檢視 SELinux Booleans 目前的設定值，可使用 <code>getsebool -a</code> 取得。</p><hr><a name="ch7.2" />7.2 Changing SELinux Modes==========================<h2 id="7-2-1-Changing-the-current-SELinux-mode"><a href="#7-2-1-Changing-the-current-SELinux-mode" class="headerlink" title="7.2.1 Changing the current SELinux mode"></a>7.2.1 Changing the current SELinux mode</h2><p>檢視目前 SELinux 狀態：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ getenforce</span><br><span class="line">Permissive</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1(enforcing), 0(permissive)</span></span><br><span class="line">[vagrant@server tmp]$ sudo setenforce 1</span><br><span class="line">[vagrant@server tmp]$ getenforce</span><br><span class="line">Enforcing</span><br></pre></td></tr></table></figure><h2 id="7-2-2-Setting-the-default-SELinux-mode"><a href="#7-2-2-Setting-the-default-SELinux-mode" class="headerlink" title="7.2.2 Setting the default SELinux mode"></a>7.2.2 Setting the default SELinux mode</h2><p>SELinux 的設定檔位於 <code>/etc/selinux/config</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ cat /etc/selinux/config</span><br><span class="line"></span><br><span class="line"><span class="comment"># This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="comment"># SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="comment">#     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="comment">#     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="comment">#     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=permissive</span><br><span class="line"><span class="comment"># SELINUXTYPE= can take one of three two values:</span></span><br><span class="line"><span class="comment">#     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="comment">#     minimum - Modification of targeted policy. Only selected processes are protected.</span></span><br><span class="line"><span class="comment">#     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure><p><code>permissive</code>：僅會 log 不會限制存取</p><p><code>enforcing</code>：會 log &amp; 限制存取</p><blockquote><p>SELINUXTYPE 預設為 targeted, 也許在少數狀況下會使用另外兩個</p></blockquote><blockquote><p>變更完設定後必須重新啟動，設定才會生效</p></blockquote><hr><a name="ch7.3" />## 7.3 Chaging SELinux Contexts<p>在開始這個部分之前，需要先安裝兩個必要套件，分別是 <code>policycoreutils</code> &amp; <code>policycoreutils-python</code>(semanage)</p><h3 id="7-3-1-Initial-SELinux-context"><a href="#7-3-1-Initial-SELinux-context" class="headerlink" title="7.3.1 Initial SELinux context"></a>7.3.1 Initial SELinux context</h3><p>在 RHEL7 中，檔案預設的 SELinux context 會由其所在的目錄所決定，新產生的檔案都會繼承目錄的 context(<code>vim</code>, <code>cp</code>, <code>touch</code> 適用)，但若是非新建的或是特別情況則不會(<code>mv</code>, <code>cp -a</code>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># context =&gt; httpd_sys_content_t</span></span><br><span class="line">[student@server0 ~]$ ls -Zd /var/www/html</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 /var/www/html</span><br><span class="line"></span><br><span class="line"><span class="comment"># 產生兩個檔案</span></span><br><span class="line">[student@server0 ~]$ sudo touch /var/www/html/index.html</span><br><span class="line">[student@server0 ~]$ sudo cp -a /tmp/rht /var/www/html/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建的繼承目錄的 context，透過 cp -a 的則保留原有的 context</span></span><br><span class="line">[student@server0 ~]$ ls -laZ /var/www/html/</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 .</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 ..</span><br><span class="line">-rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 index.html</span><br><span class="line">-rw-r--r--. root root system_u:object_r:init_tmp_t:s0  rht</span><br></pre></td></tr></table></figure><h3 id="7-3-2-Changing-the-SELinux-context-of-a-file"><a href="#7-3-2-Changing-the-SELinux-context-of-a-file" class="headerlink" title="7.3.2 Changing the SELinux context of a file"></a>7.3.2 Changing the SELinux context of a file</h3><p>有兩種方式可以改變 SELinux context：</p><ol><li><p><code>chcon</code>：搭配 <code>-t</code> 參數指定所要變更的 context</p></li><li><p><code>restorecon</code>：直接將 context 改為預設 context (根據檔案 or 目錄所在的位置而定)</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示目錄(/var/www/html)的預設 SELinux context</span></span><br><span class="line">[student@server0 ~]$ ls -laZ /var/www/html/</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 .</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 ..</span><br><span class="line">-rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 index.html</span><br><span class="line">-rw-r--r--. root root system_u:object_r:init_tmp_t:s0  rht</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 chcon 變更檔案的 context</span></span><br><span class="line">[student@server0 ~]$ sudo chcon -t tmp_t /var/www/html/index.html</span><br><span class="line">[student@server0 ~]$ ls -laZ /var/www/html/</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 .</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 ..</span><br><span class="line">-rw-r--r--. root root unconfined_u:object_r:tmp_t:s0   index.html</span><br><span class="line">-rw-r--r--. root root system_u:object_r:init_tmp_t:s0  rht</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 restorecon 直接將檔案變成為預設值</span></span><br><span class="line">[student@server0 ~]$ sudo restorecon -vR /var/www/html</span><br><span class="line">restorecon reset /var/www/html/index.html context unconfined_u:object_r:tmp_t:s0-&gt;unconfined_u:object_r:httpd_sys_content_t:s0</span><br><span class="line">restorecon reset /var/www/html/rht context system_u:object_r:init_tmp_t:s0-&gt;system_u:object_r:httpd_sys_content_t:s0</span><br><span class="line">[student@server0 ~]$ ls -laZ /var/www/html/</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 .</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 ..</span><br><span class="line">-rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 index.html</span><br><span class="line">-rw-r--r--. root root system_u:object_r:httpd_sys_content_t:s0 rht</span><br></pre></td></tr></table></figure><h3 id="7-3-3-Defining-SELinux-default-file-context-rules"><a href="#7-3-3-Defining-SELinux-default-file-context-rules" class="headerlink" title="7.3.3 Defining SELinux default file context rules"></a>7.3.3 Defining SELinux default file context rules</h3><p>使用 <code>semanage fcontext</code> 可以顯示(<code>-l</code>) or 修改(<code>-a</code>) 目錄的預設 SELinux context：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 / 建立新的目錄，其預設 SELinux context 為 default_t</span></span><br><span class="line">[student@server0 ~]$ sudo mkdir /virtual</span><br><span class="line"><span class="comment"># 在此目錄建立的檔案，context 也會變成 default_t</span></span><br><span class="line">[student@server0 ~]$ sudo touch /virtual/index.html</span><br><span class="line">[student@server0 ~]$ ls -alZ /virtual</span><br><span class="line">drwxr-xr-x. root root unconfined_u:object_r:default_t:s0 .</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:root_t:s0      ..</span><br><span class="line">-rw-r--r--. root root unconfined_u:object_r:default_t:s0 index.html</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 semanage fcontext 修改 /virtual 目錄的預設 SELinux context</span></span><br><span class="line">[student@server0 ~]$ sudo semanage fcontext -a -t httpd_sys_content_t <span class="string">&#x27;/virtual(/.*)?&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 restorecon 將 /virtual 目錄下的檔案還原為預設值</span></span><br><span class="line">[student@server0 ~]$ sudo restorecon -Rv /virtual</span><br><span class="line">restorecon reset /virtual context unconfined_u:object_r:default_t:s0-&gt;unconfined_u:object_r:httpd_sys_content_t:s0</span><br><span class="line">restorecon reset /virtual/index.html context unconfined_u:object_r:default_t:s0-&gt;unconfined_u:object_r:httpd_sys_content_t:s0</span><br><span class="line">[student@server0 ~]$ ls -alZ /virtual</span><br><span class="line">drwxr-xr-x. root root unconfined_u:object_r:httpd_sys_content_t:s0 .</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:root_t:s0      ..</span><br><span class="line">-rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 index.html</span><br></pre></td></tr></table></figure><hr><a name="ch7.3" />7.4 Chaging SELinux Booleans============================<p>安裝 <code>selinux-policy-devel</code> 套件可取得與 SELinux Booleans 相關的說明資訊，位於 **selinux(8)**，可使用 <code>man -k _selinux</code> 來查詢目前系統中存在的文件。</p><p>SELinux Booleans 是用來決定 rule 是否啟用的設定值，可透過 <code>getsebool</code> &amp; <code>setsebool</code> 兩個指令來設定：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得 SELinux Booleans 資訊(&#x27;-a&#x27; 參數表示顯示全木)</span></span><br><span class="line">[student@server0 ~]$ getsebool -a | grep httpd_enable_homedir</span><br><span class="line">httpd_enable_homedirs --&gt; off</span><br><span class="line">[student@server0 ~]$ getsebool httpd_enable_homedirs</span><br><span class="line">httpd_enable_homedirs --&gt; off</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 SELinux Boolean</span></span><br><span class="line">[student@server0 ~]$ sudo setsebool httpd_enable_homedirs on</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 &#x27;semanage boolean -l&#x27; 可以查詢 SELinux Boolean 是否永久被更改(第二個結果)</span></span><br><span class="line">[student@server0 ~]$ sudo semanage boolean -l | grep httpd_enable_homedirs</span><br><span class="line">httpd_enable_homedirs          (on   ,  off)  Allow httpd to <span class="built_in">read</span> home directories</span><br><span class="line">[student@server0 ~]$ getsebool -a | grep httpd_enable_homedir</span><br><span class="line">httpd_enable_homedirs --&gt; on</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 &#x27;-P&#x27; 參數永久變更 SELinux Boolean 的設定</span></span><br><span class="line">[student@server0 ~]$ sudo setsebool -P httpd_enable_homedirs on</span><br><span class="line">[student@server0 ~]$ getsebool -a | grep httpd_enable_homedir</span><br><span class="line">httpd_enable_homedirs --&gt; on</span><br><span class="line">[student@server0 ~]$ sudo semanage boolean -l | grep httpd_enable_homedirs</span><br><span class="line">httpd_enable_homedirs          (on   ,   on)  Allow httpd to <span class="built_in">read</span> home directories</span><br></pre></td></tr></table></figure><hr><a name="ch7.3" />7.5 Troublshooting SELinux==========================<h2 id="7-5-1-Troubleshooting-SELinux-issues"><a href="#7-5-1-Troubleshooting-SELinux-issues" class="headerlink" title="7.5.1 Troubleshooting SELinux issues"></a>7.5.1 Troubleshooting SELinux issues</h2><p>關於 SELinux 會造成的 issue，大概會有幾個原因 &amp; 方向可以思考：</p><ol><li><p>服務是否有特定目錄 or 檔案的存取權限</p></li><li><p>可能是錯誤的 file context 造成的，此時可用 <code>restorecon</code> 解決</p></li><li><p>可能是太嚴格的 SELinux Boolean 設定所造成(例如 <code>ftpd_anon_write</code> 限制了匿名使用者對 FTP 服務的存取)</p></li><li><p>可能是 SELinux policy 的 bug 所造成</p></li></ol><h2 id="7-5-2-Monitoring-SELinux-violations"><a href="#7-5-2-Monitoring-SELinux-violations" class="headerlink" title="7.5.2 Monitoring SELinux violations"></a>7.5.2 Monitoring SELinux violations</h2><p>套件 <code>setroubleshoot-server</code> 必須安裝，才有辦法紀錄 SELinux 所產生的相關 log 資訊(存在於 <strong>/var/log/audit/audit.log</strong>)。</p><ul><li><p><code>sealert -l UUID</code>：檢視指定 UUID 的報告</p></li><li><p><code>sealert -a /var/log/audit/audit.log</code>：檢視 log 中所有的稽核報告</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">[student@server0 ~]$ sudo systemctl start httpd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 /root 下新增檔案，並移到 /var/www/html 目錄</span></span><br><span class="line">[student@server0 ~]$ sudo touch /root/file3</span><br><span class="line">[student@server0 ~]$ sudo mv /root/file3 /var/www/html/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 嘗試透過瀏覽器存取 file3</span></span><br><span class="line">[student@server0 ~]$ curl http://localhost/file3</span><br><span class="line">&lt;!DOCTYPE HTML PUBLIC <span class="string">&quot;-//IETF//DTD HTML 2.0//EN&quot;</span>&gt;</span><br><span class="line">&lt;html&gt;&lt;head&gt;</span><br><span class="line">&lt;title&gt;403 Forbidden&lt;/title&gt;</span><br><span class="line">&lt;/head&gt;&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Forbidden&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;You don\<span class="string">&#x27;t have permission to access /file3</span></span><br><span class="line"><span class="string">on this server.&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;/body&gt;&lt;/html&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 從 /var/log/audit/audit.log 中尋找相關的錯誤資訊</span></span><br><span class="line"><span class="string">[student@server0 ~]$ sudo tail /var/log/audit/audit.log</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="string">type=AVC msg=audit(1462720907.786:517): avc:  denied  &#123; getattr &#125; for  pid=1754 comm=&quot;httpd&quot; path=&quot;/var/www/html/file3&quot; dev=&quot;vda1&quot; ino=8846767 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file</span></span><br><span class="line"><span class="string">type=SYSCALL msg=audit(1462720907.786:517): arch=c000003e syscall=4 success=no exit=-13 a0=7fd102869b48 a1=7fff0049e4f0 a2=7fff0049e4f0 a3=7fd0f7202752 items=0 ppid=1751 pid=1754 auid=4294967295 uid=48 gid=48 euid=48 suid=48 fsuid=48 egid=48 sgid=48 fsgid=48 tty=(none) ses=4294967295 comm=&quot;httpd&quot; exe=&quot;/usr/sbin/httpd&quot; subj=system_u:system_r:httpd_t:s0 key=(null)</span></span><br><span class="line"><span class="string">type=AVC msg=audit(1462720907.786:518): avc:  denied  &#123; getattr &#125; for  pid=1754 comm=&quot;httpd&quot; path=&quot;/var/www/html/file3&quot; dev=&quot;vda1&quot; ino=8846767 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file</span></span><br><span class="line"><span class="string">type=SYSCALL msg=audit(1462720907.786:518): arch=c000003e syscall=6 success=no exit=-13 a0=7fd102869c18 a1=7fff0049e4f0 a2=7fff0049e4f0 a3=0 items=0 ppid=1751 pid=1754 auid=4294967295 uid=48 gid=48 euid=48 suid=48 fsuid=48 egid=48 sgid=48 fsgid=48 tty=(none) ses=4294967295 comm=&quot;httpd&quot; exe=&quot;/usr/sbin/httpd&quot; subj=system_u:system_r:httpd_t:s0 key=(null)</span></span><br><span class="line"><span class="string">.......</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 找到 SELinux 相關的 UUID</span></span><br><span class="line"><span class="string">[student@server0 ~]$ sudo tail -50 /var/log/messages</span></span><br><span class="line"><span class="string">.....</span></span><br><span class="line"><span class="string">May  8 11:21:48 localhost setroubleshoot: Plugin Exception restorecon_source</span></span><br><span class="line"><span class="string">May  8 11:21:48 localhost setroubleshoot: SELinux is preventing /usr/sbin/httpd from getattr access on the file . For complete SELinux messages. run sealert -l 26a423d9-3dbd-413b-8048-3e7abec01df1</span></span><br><span class="line"><span class="string">May  8 11:21:48 localhost python: SELinux is preventing /usr/sbin/httpd from getattr access on the file .</span></span><br><span class="line"><span class="string">.....</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 透過 sealert -l 檢視詳細的報告內容</span></span><br><span class="line"><span class="string">[student@server0 ~]$ sudo sealert -l 26a423d9-3dbd-413b-8048-3e7abec01df1</span></span><br><span class="line"><span class="string">SELinux is preventing /usr/sbin/httpd from getattr access on the file .</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">*****  Plugin catchall (100. confidence) suggests   **************************</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">If you believe that httpd should be allowed getattr access on the  file by default.</span></span><br><span class="line"><span class="string">Then you should report this as a bug.</span></span><br><span class="line"><span class="string">You can generate a local policy module to allow this access.</span></span><br><span class="line"><span class="string">Do</span></span><br><span class="line"><span class="string">allow this access for now by executing:</span></span><br><span class="line"><span class="string"># grep httpd /var/log/audit/audit.log | audit2allow -M mypol</span></span><br><span class="line"><span class="string"># semodule -i mypol.pp</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Additional Information:</span></span><br><span class="line"><span class="string">Source Context                system_u:system_r:httpd_t:s0</span></span><br><span class="line"><span class="string">Target Context                unconfined_u:object_r:admin_home_t:s0</span></span><br><span class="line"><span class="string">Target Objects                 [ file ]</span></span><br><span class="line"><span class="string">Source                        httpd</span></span><br><span class="line"><span class="string">Source Path                   /usr/sbin/httpd</span></span><br><span class="line"><span class="string">Port                          &lt;Unknown&gt;</span></span><br><span class="line"><span class="string">Host                          localhost</span></span><br><span class="line"><span class="string">Source RPM Packages           httpd-2.4.6-17.el7.x86_64</span></span><br><span class="line"><span class="string">Target RPM Packages</span></span><br><span class="line"><span class="string">Policy RPM                    selinux-policy-3.12.1-153.el7.noarch</span></span><br><span class="line"><span class="string">Selinux Enabled               True</span></span><br><span class="line"><span class="string">Policy Type                   targeted</span></span><br><span class="line"><span class="string">Enforcing Mode                Enforcing</span></span><br><span class="line"><span class="string">Host Name                     server0.example.com</span></span><br><span class="line"><span class="string">Platform                      Linux server0.example.com 3.10.0-123.el7.x86_64 #1</span></span><br><span class="line"><span class="string">                              SMP Mon May 5 11:16:57 EDT 2014 x86_64 x86_64</span></span><br><span class="line"><span class="string">Alert Count                   1</span></span><br><span class="line"><span class="string">First Seen                    2016-05-09 00:21:47 JST</span></span><br><span class="line"><span class="string">Last Seen                     2016-05-09 00:21:47 JST</span></span><br><span class="line"><span class="string">Local ID                      26a423d9-3dbd-413b-8048-3e7abec01df1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Raw Audit Messages</span></span><br><span class="line"><span class="string">type=AVC msg=audit(1462720907.786:518): avc:  denied  &#123; getattr &#125; for  pid=1754 comm=&quot;httpd&quot; path=&quot;/var/www/html/file3&quot; dev=&quot;vda1&quot; ino=8846767 scontext=system_u:system_r:httpd_t:s0 tcontext=unconfined_u:object_r:admin_home_t:s0 tclass=file</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">type=SYSCALL msg=audit(1462720907.786:518): arch=x86_64 syscall=lstat success=no exit=EACCES a0=7fd102869c18 a1=7fff0049e4f0 a2=7fff0049e4f0 a3=0 items=0 ppid=1751 pid=1754 auid=4294967295 uid=48 gid=48 euid=48 suid=48 fsuid=48 egid=48 sgid=48 fsgid=48 tty=(none) ses=4294967295 comm=httpd exe=/usr/sbin/httpd subj=system_u:system_r:httpd_t:s0 key=(null)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Hash: httpd,httpd_t,admin_home_t,file,getattr</span></span><br></pre></td></tr></table></figure><hr><h1 id="補充教材"><a href="#補充教材" class="headerlink" title="補充教材"></a>補充教材</h1><h2 id="初階篇"><a href="#初階篇" class="headerlink" title="初階篇"></a>初階篇</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ id -Z</span><br><span class="line">unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ ls</span><br><span class="line">ABC  hosts  passwd  vagrant-shell</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ ls -Z</span><br><span class="line">drwxrwxrwx+ vagrant vagrant unconfined_u:object_r:user_tmp_t:s0 ABC</span><br><span class="line">-rw-r--r--. vagrant vagrant unconfined_u:object_r:user_tmp_t:s0 hosts</span><br><span class="line">-rw-r-xr--+ vagrant vagrant unconfined_u:object_r:user_tmp_t:s0 passwd</span><br><span class="line">-rwx--x--x. vagrant vagrant unconfined_u:object_r:user_tmp_t:s0 vagrant-shell</span><br></pre></td></tr></table></figure><p>以上權限對應 =&gt; system_r:object_r:var_t:s0</p><ol><li>User</li><li>Role</li><li>Type<br>若 SELinux 設定為 enforcing，只要看 type 即可</li></ol><p>SELinux 的 check policy 存放於 <code>/etc/selinux/targeted/policy/policy.29</code></p><p>安裝 <code>setools-console</code> 後就可以查詢 SELinux policy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ ps auxZ | grep httpd</span><br><span class="line">system_u:system_r:httpd_t:s0    root     14773  0.0  0.2 221904  4968 ?        Ss   05:44   0:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">system_u:system_r:httpd_t:s0    apache   14774  0.0  0.1 221904  2972 ?        S    05:44   0:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">system_u:system_r:httpd_t:s0    apache   14775  0.0  0.1 221904  2972 ?        S    05:44   0:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">system_u:system_r:httpd_t:s0    apache   14776  0.0  0.1 221904  2972 ?        S    05:44   0:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">system_u:system_r:httpd_t:s0    apache   14777  0.0  0.1 221904  2972 ?        S    05:44   0:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line">system_u:system_r:httpd_t:s0    apache   14778  0.0  0.1 221904  2972 ?        S    05:44   0:00 /usr/sbin/httpd -DFOREGROUND</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可查詢來源端為 httpd_t 相關的權限設定</span></span><br><span class="line">[vagrant@server tmp]$ sesearch -A -s httpd_t</span><br><span class="line">....more</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢來源端為 httpd_t，目的端為 lib_t 的權限設定</span></span><br><span class="line">[vagrant@server tmp]$ sesearch -A -s httpd_t -t lib_t</span><br><span class="line">Found 13 semantic av rules:</span><br><span class="line">   allow domain base_ro_file_type : file \&#123; ioctl <span class="built_in">read</span> getattr lock open \&#125; ;</span><br><span class="line">   allow domain base_ro_file_type : dir \&#123; ioctl <span class="built_in">read</span> getattr lock search open \&#125; ;</span><br><span class="line">   allow domain base_ro_file_type : lnk_file \&#123; <span class="built_in">read</span> getattr \&#125; ;</span><br><span class="line">.... more</span><br></pre></td></tr></table></figure><h2 id="進階篇"><a href="#進階篇" class="headerlink" title="進階篇"></a>進階篇</h2><h3 id="限制-Process-對檔案目錄的存取"><a href="#限制-Process-對檔案目錄的存取" class="headerlink" title="限制 Process 對檔案目錄的存取"></a>限制 Process 對檔案目錄的存取</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ ls -ldZ /var/www/html</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 /var/www/html</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ ls -lZ passwd</span><br><span class="line">-rw-r--r--. vagrant vagrant unconfined_u:object_r:user_tmp_t:s0 passwd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 變更 SELinux context user/role/type</span></span><br><span class="line">[vagrant@server tmp]$ chcon -u system_u passwd</span><br><span class="line">[vagrant@server tmp]$ chcon -r object_r passwd</span><br><span class="line">[vagrant@server tmp]$ chcon -t httpd_sys_content_t passwd</span><br><span class="line">[vagrant@server tmp]$ ls -lZ passwd</span><br><span class="line">-rw-r--r--. vagrant vagrant system_u:object_r:httpd_sys_content_t:s0 passwd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同時變更 SELinux context user/role/type</span></span><br><span class="line">[vagrant@server tmp]$ chcon system_u:object_r:httpd_sys_content_t:s0 passwd</span><br></pre></td></tr></table></figure><p><code>policycoreutils-python</code> 套件是用來尋找正確的 SELinux Context type 之用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尋找系統中已經存在的檔案 or 目錄的 context type</span></span><br><span class="line">[vagrant@server tmp]$ sudo semanage fcontext -l | grep /var/www</span><br><span class="line">/var/www(/.*)?                                     all files          system_u:object_r:httpd_sys_content_t:s0</span><br><span class="line">/var/www(/.*)?/logs(/.*)?                          all files          system_u:object_r:httpd_log_t:s0</span><br><span class="line">/var/www/[^/]*/cgi-bin(/.*)?                       all files          system_u:object_r:httpd_sys_script_exec_t:s0</span><br><span class="line">/var/www/svn(/.*)?                                 all files          system_u:object_r:httpd_sys_rw_content_t:s0</span><br><span class="line">/var/www/git(/.*)?                                 all files          system_u:object_r:git_content_t:s0</span><br><span class="line">/var/www/perl(/.*)?                                all files          system_u:object_r:httpd_sys_script_exec_t:s0</span><br><span class="line">.... more</span><br><span class="line"></span><br><span class="line"><span class="comment"># 參考同性質檔案 or 目錄的 context type</span></span><br><span class="line">[vagrant@server tmp]$ ls -ldZ /var/www</span><br><span class="line">drwxr-xr-x. root root system_u:object_r:httpd_sys_content_t:s0 /var/www</span><br></pre></td></tr></table></figure><p><code>policycoreutils</code> 中的 <code>restorecon</code> 工具可幫助恢復成原有的標籤：例如：<code>restorecon -Rv /var/www/html/</code></p><blockquote><p>前提是資料庫必須要有相關資料</p></blockquote><h3 id="修改資料庫"><a href="#修改資料庫" class="headerlink" title="修改資料庫"></a>修改資料庫</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ sudo mkdir /WWW</span><br><span class="line">[vagrant@server tmp]$ ls -ldZ /WWW</span><br><span class="line">drwxr-xr-x. root root unconfined_u:object_r:default_t:s0 /WWW</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 semanage fcontext 修改目錄預設的 SELinux context</span></span><br><span class="line">[vagrant@server tmp]$ sudo semanage fcontext -a -f a -t httpd_sys_content_t <span class="string">&#x27;/WWW(/.*)?&#x27;</span></span><br><span class="line">[vagrant@server tmp]$ sudo semanage fcontext -l | grep WWW</span><br><span class="line">/WWW(/.*)?                                         all files          system_u:object_r:httpd_sys_content_t:s0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢復原有設定</span></span><br><span class="line">[vagrant@server tmp]$ sudo restorecon -Rv /WWW</span><br><span class="line">restorecon reset /WWW context unconfined_u:object_r:default_t:s0-&gt;unconfined_u:object_r:httpd_sys_content_t:s0</span><br><span class="line">restorecon reset /WWW/aaa context unconfined_u:object_r:default_t:s0-&gt;unconfined_u:object_r:httpd_sys_content_t:s0</span><br><span class="line">[vagrant@server tmp]$ ls -ldZ /WWW</span><br><span class="line">drwxr-xr-x. root root unconfined_u:object_r:httpd_sys_content_t:s0 /WWW</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ sudo cp /etc/shadow /WWW/</span><br><span class="line">[vagrant@server tmp]$ ls -ldZ /WWW/</span><br><span class="line">drwxr-xr-x. root root unconfined_u:object_r:httpd_sys_content_t:s0 /WWW/</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ ls -ldZ /WWW/*</span><br><span class="line">----------. root root unconfined_u:object_r:httpd_sys_content_t:s0 /WWW/shadow</span><br><span class="line"></span><br><span class="line"><span class="comment"># -a 會保留原檔案的相關 metadata，因此不會被 dest dir 的 SELinux context 複寫</span></span><br><span class="line">[vagrant@server tmp]$ sudo cp -a /etc/shadow /WWW/shadow_a</span><br><span class="line">[vagrant@server tmp]$ ls -ldZ /WWW/*</span><br><span class="line">----------. root root unconfined_u:object_r:httpd_sys_content_t:s0 /WWW/shadow</span><br><span class="line">----------. root root system_u:object_r:shadow_t:s0    /WWW/shadow_a</span><br></pre></td></tr></table></figure><p>查詢 SELinux 相關的 log 可到 /var/log/audit/audit.log 查詢</p><h3 id="限制應用程式的特定功能是否能夠啟用"><a href="#限制應用程式的特定功能是否能夠啟用" class="headerlink" title="限制應用程式的特定功能是否能夠啟用"></a>限制應用程式的特定功能是否能夠啟用</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ sudo semanage boolean -l | grep ftp</span><br><span class="line">ftp_home_dir                   (off  ,  off)  Allow ftp to home dir</span><br><span class="line">ftpd_use_cifs                  (off  ,  off)  Allow ftpd to use cifs</span><br><span class="line">sftpd_write_ssh_home           (off  ,  off)  Allow sftpd to write ssh home</span><br><span class="line">ftpd_use_fusefs                (off  ,  off)  Allow ftpd to use fusefs</span><br><span class="line">..... more</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加了 -P 會永久儲存，沒加就只會存到記憶體中</span></span><br><span class="line">[vagrant@server tmp]$ sudo setsebool -P ftp_home_dir 1</span><br><span class="line">[vagrant@server tmp]$ sudo semanage boolean -l | grep ftp</span><br><span class="line">ftp_home_dir                   (on   ,   on)  Allow ftp to home dir</span><br></pre></td></tr></table></figure><h3 id="限制應用程式所能夠存取的-port"><a href="#限制應用程式所能夠存取的-port" class="headerlink" title="限制應用程式所能夠存取的 port"></a>限制應用程式所能夠存取的 port</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ sudo semanage port -l | grep http</span><br><span class="line">http_cache_port_t              tcp      8080, 8118, 8123, 10001-10010</span><br><span class="line">http_cache_port_t              udp      3130</span><br><span class="line">http_port_t                    tcp      80, 81, 443, 488, 8008, 8009, 8443, 9000</span><br><span class="line">pegasus_http_port_t            tcp      5988</span><br><span class="line">pegasus_https_port_t           tcp      5989</span><br><span class="line"></span><br><span class="line"><span class="comment"># -a 增加 port, -d 刪除 port</span></span><br><span class="line">[vagrant@server tmp]$ sudo semanage port -a -t http_port_t -p tcp 5678</span><br><span class="line">[vagrant@server tmp] manage port -l | grep http_port_t</span><br><span class="line">http_port_t                    tcp      5678, 80, 81, 443, 488, 8008, 8009, 8443, 9000</span><br><span class="line">pegasus_http_port_t            tcp      5988</span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Changing-SELinux-Booleans"><a href="#Practice-Changing-SELinux-Booleans" class="headerlink" title="Practice: Changing SELinux Booleans"></a>Practice: Changing SELinux Booleans</h1><p>啟用 web server 的使用者家目錄 web 功能</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安裝 apache</span></span><br><span class="line">$ sudo yum -y install httpd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟動 apache server</span></span><br><span class="line">$ sudo systemctl <span class="built_in">enable</span> httpd.service</span><br><span class="line">ln -s <span class="string">&#x27;/usr/lib/systemd/system/httpd.service&#x27;</span> <span class="string">&#x27;/etc/systemd/system/multi-user.target.wants/httpd.service&#x27;</span></span><br><span class="line">$ sudo systemctl start httpd.service</span><br><span class="line">$ sudo systemctl status httpd.service</span><br><span class="line">httpd.service - The Apache HTTP Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled)</span><br><span class="line">   Active: active (running) since Mon 2016-05-16 17:55:59 JST; 5s ago</span><br><span class="line">.......</span><br><span class="line"></span><br><span class="line"><span class="comment"># 開啟使用者 web 目錄為 ~/public_html</span></span><br><span class="line">$ sudo sed -i <span class="string">&#x27;s/^\s*#\(UserDir public_html\)/    \1/g&#x27;</span> /etc/httpd/conf.d/userdir.conf</span><br><span class="line">$ sudo sed -i <span class="string">&#x27;s/^\s*\(UserDir\sdisabled\)/    # \1/g&#x27;</span> /etc/httpd/conf.d/userdir.conf</span><br><span class="line">$ sudo systemctl restart httpd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 index.html，但還是沒有權限可以存取</span></span><br><span class="line">$ mkdir ~/public_html</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;Hello Internet&quot;</span> | sudo tee /home/student/public_html/index.html</span><br><span class="line">Hello Internet</span><br><span class="line">$ chmod 711 /home/student</span><br><span class="line">$ curl http://localhost/~student/index.html</span><br><span class="line">&lt;!DOCTYPE HTML PUBLIC <span class="string">&quot;-//IETF//DTD HTML 2.0//EN&quot;</span>&gt;</span><br><span class="line">&lt;html&gt;&lt;head&gt;</span><br><span class="line">&lt;title&gt;403 Forbidden&lt;/title&gt;</span><br><span class="line">&lt;/head&gt;&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Forbidden&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;You don\<span class="string">&#x27;t have permission to access /~student/index.html</span></span><br><span class="line"><span class="string">on this server.&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;/body&gt;&lt;/html&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 檢視 SELinux Boolean 設定</span></span><br><span class="line"><span class="string">$ getsebool -a | grep httpd | grep home</span></span><br><span class="line"><span class="string">httpd_enable_homedirs --&gt; off</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 開啟 httpd_enable_homedirs</span></span><br><span class="line"><span class="string">$ sudo setsebool -P httpd_enable_homedirs on</span></span><br><span class="line"><span class="string">$ getsebool -a | grep httpd | grep home</span></span><br><span class="line"><span class="string">httpd_enable_homedirs --&gt; on</span></span><br><span class="line"><span class="string">$ curl http://localhost/~student/index.html</span></span><br><span class="line"><span class="string">Hello Internet</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 6. Controlling Access to Files with Access Control Lists(ACLs) 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH06_ControllingAccessToFilesWithAccessControlLists/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH06_ControllingAccessToFilesWithAccessControlLists/</url>
      
        <content type="html"><![CDATA[<p>透過 ACL 的機制，可以讓一個檔案同時有多個 Owner or Group</p><a name="ch6.1" />6.1 POSIX Access Control Lists (ACLs)=====================================<p>若 partition 格式為 ext4，mount 的時候必須加上 <code>-o acl</code> 參數：(以下為範例)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mount -o acl /dev/vdb1 /mnt</span><br></pre></td></tr></table></figure><blockquote><p>或是在 /etc/fstab 上的參數設定加上 acl 也行</p></blockquote><p>也可以透過指令 <code>sudo tune2fs -o user_xattr,acl /dev/vdb1</code> 直接把屬性加入到 partition 的 superblock 中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取得 ACL 資訊</span></span><br><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">group::rw-</span><br><span class="line">other::r--</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入 User(user1) 權限 (使用 -m)</span></span><br><span class="line">[vagrant@server tmp]$ setfacl -m u:user1:r-x acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">user:user1:r-x</span><br><span class="line">group::rw-</span><br><span class="line">mask::rwx</span><br><span class="line">other::r--</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入 User(user2 &amp; user3) 權限 (使用 -m)</span></span><br><span class="line">[vagrant@server tmp]$ setfacl -m u:user2:r-- acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ setfacl -m u:user3:rwx acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">user:user1:r-x</span><br><span class="line">user:user2:r--</span><br><span class="line">user:user3:rwx</span><br><span class="line">group::rw-</span><br><span class="line">mask::rwx</span><br><span class="line">other::r--</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入 Group(user4) 權限 (使用 -m)</span></span><br><span class="line">[vagrant@server tmp]$ setfacl -m g:user4:r-x acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">user:user1:r-x</span><br><span class="line">user:user2:r--</span><br><span class="line">user:user3:rwx</span><br><span class="line">group::rw-</span><br><span class="line">group:user4:r-x</span><br><span class="line">mask::rwx</span><br><span class="line">other::r--</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 User(user1 &amp; user2) 權限 (使用 -x)</span></span><br><span class="line">[vagrant@server tmp]$ setfacl -x u:user1 acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ setfacl -x u:user2 acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">user:user3:rwx</span><br><span class="line">group::rw-</span><br><span class="line">group:user4:r-x</span><br><span class="line">mask::rwx</span><br><span class="line">other::r--</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 User(user3) &amp; Group(user4) 權限 (使用 -x)</span></span><br><span class="line">[vagrant@server tmp]$ setfacl -x u:user3 acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ setfacl -x g:user4 acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">group::rw-</span><br><span class="line">mask::rw-</span><br><span class="line">other::r--</span><br><span class="line"></span><br><span class="line"><span class="comment"># 複製指定檔案的 ACL 權限到另外一個檔案</span></span><br><span class="line">$ getfacl acl_test.txt | setfacl --set-file=- acl_clone.txt</span><br></pre></td></tr></table></figure><h2 id="補充-使用-setfacl-變更檔案的傳統權限"><a href="#補充-使用-setfacl-變更檔案的傳統權限" class="headerlink" title="補充 (使用 setfacl 變更檔案的傳統權限)"></a>補充 (使用 setfacl 變更檔案的傳統權限)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ ll</span><br><span class="line">total 8</span><br><span class="line">-rw-rw-r--+ 1 vagrant vagrant  0 Feb 23 03:09 acl_test.txt</span><br><span class="line">-rwx--x--x. 1 vagrant vagrant 22 Feb 23 01:45 vagrant-shell</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ setfacl -m u::rwx acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ setfacl -m g::rwx acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ setfacl -m o::rwx acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ ll</span><br><span class="line">total 8</span><br><span class="line">-rwxrwxrwx+ 1 vagrant vagrant  0 Feb 23 03:09 acl_test.txt</span><br><span class="line">-rwx--x--x. 1 vagrant vagrant 22 Feb 23 01:45 vagrant-shell</span><br></pre></td></tr></table></figure><p><code>flags</code> 表示 SUID, SGID, StickyBit：(下面的範例表示檔案有 SUID 的屬性)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ getfacl /bin/passwd</span><br><span class="line">getfacl: Removing leading <span class="string">&#x27;/&#x27;</span> from absolute path names</span><br><span class="line"><span class="comment"># file: bin/passwd</span></span><br><span class="line"><span class="comment"># owner: root</span></span><br><span class="line"><span class="comment"># group: root</span></span><br><span class="line"><span class="comment"># flags: s--</span></span><br><span class="line">user::rwx</span><br><span class="line">group::r-x</span><br><span class="line">other::r-x</span><br></pre></td></tr></table></figure><h2 id="補充-使用-setfacl-修改-mask-設定"><a href="#補充-使用-setfacl-修改-mask-設定" class="headerlink" title="補充(使用 setfacl 修改 mask 設定)"></a>補充(使用 setfacl 修改 mask 設定)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rwx</span><br><span class="line">group::rwx</span><br><span class="line">mask::rwx</span><br><span class="line">other::rwx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 mask 可以限制 group 僅剩下 rw 的權限</span></span><br><span class="line">[vagrant@server tmp]$ setfacl -m m::r-x acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rwx</span><br><span class="line">group::rwx            <span class="comment">#effective:r-x</span></span><br><span class="line">mask::r-x</span><br><span class="line">other::rwx</span><br></pre></td></tr></table></figure><h2 id="補充-已經有-ACL-的設定就不要再用-chmod"><a href="#補充-已經有-ACL-的設定就不要再用-chmod" class="headerlink" title="補充(已經有 ACL 的設定就不要再用 chmod)"></a>補充(已經有 ACL 的設定就不要再用 chmod)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ ll</span><br><span class="line">total 8</span><br><span class="line">-rwxr-xrwx+ 1 vagrant vagrant  0 Feb 23 03:09 acl_test.txt</span><br><span class="line">-rwx--x--x. 1 vagrant vagrant 22 Feb 23 01:45 vagrant-shell</span><br><span class="line">[vagrant@server tmp]$ chmod 123 acl_test.txt</span><br><span class="line">[vagrant@server tmp]$ ll</span><br><span class="line">total 8</span><br><span class="line">---x-w--wx+ 1 vagrant vagrant  0 Feb 23 03:09 acl_test.txt</span><br><span class="line">-rwx--x--x. 1 vagrant vagrant 22 Feb 23 01:45 vagrant-shell</span><br><span class="line">[vagrant@server tmp]$ getfacl acl_test.txt</span><br><span class="line"><span class="comment"># file: acl_test.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::--x</span><br><span class="line">group::rwx            <span class="comment">#effective:-w-</span></span><br><span class="line">mask::-w-</span><br><span class="line">other::-wx</span><br></pre></td></tr></table></figure><h2 id="補充-若要稽核使用者使用檔案的狀況"><a href="#補充-若要稽核使用者使用檔案的狀況" class="headerlink" title="補充(若要稽核使用者使用檔案的狀況)"></a>補充(若要稽核使用者使用檔案的狀況)</h2><p>可使用 kernel 中的 Audit 功能，設定可參考 <code>/etc/audit</code> 目錄中的設定</p><h2 id="補充-其他"><a href="#補充-其他" class="headerlink" title="補充(其他)"></a>補充(其他)</h2><p>tar 打包時要包含 ACL &amp; SELinux 的權限，要加入 <code>-xattr</code> 參數</p><hr><a name="ch6.2" />6.2 Securing Files with ACLs============================<p>透過 <code>-b</code> 參數可回復沒有 ACL 權限設定的狀態，例如 <code>setfacl -b filename</code></p><h2 id="Setting-an-explicit-ACL-mask"><a href="#Setting-an-explicit-ACL-mask" class="headerlink" title="Setting an explicit ACL mask"></a>Setting an explicit ACL mask</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ setfacl -m u:user1:rwx passwd</span><br><span class="line">[vagrant@server tmp]$ setfacl -m u:user2:r-x passwd</span><br><span class="line">[vagrant@server tmp]$ setfacl -m g:user3:rwx passwd</span><br><span class="line">[vagrant@server tmp]$ setfacl -m g:user4:r-x passwd</span><br><span class="line">[vagrant@server tmp]$ getfacl passwd</span><br><span class="line"><span class="comment"># file: passwd</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">user:user1:rwx</span><br><span class="line">user:user2:r-x</span><br><span class="line">group::r--</span><br><span class="line">group:user3:rwx</span><br><span class="line">group:user4:r-x</span><br><span class="line">mask::rwx</span><br><span class="line">other::r--</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 mask，限定所能設定的最大權限(注意 rwx 實際只有 r-x 可用)</span></span><br><span class="line">[vagrant@server tmp]$ setfacl -m m::r-x passwd</span><br><span class="line">[vagrant@server tmp]$ getfacl passwd</span><br><span class="line"><span class="comment"># file: passwd</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">user:user1:rwx            <span class="comment">#effective:r-x</span></span><br><span class="line">user:user2:r-x</span><br><span class="line">group::r--</span><br><span class="line">group:user3:rwx            <span class="comment">#effective:r-x</span></span><br><span class="line">group:user4:r-x</span><br><span class="line">mask::r-x</span><br><span class="line">other::r--</span><br></pre></td></tr></table></figure><h2 id="設定檔案建立時的預設-ACL-權限-d"><a href="#設定檔案建立時的預設-ACL-權限-d" class="headerlink" title="設定檔案建立時的預設 ACL 權限(d)"></a>設定檔案建立時的預設 ACL 權限(<code>d</code>)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ mkdir ABC</span><br><span class="line">[vagrant@server tmp]$ chmod 777 ABC/</span><br><span class="line">[vagrant@server tmp]$ touch ABC/file.txt</span><br><span class="line">[vagrant@server tmp]$ getfacl ABC/file.txt</span><br><span class="line"><span class="comment"># file: ABC/file.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">group::rw-</span><br><span class="line">other::r--</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ setfacl -m d:u:user1:rwx ABC/</span><br><span class="line">[vagrant@server tmp]$ touch ABC/file2.txt</span><br><span class="line">[vagrant@server tmp]$ getfacl ABC/file2.txt</span><br><span class="line"><span class="comment"># file: ABC/file2.txt</span></span><br><span class="line"><span class="comment"># owner: vagrant</span></span><br><span class="line"><span class="comment"># group: vagrant</span></span><br><span class="line">user::rw-</span><br><span class="line">user:user1:rwx            <span class="comment">#effective:rw-</span></span><br><span class="line">group::rwx            <span class="comment">#effective:rw-</span></span><br><span class="line">mask::rw-</span><br><span class="line">other::rw-</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ ls -l ABC/</span><br><span class="line">total 4</span><br><span class="line">-rw-rw-rw-+ 1 vagrant vagrant 0 Feb 23 03:50 file2.txt</span><br><span class="line">-rw-rw-r--. 1 vagrant vagrant 0 Feb 23 03:50 file.txt</span><br></pre></td></tr></table></figure><h2 id="遞迴設定-ACL-權限-R-X"><a href="#遞迴設定-ACL-權限-R-X" class="headerlink" title="遞迴設定 ACL 權限(-R + X)"></a>遞迴設定 ACL 權限(-R + X)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 大寫 X =&gt; 表示 user1 對於檔案沒有 exec 的權限，但目錄則有 exec 的權限(才可瀏覽)</span></span><br><span class="line"><span class="comment"># -R =&gt; recusive</span></span><br><span class="line">$ setfacl -R -m u:user1:rX /dir</span><br></pre></td></tr></table></figure><h2 id="移除-ACL-權限-x"><a href="#移除-ACL-權限-x" class="headerlink" title="移除 ACL 權限(-x)"></a>移除 ACL 權限(<code>-x</code>)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ getfacl acl_clone.txt</span><br><span class="line"><span class="comment"># file: acl_clone.txt</span></span><br><span class="line"><span class="comment"># owner: student</span></span><br><span class="line"><span class="comment"># group: student</span></span><br><span class="line">user::rw-</span><br><span class="line">user:user1:r-x</span><br><span class="line">group::rw-</span><br><span class="line">group:group1:rw-</span><br><span class="line">group:group2:r-x</span><br><span class="line">mask::rwx</span><br><span class="line">other::---</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 -x 參數，指定移除 user1 &amp; group2 的權限</span></span><br><span class="line">$ setfacl -x u:user1,g:group2 acl_clone.txt</span><br><span class="line">$ getfacl acl_clone.txt</span><br><span class="line"><span class="comment"># file: acl_clone.txt</span></span><br><span class="line"><span class="comment"># owner: student</span></span><br><span class="line"><span class="comment"># group: student</span></span><br><span class="line">user::rw-</span><br><span class="line">group::rw-</span><br><span class="line">group:group1:rw-</span><br><span class="line">mask::rw-</span><br><span class="line">other::---</span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Using-ACLs-to-Grant-and-Limit-Access"><a href="#Practice-Using-ACLs-to-Grant-and-Limit-Access" class="headerlink" title="Practice: Using ACLs to Grant and Limit Access"></a>Practice: Using ACLs to Grant and Limit Access</h1><p>實作結果：</p><ol><li>讓 <strong>sodor</strong> group 與 <strong>controller</strong> group 在 <strong>/shares/steamies</strong> 目錄有相同的權限，但 user <strong>james</strong> 則是例外沒有任何權限</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 確認 controller 的 ACL 權限</span></span><br><span class="line">[student@server0 ~]$ sudo getfacl /shares/steamies/</span><br><span class="line">getfacl: Removing leading <span class="string">&#x27;/&#x27;</span> from absolute path names</span><br><span class="line"><span class="comment"># file: shares/steamies/</span></span><br><span class="line"><span class="comment"># owner: root</span></span><br><span class="line"><span class="comment"># group: controller</span></span><br><span class="line"><span class="comment"># flags: -s-</span></span><br><span class="line">user::rwx</span><br><span class="line">group::rwx</span><br><span class="line">other::---</span><br><span class="line">default:user::rwx</span><br><span class="line">default:group::rwx</span><br><span class="line">default:other::---</span><br><span class="line"></span><br><span class="line"><span class="comment"># 讓 sodor group 與 controller group 有相同的權限</span></span><br><span class="line">[student@server0 ~]$ sudo setfacl -Rm g:sodor:rwX /shares/steamies</span><br><span class="line">[student@server0 ~]$ sudo getfacl /shares/steamies/</span><br><span class="line">getfacl: Removing leading <span class="string">&#x27;/&#x27;</span> from absolute path names</span><br><span class="line"><span class="comment"># file: shares/steamies/</span></span><br><span class="line"><span class="comment"># owner: root</span></span><br><span class="line"><span class="comment"># group: controller</span></span><br><span class="line"><span class="comment"># flags: -s-</span></span><br><span class="line">user::rwx</span><br><span class="line">group::rwx</span><br><span class="line">group:sodor:rwx</span><br><span class="line">mask::rwx</span><br><span class="line">other::---</span><br><span class="line">default:user::rwx</span><br><span class="line">default:group::rwx</span><br><span class="line">default:other::---</span><br><span class="line"></span><br><span class="line"><span class="comment"># 讓 user james 沒有任何權限</span></span><br><span class="line">[student@server0 ~]$ sudo setfacl -Rm u:james:- /shares/steamies</span><br><span class="line">[student@server0 ~]$ sudo getfacl /shares/steamies/</span><br><span class="line">getfacl: Removing leading <span class="string">&#x27;/&#x27;</span> from absolute path names</span><br><span class="line"><span class="comment"># file: shares/steamies/</span></span><br><span class="line"><span class="comment"># owner: root</span></span><br><span class="line"><span class="comment"># group: controller</span></span><br><span class="line"><span class="comment"># flags: -s-</span></span><br><span class="line">user::rwx</span><br><span class="line">user:james:---</span><br><span class="line">group::rwx</span><br><span class="line">group:sodor:rwx</span><br><span class="line">mask::rwx</span><br><span class="line">other::---</span><br><span class="line">default:user::rwx</span><br><span class="line">default:group::rwx</span><br><span class="line">default:other::---</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>/shares/steamies</strong> 目錄底下的現有檔案都需要設定成上面的 ACL 權限</li></ol><blockquote><p>因為上面已經使用 <code>-R</code> 參數，因此這個部分已經完成</p></blockquote><ol start="3"><li>新增的目錄 &amp; 檔案也會有相同的 ACL 權限</li></ol><p>表示 <strong>sodor</strong> group 還是會有 rwx 權限，<strong>james</strong> user 也是同樣沒權限：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[student@server0 ~]$ sudo setfacl -m d:g:sodor:rwx /shares/steamies</span><br><span class="line">[student@server0 ~]$ sudo setfacl -m d:u:james:- /shares/steamies</span><br><span class="line">[student@server0 ~]$ sudo getfacl /shares/steamies</span><br><span class="line">getfacl: Removing leading <span class="string">&#x27;/&#x27;</span> from absolute path names</span><br><span class="line"><span class="comment"># file: shares/steamies</span></span><br><span class="line"><span class="comment"># owner: root</span></span><br><span class="line"><span class="comment"># group: controller</span></span><br><span class="line"><span class="comment"># flags: -s-</span></span><br><span class="line">user::rwx</span><br><span class="line">user:james:---</span><br><span class="line">group::rwx</span><br><span class="line">group:sodor:rwx</span><br><span class="line">mask::rwx</span><br><span class="line">other::---</span><br><span class="line">default:user::rwx</span><br><span class="line">default:user:james:---</span><br><span class="line">default:group::rwx</span><br><span class="line">default:group:sodor:rwx</span><br><span class="line">default:mask::rwx</span><br><span class="line">default:other::---</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 5 Managing Priority of Linux Processes 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH05_ManagingPriorityOfLinuxProcesses/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH05_ManagingPriorityOfLinuxProcesses/</url>
      
        <content type="html"><![CDATA[<a name="ch5.1" />5.1 Process Priority and "nice" Concepts========================================<p>nice 值範圍 <code>-20</code> ~ <code>19</code>，沒指定預設就是 0，愈小表示 priority 愈高</p><blockquote><p>root 可以調大 or 調小 nice value，但一般使用者只能調大</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加上參數 -l 可取得 nice value(NI)</span></span><br><span class="line">[vagrant@server ~]$ ps -l</span><br><span class="line">F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD</span><br><span class="line">0 S  1000 11502 11501  0  80   0 - 28845 <span class="built_in">wait</span>   pts/0    00:00:00 bash</span><br><span class="line">0 R  1000 11572 11502  0  80   0 - 34343 -      pts/0    00:00:00 ps</span><br></pre></td></tr></table></figure><blockquote><p>PR 20 = nice 0</p></blockquote><hr><a name="ch5.2" />5.2 Using nice and renice to Influence Process Priority=======================================================<p>★★★★★★★★★ Very Important! ★★★★★★★★★ (start)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># o format =&gt; 使用者自訂顯示欄位</span></span><br><span class="line">$ ps axo pid,comm,nice --sort=-nice</span><br><span class="line"> PID COMMAND          NI</span><br><span class="line">   26 khugepaged       19</span><br><span class="line">  476 alsactl          19</span><br><span class="line">   25 ksmd              5</span><br><span class="line">  537 rtkit-daemon      1</span><br><span class="line">    1 systemd           0</span><br><span class="line">......</span><br><span class="line">   11 rcuos/0           0</span><br><span class="line">   12 watchdog/0        -</span><br><span class="line">......</span><br><span class="line">29921 ps                0</span><br><span class="line">  449 auditd           -4</span><br><span class="line">  467 sedispatch       -4</span><br><span class="line">  462 audispd          -8</span><br><span class="line"> 1583 pulseaudio      -11</span><br><span class="line">    5 kworker/0:0H    -20</span><br><span class="line">.......</span><br></pre></td></tr></table></figure><p>★★★★★★★★★ Very Important! ★★★★★★★★★ (end)</p><p>啟動程式時指定 nice 值：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nice 示範</span></span><br><span class="line">$ nice -n 10 top</span><br><span class="line">$ ps -al</span><br><span class="line">F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD</span><br><span class="line">0 S  1000 11626 11502  0  90  10 - 36537 poll_s pts/0    00:00:00 top</span><br><span class="line">0 R  1000 11627 11604  0  80   0 - 34343 -      pts/1    00:00:00 ps</span><br><span class="line"></span><br><span class="line"><span class="comment"># renice 示範</span></span><br><span class="line">$ renice -n 15 11604</span><br><span class="line">11604 (process ID) old priority 0, new priority 15</span><br><span class="line">$ ps -al</span><br><span class="line">F S   UID   PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY          TIME CMD</span><br><span class="line">0 S  1000 11626 11502  0  90  10 - 36537 poll_s pts/0    00:00:00 top</span><br><span class="line">0 R  1000 11630 11604  0  95  15 - 34343 -      pts/1    00:00:00 ps</span><br></pre></td></tr></table></figure><hr><h1 id="補充：Hash-Algorithm-特性"><a href="#補充：Hash-Algorithm-特性" class="headerlink" title="補充：Hash Algorithm 特性"></a>補充：Hash Algorithm 特性</h1><ol><li><p>計算的資料來源沒有長度限制</p></li><li><p>雜湊長度永遠固定</p></li><li><p>雜湊演算法為單向運算</p></li><li><p>不同的資料來源不會產出相同的雜湊值</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sha1sum &#x2F;etc&#x2F;passwd</span><br><span class="line">524dab4b600e8d87cb7aa5b21e4d57f8996a4374  &#x2F;etc&#x2F;passwd</span><br><span class="line"></span><br><span class="line">$ echo &quot;1234&quot; | sha1sum</span><br><span class="line">1be168ff837f043bde17c0314341c84271047b31  -</span><br></pre></td></tr></table></figure><hr><h1 id="Practice-Discovery-Process-Properties"><a href="#Practice-Discovery-Process-Properties" class="headerlink" title="Practice: Discovery Process Properties"></a>Practice: Discovery Process Properties</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根據 cpu 的核心數，執行兩倍數量的運算工作</span></span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> $( seq $(($(grep -c &#x27;^processor&#x27; /proc/cpuinfo) * <span class="number">2</span>)) ); <span class="keyword">do</span> sha1sum /dev/zero &amp; <span class="keyword">done</span></span><br><span class="line">[1] 31701</span><br><span class="line">[2] 31702</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">jobs</span></span><br><span class="line">[1]-  Running                 sha1sum /dev/zero &amp;</span><br><span class="line">[2]+  Running                 sha1sum /dev/zero &amp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用參數 u 可檢視資源使用率</span></span><br><span class="line">$ ps u $(pgrep sha1sum)</span><br><span class="line">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">student  31701 50.0  0.0 116096  1044 pts/0    R    14:26   1:29 sha1sum /dev/zero</span><br><span class="line">student  31702 50.0  0.0 116096  1044 pts/0    R    14:26   1:29 sha1sum /dev/zero</span><br><span class="line"></span><br><span class="line">$ nice -n 10 sha1sum /dev/zero &amp;</span><br><span class="line">[3] 31764</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過參數 o 指定顯示欄位 (nice value = 10 的 process 所佔的資源相對低)</span></span><br><span class="line">$ ps o pid,pcpu,nice,comm $(pgrep sha1sum)</span><br><span class="line">  PID %CPU  NI COMMAND</span><br><span class="line">31701 49.8   0 sha1sum</span><br><span class="line">31702 49.8   0 sha1sum</span><br><span class="line">31764  4.0  10 sha1sum</span><br><span class="line"></span><br><span class="line"><span class="comment"># 調整 process nice value，可看出資源使用率有提升</span></span><br><span class="line">$ sudo renice -n 5 31764</span><br><span class="line">$ ps o pid,pcpu,nice,comm $(pgrep sha1sum)</span><br><span class="line">  PID %CPU  NI COMMAND</span><br><span class="line">31701 48.8   0 sha1sum</span><br><span class="line">31702 48.8   0 sha1sum</span><br><span class="line">31764  6.2   5 sha1sum</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除所有使用 sha1sum 指令產生的 process</span></span><br><span class="line">$ killall sha1sum</span><br><span class="line">$ ps o pid,pcpu,nice,comm $(pgrep sha1sum)</span><br><span class="line">  PID %CPU  NI COMMAND</span><br><span class="line"> 2052  0.0   0 bash</span><br><span class="line">31825  0.0   0 ps</span><br><span class="line">[1]   Terminated              sha1sum /dev/zero</span><br><span class="line">[2]-  Terminated              sha1sum /dev/zero</span><br><span class="line">[3]+  Terminated              nice -n 10 sha1sum /dev/zero</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 4 Scheduling Future Linux Tasks 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH04_SchedulingFutureLinuxTasks/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH04_SchedulingFutureLinuxTasks/</url>
      
        <content type="html"><![CDATA[<a name="ch4.1" />4.1 Scheduling One-Time Tasks with at=====================================<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ at now+5minutes</span><br><span class="line">at&gt; cp /etc/yum.conf /tmp</span><br><span class="line">at&gt; &lt;EOT&gt;</span><br><span class="line">job 1 at Mon Feb 22 08:23:00 2016</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看目前的排程</span></span><br><span class="line">$ atq</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看指定排程的內容</span></span><br><span class="line">$ at -c 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刪除</span></span><br><span class="line">$ atrm 1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ <span class="built_in">echo</span> <span class="string">&quot;cp /etc/passwd /tmp&quot;</span> &gt; file.txt</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ at now+10minutes &lt; file.txt</span><br><span class="line">job 2 at Mon Feb 22 08:31:00 2016</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ atq</span><br><span class="line">1    Mon Feb 22 08:23:00 2016 a vagrant</span><br><span class="line">2    Mon Feb 22 08:31:00 2016 a vagrant</span><br></pre></td></tr></table></figure><h3 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ <span class="built_in">echo</span> <span class="string">&quot;date &gt; ~/myjob&quot;</span> | at now+3min</span><br><span class="line">job 3 at Mon Feb 22 08:33:00 2016</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ atq</span><br><span class="line">3    Mon Feb 22 08:33:00 2016 a vagrant</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意 while 的條件式中左右各要帶一個 space</span></span><br><span class="line">[vagrant@server tmp]$ <span class="keyword">while</span> [ $(atq | wc -l) -gt 0 ]; <span class="keyword">do</span> sleep 1s; <span class="keyword">done</span></span><br><span class="line">[vagrant@server tmp]$ cat ~/myjob</span><br><span class="line">Mon Feb 22 08:33:00 UTC 2016</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定 queue</span></span><br><span class="line">[vagrant@server tmp]$ at -q g teatime tomorrow</span><br><span class="line">at&gt; touch ~/cookies</span><br><span class="line">at&gt; &lt;EOT&gt;</span><br><span class="line">job 4 at Tue Feb 23 16:00:00 2016</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ at -q b 16:05 tomorrow</span><br><span class="line">at&gt; touch ~/cookies</span><br><span class="line">at&gt; &lt;EOT&gt;</span><br><span class="line">job 5 at Tue Feb 23 16:05:00 2016</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ atq</span><br><span class="line">4    Tue Feb 23 16:00:00 2016 g vagrant</span><br><span class="line">5    Tue Feb 23 16:05:00 2016 b vagrant</span><br></pre></td></tr></table></figure><hr><a name="ch4.2" />4.2 Scheduling Recurring Jobs with cron (User cron)===================================================<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 透過 crontab -e 來設定 user cron jobs</span></span><br><span class="line">[vagrant@server tmp]$ crontab -e</span><br><span class="line">01 * * * *  ~/test.sh</span><br><span class="line">30 2 * * *  run-parts   ~/cron.d</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ sudo ls /var/spool/cron/</span><br><span class="line">vagrant</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 root 身份檢視 vagrant 使用者的 cron jobs</span></span><br><span class="line">[vagrant@server tmp]$ sudo crontab -u vagrant -l</span><br><span class="line">01 * * * *  ~/test.sh</span><br><span class="line">30 2 * * *  run-parts   ~/cron.d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 vagrant 的 user cron jobs</span></span><br><span class="line">[vagrant@server tmp]$ sudo crontab -u vagrant -r</span><br><span class="line">[vagrant@server tmp]$ sudo crontab -u vagrant -l</span><br><span class="line">no crontab <span class="keyword">for</span> vagrant</span><br></pre></td></tr></table></figure><h3 id="Practice-1"><a href="#Practice-1" class="headerlink" title="Practice"></a>Practice</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ crontab -e</span><br><span class="line">*/2 9-16 * * 1-5 date &gt;&gt; /home/vagrant/my_first_cron_job</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ crontab -l</span><br><span class="line">*/2 9-16 * * 1-5 date &gt;&gt; /home/vagrant/my_first_cron_job</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ cat ~/my_first_cron_job</span><br><span class="line">Mon Feb 22 09:12:01 UTC 2016</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ crontab -r</span><br><span class="line">[vagrant@server ~]$ crontab -l</span><br><span class="line">no crontab <span class="keyword">for</span> vagrant</span><br></pre></td></tr></table></figure><hr><a name="ch4.3" />4.3 Scheduling System cron Jobs===============================<p>cron job 設定存在於 <code>/etc/crontab</code> 中，可透過 <code>man 5 crontab</code> 查詢格式 &amp; 設定方式</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每個小時的第 1 分鐘</span></span><br><span class="line">01 * * * * *    root  /tmp/aa.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 星期天 02:01</span></span><br><span class="line">01 2 * * * 0    vagrant   /tmp/bb.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每個小時的第 2 分鐘，執行指定目錄中的所有檔案</span></span><br><span class="line">02 * * * * *    root  run-parts   /root/cron.d</span><br></pre></td></tr></table></figure><blockquote><p>指定的檔案都必須要有 <strong>execute</strong>(chmod +x) 的權限才會正確執行</p></blockquote><p><code>/etc/anacrontab</code> &amp; <code>/var/spool/anacron</code>(目錄)：用來處理未執行的 cron job</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ sudo cat /etc/anacrontab</span><br><span class="line"><span class="comment"># /etc/anacrontab: configuration file for anacron</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># See anacron(8) and anacrontab(5) for details.</span></span><br><span class="line"></span><br><span class="line">SHELL=/bin/sh</span><br><span class="line">PATH=/sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">MAILTO=root</span><br><span class="line"><span class="comment"># the maximal random delay added to the base delay of the jobs</span></span><br><span class="line">RANDOM_DELAY=45</span><br><span class="line"><span class="comment"># the jobs will be started during the following hours only</span></span><br><span class="line">START_HOURS_RANGE=3-22</span><br><span class="line"></span><br><span class="line"><span class="comment">#period in days   delay in minutes   job-identifier   command</span></span><br><span class="line">1    5    cron.daily        nice run-parts /etc/cron.daily</span><br><span class="line">7    25    cron.weekly        nice run-parts /etc/cron.weekly</span><br><span class="line">@monthly 45    cron.monthly        nice run-parts /etc/cron.monthly</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過比較日期的方式，判斷有哪些該執行的 cron job 未執行</span></span><br><span class="line">[vagrant@server tmp]$ ls /var/spool/anacron/</span><br><span class="line">cron.daily  cron.monthly  cron.weekly</span><br><span class="line">[vagrant@server tmp]$ sudo cat /var/spool/anacron/cron.daily</span><br><span class="line">20160222</span><br><span class="line">[vagrant@server tmp]$ sudo cat /var/spool/anacron/cron.monthly</span><br><span class="line">20160222</span><br><span class="line">[vagrant@server tmp]$ sudo cat /var/spool/anacron/cron.weekly</span><br><span class="line">20160222</span><br></pre></td></tr></table></figure><hr><a name="ch4.4" />4.4 Managing Temporary Files============================<p>從 RHEL 7 開始，<code>init</code> 已經被 <code>systemd</code> 所取代</p><p>在 RHEL 7 之前，暫存檔的監控 &amp; 移除由 <code>tmpwatch</code> 套件來管理；但在 RHEL 7，systemd 提供了一個稱為 <code>systemd-tmpfiles</code> 的服務來監控 &amp; 管理所指定的目錄</p><p>透過 <code>stat filename</code> 可以檢視 inode 的內容，與 systemd-tmpfiles 相關的為 <code>Access Time</code>(atime)、<code>Modify Time</code>(mtime)、以及 <code>Change Time</code>(ctime)</p><h3 id="4-4-1-Managing-temporary-files-with-systemd-tmpfiles"><a href="#4-4-1-Managing-temporary-files-with-systemd-tmpfiles" class="headerlink" title="4.4.1 Managing temporary files with systemd-tmpfiles"></a>4.4.1 Managing temporary files with systemd-tmpfiles</h3><p><code>systemd-tmpfiles</code> 的功能在於定期(並非依賴 system cron，而是透過自身的 Timer 機制)的清除指定的目錄內容，或是恢復指定監控的目錄下被務刪的檔案，以下是設定範例(<code>/usr/lib/systemd/system/systemd-tmpfiles-clean.timer</code>)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Timer]</span><br><span class="line">OnBootSec&#x3D;15min   # 開機後的 15 分鐘執行一次</span><br><span class="line">OnUnitActiveSec&#x3D;1d  # 之後每天執行一次</span><br></pre></td></tr></table></figure><p><code>systemd-tmpfiles</code> 的設定檔有 3 個地方：</p><ul><li><p>/etc/tmpfiles.d/*.conf</p></li><li><p>/run/tmpfiles.d/*.conf</p></li><li><p>/usr/lib/tmpfiles.d/*.conf</p></li></ul><blockquote><p>下面兩個是屬於預設的設定檔，建議從下面兩個複製到第一個目錄後再修改，因為系統讀取到第一個目錄中有設定後，就不會執行下面兩個目錄的設定檔</p></blockquote><p>以上設定表示 <code>systemd-tmpfiles-clean.service</code> 會在 systemd 啟動後的 15 分鐘後啟動執行，並在每 24 小時後重新執行一次，並根據上面三個目錄中的 <code>*.conf</code> 的設定，執行 <code>systemd-tmpfiles --clean</code> 來清除不需要的檔案(藉由比對檔案的 atime/mtime/ctime)。</p><p>以下是設定檔範例說明：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 若 /run/tuned 目錄不存在，則建立，user/group 皆為 root，權限為 0755</span></span><br><span class="line">d /run/tuned 0755 root root -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除 /var/run/lsm 目錄中的內容</span></span><br><span class="line">D /var/run/lsm 0755 libstoragemgmt libstoragemgmt -</span><br></pre></td></tr></table></figure><blockquote><p>tmpfiles.d 詳細的設定檔撰寫方式可參考 tmpfiles.d(5), systemd-tmpfiles(8) 等文件</p></blockquote><h3 id="補充"><a href="#補充" class="headerlink" title="補充"></a>補充</h3><p>RHEL 7 之前：standalone(daemon) service + xinetd(短暫式服務)</p><p>RHEL 7 之後：systemd service unit = service(對應到原先的 daemon) + socket(對應到原先的 xinetd) + path(監控目錄中檔案的變化，來決定執行的程式，例如：cups.path)</p><hr><h1 id="Practice-Managing-Temporary-Files"><a href="#Practice-Managing-Temporary-Files" class="headerlink" title="Practice: Managing Temporary Files"></a>Practice: Managing Temporary Files</h1><p>RHEL7 中，暫存檔由 <code>systemd-tmpfiles</code> 這個服務來進行管理。</p><p>將 /tmp 中的自動清除設定由 10 天改為 5 天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 複製 template</span></span><br><span class="line">$ sudo cp /usr/lib/tmpfiles.d/tmp.conf /etc/tmpfiles.d/</span><br><span class="line">$ cat /etc/tmpfiles.d/tmp.conf</span><br><span class="line">.......</span><br><span class="line">d /tmp 1777 root root 5d</span><br><span class="line">d /var/tmp 1777 root root 30d</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 /etc/tmpfiles.d/tmp.conf 檔案，將 10d 改為 5d</span></span><br><span class="line">$ sudo sed -i <span class="string">&#x27;/^d .tmp /s/10d/5d/&#x27;</span> /etc/tmpfiles.d/tmp.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試設定是否成功</span></span><br><span class="line">$ sudo systemd-tmpfiles --clean tmp.conf</span><br></pre></td></tr></table></figure><p>設定 <strong>/run/gallifrey</strong> 每 30 秒清空一次：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 編輯設定檔</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;d /run/gallifrey 0700 root root 30s&quot;</span> | sudo tee /etc/tmpfiles.d/gallifrey.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立規則</span></span><br><span class="line">$ sudo systemd-tmpfiles create gallifrey.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 測試是否有效</span></span><br><span class="line">$ sudo touch /run/gallifrey/helloworld</span><br><span class="line">$ sudo systemd-tmpfiles --clean gallifrey.conf</span><br><span class="line">$ sudo ls -l /run/gallifrey/</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 3 Creating and Editing Text Files with vim 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH03_CreatingAndEditingTextFilesWithVim/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH03_CreatingAndEditingTextFilesWithVim/</url>
      
        <content type="html"><![CDATA[<h1 id="3-3-Basic-vim-Workflow"><a href="#3-3-Basic-vim-Workflow" class="headerlink" title="3.3 Basic vim Workflow"></a>3.3 Basic vim Workflow</h1><h2 id="Editor-basics"><a href="#Editor-basics" class="headerlink" title="Editor basics"></a>Editor basics</h2><p><code>i</code>：進入 insert mode</p><p><code>I</code>：進入 insert mode，從游標所在行首插入新字元(<code>i</code> + <code>HOME</code>)</p><p><code>A</code>：<code>i</code> + <code>END</code></p><p><code>R</code>：replace mode，所輸入的會取代原本的內容</p><p><code>o</code>：在游標上方插入新的一行，並進入 insert mode</p><p><code>O</code>：在游標下方插入新的一行，並進入 insert mode</p><p><code>:n</code>：移到第 n 行</p><p><code>:$</code>：移到最後一行</p><p><code>u</code>：undo</p><p><code>Ctrl + r</code>： redo</p><p><code>w</code>(往後移動一個 word) &amp; <code>b</code>(往前移動一個 word)：可用 Ctrl + 左右鍵 取代</p><p><code>DEL</code> or <code>x</code>：刪除一個字元</p><p><code>20dd</code>：刪除 20 行</p><p><code>5yy</code>：複製 5 行</p><hr><h1 id="3-4-Editing-with-Vim"><a href="#3-4-Editing-with-Vim" class="headerlink" title="3.4 Editing with Vim"></a>3.4 Editing with Vim</h1><h2 id="Search-and-Replace"><a href="#Search-and-Replace" class="headerlink" title="Search and Replace"></a>Search and Replace</h2><p>簡單來說，就是在 vim 中使用 sed 的功能</p><p><code>:1,$s/the/*****/g</code>：將檔案中的 then 全部換成星號</p>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 1 Automating Installation with Kickstart 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH01_AutomatingInstallationWithKickstart/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH01_AutomatingInstallationWithKickstart/</url>
      
        <content type="html"><![CDATA[<p><code>dmesg</code> 觀察由 Linux Kernel 所產生的 log 檔</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 製作 USB 安裝裝置</span></span><br><span class="line"><span class="comment"># /dev/sr0 為光碟機裝置</span></span><br><span class="line"><span class="comment"># /dev/sdb1 為 USB 裝置</span></span><br><span class="line">dd <span class="keyword">if</span>=/dev/sr0 of=/dev/sdb1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 將 DVD 光碟轉成 iso 檔</span></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/sr0 of=/tmp/rhel_dvd.iso</span><br></pre></td></tr></table></figure><p>安裝 Linux 方式：</p><ul><li>DVD/USB</li><li>Hard Disk</li><li>Network (必須先安裝 YUM server)<ul><li>FTP</li><li>HTTP</li><li>NFS</li></ul></li><li>PXE boot (找 CentOS PXE server)</li></ul><p>Kickstart:</p><ol><li>使用 boot.iso 開機</li><li>準備好 <code>ks.cfg</code>(安裝中需要的設定參數)</li><li>會進入到 <code>Boot:</code>，並輸入 <code>Boot: linux ks=floppy</code>，接著程式就會去找 <strong><font color='red'>ks.cfg</font></strong> 並開始安裝</li></ol><blockquote><p>ks.cfg 也可以放置於其他地方，不一定要放在磁碟片中</p></blockquote><a name="ch1.1" />1.1 Defining the Anaconda Kickstart System==========================================<h2 id="1-1-1-Introduction-to-Kickstart-installations"><a href="#1-1-1-Introduction-to-Kickstart-installations" class="headerlink" title="1.1.1 Introduction to Kickstart installations"></a>1.1.1 Introduction to Kickstart installations</h2><p>每個 seciton 由 <code>%</code> 開頭，並用 <code>%end</code> 結尾</p><p><code>%package</code> section 指定要所安裝的軟體</p><p><code>@</code> 開頭的設定表示指定 <code>package group</code>，可指定安裝 RedHat 預先設定的軟體群組，例如 core、Web Server …. 等等</p><p><code>@^</code>開頭表示指定 <code>enrironmental group</code>(group in package group)</p><p>其他客製化的需求可以放到 <code>%pre</code> &amp; <code>%post</code> script 中</p><h2 id="1-1-2-Kickstart-configuration-file-commands"><a href="#1-1-2-Kickstart-configuration-file-commands" class="headerlink" title="1.1.2 Kickstart configuration file commands"></a>1.1.2 Kickstart configuration file commands</h2><h3 id="Installation-commands"><a href="#Installation-commands" class="headerlink" title="Installation commands:"></a>Installation commands:</h3><ul><li><p><code>url</code>：用來指定安裝媒體的位置(FTP/HTTP/NFS …. etc)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 --url 指定來源</span></span><br><span class="line">url --url=<span class="string">&quot;ftp://installserver.example.com/pub/RHEL7/dvd&quot;</span></span><br></pre></td></tr></table></figure></li><li><p><code>repo</code>：用來指定要額外安裝的 package repository</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --name 指定名稱，--baseurl 指定 repository 位置</span></span><br><span class="line">repo --name=<span class="string">&quot;Custom Packages&quot;</span> --baseurl=<span class="string">&quot;ftp://repo.example.com/custom&quot;</span></span><br></pre></td></tr></table></figure></li><li><p><code>text</code>：預設以圖形模式顯示，用 text 可改成強制文字模式顯示</p></li><li><p><code>vnc</code>：設定 vnc 密碼</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vnc --password=redhat</span><br></pre></td></tr></table></figure></li></ul><h3 id="Partition-commands"><a href="#Partition-commands" class="headerlink" title="Partition commands"></a>Partition commands</h3><ul><li><p><code>clearpart</code>：安裝前清除硬碟上所有的 partition</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定清除 sda, sdb 兩顆硬碟</span></span><br><span class="line">clearpart --all --drivers=sda,sdb --initlabel</span><br></pre></td></tr></table></figure></li><li><p><code>part</code>：設定 partition 要如何分割</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 partition 目錄、檔案型態、大小....等資訊</span></span><br><span class="line">part /home --fstype=ext4 --label=homes --size=4096 --maxsize=8192 --grow</span><br></pre></td></tr></table></figure></li><li><p><code>ignoredisk</code>：安裝時忽略特定硬碟</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 忽略 sdc</span></span><br><span class="line">ignoredisk --drivers=sdc</span><br></pre></td></tr></table></figure></li><li><p><code>bootloader</code>：指定安裝 bootloader 的地方</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 sda 上的 mbr 位置安裝 bootloader</span></span><br><span class="line">bootloader --location=mbr --boot-driver=sda</span><br></pre></td></tr></table></figure></li><li><p><code>volgroup</code>, <code>logvol</code>：建立 LVM volume groups &amp; logical volumes</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">part pv.01 --size=8192</span><br><span class="line">volgroup myvg pv.01</span><br><span class="line">logvol / --vgname=myvg --fstype=xfs --size=2048 --name=rootvol -grow</span><br><span class="line">logvol /var --vgname=myvg --fstype=xfs --size=4096 --name=varvol</span><br></pre></td></tr></table></figure></li><li><p><code>zerombr</code>：清除原有的 mbr 設定</p></li></ul><h3 id="Network-commands"><a href="#Network-commands" class="headerlink" title="Network commands"></a>Network commands</h3><ul><li><p><code>network</code>：設定網路</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定 eth0 為 DHCP</span></span><br><span class="line">network --device=eth0 --bootproto=dhcp</span><br></pre></td></tr></table></figure></li><li><p><code>firewall</code>：設定防火牆，指定開啟(or 關閉)特定服務</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall --enabled --services=ssh,cups</span><br></pre></td></tr></table></figure></li><li><p><code>lang</code>：語系設定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lang en_US.UTF-8</span><br></pre></td></tr></table></figure></li><li><p><code>keyboard</code>：鍵盤設定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyboard --vckeymap=us --xlayouts=<span class="string">&#x27;us&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p><code>timezon</code>：設定時區、NTP server 以及是否使用 UTC</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 UTC, 指定 NTP server, 並設定時區</span></span><br><span class="line">timezon --utc --ntpservers=time.example.com Asia/Taipei</span><br></pre></td></tr></table></figure></li><li><p><code>auth</code>：認證方式的設定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 設定使用一般登入方式 &amp; 加密強度</span></span><br><span class="line">auth --useshadow --enablemd5 --passalgo=sha512</span><br></pre></td></tr></table></figure></li><li><p><code>rootpw</code>：設定 root 密碼</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以使用 &quot;--uscrypted&quot; 參數搭配加密過的密碼</span></span><br><span class="line">rootpwd --plaintext redhat</span><br></pre></td></tr></table></figure></li><li><p><code>selinux</code>：設定 SELinux 的狀態</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">selinux --enforcing</span><br></pre></td></tr></table></figure></li><li><p><code>services</code>：設定各種 service 的預設狀態</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">services --disabled=network,iptables,ip6tables</span><br></pre></td></tr></table></figure></li><li><p><code>group</code>, <code>user</code>：建立指定的群組與使用者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">group --name=admins --gid=10001</span><br><span class="line">user --name=jdoe --gecos=<span class="string">&quot;John Doe&quot;</span> --group=admins --password=changeme --plaintext</span><br></pre></td></tr></table></figure></li></ul><h3 id="Miscelaneous-commands"><a href="#Miscelaneous-commands" class="headerlink" title="Miscelaneous commands"></a>Miscelaneous commands</h3><ul><li><p><code>logging</code>：定義安裝時的 log 如何處理，可指定 remote logging server</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定 log level &amp; 要儲存的地方</span></span><br><span class="line">logging --host=loghost.example.com --level=INFO</span><br></pre></td></tr></table></figure></li><li><p><code>reboot</code>, <code>poweroff</code>, <code>halt</code>：系統安裝完執行的動作</p></li></ul><blockquote><p>一堆設定不用記，需要的時候再到 <a href="https://access.redhat.com/documentation">RedHat 官方網站</a> 查詢(Getting Started -&gt; Installation Guide)就好</p></blockquote><p>產生 ks.cfg 方式：</p><ol><li><p>透過 <code>system-config-kickstart</code> 可以透過圖形介面產生 ks.cfg</p><blockquote><p>必須在 /etc/yum.repos.d/rhel-dvd.repo 中要有 <code>rawhide</code> section 的設定，不然出來的圖形介面會沒有 package 可以選</p></blockquote></li><li><p>安裝好一台新的 RHEL，並找到 <code>/root/anaconda-ks.cfg</code> 檔案，拿出來用</p></li></ol><hr><a name="ch1.2" />1.2 Deploying a New Virtual System with Kickstart=================================================<p><code>ksvalidator</code> 可用來檢查 ks.cfg 的格式是否正確 (什麼都結果都沒有表示正確)</p><h3 id="Publish-the-Kickstart-configuration-file-to-Anaconda"><a href="#Publish-the-Kickstart-configuration-file-to-Anaconda" class="headerlink" title="Publish the Kickstart configuration file to Anaconda"></a>Publish the Kickstart configuration file to Anaconda</h3><p>ks.cfg 可以放在很多不同的地方：</p><ul><li><p>可放在 FTP/HTTP/NFS … 等服務上</p></li><li><p>DHCP/TFTP server</p></li><li><p>USB disk or CD-ROM</p></li><li><p>Local disk</p></li><li><p>與 PXE server 結合</p></li></ul><h2 id="補充說明"><a href="#補充說明" class="headerlink" title="補充說明"></a>補充說明</h2><h3 id="遠端安裝-scenario-1"><a href="#遠端安裝-scenario-1" class="headerlink" title="遠端安裝 scenario 1"></a>遠端安裝 scenario 1</h3><ul><li><p>client: private/puiblic IP</p></li><li><p>remote server: public IP</p></li></ul><p>在 remote server 端執行如下：(光碟開機 -&gt; ESC 跳到 boot 選項)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 光碟開機 -&gt; ESC 跳到 boot 選項：</span></span><br><span class="line">boot: linux vncpassword=redhat ip=172.25.0.11 netmask=255.255.255.0 gateway=172.25.0.254</span><br></pre></td></tr></table></figure><blockquote><p>以上 IP 組態設定會根據不同的地點而不同</p></blockquote><h3 id="遠端安裝-scenario-2"><a href="#遠端安裝-scenario-2" class="headerlink" title="遠端安裝 scenario 2"></a>遠端安裝 scenario 2</h3><ul><li><p>client: public IP</p></li><li><p>remote server: private IP</p></li></ul><p>在 client 端下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vncviewer --listen</span><br></pre></td></tr></table></figure><p>在 remote server 端執行如下：(光碟開機 -&gt; ESC 跳到 boot 選項)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boot: linux vnc vncconnect=172.25.254.250 ip=172.25.0.11 netmask=255.255.255.0 gateway=172.25.0.254 dns=8.8.8.8</span><br></pre></td></tr></table></figure><p>按下 Enter 後，client 會自動跑一個 VNC console 出來，並顯示 remote server 的安裝畫面。</p><blockquote><p>也可以通過 direct TCP port 5901 達成第一個方式</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE7] RH134 Chapter 2 Using Regular Expressions with grep 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH134-LearningNotes-CH02_UsingRegularExpressionsWithGrep/"/>
      <url>/blog/RHCE/RHCE7-RH134-LearningNotes-CH02_UsingRegularExpressionsWithGrep/</url>
      
        <content type="html"><![CDATA[<a name="ch2.2" />2.2 Matching Text with grep===========================<p><code>.</code>(單一任何字元) &amp; <code>\</code>(跳脫字元) 的用法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 尋找 rcx.d</span><br><span class="line">[vagrant@server etc]$ ls | grep &#39;rc.\.d&#39;</span><br><span class="line">rc0.d</span><br><span class="line">rc1.d</span><br><span class="line">....</span><br></pre></td></tr></table></figure><p><code>[]</code>(中括號，符合其中一個字元) &amp; <code>[^]</code> 的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server etc]$ ls | grep <span class="string">&#x27;rc[134]\.d&#x27;</span></span><br><span class="line">rc1.d</span><br><span class="line">rc3.d</span><br><span class="line">rc4.d</span><br><span class="line"></span><br><span class="line">[vagrant@server etc]$ ls | grep <span class="string">&#x27;rc[1-3]\.d&#x27;</span></span><br><span class="line">rc1.d</span><br><span class="line">rc2.d</span><br><span class="line">rc3.d</span><br><span class="line"></span><br><span class="line">[vagrant@server etc]$ ls | grep <span class="string">&#x27;rc[^1-5]\.d&#x27;</span></span><br><span class="line">rc0.d</span><br><span class="line">rc6.d</span><br></pre></td></tr></table></figure><p><code>*</code>(零個或多個前面的字元) &amp; <code>\+</code>(一個或多個前面的字元) &amp; <code>\?</code>(零個或一個前面的字元) 的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ cat file.txt</span><br><span class="line">ac</span><br><span class="line">abc</span><br><span class="line">abbc</span><br><span class="line">abbbc</span><br><span class="line">abbbbc</span><br><span class="line">abbbbbc</span><br><span class="line">abbbbbbc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到全部</span></span><br><span class="line">[vagrant@server tmp]$ cat file.txt | grep <span class="string">&#x27;ab*c&#x27;</span></span><br><span class="line">ac</span><br><span class="line">abc</span><br><span class="line">abbc</span><br><span class="line">abbbc</span><br><span class="line">abbbbc</span><br><span class="line">abbbbbc</span><br><span class="line">abbbbbbc</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ cat file.txt | grep <span class="string">&#x27;ab\+c&#x27;</span></span><br><span class="line">abc</span><br><span class="line">abbc</span><br><span class="line">abbbc</span><br><span class="line">abbbbc</span><br><span class="line">abbbbbc</span><br><span class="line">abbbbbbc</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ cat file.txt | grep <span class="string">&#x27;ab\?c&#x27;</span></span><br><span class="line">ac</span><br><span class="line">abc</span><br></pre></td></tr></table></figure><p><code>\&#123;i\&#125;</code>(i 個前面的字元) &amp; <code>\&#123;i,\&#125;</code>(大於等於 i 個前面的字元) &amp; <code>\&#123;i,j\&#125;</code>(i 到 j 的前面的字元) 的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ cat file.txt | grep <span class="string">&#x27;ab\&#123;3\&#125;c&#x27;</span></span><br><span class="line">abbbc</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ cat file.txt | grep <span class="string">&#x27;ab\&#123;3,\&#125;c&#x27;</span></span><br><span class="line">abbbc</span><br><span class="line">abbbbc</span><br><span class="line">abbbbbc</span><br><span class="line">abbbbbbc</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ cat file.txt | grep <span class="string">&#x27;ab\&#123;3,4\&#125;c&#x27;</span></span><br><span class="line">abbbc</span><br><span class="line">abbbbc</span><br></pre></td></tr></table></figure><h3 id="練習：尋找包含-IP-address-的行"><a href="#練習：尋找包含-IP-address-的行" class="headerlink" title="練習：尋找包含 IP address 的行"></a>練習：尋找包含 IP address 的行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 尋找 IP address</span></span><br><span class="line">[vagrant@server ~]$ ip addr | grep <span class="string">&#x27;[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;&#x27;</span></span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3</span><br><span class="line">    inet 172.25.25.11/24 brd 172.25.25.255 scope global enp0s8</span><br></pre></td></tr></table></figure><p><code>^</code>(一行的開頭) &amp; <code>$</code>(一行的結尾)的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ cat passwd | grep <span class="string">&#x27;root&#x27;</span></span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">operator:x:11:0:operator:/root:/sbin/nologin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 root 為開頭</span></span><br><span class="line">[vagrant@server tmp]$ cat passwd | grep <span class="string">&#x27;^root&#x27;</span></span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 bsah 為結尾</span></span><br><span class="line">[vagrant@server tmp]$ cat passwd | grep <span class="string">&#x27;bash$&#x27;</span></span><br><span class="line">root:x:0:0:root:/root:/bin/bash</span><br><span class="line">vagrant:x:1000:1000:vagrant:/home/vagrant:/bin/bash</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ cat ff</span><br><span class="line">aaaaa cataaa aaaa</span><br><span class="line">aaaaa cat aaaaa</span><br><span class="line">aaaaa aaaacat aaaa</span><br><span class="line"></span><br><span class="line">[vagrant@server tmp]$ cat ff | grep <span class="string">&#x27;\&lt;cat\&gt;&#x27;</span></span><br><span class="line">aaaaa cat aaaaa</span><br></pre></td></tr></table></figure><p><code>-v</code>：反向(<strong>顯示沒符合的</strong>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜尋沒有 root 的行</span></span><br><span class="line">[vagrant@server tmp]$ grep -v <span class="string">&#x27;root&#x27;</span> passwd</span><br><span class="line">bin:x:1:1:bin:/bin:/sbin/nologin</span><br><span class="line">daemon:x:2:2:daemon:/sbin:/sbin/nologin</span><br><span class="line">adm:x:3:4:adm:/var/adm:/sbin/nologin</span><br></pre></td></tr></table></figure><p><code>-n</code>：搜尋結果加上行號</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ grep -n <span class="string">&#x27;nologin&#x27;</span> passwd</span><br><span class="line">2:bin:x:1:1:bin:/bin:/sbin/nologin</span><br><span class="line">3:daemon:x:2:2:daemon:/sbin:/sbin/nologin</span><br><span class="line">4:adm:x:3:4:adm:/var/adm:/sbin/nologin</span><br></pre></td></tr></table></figure><p><code>-c</code>：列出符合條件的數量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 搜尋 /etc 有多少個目錄</span></span><br><span class="line">[vagrant@server tmp]$ sudo ls -lR /etc | grep -c <span class="string">&#x27;^d&#x27;</span></span><br><span class="line">179</span><br></pre></td></tr></table></figure><p><code>-l</code>：只列出符合條件的檔名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server etc]$ grep <span class="string">&#x27;password&#x27;</span> /etc/* 2&gt;/dev/null</span><br><span class="line">/etc/dnsmasq.conf:<span class="comment">#dhcp-option=encap:175, 191, pass     # iSCSI password</span></span><br><span class="line">/etc/login.defs:<span class="comment">#    PASS_MAX_DAYS    Maximum number of days a password may be used.</span></span><br><span class="line">/etc/login.defs:<span class="comment">#    PASS_MIN_DAYS    Minimum number of days allowed between password changes.</span></span><br><span class="line">....</span><br><span class="line">/etc/login.defs:<span class="comment"># Use SHA512 to encrypt password.</span></span><br><span class="line">/etc/services:shell           514/tcp         cmd             <span class="comment"># no passwords used</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">[vagrant@server etc]$ grep -l <span class="string">&#x27;password&#x27;</span> /etc/* 2&gt;/dev/null</span><br><span class="line">/etc/dnsmasq.conf</span><br><span class="line">/etc/login.defs</span><br><span class="line">/etc/services</span><br></pre></td></tr></table></figure><p><code>-r</code>：搜尋整個路徑下的檔案</p><p><code>-i</code>：不區分大小寫</p><p><code>-e</code>：可同時給多個搜尋條件</p><h3 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h3><ol><li><p><code>sed &#39;s/cat/dog/&#39; file.txt</code>：將檔案中每一行左邊的第一個 cat 換成 dog</p></li><li><p><code>sed &#39;s/cat/dog/gi&#39; file.txt</code>：同上，但全部一起置換，且不區分大小寫</p></li><li><p><code>sed &#39;s/[Cc]at/dog/gi&#39; file.txt</code>：Cat or cat 都會置換</p></li><li><p><code>sed &#39;s/\&lt;[Cc]at\&gt;/dog/gi&#39; file.txt</code>：只有精準的 Cat or cat 會被置換</p></li><li><p><code>sed &#39;1,30s/cat/dog/gi&#39; file.txt</code>：同 2，但僅處理 1~30 行</p></li><li><p><code>sed &#39;/begin/,/end/s/cat/dog/gi&#39; file.txt</code>：同 2，但僅處理 begin 開頭的行到 end 開頭的行</p></li><li><p><code>sed -e &#39;s/cat/dog/g&#39; -e &#39;s/Cat/dog/g&#39; file.txt</code>：同時給多個條件</p></li><li><p><code>set &#39;/^root/d&#39; file.txt</code>：開頭為 root 的行刪除</p></li><li><p><code>set &#39;/^root/!d&#39; file.txt</code>：開頭為 root 的行不刪除</p></li></ol><blockquote><p>加上 <strong><font color='red'>-i</font></strong> 參數會將實際的改變反應到檔案中(原本預設是不會變更檔案內容)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH134 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 14 Accessing Linux File Systems 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH14_AccessingLinuxFileSystems/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH14_AccessingLinuxFileSystems/</url>
      
        <content type="html"><![CDATA[<a name="ch14.1" />14.1 Identifying File Systems and Devices=========================================<p>常用指令：</p><ul><li><code>sudo du -h / --max-depth=1 2&gt;/dev/null | sort -h</code>：檢查 root directory 每個目錄所使用的容量</li></ul><hr><a name="ch14.2" />14.2 Mounting and Unmounting File Systems=========================================<p>常用指令：</p><ul><li><p><code>blkid</code>：顯示所有 block device 資訊</p></li><li><p><code>mount source_device destination_dir</code>：透過 device name 掛載</p></li><li><p><code>mount UUID destination_dir</code>：透過 UUID 掛載</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢機器上的 block device</span></span><br><span class="line">$ sudo blkid</span><br><span class="line">/dev/vda1: UUID=<span class="string">&quot;9bf6b9f7-92ad-441b-848e-0257cbb883d1&quot;</span> TYPE=<span class="string">&quot;xfs&quot;</span> </span><br><span class="line">/dev/vdb1: UUID=<span class="string">&quot;bffdaa4a-34f2-4a74-8455-a11aca40a6e1&quot;</span> TYPE=<span class="string">&quot;xfs&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定 UUID 掛載 block device</span></span><br><span class="line">$ sudo mkdir /mnt/newspace &amp;&amp; sudo mount UUID=<span class="string">&quot;bffdaa4a-34f2-4a74-8455-a11aca40a6e1&quot;</span> /mnt/newspace</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸離 block device</span></span><br><span class="line">$ sudo umount /mnt/newspace</span><br></pre></td></tr></table></figure><hr><a name="ch14.3" />14.3 Making Links Between Files===============================<h2 id="Partition-UUID-amp-inode"><a href="#Partition-UUID-amp-inode" class="headerlink" title="Partition UUID &amp; inode"></a>Partition UUID &amp; inode</h2><blockquote><p>UUID 存於 super block 中</p></blockquote><p>inode block -&gt; inode table 結構：</p><ol><li>inode number</li><li>Permission</li><li>Hard Link Subdirectory 數量</li><li>UID</li><li>GID</li><li>Size</li><li>Timestamp</li><li>Filename</li><li>Pointer</li></ol><h2 id="Hard-Link"><a href="#Hard-Link" class="headerlink" title="Hard Link"></a>Hard Link</h2><p><strong><font color='red'>inode 在 Linux 中是真正指向檔案實際內容的指標</font></strong></p><p>透過 <code>ln</code> 可建立 Hard Link，這是個指向 inode 的連結</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 產生檔案</span></span><br><span class="line">[student@server0 ~]$ <span class="built_in">echo</span> <span class="string">&quot;Hello World&quot;</span> &gt; newfile.txt</span><br><span class="line">[student@server0 ~]$ ls -l</span><br><span class="line">total 4</span><br><span class="line">-rw-rw-r--. 1 student student 12  4月 28 15:05 newfile.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立 hard link (注意數字從 1 變成 2, inode number 相同)</span></span><br><span class="line">[student@server0 ~]$ ln newfile.txt ~/newfile-hlink.txt</span><br><span class="line">[student@server0 ~]$ ls -li</span><br><span class="line">total 8</span><br><span class="line">12889 -rw-rw-r--. 2 student student 12  4月 28 15:05 newfile-hlink.txt</span><br><span class="line">12889 -rw-rw-r--. 2 student student 12  4月 28 15:05 newfile.txt</span><br></pre></td></tr></table></figure><p>Hard Link 特性 &amp; 說明：</p><ul><li>上面建立 Hard Link 的示範，可看出指向同一個 inode 的連結，從一個變成兩個(可防止檔案誤刪)</li><li>增加 hard link 不會增加磁碟空間</li><li>不能跨 File System</li><li>不能 link 目錄，只能建立在檔案上</li></ul><h2 id="Symbolic-Link"><a href="#Symbolic-Link" class="headerlink" title="Symbolic Link"></a>Symbolic Link</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 symbolic link</span></span><br><span class="line">$ ln -s newfile.txt ~/newfile-symlink.txt</span><br><span class="line">$ ls -l</span><br><span class="line">total 8</span><br><span class="line">-rw-rw-r--. 2 student student 12  4月 28 15:05 newfile-hlink.txt</span><br><span class="line">lrwxrwxrwx. 1 student student 11  4月 28 15:19 newfile-symlink.txt -&gt; newfile.txt</span><br><span class="line">-rw-rw-r--. 2 student student 12  4月 28 15:05 newfile.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除 symbolic link 指向的檔案(系統會標示連結失效)</span></span><br><span class="line">$ rm newfile.txt </span><br><span class="line">$ ls -l</span><br><span class="line">total 4</span><br><span class="line">-rw-rw-r--. 1 student student 12  4月 28 15:05 newfile-hlink.txt</span><br><span class="line">lrwxrwxrwx. 1 student student 11  4月 28 15:19 newfile-symlink.txt -&gt; newfile.txt (這裡會有底色標記連結失效)</span><br><span class="line"></span><br><span class="line"><span class="comment"># link 目錄</span></span><br><span class="line">$ ln -s /etc ~/config_files</span><br></pre></td></tr></table></figure><p>特色：</p><ol><li>類似捷徑</li><li>不能防止檔案誤刪</li></ol><hr><a name="ch14.4" />14.4 Locating Files on the System=================================<h2 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h2><p>要使用 locate 之前必須先執行 <code>sudo updatedb</code>，才會有檔案資料庫可用，若要搜尋最新的檔案，也必須要執行 updatedb</p><ul><li><p><code>sudo locate passwd</code>：尋找檔名為 passwd 的檔案</p></li><li><p><code>sudo locate -n 5 passwd</code>：同上，但只列出 5 筆資料</p></li><li><p><code>sudo locate -i messages</code>：以 case-insensitive 的方式搜尋</p></li></ul><h2 id="find"><a href="#find" class="headerlink" title="find"></a>find</h2><p>即時搜尋，可找到剛新增的檔案</p><ul><li><p><code>sudo find / -name sshd_config</code>：搜尋檔名為 sshd_config 的檔案</p></li><li><p><code>sudo find / -name &#39;*.txt&#39;</code>：在 / 目錄下尋找副檔名為 txt 的檔案</p></li><li><p><code>sudo find / -iname &#39;*messages*&#39;</code>：在 / 目錄下以 case-insensitive 的方式檔名尋找 <em>messages</em> 的檔案</p></li><li><p><code>sudo find -user student</code>：尋找 /home 目錄中，user 為 student 的檔案</p></li><li><p><code>sudo find -group student</code>：尋找 /home 目錄中，group 為 student 的檔案</p></li><li><p><code>sudo find / -user root -group mail</code>：在 / 目錄中尋找 user=root, group=mail 的檔案</p></li><li><p><code>sudo find /home -perm 764</code>：尋找 /home 中 permission=764 檔案</p></li><li><p><code>sudo find /home -perm -324</code>：尋找 /home 中，**<font color='red'>至少</font>**有指定權限的檔案</p></li><li><p><code>sudo find /home -perm /442</code>：尋找 /home 中，user(read)/group(read)/others(write) 至少其中一個符合指定權限的檔案</p></li><li><p><code>sudo find / -perm /7000</code>：搜尋檔案當中含有 SGID 或 SUID 或 SBIT 的屬性</p></li><li><p><code>sudo find /run -type s</code>：找出 /run 目錄中，檔案類型為 Socket 的檔名有哪些</p><blockquote><p>type 選項可以有 f(一般檔案) / d(目錄) / l(symbolic link) / b(block device)</p></blockquote></li><li><p><code>sudo find -size -10M</code>：尋找小於 10MB 的檔案</p></li><li><p><code>sudo find / -type f -links +1</code>：尋找擁有超過 1 個 hard link 的一般檔案</p></li></ul><h3 id="find-的特別功能"><a href="#find-的特別功能" class="headerlink" title="find 的特別功能"></a>find 的特別功能</h3><p>find 還可以針對搜尋結果加上 action：</p><p><code>sudo find /etc/yum.repos.d/ -type f -exec mv &#123;&#125; &#123;&#125;1 \;</code></p><p>以上指令表示：</p><ol><li><p>搜尋 /etc/yum.repos.d/ 目錄中的一般檔案</p></li><li><p>將每個檔案進行改名，在檔名後面多加一個 1</p></li></ol><hr><h1 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h1><ul><li><a href="http://linux.vbird.org/linux_basic/0230filesystem.php#link">鳥哥的 Linux 私房菜 – 第七章、Linux 磁碟與檔案系統管理 &gt;&gt; 7.2.2 實體連結與符號連結： ln</a></li><li><a href="http://linux.vbird.org/linux_basic/0220filemanager.php#file_find">鳥哥的 Linux 私房菜 – 第六章、Linux 檔案與目錄管理 &gt;&gt; 6.5 指令與檔案的搜尋</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 13 Installing and Updating Software Packages 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH13_InstallingAndUpdatingSoftwarePackages/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH13_InstallingAndUpdatingSoftwarePackages/</url>
      
        <content type="html"><![CDATA[<a name="ch13.2" />13.2 RPM Software Packages and YUM==================================<table><thead><tr><th>功能</th><th>yum</th><th>rpm</th></tr></thead><tbody><tr><td>查詢</td><td>yum list &#124; grep <font color='blue'><em>KEYWORD</em></font> <br />yum search <font color='blue'><em>KEYWORD</em></font> <br />yum info <font color='blue'><em>PACKAGE_NAME</em></font> <br />yum provides <font color='blue'><em>FILE_PATH_NAME</em></font></td><td>rpm -qa &#124; grep <font color='blue'><em>KEYWORD</em></font> <br />rpm [ -qi &#124; -ql &#124; -qc &#124; -gd &#124; -q –scripts &#124; -q –changelog ] <font color='blue'><em>PACKAGE_NAME</em></font> <br /> rpm -qf <font color='blue'><em>FILE_PATH</em></font></td></tr><tr><td>查詢(Group)</td><td>yum groups [ list &#124; info ]</td><td>rpm [ -qpi &#124; -qpl &#124; -qpc &#124; -qpd &#124; -qp –scripts &#124; -qp –changelog] <font color='blue'><em>PACKAGE_NAME</em></font></td></tr><tr><td>安裝</td><td>yum -y [group] install <font color='blue'><em>PACKAGE_NAME</em></font></td><td>rpm -ivh <font color='blue'><em>PACKAGE_NAME</em></font> <br />yum -y localinstall <font color='blue'><em>PACKAGE_NAME</em></font></td></tr><tr><td>更新</td><td>yum -y update <font color='blue'><em>PACKAGE_NAME</em></font></td><td>rpm -Uvh <font color='blue'><em>PACKAGE_NAME</em></font></td></tr><tr><td>移除</td><td>yum -y [group] remove <font color='blue'><em>PACKAGE_NAME</em></font></td><td>rpm -e <font color='blue'><em>PACKAGE_NAME</em></font></td></tr></tbody></table><hr><a name="ch13.3" />13.3 Managing Software Updates with yum=======================================<ul><li><p><code>sudo yum group install &quot;Development Tools&quot;</code>：安裝整包 Development Tools</p></li><li><p><code>yum list kernel</code>：列出 kernel 清單 (包含已經安裝 &amp; 可安裝的)</p></li><li><p><code>uname -r</code>：列出 kenal 版本</p></li><li><p><code>uname -a</code>：列出 kernel 詳細資訊</p></li><li><p><code>sudo yum history</code>：檢視 yum 歷程記錄</p></li><li><p><code>sudo yum undo 5</code>：取消 ID=5 所紀錄的 yum 工作</p></li></ul><hr><a name="ch13.4" />13.4 Enabling yum Software Repositories=======================================<p><code>/etc/yum.repos.d/*.repo</code>：此目錄內的附檔名必須都是 <strong>repo</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[ID]</span></span><br><span class="line">name=</span><br><span class="line"><span class="attr">baseurl</span>= YUM server 上的容器</span><br><span class="line">enabled=</span><br><span class="line">gpgcheck=</span><br></pre></td></tr></table></figure><p>常用指令：</p><ul><li><p><code>yum repolist all</code>：列出目前所有 repository</p></li><li><p><code>sudo yum-config-manager --disable rhel_dvd</code>：停用 “rhel_dvd” repository</p></li><li><p><code>sudo yum-config-manager --add-repo=&quot;http://content.example.com/rhel7.0/x86_64/rht/&quot;</code>：直接指定路徑增加 repository</p></li><li><p><code>sudo rpm --import http://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7</code>：加入 GPG Key</p></li><li><p><code>sudo yum -y install http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</code>：透過 rpm 安裝方式加入 repository</p></li></ul><p>其他：</p><ul><li><p><code>EPEL</code>：Extra Package for Enterprise Linux</p></li><li><p>使用 yum-config-manager 搭配 <code>--nogpgcheck</code> 表示忽略 GPG key 的檢查，可能會有安全性上的風險</p></li></ul><h3 id="非常重要-Practice-非常重要"><a href="#非常重要-Practice-非常重要" class="headerlink" title="(非常重要) ===== Practice ====== (非常重要)"></a>(<strong><font color='red'>非常重要</font></strong>) ===== Practice ====== (<strong><font color='red'>非常重要</font></strong>)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum-config-manager --add-repo=<span class="string">&quot;http://content.example.com/rhel7.0/x86_64/rht&quot;</span></span><br><span class="line">[content.example.com_rhel7.0_x86_64_rht]</span><br><span class="line">name=added from: http://content.example.com/rhel7.0/x86_64/rht</span><br><span class="line">baseurl=http://content.example.com/rhel7.0/x86_64/rht</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure><p>編輯 <strong><font color='red'>/etc/yum.repo.d/errata.repo</font></strong> 內容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[updates]</span><br><span class="line">name=RedHat updates</span><br><span class="line">baseurl=http://content.example.com/rhel7.0/x86_64/errata</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢查詢所有的 repository (包含 enabled &amp; disabled)</span></span><br><span class="line">$ yum repolist all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停用指定 repository</span></span><br><span class="line">$ sudo yum-config-manager --<span class="built_in">disable</span> content.example.com_rhel7.0_x86_64_rht</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再次確認 repository 狀態</span></span><br><span class="line">$ yum repolist all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 套件升級(會發現有個 kernel 的 update 來自剛剛的 errata repo)</span></span><br><span class="line">$ sudo yum -y update</span><br><span class="line"><span class="comment"># 檢視目前系統中所有的 kernel 清單</span></span><br><span class="line">$ yum list kernel</span><br></pre></td></tr></table></figure><hr><a name="ch13.5" />13.5 Examining RPM Package Files================================<p>rpm 常用參數：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢指定套件</span></span><br><span class="line">$ rpm -q yum</span><br><span class="line">yum-3.4.3-118.el7.noarch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢指定檔案(or 目錄)屬於哪個套件</span></span><br><span class="line">$ rpm -q -f /etc/yum.repos.d</span><br><span class="line">yum-3.4.3-118.el7.noarch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢套件資訊，類似 &quot;yum info&quot; 的功能</span></span><br><span class="line">$ rpm -q -i yum</span><br><span class="line">Name        : yum</span><br><span class="line">Version     : 3.4.3</span><br><span class="line">Release     : 118.el7</span><br><span class="line">Architecture: noarch</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出安裝指定套件所產生的檔案列表</span></span><br><span class="line">$ rpm -q -l yum</span><br><span class="line">.....</span><br><span class="line">/etc/yum.conf</span><br><span class="line">/etc/yum.repos.d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出指定套件相關的文件資訊</span></span><br><span class="line">$ rpm -q -d yum</span><br><span class="line">/usr/share/doc/yum-3.4.3/AUTHORS</span><br><span class="line">/usr/share/doc/yum-3.4.3/COPYING</span><br><span class="line">/usr/share/doc/yum-3.4.3/ChangeLog</span><br><span class="line">/usr/share/doc/yum-3.4.3/INSTALL</span><br><span class="line">.......</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出安裝指定套件所會執行的相關 script 內容</span></span><br><span class="line">$ rpm -q --scripts openssh-server</span><br><span class="line">preinstall scriptlet (using /bin/sh):</span><br><span class="line">getent group sshd &gt;/dev/null || groupadd -g 74 -r sshd || :</span><br><span class="line">.......</span><br><span class="line">postinstall scriptlet (using /bin/sh):</span><br><span class="line">.......</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢套件所包含的設定檔</span></span><br><span class="line">$ rpm -q -c yum</span><br><span class="line">/etc/logrotate.d/yum</span><br><span class="line">/etc/yum.conf</span><br><span class="line">/etc/yum/version-groups.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢指定套件的 changelog 資訊</span></span><br><span class="line">$ rpm -q --changelog yum</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 12 Archiving And Copying Files Between Systems 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH12-ArchvingAndCopyingFilesBetweenSystems/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH12-ArchvingAndCopyingFilesBetweenSystems/</url>
      
        <content type="html"><![CDATA[<a name="ch12.1" />12.1 Managing Compressed tar Archives=====================================<h2 id="12-1-2-Archive-files-and-directories-with-tar"><a href="#12-1-2-Archive-files-and-directories-with-tar" class="headerlink" title="12.1.2 Archive files and directories with tar"></a>12.1.2 Archive files and directories with tar</h2><p>tar 指令要先輸入目的地檔案，後面才是來源檔案 (跟 cp &amp; mv 等指令相反)</p><p>tar 打包檔案中若包含絕對路徑檔案，則會把最開頭的 <code>/</code> 拿掉 (安全因素)</p><p>tar 打包檔案時會包含檔案的修改時間、權限 … 等資訊，但預設不儲存檔案 SELinux conext &amp; ACL 屬性(若要一起打包則要在指令最後方加上 <code>--xattrs</code> 參數)</p><p>指令參考：</p><ul><li><p><code>tar cf archive.tar file1 file2 file3</code>：將三個檔案打包為 archive.tar</p></li><li><p><code>tar tf archive.tar</code>：列出 archive.tar 中的內容</p></li><li><p><code>sudo tar cf /root/etc.tar /etc</code>：打包整個 /etc 目錄成 /root/etc.tar</p></li></ul><h2 id="12-1-3-Extract-an-archive-created-with-tar"><a href="#12-1-3-Extract-an-archive-created-with-tar" class="headerlink" title="12.1.3 Extract an archive created with tar"></a>12.1.3 Extract an archive created with tar</h2><p><code>mkdir test &amp;&amp; sudo sudo tar xf /root/etc.tar -C test/</code>：將 /root/etc.tar 解開後放到 test 目錄中</p><blockquote><p>用 tar 解開打包檔，若要保留原有檔案的權限資訊，要加上 <code>-p</code> 參數 (這是 root 預設就會包含的選項)</p></blockquote><h2 id="12-1-4-Create-a-compressed-tar-archive"><a href="#12-1-4-Create-a-compressed-tar-archive" class="headerlink" title="12.1.4 Create a compressed tar archive"></a>12.1.4 Create a compressed tar archive</h2><ul><li><p>‘z’：gzip (archive.tgz / archive.tar.gz)</p></li><li><p>‘j’：bz2 (archive.tar.bz2)</p></li><li><p>‘J’：xz (archive.tar.xz) (壓縮效率最好，但速度最慢)</p></li></ul><p>參考指令：</p><ul><li><p><code>sudo tar czf /root/etc.tar.gz /etc</code>：以 /etc 為資料來源，建立 gzip 打包壓縮檔</p></li><li><p><code>sudo tar cjf /root/etc.tar.bz2 /etc</code>：以 /etc 為資料來源，建立 bzip2 打包壓縮檔</p></li><li><p><code>sudo tar cJf /root/etc.tar.xz /etc</code>：以 /etc 為資料來源，建立 xz 打包壓縮檔</p></li></ul><h2 id="12-1-5-Extract-a-compressed-tar-archive"><a href="#12-1-5-Extract-a-compressed-tar-archive" class="headerlink" title="12.1.5 Extract a compressed tar archive"></a>12.1.5 Extract a compressed tar archive</h2><p>系統會清楚知道檔案壓縮的格式，因此解壓縮時不用加上 <code>z</code> or <code>j</code> or <code>J</code> 也沒關係</p><p>參考指令：</p><ul><li><code>sudo tar xJf /root/etc.tar.xz -C test/</code>：將上述 xz 壓縮檔解壓縮到 test 目錄下</li></ul><hr><a name="ch12.2" />12.2 Copying Files Between Systems Securely===========================================<p><code>scp</code>：適合用於單一檔案</p><p><code>rsync</code>：可用於單一檔案，但特色是目錄的同步與屬性的保留</p><p><code>sftp</code>：互動功能，Windows 作業系統上較常用</p><p>參考指令：</p><ul><li><code>scp /etc/yum.conf /etc/hosts student@172.25.0.11:/home/student</code>：透過 scp 直接傳兩個檔案到遠端主機</li></ul><hr><a name="ch12.3" />12.3 Synchronizing Files Between Systems Securely=================================================<p><code>-n</code>：Dry run，不會真的執行<br><code>-a</code>：<code>-r</code> + <code>-l</code> + <code>-p</code> + <code>-t</code> + <code>-g</code> + <code>-o</code> + <code>-D</code><br><code>-H</code>：保留 Hard Link<br><code>-A</code>：保留 ACLs 設定<br><code>-X</code>：保留 SELinux context 設定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo rm -rf /tmp/*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步目錄下所有檔案(只會有一個 log 目錄)</span></span><br><span class="line">$ sudo rsync -av /var/<span class="built_in">log</span> /tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步目錄下所有檔案(會跑出很多檔案 &amp; 目錄)</span></span><br><span class="line">$ sudo rsync -av /var/<span class="built_in">log</span>/ /tmp</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 11 Managing Red Hat Enterprise Linux Networking 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH11_ManagingRedHatEnterpriseLinuxNetworking/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH11_ManagingRedHatEnterpriseLinuxNetworking/</url>
      
        <content type="html"><![CDATA[<a name="ch11.1" />11.1 Network Concepts=====================<h2 id="11-1-2-Network-interface-names"><a href="#11-1-2-Network-interface-names" class="headerlink" title="11.1.2 Network interface names"></a>11.1.2 Network interface names</h2><p>網卡命名原則：</p><ul><li><p>Ethernet 介面卡，開頭為 <code>en</code></p><blockquote><p>onboard 的網卡名稱為 eno1, eno2 … etc<br>可插拔(PCI 介面)的網卡名稱為 enp2s0</p></blockquote></li><li><p>無線網路卡，開頭為 <code>wl</code></p></li><li><p>3G/4G 網路卡，開頭為 <code>ww</code></p></li></ul><blockquote><p>虛擬機則一律為 <code>eth0</code>, <code>eth1</code>, <code>eth2</code> … etc</p></blockquote><hr><a name="ch11.2" />11.2 Validating Network Configuration=====================================<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 顯示 enp0s8 資訊</span></span><br><span class="line">[student@server0 ~]$ ip addr show eth0</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000</span><br><span class="line">    link/ether 52:54:00:00:00:0b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.25.0.11/24 brd 172.25.0.255 scope global dynamic eth0</span><br><span class="line">       valid_lft 16494sec preferred_lft 16494sec</span><br><span class="line">    inet6 fe80::5054:ff:fe00:b/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"><span class="comment"># 顯示網路介面的統計紀錄</span></span><br><span class="line">[student@server0 ~]$ ip -s link show eth0</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000</span><br><span class="line">    link/ether 52:54:00:00:00:0b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    RX: bytes  packets  errors  dropped overrun mcast   </span><br><span class="line">    5641965    4752     0       0       0       0      </span><br><span class="line">    TX: bytes  packets  errors  dropped carrier collsns</span><br><span class="line">    611742     3112     0       0       0       0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視路由</span></span><br><span class="line">[student@server0 ~]$ ip route</span><br><span class="line">default via 172.25.0.254 dev eth0  proto static  metric 1024</span><br><span class="line">172.25.0.0/24 dev eth0  proto kernel  scope link  src 172.25.0.11</span><br><span class="line">172.25.253.254 via 172.25.0.254 dev eth0  proto static  metric 1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出所有資訊</span></span><br><span class="line">$ sudo ss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出已經建立的 connection</span></span><br><span class="line">$ sudo ss -t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出包含 listen 的 port &amp; connection</span></span><br><span class="line">$ sudo ss -ta</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出 listening 的 tcp socket</span></span><br><span class="line">$ sudo ss -lt</span><br></pre></td></tr></table></figure><hr><a name="ch11.3" />11.3 Configuring Network with nmcli===================================<p>在 RHEL 7 中提供了 <strong><font color='red'>nmcli</font></strong>(NetworkManager) 作為網路設定管理之用。</p><h2 id="11-3-1-Network-Manager"><a href="#11-3-1-Network-Manager" class="headerlink" title="11.3.1 Network Manager"></a>11.3.1 Network Manager</h2><p><code>nmcli</code> 命令是修改 <code>/etc/sysconfig/network-scripts/ifcfg-*</code> 中的內容，有兩個觀念必須弄清楚，分別是 <strong><font color='red'>device</font></strong> &amp; **<font color='red'>connection</font>**：</p><ul><li><p><code>device</code>：每一個網路卡(介面)都屬於一個 device</p></li><li><p><code>connection</code>：每一個 device 可以同時有多個 connection 設定(每次只有一種可以生效)，可快速因應在不同場景所需要的網路設定變更</p></li></ul><h2 id="11-3-2-Viewing-network-information-with-nmcli"><a href="#11-3-2-Viewing-network-information-with-nmcli" class="headerlink" title="11.3.2 Viewing network information with nmcli"></a>11.3.2 Viewing network information with nmcli</h2><ul><li><p><code>sudo systemctl status NetworkManager.service</code>：檢查 Network Manager 目前服務狀態</p></li><li><p><code>nmcli connection show</code>：列出目前所有的 connection</p></li><li><p><code>nmcli connection show --active</code>：顯示出目前狀態為 active 的 connection</p></li><li><p><code>nmcli connection show &quot;System eth0&quot;</code>：顯示指定 connection 的詳細內容 (小寫的部分可以變更、大寫的部分無法變更)</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[student@server0 ~]$ nmcli connection show --active</span><br><span class="line">NAME         UUID                                  TYPE            DEVICE</span><br><span class="line">System eth0  5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03  802-3-ethernet  eth0</span><br><span class="line">[student@server0 ~]$ nmcli connection show <span class="string">&quot;System eth0&quot;</span></span><br><span class="line">connection.id:                          System eth0</span><br><span class="line">connection.uuid:                        5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03</span><br><span class="line">connection.interface-name:              eth0</span><br><span class="line">connection.type:                        802-3-ethernet</span><br><span class="line">.....</span><br><span class="line">GENERAL.NAME:                           System eth0</span><br><span class="line">GENERAL.UUID:                           5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03</span><br><span class="line">GENERAL.DEVICES:                        eth0</span><br><span class="line">GENERAL.STATE:                          activated</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><ul><li><p><code>nmcli device status</code>：顯示目前 device 的狀態</p></li><li><p><code>nmcli device show eth0</code>：顯示指定 device 的詳細狀態</p></li></ul><h2 id="11-3-3-Creating-network-connections-with-nmcli"><a href="#11-3-3-Creating-network-connections-with-nmcli" class="headerlink" title="11.3.3 Creating network connections with nmcli"></a>11.3.3 Creating network connections with nmcli</h2><ul><li><p><code>sudo nmcli connection add con-name &quot;my-connect-name&quot; type ethernet ifname eth0</code></p><blockquote><p>device: eth0<br /><br>connection: my-connect-name<br /><br>設定內容：DHCP</p></blockquote></li><li><p><code>sudo nmcli connection add con-name &quot;static&quot; ifname eth0 type ethernet autoconnect no ip4 172.25.40.11/24 gw4 172.25.40.254</code></p><blockquote><p>device: eth0 <br /><br>connection: static <br /><br>設定內容: 開機時不套用 | IPv4 | IP: 172.25.40.11/24 | Gateway: 172.25.40.254 <br /><br><strong><font color='red'>但 connection add 無法增加 DNS 設定</font></strong></p></blockquote></li><li><p><code>sudo nmcli connection up static</code>：套用 “staic” connection 設定</p></li><li><p><code>sudo nmcli connection reload</code>：reload 所有的 connection(設定檔)，不會套用到網路介面上(設定完建議 reload 以確保設定有被 Network Manager 抓到)</p></li></ul><h2 id="11-3-4-Modifying-network-interfaces-with-nmcli"><a href="#11-3-4-Modifying-network-interfaces-with-nmcli" class="headerlink" title="11.3.4 Modifying network interfaces with nmcli"></a>11.3.4 Modifying network interfaces with nmcli</h2><ul><li><p><code>sudo nmcli connection modify &quot;static&quot; ipv4.dns 8.8.8.8</code>：在指定的 connection 中設定 DNS(作完要重新 up connection 才會生效)</p></li><li><p><code>sudo nmcli connection modify &quot;static&quot; +ipv4.dns 8.8.4.4</code>：在指定的 connection 中增加 DNS 設定</p></li><li><p><code>sudo nmcli connection modify &quot;static&quot; connection.autoconnect on</code>：設定開機自動套用指定 connection</p></li><li><p><code>sudo nmcli connection delete &quot;static&quot;</code>：刪除指定的 connection</p></li><li><p><code>sudo nmcli connection down</code>：網路斷掉後，Network Manager 會嘗試找到另外一個 autoconnect=on 的 connection 並套用其設定</p></li><li><p><code>sudo nmcli device disconnect eth0</code>：強制停用指定 device 的網路設定(不會自動套用設定)</p></li><li><p><code>sudo nmcli net off</code>：停止所有的網路介面</p></li></ul><hr><a name="ch11.4" />11.4 Editing Network Configuration Files========================================<p>直接修改 <code>/etc/sysconfig/network-scripts/ifcfg-*</code> 檔案中的內容，再使用 <code>sudo nmcli connection reload</code>，就可以讓 Network Manager 取得新的設定。</p><hr><a name="ch11.5" />11.5 Configuring Host Names and Name Resolution===============================================<h2 id="11-5-1-Changing-the-System-host-name"><a href="#11-5-1-Changing-the-System-host-name" class="headerlink" title="11.5.1 Changing the System host name"></a>11.5.1 Changing the System host name</h2><p>若 <code>/etc/hostname</code> 不存在，則系統在網卡被分配到 ip 後，就會進行一個 DNS 的反向查詢</p><p>hostname 可透過 <code>hostnamectl</code> 命令來設定</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ hostname</span><br><span class="line">server5.example.com</span><br><span class="line"></span><br><span class="line">$ sudo hostnamectl set-hostname sercerX.example.com</span><br><span class="line"></span><br><span class="line">$ hostnamectl status</span><br><span class="line">   Static hostname: sercerx.example.com</span><br><span class="line">   Pretty hostname: sercerX.example.com</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: adf65a29af58497b8bb516fc6d366b8d</span><br><span class="line">           Boot ID: 963e5bc0e26a42e8acf285616ed9c9b6</span><br><span class="line">    Virtualization: kvm</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-327.3.1.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br></pre></td></tr></table></figure><h2 id="11-5-2-Configuring-name-resolution"><a href="#11-5-2-Configuring-name-resolution" class="headerlink" title="11.5.2 Configuring name resolution"></a>11.5.2 Configuring name resolution</h2><blockquote><p>若查詢簡短名稱，系統會自動戴上 <code>/etc/resolv.conf</code> 中的 <code>domain</code> or <code>search</code> 的值再查詢</p></blockquote><p>使用 <code>getent</code> &amp; <code>host</code> 測試 DNS 設定：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># getenv 主要以 IPv6 為主</span></span><br><span class="line">$ getent hosts tw.yahoo.com</span><br><span class="line">2406:2000:ec:601::1009 fd-fp3.wg1.b.yahoo.com tw.yahoo.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># 沒有 IPv6 的設定，則回傳 IPv4</span></span><br><span class="line">$ getent hosts ptt.cc</span><br><span class="line">140.112.172.3   ptt.cc</span><br><span class="line">140.112.172.4   ptt.cc</span><br><span class="line">140.112.172.2   ptt.cc</span><br><span class="line">140.112.172.11  ptt.cc</span><br><span class="line">140.112.172.5   ptt.cc</span><br><span class="line">140.112.172.1   ptt.cc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 host 查詢 google</span></span><br><span class="line">$ host www.google.com</span><br><span class="line">www.google.com has address 210.242.127.104</span><br><span class="line">www.google.com has address 210.242.127.88</span><br><span class="line">........</span><br><span class="line">www.google.com has address 210.242.127.109</span><br><span class="line">www.google.com has IPv6 address 2404:6800:4008:c01::6a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 getent 查詢 google</span></span><br><span class="line">$ getent hosts www.google.com</span><br><span class="line">2404:6800:4008:c04::6a www.google.com</span><br></pre></td></tr></table></figure><p>另外，一般網路設定若使用 DHCP，會把原有的 DNS 設定覆蓋，若要避免此情況，可用 <code>sudo nmcli connection &quot;System eth0&quot; ipv4.ignore-auto-dns yes</code> 來避免這樣的狀況發生。</p><blockquote><p>也可以用 nslookup 來測試 DNS，但需要額外加裝 <code>bind-utils</code> 套件</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 10 Analyzing and Storing Logs 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH10_AnalyzingAndStoringLogs/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH10_AnalyzingAndStoringLogs/</url>
      
        <content type="html"><![CDATA[<h1 id="10-1-System-Log-Architecture"><a href="#10-1-System-Log-Architecture" class="headerlink" title="10.1 System Log Architecture"></a>10.1 System Log Architecture</h1><h2 id="10-1-1-System-logging"><a href="#10-1-1-System-logging" class="headerlink" title="10.1.1 System logging"></a>10.1.1 System logging</h2><hr><p>RHEL 7 有兩支 daemon 管理 log：</p><ul><li>systemd-journald</li><li>rsyslog<ul><li>紀錄 message type(or facility) &amp; priority 在 <code>/var/log</code> 內</li><li>facility.severity (facility 很多種，severity 則是標準)</li></ul></li></ul><h3 id="systemd-journald"><a href="#systemd-journald" class="headerlink" title="systemd-journald"></a>systemd-journald</h3><p>systemd-journald daemon 蒐集以下的訊息:(並且會把資料寫進 structured database)</p><ul><li><p>kernel 相關的訊息</p></li><li><p>開機流程中早期的訊息</p></li><li><p>daemon 啟動時的標準輸出 &amp; 錯誤訊息</p></li><li><p>syslog (會被 forward 給 rsyslog 處理)</p></li></ul><h3 id="rsyslog"><a href="#rsyslog" class="headerlink" title="rsyslog"></a>rsyslog</h3><p>rsyslog 會把從 syslog 來的資料放到 <strong>/var/log</strong> 中，除了 <strong>/var/log/secure</strong>、**/var/log/maillog<strong>、</strong>/var/log/cron<strong>、</strong>/var/log/boot.log** 四類訊息，其他訊息都會放在 <strong>/var/log/messages</strong> 中</p><hr><h1 id="10-2-Reviewing-Syslog-Files"><a href="#10-2-Reviewing-Syslog-Files" class="headerlink" title="10.2 Reviewing Syslog Files"></a>10.2 Reviewing Syslog Files</h1><p>rsyslogd 使用 <code>facility</code>(type) &amp; <code>priority</code>(severity) 來決定如何處理 log message，透過設定檔 <code>/etc/rsyslog.config</code> &amp; <code>/etc/rsyslog.d/\*.conf</code> 定義處理方式</p><h2 id="10-2-2-Sample-rule-section-of-rsyslog-config"><a href="#10-2-2-Sample-rule-section-of-rsyslog-config" class="headerlink" title="10.2.2 Sample rule section of rsyslog.config"></a>10.2.2 Sample rule section of rsyslog.config</h2><p>左邊的部份指定哪些 facility.severity 要被記錄，右邊的部份則是指定 log message 要存到哪個檔案</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有 severity INFO 以上的訊息都送往 /var/log/message</span></span><br><span class="line"><span class="comment"># 但 facility mail / authpriv / cron 除外</span></span><br><span class="line"><span class="comment"># boot log message 也會送</span></span><br><span class="line">*.info;mail.none;authpriv.none;cron.none    /var/<span class="built_in">log</span>/messages</span><br><span class="line"></span><br><span class="line"><span class="comment"># kernel facility 的 log message 會往 /dev/console 送</span></span><br><span class="line">kernel.*    /dev/console</span><br><span class="line"></span><br><span class="line"><span class="comment"># authpriv facility 相關的 log message 都會往 /var/log/secure 送</span></span><br><span class="line">authpriv.*  /var/<span class="built_in">log</span>/secure</span><br><span class="line"></span><br><span class="line"><span class="comment"># mail facility 的訊息會先存在於記憶體中，一段時間後詞才會存到檔案中</span></span><br><span class="line">mail.*      -/var/<span class="built_in">log</span>/maillog</span><br><span class="line"></span><br><span class="line"><span class="comment"># severity Emergency 的 log message 會送到目前所有登入使用者的 terminal 上</span></span><br><span class="line">*.emerg     :omusrmsg:*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 開機相關的訊息送往 /var/log/boot.log</span></span><br><span class="line">local7.*    /var/<span class="built_in">log</span>/boot.log</span><br></pre></td></tr></table></figure><p>設定完成後，可透過 <code>logger -p [facility].[severity] &quot;log message&quot;</code> 來測試，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ logger -p local7.emerg <span class="string">&quot;boot emergency log message test&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>可安裝 <strong><font color='red'>rsyslog-doc</font></strong> 套件取得 rsyslogd 相關的 man page，很有幫助</p></blockquote><h3 id="10-2-3-Log-file-rotation"><a href="#10-2-3-Log-file-rotation" class="headerlink" title="10.2.3 Log file rotation"></a>10.2.3 Log file rotation</h3><p>透過 <code>/etc/logrotate.conf</code> 進行設定的修改</p><p><code>/etc/cron.daily</code> 目錄中有一支 logrotate 的 shell script 作為每天執行的工作</p><h3 id="10-2-6-Send-a-syslog-message-with-logger"><a href="#10-2-6-Send-a-syslog-message-with-logger" class="headerlink" title="10.2.6 Send a syslog message with logger"></a>10.2.6 Send a syslog message with logger</h3><p>若是有修改過 rsyslog 的設定後，可用 <code>logger</code> 程式手動發送 log 進行驗證，是否 log 有正確的被記錄：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[student@server0 ~]$ sudo tail -3 /var/<span class="built_in">log</span>/boot.log</span><br><span class="line">[  OK  ] Started GNOME Display Manager.</span><br><span class="line">[  OK  ] Started LSB: Start the ipr dump daemon.</span><br><span class="line">[  OK  ] Started Dynamic System Tuning Daemon.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手動產生一個 local7.notice 的 log message</span></span><br><span class="line">[student@server0 ~]$ logger -p local7.notice <span class="string">&quot;Log entry created on server0&quot;</span></span><br><span class="line"></span><br><span class="line">[student@server0 ~]$ tail -3 /var/<span class="built_in">log</span>/boot.log</span><br><span class="line">[  OK  ] Started LSB: Start the ipr dump daemon.</span><br><span class="line">[  OK  ] Started Dynamic System Tuning Daemon.</span><br><span class="line">Apr 23 23:26:04 server0 student: Log entry created on server0</span><br></pre></td></tr></table></figure><hr><h1 id="10-3-Reviewing-systemd-Journal-Entries"><a href="#10-3-Reviewing-systemd-Journal-Entries" class="headerlink" title="10.3 Reviewing systemd Journal Entries"></a>10.3 Reviewing systemd Journal Entries</h1><p>特點：</p><ul><li><p>存放在 <code>/run/log</code> 目錄中 (表示重開機之後就會消失)</p></li><li><p>最多使用系統 10% 的空間，超過的話，舊的 journal 就會被砍掉</p></li><li><p>使用 <code>journalctl</code> 命令來查詢 (<font color='red'><strong>只有 root 可用</strong></font>)</p></li><li><p>severity <code>notice</code> or <code>warning</code> 會以粗體顯示，<code>error</code> 以上會以紅色表示</p></li></ul><h2 id="10-3-1-Finding-events-with-journalctl"><a href="#10-3-1-Finding-events-with-journalctl" class="headerlink" title="10.3.1 Finding events with journalctl"></a>10.3.1 Finding events with journalctl</h2><p><strong><font color='red'>journalctl</font></strong> 的使用方式：</p><ul><li><p><code>sudo journalctl</code>：顯示所有的 system journal</p></li><li><p><code>sudo journalctl -n 5</code>：使用 <code>-n</code> 參數，顯示最新五筆的 system journal</p></li><li><p><code>sudo journalctl -p err</code>：使用 <code>-p</code> 參數，指定要顯示 priority 為 error 的 system journal</p></li><li><p><code>sudo journalctl -f</code>：類似 <code>tail -f</code>，但持續顯示最新十筆</p></li><li><p><code>sudo journalctl --since today</code>：顯示今天發生的 system journal</p></li><li><p><code>sudo journalctl --since &#39;2016-04-24 00:00:00&#39; --until &#39;2016-04-24 01:00:00&#39;</code>：顯示特定時段內的 system journal</p></li><li><p><code>sudo journalctl --since=&quot;$(date -d &quot;-30 minutes&quot; +%F&#39; &#39;%H:%M:%S)&quot;</code>：尋找 30 分鐘前的 system journal</p></li><li><p><code>sudo journalctl -o verbose</code>：顯示完整 system journal 訊息</p></li><li><p><code>sudo journalctl _PID=1</code>：顯示 pid=1 的 system journal</p></li><li><p><code>sudo journalctl -b</code>：顯示上一次開機到目前所存在的 system journal</p></li></ul><hr><h1 id="10-4-Preserving-the-systemd-Journal"><a href="#10-4-Preserving-the-systemd-Journal" class="headerlink" title="10.4 Preserving the systemd Journal"></a>10.4 Preserving the systemd Journal</h1><p>保留 systemd journal 的方式：</p><ol><li><code>/var/log/journal</code> 目錄存在</li><li>目錄的 owner 必須為 <code>root</code>，owner_group 必須為 <code>systemd-journal</code>，並設定權限為 <code>2755</code></li></ol><hr><h1 id="10-5-Maintain-Accurate-Time"><a href="#10-5-Maintain-Accurate-Time" class="headerlink" title="10.5 Maintain Accurate Time"></a>10.5 Maintain Accurate Time</h1><h2 id="10-5-1-Set-local-clocks-and-time-zone"><a href="#10-5-1-Set-local-clocks-and-time-zone" class="headerlink" title="10.5.1 Set local clocks and time zone"></a>10.5.1 Set local clocks and time zone</h2><ul><li><p><code>timedatectl</code>：顯示目前時區設定</p></li><li><p><code>timedatectl list-timezones</code>：顯示所有時區</p></li><li><p><code>timedatectl set-timezone Asia/Taipei</code>：更改時區</p></li><li><p><code>sudo timedatectl set-ntp true</code>：設定自動 NTP 教時</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 看目前機器上的時間</span></span><br><span class="line">$ timedatectl</span><br><span class="line">      Local time: Thu 2016-01-21 06:35:27 UTC</span><br><span class="line">  Universal time: Thu 2016-01-21 06:35:27 UTC</span><br><span class="line">        RTC time: Thu 2016-01-21 06:35:26</span><br><span class="line">       Time zone: UTC (UTC, +0000)</span><br><span class="line">     NTP enabled: n/a</span><br><span class="line">NTP synchronized: no</span><br><span class="line"> RTC <span class="keyword">in</span> <span class="built_in">local</span> TZ: yes</span><br><span class="line">      DST active: n/a</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定時區</span></span><br><span class="line">$ sudo timedatectl set-timezone Asis/Taipei</span><br><span class="line"></span><br><span class="line"><span class="comment"># 啟用 NTP 校時</span></span><br><span class="line">$ sudo timedatectl set-ntp <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過互動的方式選擇時區(Time Zone)</span></span><br><span class="line">$ tzselect</span><br></pre></td></tr></table></figure><h2 id="10-5-2-Configuring-and-monitoring-chronyd"><a href="#10-5-2-Configuring-and-monitoring-chronyd" class="headerlink" title="10.5.2 Configuring and monitoring chronyd"></a>10.5.2 Configuring and monitoring chronyd</h2><p>這個是用在沒有網路環境時，會由一台主要的 NTP server 去取得正確的資料，其他主機再透過 chronyd.service 進行時間同步。</p><p>加上 <code>iburst</code> 選項會讓網路校時更快，且更正確</p><p><code>sudo hwclock -w</code>：強制將時間資訊寫入硬體</p><h2 id="補充：設定-NTP-校時的完整步驟-很重要"><a href="#補充：設定-NTP-校時的完整步驟-很重要" class="headerlink" title="補充：設定 NTP 校時的完整步驟 (很重要)"></a>補充：設定 NTP 校時的完整步驟 (<strong>很重要</strong>)</h2><ul><li>NTP server：<code>classroom.example.com</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ sudo timedatectl set-ntp <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;server classroom.example.com iburst&quot;</span> | sudo tee --append /etc/chrony.conf</span><br><span class="line">$ sudo systemctl restart chronyd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 驗證設定結果</span></span><br><span class="line">$ chronyc sources -v</span><br><span class="line">210 Number of sources = 1</span><br><span class="line"></span><br><span class="line">  .-- Source mode  <span class="string">&#x27;^&#x27;</span> = server, <span class="string">&#x27;=&#x27;</span> = peer, <span class="string">&#x27;#&#x27;</span> = <span class="built_in">local</span> clock.</span><br><span class="line"> / .- Source state <span class="string">&#x27;*&#x27;</span> = current synced, <span class="string">&#x27;+&#x27;</span> = combined , <span class="string">&#x27;-&#x27;</span> = not combined,</span><br><span class="line">| /   <span class="string">&#x27;?&#x27;</span> = unreachable, <span class="string">&#x27;x&#x27;</span> = time may be <span class="keyword">in</span> error, <span class="string">&#x27;~&#x27;</span> = time too variable.</span><br><span class="line">||                                                 .- xxxx [ yyyy ] +/- zzzz</span><br><span class="line">||                                                /   xxxx = adjusted offset,</span><br><span class="line">||         Log2(Polling interval) -.             |    yyyy = measured offset,</span><br><span class="line">||                                  \            |    zzzz = estimated error.</span><br><span class="line">||                                   |           |                         </span><br><span class="line">MS Name/IP address         Stratum Poll Reach LastRx Last sample</span><br><span class="line">===============================================================================</span><br><span class="line">^* classroom.example.com         8   6    17    18  +2830us[+3050us] +/- 3509us</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 9 Controlling and Securing OpenSSH Service 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH09_ConfiguringAndSecuringOpenSSHService/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH09_ConfiguringAndSecuringOpenSSHService/</url>
      
        <content type="html"><![CDATA[<h1 id="9-1-Accessing-the-Remote-Command-Line-with-SSH"><a href="#9-1-Accessing-the-Remote-Command-Line-with-SSH" class="headerlink" title="9.1 Accessing the Remote Command Line with SSH"></a>9.1 Accessing the Remote Command Line with SSH</h1><h2 id="9-1-1-SSH-Host-Keys"><a href="#9-1-1-SSH-Host-Keys" class="headerlink" title="9.1.1 SSH Host Keys"></a>9.1.1 SSH Host Keys</h2><p>server 會將 public key copy 送到 client 端，有兩個功能：</p><ol><li>用來加密 ssh connection 用</li><li>用來驗證 server</li></ol><ul><li><p>server public key 會存在於 client 端的 <code>~/.ssh/known_hosts</code> 檔案中</p></li><li><p>server 端會把 key pair 儲存在 <code>/etc/ssh/ssh_host_key*</code> 目錄下</p></li><li><p>當 client 透過 ssh 連到 server 時，會把 server 的 public 儲存在 <strong><font color='red'>~/.ssh/known_hosts</font></strong> 內，且每次連線都會檢查，若發現內容不會就會警告且中斷連線!</p></li></ul><hr><h1 id="9-2-Conguring-SSH-Key-based-Authentication"><a href="#9-2-Conguring-SSH-Key-based-Authentication" class="headerlink" title="9.2 Conguring SSH Key-based Authentication"></a>9.2 Conguring SSH Key-based Authentication</h1><p><code>ssh-copy-id</code>：上傳 <font color='red'><strong>~/.ssh/id_rsa.pub</strong></font> 到 remote server 的 <font color='red'><strong>~user/.ssh/authorized_keys</strong></font> 檔案中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 將指定的 public key 加入到 remote server 的 student 帳號下</span></span><br><span class="line">[student@server0 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub student@172.25.0.10</span><br><span class="line"><span class="comment"># 將指定的 public key 加入到 remote server 的 root 帳號下</span></span><br><span class="line">[student@server0 ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@172.25.0.10</span><br></pre></td></tr></table></figure><blockquote><p>若沒有使用 <code>-i</code> 指定 public key 位置，則就預設為 <code>~/.ssh/id_rsa.pub</code></p></blockquote><p>若要使用 key-based 認證但又希望在 private key 上加密碼，並達成 password-less 的效果時：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 產生一個新的 ssh agent 並將 private key 驗證加入</span></span><br><span class="line">$ ssh-agent bash</span><br><span class="line">$ ssh-add</span><br></pre></td></tr></table></figure><hr><h1 id="9-3-Customize-SSH-Service-Configuration"><a href="#9-3-Customize-SSH-Service-Configuration" class="headerlink" title="9.3 Customize SSH Service Configuration"></a>9.3 Customize SSH Service Configuration</h1><p>修改 <code>/etc/ssh/sshd_config</code> 中，調整使用者登入方式：</p><ol><li><p><code>PermitRootLogin no</code>：禁止 root 使用 ssh 登入</p></li><li><p><code>PermitRootLogin without-password</code>：root 只能透過 key-based 的方式登入</p></li><li><p><code>PasswordAuthentication no</code>：關閉密碼登入功能(只能透過 key-based 的方式登入)</p></li></ol><blockquote><p>要重新 reload sshd.service 讓設定變更生效 (<code>sudo systemctl restart sshd.service</code>)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 8 Controlling Services and Daemons 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH08_ControllingServicesAndDaemons/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH08_ControllingServicesAndDaemons/</url>
      
        <content type="html"><![CDATA[<h1 id="8-1-Identifying-Automatically-Started-System-Processes"><a href="#8-1-Identifying-Automatically-Started-System-Processes" class="headerlink" title="8.1 Identifying Automatically Started System Processes"></a>8.1 Identifying Automatically Started System Processes</h1><h2 id="8-1-1-Introduction-to-systemd"><a href="#8-1-1-Introduction-to-systemd" class="headerlink" title="8.1.1 Introduction to systemd"></a>8.1.1 Introduction to systemd</h2><p>systemd(是一個小型的 kernel) 跟開機流程很有關係</p><p>通常一個 service 是由一個或多個 daemon 提供。</p><p>多年前，Linux &amp; UNIX 系統中 PID=1 是屬於一支稱為 <code>init</code>(載入 kernel 之後執行) 的 process；當 kernel 載入後執行。</p><p>在 RHEL 7 中，PID=1 的 process 已經變成 systemd：</p><ul><li>有平行處理的能力，可提升系統開機的速度</li><li>有些先前需要的 daemon 會自動啟動</li><li>自動管理 service 相依性</li><li>LCG (Linux control group)</li></ul><blockquote><p>現在的 <code>/usr/sbin/init</code> 已經改由 symbolic link 指到 systemd</p></blockquote><p>systemd 用來管理各式各樣不同的型態的 object，稱為 <strong>systemd unit</strong>，可以用 <code>systemctl -t help</code> 查詢目前 systemd 可管理的 unit 有那些：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[student@server0 ~]$ systemctl -t <span class="built_in">help</span></span><br><span class="line">Available unit types:</span><br><span class="line">service</span><br><span class="line">socket</span><br><span class="line">target  <span class="comment"># 取代原先 Run Level 的概念</span></span><br><span class="line">device</span><br><span class="line">mount</span><br><span class="line">automount</span><br><span class="line">snapshot</span><br><span class="line">timer   <span class="comment"># 自動排程的功能</span></span><br><span class="line">.....</span><br></pre></td></tr></table></figure><ul><li><code>service</code>：用來表示系統服務</li><li><code>socket</code>：表示 IPC(inter-process communication) socket</li><li><code>timer</code>：自動排程工作是由 timer unit 來處理</li><li><code>target</code>：取代原先 Run Level 的概念</li><li><code>path</code>：通常用來以特定檔案有無變化為前提下，延遲服務發生，例如：列印服務(print pool 的概念)</li></ul><h3 id="Service-status"><a href="#Service-status" class="headerlink" title="Service status"></a>Service status</h3><p>systemd service unit 有以下幾種狀態：</p><ul><li><strong>loaded</strong>：unit 設定被處理後(套件剛裝好時，會處於此狀態)</li><li><strong>active</strong>：啟用狀態</li><li><strong>inactive</strong>：非啟用狀態</li><li><strong>enabled</strong>：開機時會自動啟動</li><li><strong>disabled</strong>：開機時不會自動啟動</li><li><strong>static</strong>：無法控管的狀態，必須由其他 unit 來自動的 enable</li></ul><h2 id="8-1-2-systemctl-使用方式"><a href="#8-1-2-systemctl-使用方式" class="headerlink" title="8.1.2 systemctl 使用方式"></a>8.1.2 systemctl 使用方式</h2><p><code>systemctl</code> 是用來管理不同 systemd unit 的指令，使用方式大概如下：</p><p>列表相關的指令：</p><table><thead><tr><th>Command</th><th>Description</th></tr></thead><tbody><tr><td><code>sudo systemctl -l</code></td><td>檢視所有的 systemd unit 的狀態，但不包含狀態為 inactive 的(加上 <code>--all</code> or <code>-a</code> 可看到全部)</td></tr><tr><td><code>sudo systemctl status sshd.service</code></td><td>檢查 ssh service unit 的狀態<p />(透過 <code>systemctl status name.type</code> 可以檢視 unit 狀態，<code>type</code> 可以不輸入，則預設為 <strong>service</strong>)</td></tr><tr><td><code>sudo systemctl --type=service</code></td><td>檢視 type 屬於 service 的 systemd unit</td></tr><tr><td><code>sudo systemctl is-active sshd.service</code></td><td>檢視 sshd service 是否為 active 狀態</td></tr><tr><td><code>sudo systemctl is-enabled sshd.service</code></td><td>檢視 sshd service 是否為 enabled 狀態</td></tr><tr><td><code>sudo systemctl list-unit-files --type=service</code></td><td>檢視 service type 所有的 unit 設定</td></tr></tbody></table><hr><h1 id="8-2-Controlling-System-Services"><a href="#8-2-Controlling-System-Services" class="headerlink" title="8.2 Controlling System Services"></a>8.2 Controlling System Services</h1><h2 id="8-2-1-Starting-and-stopping-daemons-on-a-running-system"><a href="#8-2-1-Starting-and-stopping-daemons-on-a-running-system" class="headerlink" title="8.2.1 Starting and stopping daemons on a running system"></a>8.2.1 Starting and stopping daemons on a running system</h2><p>安裝 apache2，檢視狀態，並啟動服務：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum -y install httpd</span><br><span class="line"></span><br><span class="line"><span class="comment"># inactive + disabled</span></span><br><span class="line">$ sudo systemctl status httpd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># active(running) + disabled</span></span><br><span class="line">$ sudo systemctl start httpd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># active(running) + enabled</span></span><br><span class="line">$ sudo systemctl <span class="built_in">enable</span> httpd.service</span><br></pre></td></tr></table></figure><p>系統中有 exited &amp; waiting 狀態的 system unit：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 觀察 active(exited) 狀態</span></span><br><span class="line">$ sudo systemctl status network.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 觀察 active(waiting) 狀態</span></span><br><span class="line">$ sudo systemctl status cups.path</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下兩行指令相同</span></span><br><span class="line">$ sudo service sshd restart</span><br><span class="line">$ sudo /bin/systemctl restart sshd.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下兩行指令相同</span></span><br><span class="line">$ sudo systemctl --<span class="built_in">type</span>=service</span><br><span class="line">$ sudo systemctl list-unit --<span class="built_in">type</span>=service</span><br></pre></td></tr></table></figure><blockquote><p>service reload 時，PID 不會換</p></blockquote><h3 id="Unit-dependencies"><a href="#Unit-dependencies" class="headerlink" title="Unit dependencies"></a>Unit dependencies</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視該 system unit 被那些 systen unit 所依賴</span></span><br><span class="line">$ sudo systemctl list-dependencies graphical.target | grep target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 檢視指定的 system unit 依賴那些其他的 system unit</span></span><br><span class="line">$ sudo systemctl list-dependencies multi-user.target --reverse</span><br></pre></td></tr></table></figure><h3 id="Masking-services"><a href="#Masking-services" class="headerlink" title="Masking services"></a>Masking services</h3><ul><li><p><code>disabled</code> 的 service 開機時不會自動啟動，但可以手動啟動</p></li><li><p><code>mask</code> 的 service 無法透過手動或自動的方式啟動(會在 <code>/etc/systemd/system</code> 目錄中產生指定 system unit 的 symbolic link 並指向 /dev/null)</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 7 Monitoring and Managing Linux Processes 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH07_MonitoringAndManagingLinuxProcesses/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH07_MonitoringAndManagingLinuxProcesses/</url>
      
        <content type="html"><![CDATA[<h1 id="7-1-Process"><a href="#7-1-Process" class="headerlink" title="7.1 Process"></a>7.1 Process</h1><h2 id="7-1-1-What-is-a-process"><a href="#7-1-1-What-is-a-process" class="headerlink" title="7.1.1 What is a process?"></a>7.1.1 What is a process?</h2><p>每個 process 都會有個獨一無二的 PID，而 parent process ID(PPID) 則為 PID 的父程序所擁有的 ID；process 可透過 <code>fork</code> 的方式產生 child process，而這些 child process 則會繼承 parent process 的 security identifiers, file descriptors, port, resource privileges, 環境變數….等等，整個 process lifeycle 可以參考下圖：</p><p><img src="https://www.freebsd.org/doc/en_US.ISO8859-1/books/design-44bsd/fig1.png" alt="Process Lifecycle"></p><p>在 RHEL7 中，所有的 process 都是 <strong><font color='red'>systemd(1)</font></strong> 的 child process。</p><h2 id="7-1-2-Process-States"><a href="#7-1-2-Process-States" class="headerlink" title="7.1.2 Process States"></a>7.1.2 Process States</h2><p><img src="http://4.bp.blogspot.com/-5jYQDgc6Z4M/UyV1ni478uI/AAAAAAAADZw/rpBf813wpbg/s1600/ProcessStates.JPG" alt="Linux Process States"></p><table><thead><tr><th>狀態</th><th>Flag</th><th>kernel-defined state and description</th></tr></thead><tbody><tr><td>Running</td><td>R</td><td>**<font color='red'>TASK_RUNNING</font>**：正在等待執行 or 正在執行的 process，正在執行的又包含執行 user routine &amp; kernel routine(system calls)。例如：Data -&gt; Memory</td></tr><tr><td>Sleeping</td><td>S</td><td>**<font color='red'>TASK_INTERRUPTIBLE</font>**：等待特定的情況(硬體要求、系統資源存取、信號….等)發生，當條件滿足時就會回到 Running 的狀態</td></tr><tr><td>Sleeping</td><td>D</td><td><strong><font color='red'>TASK_UNINTERRUPTIBLE</font>**：與 **<font color='red'>S</font></strong> 類似，但不會回應從其他地方送來的信號。例如：寫資料到外接儲存裝置時(Memory -&gt; USB)</td></tr><tr><td>Sleeping</td><td>K</td><td><strong><font color='red'>TASK_KILLABLE</font>**：與 **<font color='red'>D</font></strong> 相反，可以接收來自其他地方的信號，例如：掛載網路磁碟機</td></tr><tr><td>Stopped</td><td>T</td><td>**<font color='red'>TASK_STOPPED</font>**：可能因為 user 或是其他 process 送來訊號而進入 Stopped 狀態，也可能因為特定訊號而返回 Running 狀態</td></tr><tr><td>Stopped</td><td>T</td><td>**<font color='red'>TASK_TRACED</font>**：同上，但可 debug</td></tr><tr><td>Zombie</td><td>Z</td><td>**<font color='red'>EXIT_ZOMBIE</font>**：已通知 parent process 準備離開後的狀態，除了 process identity 之外的資源都會被釋放</td></tr><tr><td>Zombie</td><td>X</td><td>**<font color='red'>EXIT_DEAD</font>**：所有資源都被釋放，ps 也看不見了</td></tr></tbody></table><h2 id="7-1-2-Listing-processes"><a href="#7-1-2-Listing-processes" class="headerlink" title="7.1.2 Listing processes"></a>7.1.2 Listing processes</h2><p>ps 指定常用的參數：</p><ul><li><code>aux</code></li><li><code>las</code></li><li><code>afx</code> (含階層)</li><li><code>-O</code> (指定要顯示的欄位)</li><li><code>--sort</code>：排序</li></ul><p><code>ps</code> 可用來觀察目前 process 的狀態，有以下幾種格式</p><ul><li><p>UNIX(POSIX)：參數加上一個 <code>-</code></p></li><li><p>BSD：參數不加上 <code>-</code></p></li><li><p>GNU long：參數加上兩個 <code>-</code></p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BSD</span></span><br><span class="line">[student@server0 ~]$ ps f</span><br><span class="line">  PID TTY      STAT   TIME COMMAND</span><br><span class="line"> 1986 pts/0    Ss     0:00 -bash</span><br><span class="line"> 2072 pts/0    R+     0:00  \_ ps f</span><br><span class="line"></span><br><span class="line"> <span class="comment"># 以階層顯示(顯示往上五階)</span></span><br><span class="line">[student@server0 ~]$ ps afx | grep -B5 ssh-agent</span><br><span class="line">  1130 ?        Ss     0:00 /usr/sbin/sshd -D</span><br><span class="line">  2823 ?        Ss     0:00  \_ sshd: student [priv]</span><br><span class="line">  2827 ?        S      0:00      \_ sshd: student@pts/0</span><br><span class="line">  2829 pts/0    Ss     0:00          \_ -bash</span><br><span class="line">  3110 pts/0    R+     0:00              \_ ps afx</span><br><span class="line">  3111 pts/0    S+     0:00              \_ grep --color=auto -B5 ssh-agent</span><br><span class="line"></span><br><span class="line"> <span class="comment"># UNIX(POSIX)</span></span><br><span class="line">[student@server0 ~]$ ps -f</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">student   1986  1983  0 05:28 pts/0    00:00:00 -bash</span><br><span class="line">student   2073  1986  0 05:30 pts/0    00:00:00 ps -f</span><br><span class="line">[student@server0 ~]$ ps --f</span><br></pre></td></tr></table></figure><p>檢視全部的 process 常用 <code>aux</code> 選項 or <code>alx</code>(較為詳細)，若想要檢視 process 之間的父子關係可使用 <code>afx</code>(關鍵是 <code>f</code> 參數)</p><p>關於 ps 還有幾個重點：</p><ul><li>若 ps 不加上任何參數，就僅會顯示與目前這個 user terminal 有關係的 process：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[student@server0 ~]$ ps</span><br><span class="line">  PID TTY          TIME CMD</span><br><span class="line"> 1986 pts/0    00:00:00 bash</span><br><span class="line"> 2326 pts/0    00:00:00 ps</span><br></pre></td></tr></table></figure><ul><li>顯示結果中，若是以 <strong><font color='red'>[ ]</font></strong> 包覆的，表示為 kernel thread</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[student@server0 ~]$ ps aux</span><br><span class="line">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">root         1  0.2  0.3  52328  6484 ?        Ss   04:55   0:06 /usr/lib/systemd/systemd --switched-root --system --deserialize 21</span><br><span class="line">root         2  0.0  0.0      0     0 ?        S    04:55   0:00 [kthreadd]</span><br><span class="line">root         3  0.0  0.0      0     0 ?        S    04:55   0:00 [ksoftirqd/0]</span><br><span class="line">root         5  0.0  0.0      0     0 ?        S&lt;   04:55   0:00 [kworker/0:0H]</span><br><span class="line">root         6  0.0  0.0      0     0 ?        S    04:55   0:00 [kworker/u2:0]</span><br></pre></td></tr></table></figure><ul><li><p>透過 <code>ps afx</code> or <code>pstree</code> 可看到 process 之間的 parent/child 關係</p></li><li><p>若要排序 ps 出來的結果，可透過 <code>--sort</code> 選項 (<a href="http://alvinalexander.com/linux/unix-linux-process-memory-sort-ps-command-cpu">參考網址(Linux process memory usage - how to sort the ps command)</a>)</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根據 CPU 遞增排序</span></span><br><span class="line">$ ps au --sort=%cpu</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根據 CPU 遞減排序</span></span><br><span class="line">$ ps au --sort=-%cpu</span><br><span class="line"></span><br><span class="line"><span class="comment"># PID 從小到大排序</span></span><br><span class="line">[student@server0 ~]$ ps aux --sort pid</span><br><span class="line">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">root         1  0.5  0.3  52328  6472 ?        Ss   19:53   0:09 /usr/lib/systemd/systemd --switched-root --system --deserialize 21</span><br><span class="line">root         2  0.0  0.0      0     0 ?        S    19:53   0:00 [kthreadd]</span><br><span class="line">root         3  0.0  0.0      0     0 ?        S    19:53   0:00 [ksoftirqd/0]</span><br><span class="line"></span><br><span class="line"><span class="comment"># PID 從大到小排序</span></span><br><span class="line">[student@server0 ~]$ ps aux --sort -pid</span><br><span class="line">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND</span><br><span class="line">student   2065  0.0  0.0 123516  1448 pts/0    R+   20:24   0:00 ps aux --sort -pid</span><br><span class="line">root      2064  0.0  0.0 107892   364 ?        S    20:24   0:00 sleep 60</span><br><span class="line">root      2001  0.0  0.0      0     0 ?        S    20:19   0:00 [kworker/0:0]</span><br></pre></td></tr></table></figure><hr><h1 id="7-2-Controlling-Jobs"><a href="#7-2-Controlling-Jobs" class="headerlink" title="7.2 Controlling Jobs"></a>7.2 Controlling Jobs</h1><p><code>jobs</code> 指令只會顯示目前 session 中的 job</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ gedit &amp;</span><br><span class="line">$ <span class="built_in">jobs</span></span><br><span class="line">$ nautilus &amp;</span><br><span class="line">$ <span class="built_in">jobs</span></span><br><span class="line">$ sleep 1000 &amp;</span><br><span class="line">$ <span class="built_in">jobs</span></span><br><span class="line">$ <span class="built_in">fg</span></span><br><span class="line">CTRL + C (終止目前前景執行的程式)</span><br><span class="line">$ <span class="built_in">jobs</span></span><br><span class="line">$ <span class="built_in">fg</span> %1</span><br><span class="line">CTRL + C (終止目前前景執行的程式)</span><br></pre></td></tr></table></figure><h2 id="7-2-1-Jobs-and-sessions"><a href="#7-2-1-Jobs-and-sessions" class="headerlink" title="7.2.1 Jobs and sessions"></a>7.2.1 Jobs and sessions</h2><p>當 terminal or console 被開啟時，會產生一個 process session，所以透過同一個 terminal or console 產生出來的 process，都會使用相同的 session ID，而每個 session 中，一次只能有一個 process 在前景執行。</p><p>service daemon 或是 kernel process thread 在 <code>ps</code> 出來的結果中，<code>TTY</code> 欄位會以 <code>?</code> 來呈現，因為此類的 background process 並沒有 controlling terminal。</p><h2 id="7-2-2-Running-jobs-in-the-background"><a href="#7-2-2-Running-jobs-in-the-background" class="headerlink" title="7.2.2 Running jobs in the background"></a>7.2.2 Running jobs in the background</h2><p><code>Ctrl + z</code>：可送出 suspend 要求，用來讓 process 進入 Stopped 狀態(T)</p><p><code>Ctrl + c</code>：中斷 process 執行</p><p><code>bg %JOB_ID</code>：可讓 process 恢復執行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一個執行中的範例</span></span><br><span class="line">$ dd &lt; /dev/zero &gt; /dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment"># R(Running) 狀態</span></span><br><span class="line">$ ps au | grep dd</span><br><span class="line"></span><br><span class="line">Ctrl + Z</span><br><span class="line"></span><br><span class="line"><span class="comment"># 變成 S(Sleeping, TASK_INTERRUPTIBLE) 狀態</span></span><br><span class="line">$ ps au | grep dd</span><br><span class="line"></span><br><span class="line">$ sleep 1000</span><br><span class="line"></span><br><span class="line"><span class="comment"># S(Sleep) 狀態</span></span><br><span class="line">$ ps au | grep sleep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用來驗證 D(Sleeping, TASK_UNINTERRUPTIBLE) 狀態</span></span><br><span class="line">$ dd <span class="keyword">if</span>=/dev/zero bs=1M count=50 of=/run/media/...../test.txt</span><br></pre></td></tr></table></figure><hr><h1 id="7-3-Killing-Processes"><a href="#7-3-Killing-Processes" class="headerlink" title="7.3 Killing Processes"></a>7.3 Killing Processes</h1><h2 id="7-3-1-Process-control-using-signals"><a href="#7-3-1-Process-control-using-signals" class="headerlink" title="7.3.1 Process control using signals"></a>7.3.1 Process control using signals</h2><p>signal 是送到 process 的一種軟體型式的中斷，有以下幾種：</p><table><thead><tr><th>Signal number</th><th>Short name</th><th>Definition</th><th>Purpose</th></tr></thead><tbody><tr><td><code>1</code></td><td><code>HUP</code></td><td>Hangup</td><td></td></tr><tr><td><code>2</code></td><td><code>INT</code></td><td>Keyboard interrupt</td><td>讓程式中止，等同按下 <code>Ctrl + c</code></td></tr><tr><td><code>3</code></td><td><code>QUIT</code></td><td>Keyboard quit</td><td>讓 process 中止，但會執行 core dump 的動作，等同按下 <code>Ctrl + \</code></td></tr><tr><td><code>9</code></td><td><code>KILL</code></td><td>Kill, unblockable</td><td>強行中止</td></tr><tr><td><code>15</code>(default)</td><td><code>TERM</code></td><td>Terminate</td><td>通知程式中止，允許 process 完成 self-cleanup 後才中止</td></tr><tr><td><code>20</code></td><td><code>TSTP</code></td><td>Keyboard stop</td><td>讓 process 中斷執行(但可恢復)，效果等同按下 <code>Ctrl + z</code></td></tr></tbody></table><blockquote><p>詳細的 signal 列表可使用 <code>kill -l</code> 查詢</p></blockquote><blockquote><p>基本上砍掉 process 時先嘗試用 <code>SIGTERM</code>，不行的話才改用 <code>SIGKILL</code>，會是比較穩妥的作法‧</p></blockquote><blockquote><p>沒指定 SIG 則預設給 <font color='red'><strong>SIGTERM(15)</strong></font></p></blockquote><h3 id="Logging-users-out-administratively"><a href="#Logging-users-out-administratively" class="headerlink" title="Logging users out administratively"></a>Logging users out administratively</h3><p><code>w</code> 指令是用來檢視目前登入到系統的使用者以及累積到現在的資源使用狀況：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[kiosk@foundation0 ~]$ w -f</span><br><span class="line"> 06:19:29 up  1:27,  3 users,  load average: 0.04, 0.07, 0.06</span><br><span class="line">USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT</span><br><span class="line">kiosk    :0       :0               04:52   ?xdm?  11:36   0.20s gdm-session-worker [pam/gdm-autologin]</span><br><span class="line">kiosk    pts/3    :0               05:27   52:14   0.03s  0.03s bash</span><br><span class="line">kiosk    pts/4    192.168.1.190    06:11    1.00s  0.03s  0.01s w -f</span><br></pre></td></tr></table></figure><p><code>pkill</code> 是個強大的關閉 process 的指令，可透過以下方式過濾：</p><ul><li>command</li><li>UID</li><li>GID</li><li>parent</li><li>terminal</li></ul><p><code>pgrep</code> 指定 user 作 grep</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 刪除使用者 bboop 相關的 process</span></span><br><span class="line">$ pkill -u bboop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 刪除使用者 bboop 相關的 process 並強制登出</span></span><br><span class="line">$ pkill -SIGKILL -u bboop</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[student@server0 ~]$ sleep 1000 &amp;</span><br><span class="line">[1] 2893</span><br><span class="line">[student@server0 ~]$ sleep 1000 &amp;</span><br><span class="line">[2] 2894</span><br><span class="line">[student@server0 ~]$ sleep 1000 &amp;</span><br><span class="line">[3] 2895</span><br><span class="line"><span class="comment"># pgrep 視特定使用者</span></span><br><span class="line">[student@server0 ~]$ pgrep -l -u student</span><br><span class="line">2827 sshd</span><br><span class="line">2829 bash</span><br><span class="line">2893 sleep</span><br><span class="line">2894 sleep</span><br><span class="line">2895 sleep</span><br><span class="line"><span class="comment"># pstree 檢視特定使用者，使用 PID</span></span><br><span class="line">[student@server0 ~]$ pstree -p student</span><br><span class="line">sshd(2827)───bash(2829)─┬─pstree(2896)</span><br><span class="line">                        ├─sleep(2893)</span><br><span class="line">                        ├─sleep(2894)</span><br><span class="line">                        └─sleep(2895)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定 PPID(parent ID) 刪除 process</span></span><br><span class="line">[student@server0 ~]$ pkill -SIGKILL -P 2829</span><br><span class="line">[1]   Killed                  sleep 1000</span><br><span class="line">[2]-  Killed                  sleep 1000</span><br><span class="line">[3]+  Killed                  sleep 1000</span><br><span class="line">[student@server0 ~]$ pstree -p student</span><br><span class="line">sshd(2827)───bash(2829)───pstree(2945)</span><br></pre></td></tr></table></figure><hr><h1 id="7-4-Monitoring-Process-Activity"><a href="#7-4-Monitoring-Process-Activity" class="headerlink" title="7.4 Monitoring Process Activity"></a>7.4 Monitoring Process Activity</h1><h2 id="7-4-1-Load-average"><a href="#7-4-1-Load-average" class="headerlink" title="7.4.1 Load average"></a>7.4.1 Load average</h2><p>靜態呈現，使用 <code>w -f</code> or <code>uptime</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查詢目前系統的 load average</span></span><br><span class="line">$ w -f</span><br><span class="line"> 16:37:45 up  7:29,  3 users,  load average: 0.20, 0.17, 0.14</span><br><span class="line">USER     TTY        LOGIN@   IDLE   JCPU   PCPU WHAT</span><br><span class="line">godleon  tty8      09:08    7:29m  9:30   0.37s mate-session</span><br><span class="line"></span><br><span class="line"><span class="comment"># $ uptime</span></span><br><span class="line"> 16:43:07 up  7:35,  3 users,  load average: 0.18, 0.15, 0.14</span><br></pre></td></tr></table></figure><blockquote><p>load average 超過 1 就表示系統負擔過重(要先除以 CPU 執行緒數量)</p></blockquote><h2 id="7-4-2-Real-time-process-monitoring"><a href="#7-4-2-Real-time-process-monitoring" class="headerlink" title="7.4.2 Real-time process monitoring"></a>7.4.2 Real-time process monitoring</h2><p>動態呈現，使用 <code>top</code>：</p><ul><li><p><code>VIRT</code>：process 消耗所有記憶體大小(包含實體 &amp; 虛擬)，等同 ps 指令中的 <code>VSZ</code></p></li><li><p><code>RES</code>：process 所使用的實體記憶體大小，等同 ps 指令中的 <code>RSS</code></p></li></ul><p>在 top 中常用的按鍵：</p><table><thead><tr><th>Key</th><th>Purpose</th></tr></thead><tbody><tr><td><code>l</code>, <code>t</code>, <code>m</code></td><td>開啟 or 關閉 load, thread, memory 資訊</td></tr><tr><td><code>M</code></td><td>記憶體使用量從大排到小</td></tr><tr><td><code>P</code></td><td>CPU 使用量從大排到小</td></tr><tr><td><code>k</code></td><td>指定 PID 並 TERM 該 process</td></tr><tr><td><code>r</code></td><td>renice 指定 process</td></tr><tr><td><code>W</code></td><td>記錄目前的 top 觀察設定，並可作為下次使用 top 時的預設設定</td></tr><tr><td><code>B</code></td><td>header &amp; running process 會以粗體顯示</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 4~6 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH04_06/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH04_06/</url>
      
        <content type="html"><![CDATA[<h1 id="Chapter-4-Creating-Viewing-and-Editing-Text-Files"><a href="#Chapter-4-Creating-Viewing-and-Editing-Text-Files" class="headerlink" title="Chapter 4. Creating, Viewing, and Editing Text Files"></a>Chapter 4. Creating, Viewing, and Editing Text Files</h1><h2 id="4-1-Redirecting-Output-to-a-File-or-Program"><a href="#4-1-Redirecting-Output-to-a-File-or-Program" class="headerlink" title="4.1 Redirecting Output to a File or Program"></a>4.1 Redirecting Output to a File or Program</h2><h3 id="Standard-input-standard-output-and-standard-error"><a href="#Standard-input-standard-output-and-standard-error" class="headerlink" title="Standard input, standard output, and standard error"></a>Standard input, standard output, and standard error</h3><p><img src="https://upload.wikimedia.org/wikipedia/commons/7/70/Stdstreams-notitle.svg" alt="Process I/O Channel"></p><p><img src="http://cs.ucla.edu/classes/fall08/cs111/scribe/4/FDT_diagram.JPG" alt="STDIN、STDOUT &amp; STDERR"></p><h3 id="Redirecting-output-to-a-file"><a href="#Redirecting-output-to-a-file" class="headerlink" title="Redirecting output to a file"></a>Redirecting output to a file</h3><table><thead><tr><th>Usage</th><th>說明</th></tr></thead><tbody><tr><td><code>&amp;&gt;file</code></td><td>stdout &amp; stderr 各自輸出到相同的檔案中<br />會複寫指定檔案，若檔案不存在則建立新檔</td></tr><tr><td><code>&gt;&gt;file 2&gt;&amp;1</code><p /><code>&amp;&gt;&gt;file</code></td><td>stderr 會導向變成 stdout 輸出，並附加內容於指定檔案<br />**<font color='red'>不會再有 stderr 輸出，而是全部皆為 stdout 輸出</font>**</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server tmp]$ find /etc -name passwd &amp;&gt;/tmp/save-both</span><br><span class="line">[vagrant@server tmp]$ cat /tmp/save-both</span><br><span class="line">....</span><br><span class="line">find: ‘/etc/lvm/backup’: Permission denied</span><br><span class="line">find: ‘/etc/lvm/cache’: Permission denied</span><br><span class="line">/etc/passwd</span><br><span class="line">find: ‘/etc/polkit-1/rules.d’: Permission denied</span><br><span class="line">find: ‘/etc/polkit-1/localauthority’: Permission denied</span><br><span class="line">....</span><br><span class="line">/etc/pam.d/passwd</span><br><span class="line">find: ‘/etc/audisp’: Permission denied</span><br><span class="line">....</span><br></pre></td></tr></table></figure><h3 id="Constructing-pipelines"><a href="#Constructing-pipelines" class="headerlink" title="Constructing pipelines"></a>Constructing pipelines</h3><p>pipeline 並沒有對 stderr 進行處理，透過以下兩個指令可以看出差別：(看有顏色的部分)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 僅有 stdout 的內容會被篩選</span></span><br><span class="line">[vagrant@server tmp]$ find /etc -name passwd | grep etc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 連同 stderr 的內容都會被篩選(因為 stderr 的內容已經導向 stdout 輸出)</span></span><br><span class="line">[vagrant@server tmp]$ find /etc -name passwd 2&gt;&amp;1 | grep etc</span><br></pre></td></tr></table></figure><p><img src="http://civilnet.cn/book/kernel/GNU.Linux.Application.Programming/images/11.1_0.jpg" alt="Linux Pipeline"></p><h4 id="tee"><a href="#tee" class="headerlink" title="tee"></a>tee</h4><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/24/Tee.svg/400px-Tee.svg.png" alt="tee pipeline"></p><p>tee 的使用範例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -a = append</span></span><br><span class="line">[vagrant@server ~]$ ps -f | tee -a ps_file.txt</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">vagrant   3372  3371  0 14:25 pts/0    00:00:00 -bash</span><br><span class="line">vagrant   3395  3372  0 14:25 pts/0    00:00:00 ps -f</span><br><span class="line">vagrant   3396  3372  0 14:25 pts/0    00:00:00 tee -a ps_file.txt</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ cat ps_file.txt</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">vagrant   3372  3371  0 14:25 pts/0    00:00:00 -bash</span><br><span class="line">vagrant   3395  3372  0 14:25 pts/0    00:00:00 ps -f</span><br><span class="line">vagrant   3396  3372  0 14:25 pts/0    00:00:00 tee -a ps_file.txt</span><br></pre></td></tr></table></figure><p>以下兩種寄 mail 的方式相同：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat filename | mail -s subject username</span><br><span class="line"></span><br><span class="line">$ mail -s subject username &lt; filename</span><br></pre></td></tr></table></figure><hr><h1 id="Chapter-5-Managing-Local-Linux-Users-and-Groups"><a href="#Chapter-5-Managing-Local-Linux-Users-and-Groups" class="headerlink" title="Chapter 5. Managing Local Linux Users and Groups"></a>Chapter 5. Managing Local Linux Users and Groups</h1><h2 id="5-1-Users-and-Groups"><a href="#5-1-Users-and-Groups" class="headerlink" title="5.1 Users and Groups"></a>5.1 Users and Groups</h2><h3 id="What-is-a-user"><a href="#What-is-a-user" class="headerlink" title="What is a user?"></a>What is a user?</h3><p><code>id</code> 用來顯示目前登入的使用者資訊：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ id</span><br><span class="line">uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),10(wheel) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023</span><br></pre></td></tr></table></figure><blockquote><p>context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 是 SELinux 安全脈絡標籤</p></blockquote><p><strong><font color='red'>/etc/passwd</font></strong> 的格式可以參考 <code>man 5 passwd</code></p><p>基本上 password 的部分已經都用 x 取代，並移到 <strong><font color='red'>/etc/shadow</font></strong> 存放</p><h3 id="What-is-a-group"><a href="#What-is-a-group" class="headerlink" title="What is a group?"></a>What is a group?</h3><ol><li><p>每個使用者只會有一個 primary group，記錄在 <code>/etc/passwd</code> 內</p></li><li><p>每個使用者可以有 0 到多個 supplementary group，資訊會記錄在 <code>/etc/group</code> 內</p></li></ol><h2 id="5-2-Gaining-Superuser-Access"><a href="#5-2-Gaining-Superuser-Access" class="headerlink" title="5.2 Gaining Superuser Access"></a>5.2 Gaining Superuser Access</h2><h3 id="The-root-user"><a href="#The-root-user" class="headerlink" title="The root user"></a>The root user</h3><p>一般使用者可透過 <code>su</code>, <code>sudo</code>, <code>PolicyKit</code>(GUI 內用，類似 Windows UAC) 來取得 root 權限</p><h3 id="Switching-users-with-su"><a href="#Switching-users-with-su" class="headerlink" title="Switching users with su"></a>Switching users with su</h3><p><code>su [-] username</code>：等同於新的 user 重新登入的效果</p><p><code>su username</code>：產生新的 shell 並使用目前的環境變數</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 -c 可達到類似 windows runas 的效果</span></span><br><span class="line">[vagrant@server ~]$ su -c <span class="string">&quot;ls /root&quot;</span> root</span><br><span class="line">Password:</span><br><span class="line">anaconda-ks.cfg</span><br></pre></td></tr></table></figure><h3 id="Running-commands-as-root-with-sudo"><a href="#Running-commands-as-root-with-sudo" class="headerlink" title="Running commands as root with sudo"></a>Running commands as root with sudo</h3><p>su 的缺點是，一次就拿到完整的 root 權限，且切換成 root 還必須知道 root 的密碼。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ sudo cat /etc/sudoers | grep <span class="string">&#x27;^[^#]&#x27;</span></span><br><span class="line">Defaults   !visiblepw</span><br><span class="line">Defaults    always_set_home</span><br><span class="line">Defaults    env_reset</span><br><span class="line">Defaults    env_keep =  <span class="string">&quot;COLORS DISPLAY HOSTNAME HISTSIZE INPUTRC KDEDIR LS_COLORS&quot;</span></span><br><span class="line">Defaults    env_keep += <span class="string">&quot;MAIL PS1 PS2 QTDIR USERNAME LANG LC_ADDRESS LC_CTYPE&quot;</span></span><br><span class="line">Defaults    env_keep += <span class="string">&quot;LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES&quot;</span></span><br><span class="line">Defaults    env_keep += <span class="string">&quot;LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE&quot;</span></span><br><span class="line">Defaults    env_keep += <span class="string">&quot;LC_TIME LC_ALL LANGUAGE LINGUAS _XKB_CHARSET XAUTHORITY&quot;</span></span><br><span class="line">Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">root    ALL=(ALL)       ALL</span><br><span class="line">%wheel  ALL=(ALL)       ALL   <span class="comment">#表示 wheel 群組內的擁有所有權限(% 開頭表示指定群組)</span></span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ cat /etc/group | grep wheel</span><br><span class="line">wheel:x:10:vagrant</span><br></pre></td></tr></table></figure><p>使用 <code>sudo</code> 的特點：</p><ol><li><p>執行 root 的系統命令不需要記住 root 密碼</p></li><li><p>所有 sudo 所執行的紀錄都會留在 <code>/var/log/secure</code> 中</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ sudo tail /var/<span class="built_in">log</span>/secure</span><br><span class="line">....</span><br><span class="line">Feb 22 13:45:26 localhost sudo: vagrant : TTY=pts/0 ; PWD=/home/vagrant ; USER=root ; COMMAND=/bin/ls /root/</span><br><span class="line">Feb 22 13:45:45 localhost sudo: vagrant : TTY=pts/0 ; PWD=/home/vagrant ; USER=root ; COMMAND=/bin/cat /var/<span class="built_in">log</span>/secure</span><br><span class="line">Feb 22 13:46:10 localhost sudo: vagrant : TTY=pts/0 ; PWD=/home/vagrant ; USER=root ; COMMAND=/bin/tail /var/<span class="built_in">log</span>/secure</span><br></pre></td></tr></table></figure><h3 id="補充"><a href="#補充" class="headerlink" title="補充"></a>補充</h3><p><code>NIS</code>：集中驗證</p><p><code>IPA</code>：集中授權管理</p><p><code>sudo su -</code> or <code>sudo -s</code>：用一般的 user 切換成 root，輸入一般 user 的密碼</p><h2 id="5-3-Managing-Local-User-Accounts"><a href="#5-3-Managing-Local-User-Accounts" class="headerlink" title="5.3 Managing Local User Accounts"></a>5.3 Managing Local User Accounts</h2><h3 id="Managing-local-users"><a href="#Managing-local-users" class="headerlink" title="Managing local users"></a>Managing local users</h3><p><code>/etc/login.defs</code> 檔案中紀錄了新增使用者時的相關預設設定，例如 UID range, 密碼有效期限 … 等</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ cat /etc/login.defs | grep <span class="string">&#x27;^[^#]&#x27;</span></span><br><span class="line">MAIL_DIR        /var/spool/mail</span><br><span class="line">PASS_MAX_DAYS   99999</span><br><span class="line">PASS_MIN_DAYS   0</span><br><span class="line">PASS_MIN_LEN    5</span><br><span class="line">PASS_WARN_AGE   7</span><br><span class="line">UID_MIN                  1000</span><br><span class="line">UID_MAX                 60000</span><br><span class="line">SYS_UID_MIN               201</span><br><span class="line">SYS_UID_MAX               999</span><br><span class="line">GID_MIN                  1000</span><br><span class="line">GID_MAX                 60000</span><br><span class="line">SYS_GID_MIN               201</span><br><span class="line">SYS_GID_MAX               999</span><br><span class="line">CREATE_HOME     yes</span><br><span class="line">UMASK           077</span><br><span class="line">USERGROUPS_ENAB yes</span><br><span class="line">ENCRYPT_METHOD SHA512</span><br></pre></td></tr></table></figure><h4 id="usermode-modifies-existing-users"><a href="#usermode-modifies-existing-users" class="headerlink" title="usermode modifies existing users"></a>usermode modifies existing users</h4><p><code>usermod</code> 用來修改使用者資訊，相關參數如下：</p><table><thead><tr><th>參數</th><th>說明</th></tr></thead><tbody><tr><td><code>-g/--gid</code> GROUP</td><td>設定 user 的 primary group</td></tr><tr><td><code>-G/--groups</code> GROUPS</td><td>設定 user 的 supplementary group</td></tr><tr><td><code>-a/--append</code></td><td>與 <code>-G</code> 搭配，用來增加 supplementary group 設定</td></tr><tr><td><code>-d/--home</code> HOME_DIR</td><td>設定 user 家目錄</td></tr><tr><td><code>-m/--move-home</code></td><td>移動 user 家目錄到新的地方，必須與 <code>-d</code> 參數同時使用</td></tr><tr><td><code>-s/--shell</code> SHELL</td><td>指定登入 shell</td></tr><tr><td><code>-L/--lock</code></td><td>鎖定 user</td></tr><tr><td><code>-U/--unlock</code></td><td>解鎖 user</td></tr></tbody></table><h4 id="userdel-deletes-users"><a href="#userdel-deletes-users" class="headerlink" title="userdel deletes users"></a>userdel deletes users</h4><p><code>userdel -r username</code>：會將 user 刪除，連同家目錄 &amp; mail 都一併移除</p><blockquote><p>userdel 命令若沒有加上 -r 參數，可能會有安全疑慮，主要是沒有 owner &amp; owner group 的檔案可能會被新增的 user 取得存取權限</p></blockquote><blockquote><p>此問題可透過 <code>sudo find / -nouser -o --nogroup 2&gt;/dev/null</code> 指令來找到if沒有 owner &amp; owner group 的檔案</p></blockquote><h4 id="id-displays-user-information"><a href="#id-displays-user-information" class="headerlink" title="id displays user information"></a>id displays user information</h4><p><code>id</code> 可用來顯示使用者資訊</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ id</span><br><span class="line">uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant),10(wheel) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ id root</span><br><span class="line">uid=0(root) gid=0(root) groups=0(root)</span><br></pre></td></tr></table></figure><h4 id="UID-ranges"><a href="#UID-ranges" class="headerlink" title="UID ranges"></a>UID ranges</h4><p>UID 在 RHEL 7 中的定義：(預設值可參考 <code>/etc/login.defs</code>)</p><ul><li><p><code>UID 0</code>：root</p></li><li><p><code>UID 1-200</code>：system users，通常用來啟動系統服務之用</p></li><li><p><code>UID 201-999</code>：保留的 system users，有其他額外未預先定義的系統服務需要時可使用</p></li><li><p><code>UID 1000</code>：一般使用者用的 UID</p></li></ul><blockquote><p>在 RHEL 7 之前，system user 使用 UID 1-499，一般使用者使用  UID 500+</p></blockquote><h3 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h3><p>使用 variable 新增 user：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ USER=juliet</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ <span class="built_in">echo</span> <span class="variable">$USER</span></span><br><span class="line">juliet</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ sudo useradd <span class="variable">$&#123;USER&#125;</span>; <span class="built_in">echo</span> <span class="variable">$&#123;USER&#125;</span> | sudo passwd --stdin <span class="variable">$&#123;USER&#125;</span></span><br><span class="line">Changing password <span class="keyword">for</span> user juliet.</span><br><span class="line">passwd: all authentication tokens updated successfully.</span><br></pre></td></tr></table></figure><h2 id="5-4-Managing-Local-Group-Accounts"><a href="#5-4-Managing-Local-Group-Accounts" class="headerlink" title="5.4 Managing Local Group Accounts"></a>5.4 Managing Local Group Accounts</h2><h3 id="5-4-1-Managing-supplementary-groups"><a href="#5-4-1-Managing-supplementary-groups" class="headerlink" title="5.4.1 Managing supplementary groups"></a>5.4.1 Managing supplementary groups</h3><h4 id="groupadd"><a href="#groupadd" class="headerlink" title="groupadd"></a>groupadd</h4><ul><li><code>-r</code>：用來增加 system group (GID &gt; 1000)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 -g 選項指定 GID</span></span><br><span class="line">$ sudo groupadd -g 5000 ateam</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 -r 選項姜群組新增成 system group</span></span><br><span class="line">$ sudo groupadd -r appusers</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查詢系統中預設 GID range</span></span><br><span class="line">[vagrant@server ~]$ cat /etc/login.defs | grep GID</span><br><span class="line">GID_MIN                  1000</span><br><span class="line">GID_MAX                 60000</span><br><span class="line">SYS_GID_MIN               201</span><br><span class="line">SYS_GID_MAX               999</span><br></pre></td></tr></table></figure><blockquote><p><code>/etc/login.defs</code> 檔案中紀錄了許多與使用者 &amp; 群組管理上相關的預設參數</p></blockquote><h4 id="groupmod"><a href="#groupmod" class="headerlink" title="groupmod"></a>groupmod</h4><ul><li><p><code>-n</code>：修改群組名稱</p></li><li><p><code>-g</code>：修改 GID (這會衍生非預期的問題)</p><ul><li>家目錄不會改</li><li>相對應的檔案 &amp; 目錄皆不會改</li></ul></li></ul><h4 id="usermod-alters-group-membership"><a href="#usermod-alters-group-membership" class="headerlink" title="usermod alters group membership"></a>usermod alters group membership</h4><ul><li><p><code>-g</code>：指定 primary group</p></li><li><p><code>sudo usermod -g NEW_PRIMARY_GROUPNAME USERNAME</code>：變更指定使用者的 primary group</p></li><li><p><code>-G</code>：指定 supplementary group</p></li><li><p><code>sudo uermod -aG SUPPLYMENTARY_GROUP USERNAME</code>：新增使用者的 supplementary group</p></li></ul><h2 id="5-5-Managing-User-Passwords"><a href="#5-5-Managing-User-Passwords" class="headerlink" title="5.5 Managing User Passwords"></a>5.5 Managing User Passwords</h2><h3 id="5-5-1-Shadow-passwords-and-password-policy"><a href="#5-5-1-Shadow-passwords-and-password-policy" class="headerlink" title="5.5.1 Shadow passwords and password policy"></a>5.5.1 Shadow passwords and password policy</h3><p>現在為了安全性，密碼都已經改存到只有 root 能讀取的 <code>/etc/shadow</code> 中，分為幾個欄位：</p><ol><li><code>name</code>：使用者名稱</li><li><code>password</code>：密碼</li><li><code>lastchange</code>：上次修改時間 (為單一數值，從 1970/01/01 作為第1天起算) (改為 0，強制使用者必須在下次登入時改密碼)</li><li><code>minage</code>：密碼存活的最小生命周期(0 表示馬上可以改回來)</li><li><code>maxage</code>：密碼存活的最大生命周期(最大為 99999，從 <code>lastchange</code> 開始算)</li><li><code>warning</code>：提醒使用者的時間 (default: 7，七天前提醒)</li><li><code>inactive</code>：寬限期，最多密碼可以存活 (maxage + inactive) 天；在寬限期登入會被強制要求更改密碼</li><li><code>expire</code>：密碼失效日期，不受前面欄位影響，過期就表示密碼完全失效</li><li><code>blank</code>：保留作為未來使用</li></ol><p>說明第 2 個欄位 <code>password</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@desktop ~]$ sudo tail -3 /etc/shadow</span><br><span class="line">juliet:<span class="variable">$6</span><span class="variable">$TFn6B61u</span><span class="variable">$sA</span>/maknNkUeghjzgrGkoAiLYzNa/KqDQDR5A0m0PxTZeac4gdXQAleR.sxWcWK5VZnnSIhSAsD/WnIZr51MZA/:16864:0:99999:7:::</span><br><span class="line">romeo:<span class="variable">$6</span><span class="variable">$vRE6wD1I</span><span class="variable">$4eh6QNLPzyMw9pVjr</span>.YwOia6s8y1zixFq2LpHSi/n.Q/A45J/jqiBCNXRJan2rl0p6MwuL.91mf3IkRNZQUVv.:16864:0:99999:7:::</span><br><span class="line">hamlet:<span class="variable">$6</span><span class="variable">$Nu8r</span>/BWW<span class="variable">$PqoSIv9iRx9oQEH2ziw4L2WH0HMXs2YXFvEH8SFHmmKd</span>/WxeAV0qOweqAXUN6TE375W7ShHHLxP6i7jBcHsD/1:16864:0:99999:7:::</span><br></pre></td></tr></table></figure><p>以 user juliet <code>$6$TFn6B61u$sA/maknNkUeghjz</code> 為例：(以 <strong><font color='red'>$</font></strong> 作為分隔)</p><ol><li><p><code>6</code>：第一個部分，表示加密用的 hash 演算法，1 = MD5，6 = SHA-512 (RHEL 7 預設為 6)</p></li><li><p><code>TFn6B61u</code>：用來在加密密碼時用的 random 字串(salt)，加密時會一起用到，為了避免同樣密碼的使用者會有相同的 hash 結果</p></li><li><p><code>sA/maknNkUeghjz....</code>：hash(password + salt) 的結果</p></li></ol><blockquote><p><code>/etc/shadow</code> 的詳細說明，可以參考 <a href="http://linux.vbird.org/linux_basic/0410accountmanager.php#shadow_file">鳥哥的 Linux 私房菜 – 第十三章、Linux 帳號管理與 ACL 權限設定</a></p></blockquote><ol><li><p><code>name</code>：使用者帳號</p></li><li><p><code>password</code>：上述的密碼資訊</p></li><li><p><code>lastchange</code>：從 1970/01/01 起算的日期(0 表示強制使用者下次登入時變更密碼)</p></li><li><p><code>min days</code>：</p></li><li><p><code>max days</code></p></li><li><p><code>warning</code></p></li><li><p><code>inactive</code></p></li><li><p><code>expires</code></p></li><li><p><code>blank</code></p></li></ol><h3 id="5-5-2-Password-aging"><a href="#5-5-2-Password-aging" class="headerlink" title="5.5.2 Password aging"></a>5.5.2 Password aging</h3><p><img src="https://lh3.googleusercontent.com/2bDqeWVGlV0vGtV0lm6TE-1KwHmNixd6pUAaDGh30pcFJGF3tI3WUsV6dxUzdRbSPh7dR3mzznf_EdinGaKeWQA1xrA9cdOJFajn-jQFCcgEfni4Sdpn4zd-6R4T-Rj0Cezry17a0m_hI61F1nD8TjQFomv6NltRNseZgPgHVM1JU0or0xr8Y322N7MfhzoQIXeLQgQMLTsKtLAr19avk7A3yUQiZn5m6RI6T0rVqmW2FsXYwGEiQiDQlmg9POKDIndNH9LknDoIGHK5PAL_03cmbuY5EdouDrBMQu1PCESLO5GvZgzG-UIWNXMYk692Rai3Cpw6zNbyMUdwqpzvUaCgsPFHYcRfA9C3U1VJ4p-O2Q9iIKq5-8DWzEzGb7HELfYNwAHny8QGnXBGnHa7qtzVUJseFngyW3I2T-rCD7rriDbNYep4-V_fSqMSCNFSpVcs6Xnaj9TmnmXoegWDt3p43VTQxqPPb5C1LpyOK8mwwzFJdrYEjRJNmRzz6o4tUTqfbeay_UdKTzARA4MrsP6k2X5wHPWlDOao9eh7cMtRsyQpk9KE3Mp2l44mIHs_tuIE=w657-h235-no" alt="Linux Password Aging"></p><p>RHEL7 用來加密密碼的 hash algorithm 已經預設改為 <code>SHA-512</code>(<font color='red'><strong>6</strong></font>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 強制使用者下次登入修改密碼</span></span><br><span class="line">$ chage -d 0 USER_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以 human readable 的方式顯示 /etc/shadow 的內容</span></span><br><span class="line">$ chage -l USER_NAME</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定密碼失效日期 (也可以改用數字)</span></span><br><span class="line">$ chage -E YYYY-MM-DD USER_NAME</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檢視 juliet 目前的密碼期限相關設定</span></span><br><span class="line">[vagrant@desktop ~]$ sudo chage -l juliet</span><br><span class="line">Last password change                                    : Mar 04, 2016</span><br><span class="line">Password expires                                        : never</span><br><span class="line">Password inactive                                       : never</span><br><span class="line">Account expires                                         : never</span><br><span class="line">Minimum number of days between password change          : 0</span><br><span class="line">Maximum number of days between password change          : 99999</span><br><span class="line">Number of days of warning before password expires       : 7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 強制 juliet 在下次登入時變更密碼</span></span><br><span class="line">[vagrant@desktop ~]$ sudo chage -d 0 juliet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 -E YYYY-MM-dd 的設定方式將帳號過期時間設定為 45 天後</span></span><br><span class="line">[vagrant@desktop ~]$ sudo chage -E $(date -d <span class="string">&quot;+45 days&quot;</span> +%F) juliet</span><br></pre></td></tr></table></figure><h3 id="5-5-3-Restricting-access"><a href="#5-5-3-Restricting-access" class="headerlink" title="5.5.3 Restricting access"></a>5.5.3 Restricting access</h3><p>限制使用者存取的方式：</p><ol><li>Lock USER_NAME</li><li>給 Expiration Date</li><li>把 shell 給成 /sbin/nologin</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下兩個功能相同</span></span><br><span class="line">$ sudo usermod -L USER_NAME</span><br><span class="line">$ sudo passwd -l USER_NAME</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock user</span></span><br><span class="line">$ sudo usermod -L juliet</span><br><span class="line"></span><br><span class="line"><span class="comment"># unlock user</span></span><br><span class="line">$ sudo usermod -U juliet</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 nologin shell 給使用者，使用者就沒有 shell 可用(例如：mail service)</span></span><br><span class="line">$ sudo usermod -s /sbin/nologin juliet</span><br></pre></td></tr></table></figure><h3 id="5-5-4-Lab-Managing-Local-Linux-Users-and-Groups"><a href="#5-5-4-Lab-Managing-Local-Linux-Users-and-Groups" class="headerlink" title="5.5.4 Lab: Managing Local Linux Users and Groups"></a>5.5.4 Lab: Managing Local Linux Users and Groups</h3><p>修改 <code>/etc/login.defs</code>(<font color='red'><strong>PASS_MAX_DAYS</strong></font>) 將密碼預設過期日改為 30</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新增一個 GID 40000，名稱為 consultants 的群組</span></span><br><span class="line">$ groupadd -g 40000 consultants</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新增使用者，指定 GID 為 40000，並設定密碼為 default</span></span><br><span class="line">$ USR=sspade; useradd -G 40000 <span class="variable">$&#123;USR&#125;</span>; <span class="built_in">echo</span> <span class="string">&quot;default&quot;</span> | passwd --stdin <span class="variable">$&#123;USR&#125;</span></span><br><span class="line">$ USR=bboop; useradd -G 40000 <span class="variable">$&#123;USR&#125;</span>; <span class="built_in">echo</span> <span class="string">&quot;default&quot;</span> | passwd --stdin <span class="variable">$&#123;USR&#125;</span></span><br><span class="line">$ USR=dtracy; useradd -G 40000 <span class="variable">$&#123;USR&#125;</span>; <span class="built_in">echo</span> <span class="string">&quot;default&quot;</span> | passwd --stdin <span class="variable">$&#123;USR&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 sspade, bboop, dreacy 的密碼過期日(expiration date)為 90 天後</span></span><br><span class="line">$ chage -E $(date -d <span class="string">&quot;+90 days&quot;</span> +%Y-%m-%d) sspade</span><br><span class="line">$ chage -E $(date -d <span class="string">&quot;+90 days&quot;</span> +%Y-%m-%d) bboop</span><br><span class="line">$ chage -E $(date -d <span class="string">&quot;+90 days&quot;</span> +%Y-%m-%d) dtracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 限制 bboop 每 15 天要改一次密碼</span></span><br><span class="line">$ chage -M 15 bboop</span><br><span class="line"></span><br><span class="line"><span class="comment"># 強制 sspade, bboop, dreacy 下次登入時要改密碼</span></span><br><span class="line">$ chage -d 0 sspade</span><br><span class="line">$ chage -d 0 bboop</span><br><span class="line">$ chage -d 0 dtracy</span><br></pre></td></tr></table></figure><h2 id="5-6-Practice-Managing-User-Password-Aging"><a href="#5-6-Practice-Managing-User-Password-Aging" class="headerlink" title="5.6 Practice: Managing User Password Aging"></a>5.6 Practice: Managing User Password Aging</h2><p>Lock User：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lock 使用者 romeo</span></span><br><span class="line">[vagrant@desktop ~]$ sudo usermod -L romeo</span><br><span class="line"><span class="comment"># 無法切換使用者，因為已經 lock</span></span><br><span class="line">[vagrant@desktop ~]$ su - romeo</span><br><span class="line">Password:</span><br><span class="line">su: Authentication failure</span><br><span class="line"></span><br><span class="line"><span class="comment"># unlock 之後便可登入</span></span><br><span class="line">[vagrant@desktop ~]$ sudo usermod -U romeo</span><br><span class="line">[vagrant@desktop ~]$ su - romeo</span><br><span class="line">Password:</span><br><span class="line">Last login: Wed Mar  9 14:39:24 UTC 2016 on pts/0</span><br><span class="line">Last failed login: Wed Mar  9 14:40:51 UTC 2016 on pts/0</span><br><span class="line">There were 2 failed login attempts since the last successful login.</span><br><span class="line">[romeo@desktop ~]$</span><br></pre></td></tr></table></figure><p>設定密碼過期時間 90 天 &amp; 下次登入時修改密碼：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@desktop ~]$ sudo chage -l romeo</span><br><span class="line">Last password change                                    : Mar 09, 2016</span><br><span class="line">Password expires                                        : never</span><br><span class="line">Password inactive                                       : never</span><br><span class="line">Account expires                                         : never</span><br><span class="line">Minimum number of days between password change          : 0</span><br><span class="line">Maximum number of days between password change          : 99999</span><br><span class="line">Number of days of warning before password expires       : 7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定密碼過期時間 90 天</span></span><br><span class="line">[vagrant@desktop ~]$ sudo chage -M 90 romeo</span><br><span class="line">[vagrant@desktop ~]$ sudo chage -l romeo</span><br><span class="line">Last password change                                    : Mar 09, 2016</span><br><span class="line">Password expires                                        : Jun 07, 2016</span><br><span class="line">Password inactive                                       : never</span><br><span class="line">Account expires                                         : never</span><br><span class="line">Minimum number of days between password change          : 0</span><br><span class="line">Maximum number of days between password change          : 90</span><br><span class="line">Number of days of warning before password expires       : 7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 強制使用者下次登入時變更密碼</span></span><br><span class="line">[vagrant@desktop ~]$ sudo chage -l romeo</span><br><span class="line">Last password change                                    : password must be changed</span><br><span class="line">Password expires                                        : password must be changed</span><br><span class="line">Password inactive                                       : password must be changed</span><br><span class="line">Account expires                                         : never</span><br><span class="line">Minimum number of days between password change          : 0</span><br><span class="line">Maximum number of days between password change          : 90</span><br><span class="line">Number of days of warning before password expires       : 7</span><br></pre></td></tr></table></figure><p>設定使用者密碼過期時間為 180 天後：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@desktop ~]$ sudo chage -E $(date -d <span class="string">&quot;+180 days&quot;</span> +%F) romeo</span><br><span class="line">[vagrant@desktop ~]$ sudo chage -l romeo</span><br><span class="line">Last password change                                    : Mar 09, 2016</span><br><span class="line">Password expires                                        : Jun 07, 2016</span><br><span class="line">Password inactive                                       : never</span><br><span class="line">Account expires                                         : Sep 05, 2016</span><br><span class="line">Minimum number of days between password change          : 0</span><br><span class="line">Maximum number of days between password change          : 90</span><br><span class="line">Number of days of warning before password expires       : 7</span><br></pre></td></tr></table></figure><hr><h1 id="Chapter-6-Controlling-Access-to-Files-with-Linux-File-System-Permissions"><a href="#Chapter-6-Controlling-Access-to-Files-with-Linux-File-System-Permissions" class="headerlink" title="Chapter 6. Controlling Access to Files with Linux File System Permissions"></a>Chapter 6. Controlling Access to Files with Linux File System Permissions</h1><h2 id="6-1-Linux-File-System-Permissions"><a href="#6-1-Linux-File-System-Permissions" class="headerlink" title="6.1 Linux File System Permissions"></a>6.1 Linux File System Permissions</h2><h3 id="6-1-1-Linux-file-system-permissions"><a href="#6-1-1-Linux-file-system-permissions" class="headerlink" title="6.1.1 Linux file system permissions"></a>6.1.1 Linux file system permissions</h3><blockquote><p>新增/刪除檔案不是看檔案本身權限，而是看上層目錄的權限</p></blockquote><p>當使用者擁有目錄的 <code>w(write)</code> &amp; <code>x(execute)</code> 權限時，可以刪除該目錄中自己也沒有權限的檔案，但這問題可以透過 <code>sticky bit</code> 來解決!</p><p>正常對目錄有存取權限的使用者，會同時有 <code>r(read)</code> &amp; <code>w(write)</code> 兩個權限：</p><ul><li><p>若沒有目錄的 <code>r(read)</code> 權限，使用者無法列出目錄中的檔案，但若知道明確檔名還是可以存取</p></li><li><p>若沒有目錄的 <code>w(write)</code> 權限，就只能列出目錄中的檔案內容，但都無法存取目錄中的任何檔案(連 timestamp 資訊都看不到)</p></li><li><p>但如果進不了目錄(<strong><font color='red'>沒有 execute 權限</font></strong>)，還是無法刪除檔案</p></li></ul><h3 id="6-1-2-Viewing-file-directory-permissions-and-ownership"><a href="#6-1-2-Viewing-file-directory-permissions-and-ownership" class="headerlink" title="6.1.2 Viewing file/directory permissions and ownership"></a>6.1.2 Viewing file/directory permissions and ownership</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出目錄中所有子目錄 &amp; 檔案的相關權限資訊</span></span><br><span class="line">[vagrant@server ~]$ ls -l /home</span><br><span class="line">total 4</span><br><span class="line">drwx------. 6 vagrant vagrant 4096 Mar 30 20:23 vagrant</span><br><span class="line"></span><br><span class="line"><span class="comment"># 透過 &#x27;-d&#x27; 參數，僅列出指定目錄的權限 (權限中的最後一個 . 是作為 ACL 控制用)</span></span><br><span class="line">[vagrant@server ~]$ ls -ld /home</span><br><span class="line">drwxr-xr-x. 3 root root 20 Jan  3 04:22 /home</span><br></pre></td></tr></table></figure><h2 id="6-2-Managing-File-System-Permissions-from-the-Command-Line"><a href="#6-2-Managing-File-System-Permissions-from-the-Command-Line" class="headerlink" title="6.2 Managing File System Permissions from the Command Line"></a>6.2 Managing File System Permissions from the Command Line</h2><h3 id="6-2-1-Changing-file-directory-permissions"><a href="#6-2-1-Changing-file-directory-permissions" class="headerlink" title="6.2.1 Changing file/directory permissions"></a>6.2.1 Changing file/directory permissions</h3><p>透過 chmod -R 以遞迴的方式指定檔案 &amp; 目錄的權限時，若包含了 x(execute) 權限，會讓檔案 &amp; 目錄同時都有 x(execute) 的權限，但這通常不是我們需要的結果；一般我們只會希望只有目錄才需要 x(execute) 權限，此時只要加上大寫 <code>W</code> 參數即可：</p><p>例如：<code>chmod -R g+rwX somedir</code> 表示 somedir 目錄下所有的檔案都給 rw 權限，目錄則是給 rwx 權限。</p><h3 id="6-2-2-Changing-file-directory-user-or-group-ownership"><a href="#6-2-2-Changing-file-directory-user-or-group-ownership" class="headerlink" title="6.2.2 Changing file/directory user or group ownership"></a>6.2.2 Changing file/directory user or group ownership</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下兩個指令功能相同，都是修改檔案的 group ownership</span></span><br><span class="line">[vagrant@server ch06]$ sudo chown :vboxsf 1st/aa</span><br><span class="line">[vagrant@server ch06]$ sudo chgrp vboxsf 1st/aa</span><br></pre></td></tr></table></figure><blockquote><p>只有 root 可以修改檔案的 ownership，一般使用者儘可以針對自己所屬的群組設定 ownership</p></blockquote><p>一般 web 網站的目錄，只會開啟目錄的 execute 權限，並且讓目錄內的檔案有 others read 的權限，讓使用者可以進入目錄，可以存取目錄中檔案的內容，但卻無法列出目錄中所有的檔案</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 檔案 file1 拿掉 group &amp; others 的 read &amp; write 權限</span></span><br><span class="line">$ chmod go-rw file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 針對多層目錄 &amp; 檔案透過遞迴的方式設定權限(group 給予所有權限)</span></span><br><span class="line">$ chmod -R g+rwx multi_layer_dir</span><br><span class="line"></span><br><span class="line"><span class="comment"># 針對多層目錄(只有目錄，沒有檔案)，使用遞迴的方式指定所有人有 execute 的權限</span></span><br><span class="line"><span class="comment"># 透過大寫 X，指定套用權限時僅會套用在目錄上，不會在檔案上</span></span><br><span class="line">$ chmod -R a+X multi_layer_dir</span><br></pre></td></tr></table></figure><p>只有 root 可以變更檔案的 ownership</p><blockquote><p>例外：檔案擁有者可以改檔案所屬群組，但只可以改成屬於自己群組</p></blockquote><h2 id="6-3-Managing-Default-Permissions-and-File-Access"><a href="#6-3-Managing-Default-Permissions-and-File-Access" class="headerlink" title="6.3 Managing Default Permissions and File Access"></a>6.3 Managing Default Permissions and File Access</h2><h3 id="6-3-1-Special-permissions"><a href="#6-3-1-Special-permissions" class="headerlink" title="6.3.1 Special permissions"></a>6.3.1 Special permissions</h3><p>若檔案的 execute 權限標示為 <code>setuid</code> or <code>setgid</code> 時(以 <code>s</code> 表示權限)，表示此檔案不論是哪個使用者執行，會以檔案 owner 的身分(<code>setgid</code> 會以群組的身分)執行，而不是執行檔案的使用者。 例如：**/etc/passwd**</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ ls -al /usr/bin/passwd</span><br><span class="line">-rwsr-xr-x. 1 root root 27832 Jun 10  2014 /usr/bin/passwd</span><br></pre></td></tr></table></figure><p>若 sticky bit(以 <code>t</code> 表示權限) 位於 directory 時，以 <strong><font color='red'>/tmp</font></strong> 為例，<strong>只有檔案的擁有者才可以刪除檔案</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ ls -ld /tmp/</span><br><span class="line">drwxrwxrwt. 7 root root 88 Apr  3 01:00 /tmp/</span><br></pre></td></tr></table></figure><table><thead><tr><th>設定方式</th><th>在檔案上的效果</th><th>在目錄上的效果</th></tr></thead><tbody><tr><td><code>u+s</code>(4)</td><td>檔案執行時會以 owner user 的權限執行</td><td>N/A</td></tr><tr><td><code>g+s</code>(2)</td><td>檔案執行時會以 owner group 的權限執行</td><td>在此目錄中新建立的檔案會擁有與目錄相同的 group 權限</td></tr><tr><td><code>o+t</code>(1)</td><td>N/A</td><td>在此目錄中，使用者僅能移除他們自己所建立的檔案</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setuid</span></span><br><span class="line">$ chmod u+s some_file</span><br><span class="line"></span><br><span class="line"><span class="comment"># setgid</span></span><br><span class="line">$ chmod g+s,o-rx some_dir</span><br><span class="line">$ chmod 2770 some_dir</span><br></pre></td></tr></table></figure><h4 id="SetUID"><a href="#SetUID" class="headerlink" title="SetUID"></a>SetUID</h4><ul><li>檔案<blockquote><p>-rwsr-xr-x 1 root root 47032 Jul 16  2015 /usr/bin/passwd -&gt; /etc/shadow<br>passwd 指令的 owner 為 root，因此執行此指令時是以 root 的權限執行</p></blockquote></li></ul><h4 id="SetGID"><a href="#SetGID" class="headerlink" title="SetGID"></a>SetGID</h4><ul><li><p>檔案</p><blockquote><p>/usr/bin/locate -&gt; /var/lib/mlocate/mlocate.db</p></blockquote></li><li><p>目錄</p><blockquote><p>SetGID 設定在目錄上，則表示在該目錄中建立的檔案 or 目錄的擁有群組都會被強制設定為該目錄的擁有群組</p></blockquote></li></ul><h4 id="Sticky-Bit"><a href="#Sticky-Bit" class="headerlink" title="Sticky Bit"></a>Sticky Bit</h4><ul><li>目錄<blockquote><p><code>o+t</code> 表示該目錄中的檔案只有擁有者可以移除</p></blockquote></li></ul><blockquote><p>**<font color='red'>T</font>**：表示 others 原本沒有 execute 權限</p></blockquote><blockquote><p>**<font color='red'>t</font>**：表示 others 原本有 execute 權限</p></blockquote><h3 id="6-3-2-Default-file-permissions"><a href="#6-3-2-Default-file-permissions" class="headerlink" title="6.3.2 Default file permissions"></a>6.3.2 Default file permissions</h3><p>Default permission (system)</p><ul><li>File: <code>666</code></li><li>Directory: <code>777</code></li></ul><p>Default Umask:</p><ul><li>root：<code>022</code></li><li>regular user：<code>002</code></li></ul><p>umask 使用 3 個數字進行修改，若少於 3 個數字，前面會被自動補 0；且更改的效果僅限於該 terminal session 中，重新登入後就會無效。</p><p>umask 的設定，若是要設定 global 的，可以到 <strong><font color='red'>/etc/profile</font></strong> &amp; <strong><font color='red'>/etc/bashrc</font></strong> 中進行調整</p><p>如果要進行個人化設定，則可以到 <strong><font color='red'>~/.bash_profile</font></strong> &amp; <strong><font color='red'>~/.bashrc</font></strong> 中進行調整</p>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[RHCE] RH124 Chapter 1~3 學習筆記</title>
      <link href="/blog/RHCE/RHCE7-RH124-LearningNotes-CH01_03/"/>
      <url>/blog/RHCE/RHCE7-RH124-LearningNotes-CH01_03/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h3 id="Internationalization"><a href="#Internationalization" class="headerlink" title="Internationalization"></a>Internationalization</h3><p>若想要讓 desktop &amp; console 環境的語系一致，可以加入以下的 script 到 ~/.bashrc 中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">i=$(grep <span class="string">&#x27;Language&#x27;</span> /var/lib/AccountsService/users/<span class="variable">$&#123;USER&#125;</span> | \</span><br><span class="line">sed <span class="string">&#x27;s/Language=//&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$i</span>&quot;</span> != <span class="string">&quot;&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">export</span> LANG=<span class="variable">$i</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><p>透過 <font color='red'><code>locale</code></font> 可以查詢目前語系相關的設定 &amp; 環境變數：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ locale</span><br><span class="line">LANG=zh_TW.UTF-8</span><br><span class="line">.....(LANG-related environment variables)</span><br><span class="line">LC_ALL=</span><br></pre></td></tr></table></figure><p>若要修改整個系統的預設語系，可以透過以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ localectl set-locale LANG=zh_TW.UTF-8</span><br></pre></td></tr></table></figure><p>或是修改 <font color='red'><code>/etc/locale.conf</code></font> 檔案的內容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/locale.conf</span><br><span class="line">LANG=zh_TW.UTF-8</span><br></pre></td></tr></table></figure><hr><a name="ch1"/>Chapter 1. ACCESSING THE COMMAND LINE=====================================<h2 id="1-1-Accessing-the-Command-Line-Using-the-Local-Console"><a href="#1-1-Accessing-the-Command-Line-Using-the-Local-Console" class="headerlink" title="1.1 Accessing the Command Line Using the Local Console"></a>1.1 Accessing the Command Line Using the Local Console</h2><h3 id="Virtual-Console"><a href="#Virtual-Console" class="headerlink" title="Virtual Console"></a>Virtual Console</h3><p>在 RHEL 7 中，若有 GUI 環境，則會預設執行在第一個 virtual console，另外還會包含 5 個文字模式的 virtual console，可使用 <code>Ctrl + Alt + F[1-6]</code> 在不同的 virtual console 間切換。</p><p>若沒有 GUI 環境，則 6 個 virtual console 都會是純文字模式。</p><p>要調整 virtual console 的數量，可修改 <code>/etc/systemd/login.conf</code> 中的 <strong><font color='red'>NAutoVTs</font></strong> 的選項，</p><h2 id="1-2-Accrssing-the-Command-Line-Using-the-Desktop"><a href="#1-2-Accrssing-the-Command-Line-Using-the-Desktop" class="headerlink" title="1.2 Accrssing the Command Line Using the Desktop"></a>1.2 Accrssing the Command Line Using the Desktop</h2><h3 id="Windows-連線至-Linux-GUI"><a href="#Windows-連線至-Linux-GUI" class="headerlink" title="Windows 連線至 Linux GUI"></a>Windows 連線至 Linux GUI</h3><p>要從 Windows 連線到 Linux GUI，可使用 <a href="http://ntu.csie.org/~piaip/pietty/">pietty</a> + <a href="http://sourceforge.net/projects/xming/">Xming</a></p><p>使用說明可參考 =&gt; <a href="http://blog.jangmt.com/2009/11/xming.html">八克里: 使用 xming 從windows 系統登入 Linux 系統</a></p><h3 id="Auto-Login"><a href="#Auto-Login" class="headerlink" title="Auto Login"></a>Auto Login</h3><p>要在 RHEL 7 作到 Auto Login，要修改 <code>/etc/gdm/custom.conf</code>，並調整 <font color='blue'><strong>daemon</font></strong> section 中的 <strong><font color='red'>AutomaticLoginEnable</font></strong> &amp; <strong><font color='red'>AutomaticLogin</font></strong> 兩個參數</p><h2 id="1-3-Executing-Commands-Using-the-Bash-Shell"><a href="#1-3-Executing-Commands-Using-the-Bash-Shell" class="headerlink" title="1.3 Executing Commands Using the Bash Shell"></a>1.3 Executing Commands Using the Bash Shell</h2><h3 id="Examples-of-simple-commands"><a href="#Examples-of-simple-commands" class="headerlink" title="Examples of simple commands"></a>Examples of simple commands</h3><p><strong><font color='red'>file</font></strong> 可用來檢查檔案的型態 &amp; 格式 (也可以用 <strong><a href="http://www.computerhope.com/unix/stat.htm">stat</a></strong>)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ file /etc/passwd</span><br><span class="line">/etc/passwd: ASCII text</span><br><span class="line"></span><br><span class="line">$ file /bin/passwd</span><br><span class="line">/bin/passwd: setuid ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked (uses shared libs), <span class="keyword">for</span> GNU/Linux 2.6.32, BuildID[sha1]=0x91a7160a019b7f5f754264d920e257522c5bce67, stripped</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">stat</span> /etc/passwd</span><br><span class="line">  File: ‘/etc/passwd’</span><br><span class="line">  Size: 961             Blocks: 8          IO Block: 4096   regular file</span><br><span class="line">Device: fd01h/64769d    Inode: 1573453     Links: 1</span><br><span class="line">Access: (0644/-rw-r--r--)  Uid: (    0/    root)   Gid: (    0/    root)</span><br><span class="line">Context: system_u:object_r:passwd_file_t:s0</span><br><span class="line">Access: 2016-01-20 07:38:22.557000000 -0500</span><br><span class="line">Modify: 2015-10-02 10:38:00.710867846 -0400</span><br><span class="line">Change: 2015-10-02 10:38:00.710867846 -0400</span><br><span class="line"> Birth: -</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">stat</span> /bin/passwd</span><br><span class="line">  File: ‘/bin/passwd’</span><br><span class="line">  Size: 27832           Blocks: 56         IO Block: 4096   regular file</span><br><span class="line">Device: fd01h/64769d    Inode: 2234382     Links: 1</span><br><span class="line">Access: (4755/-rwsr-xr-x)  Uid: (    0/    root)   Gid: (    0/    root)</span><br><span class="line">Context: system_u:object_r:passwd_exec_t:s0</span><br><span class="line">Access: 2014-06-10 02:27:56.000000000 -0400</span><br><span class="line">Modify: 2014-06-10 02:27:56.000000000 -0400</span><br><span class="line">Change: 2015-10-02 10:30:36.743867846 -0400</span><br><span class="line"> Birth: -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 甚至可以直接針對目錄來檢查</span></span><br><span class="line">$ file /home</span><br><span class="line">/home: directory</span><br></pre></td></tr></table></figure><h3 id="Editing-the-command-Line"><a href="#Editing-the-command-Line" class="headerlink" title="Editing the command Line"></a>Editing the command Line</h3><p>使用 command line 的實用快速鍵：</p><table><thead><tr><th>快速鍵</th><th>說明</th></tr></thead><tbody><tr><td><code>Ctrl + a</code></td><td>游標跳至最前</td></tr><tr><td><code>Ctrl + e</code></td><td>游標跳至最後</td></tr><tr><td><code>Ctrl + u</code></td><td>清除整個命令中，從游標到最前面的內容</td></tr><tr><td><code>Ctrl + k</code></td><td>清除整個命令中，從游標到最後面的內容</td></tr><tr><td>`ESC + .</td><td>複製上一個命令中的最後一個參數到目前的命令中</td></tr><tr><td>`Alt + .</td><td>同上</td></tr><tr><td><code>Ctrl + r</code></td><td>可用 keyword 來尋找最近使用過的命令</td></tr><tr><td><code>Ctrl + l(小寫 L)</code></td><td>清除螢幕內容(效果等同 <code>clear</code>)</td></tr></tbody></table><hr><a name="ch2"/>Chapter 2. MANAGING FILES FROM THE COMMAND LINE===============================================<h2 id="2-1-The-file-system-hierarchy"><a href="#2-1-The-file-system-hierarchy" class="headerlink" title="2.1 The file system hierarchy"></a>2.1 The file system hierarchy</h2><p>RHEL 中的重要目錄：</p><table><thead><tr><th>路徑</th><th>目的</th></tr></thead><tbody><tr><td><code>/usr</code><br />(Unix Software Resource)</td><td>安裝的軟體、shared library … 等資料都會放在此處，其中幾個重要目錄：<br /><code>/usr/bin</code>：使用者用指令<br /><code>/usr/sbin</code>：系統管理者用指令<br /><code>/usr/local</code>：使用者自行安裝的軟體</td></tr><tr><td><code>/etc</code></td><td>設定檔存放路徑</td></tr><tr><td><code>/var</code></td><td>持續不斷變動的資料，例如 log、print spool、資料庫檔案 … 等等</td></tr><tr><td><code>/run</code></td><td>從上次開機以來的 runtime 資訊(<strong><font color='red'>此目錄的資料在每次重開機都會清空</font></strong>)</td></tr><tr><td><code>/tmp</code></td><td>所有人都有權限存取的站存資料目錄(<strong><font color='red'>此目錄中日期大於 10 天的資料會被自動清除</font></strong>)，若是在目錄 <code>/var/tmp</code> 中的資料，則是超過 30 天的資料會被清除</td></tr><tr><td><code>/boot</code></td><td>系統開機所需要的檔案</td></tr><tr><td><code>/dev</code></td><td>存放系統用來存取硬體裝置所需要的檔案</td></tr></tbody></table><blockquote><p>原本在 <code>/</code> 下的某些目錄，在 RHEL 7 後都被移到 <code>/usr</code> 下了，包含 <code>/bin</code>(=&gt; <code>/usr/bin</code>)、<code>/sbin</code>(=&gt; <code>/usr/sbin</code>)、<code>/lib</code>(=&gt; <code>/usr/lib</code>)、<code>/lib64</code>(=&gt; <code>/usr/lib64</code>)<br>但原本在 <code>/</code> 的以上四個目錄都還存在，只是改成用 symbolic link 的方式連到 <code>/usr</code> 中的子目錄</p></blockquote><p>詳細資料可查詢 <font color='blue'><strong>hier(7)</font></strong> man page</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ man 7 hier</span><br></pre></td></tr></table></figure><h2 id="2-2-Locating-Files-by-name"><a href="#2-2-Locating-Files-by-name" class="headerlink" title="2.2 Locating Files by name"></a>2.2 Locating Files by name</h2><p>檔名限制 &amp; 特性：</p><ol><li><p>完整檔案路徑長度不能超過 4095 bytes (含 <code>/</code>)</p></li><li><p>兩個 <code>/</code> 之間的長度不能超過 255 bytes</p></li><li><p>檔名可以是任意的 UTF-8 字元，但不能是 <code>/</code> &amp; <code>NUL</code></p></li><li><p>Case-Sensative</p></li></ol><h3 id="Navigating-paths"><a href="#Navigating-paths" class="headerlink" title="Navigating paths"></a>Navigating paths</h3><ul><li><strong>touch</strong><blockquote><p>touch 會更新檔案的 timestamp 到目前的時間，而不會改變檔案內容<br>若是不存在的檔案，則會建立一個空白檔案</p></blockquote></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ touch /tmp/<span class="built_in">test</span>&#123;1,2&#125;.txt</span><br><span class="line">[vagrant@server ~]$ ls -l /tmp/</span><br><span class="line">total 0</span><br><span class="line">-rw-rw-r--. 1 vagrant vagrant 0 Jan 30 00:23 test1.txt</span><br><span class="line">-rw-rw-r--. 1 vagrant vagrant 0 Jan 30 00:23 test2.txt</span><br></pre></td></tr></table></figure><h2 id="2-3-Managing-Files-Using-command-Line-Tools"><a href="#2-3-Managing-Files-Using-command-Line-Tools" class="headerlink" title="2.3 Managing Files Using command-Line Tools"></a>2.3 Managing Files Using command-Line Tools</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保留複製檔案的屬性</span></span><br><span class="line">$ cp -a</span><br></pre></td></tr></table></figure><h2 id="2-4-Matching-File-Names-Using-Path-Name-Expansion"><a href="#2-4-Matching-File-Names-Using-Path-Name-Expansion" class="headerlink" title="2.4 Matching File Names Using Path Name Expansion"></a>2.4 Matching File Names Using Path Name Expansion</h2><h3 id="File-globbing-path-name-Expansion"><a href="#File-globbing-path-name-Expansion" class="headerlink" title="File globbing: path name Expansion"></a>File globbing: path name Expansion</h3><table><thead><tr><th>Pattern</th><th>Matches</th></tr></thead><tbody><tr><td><code>~+</code></td><td>目前工作目錄</td></tr><tr><td><code>~-</code></td><td>上一個工作目錄</td></tr><tr><td><code>[abc...]</code></td><td>任何在中括號中的字母都符合</td></tr><tr><td><code>[!abc...]</code></td><td>不包含中括號中的任何一個字母</td></tr><tr><td><code>[^abc...]</code></td><td>同上</td></tr><tr><td><code>[[:punct:]]</code></td><td>任何可印出來的字元(但不包含空白 or 英文字母)</td></tr></tbody></table><h4 id="Brace-Expansion"><a href="#Brace-Expansion" class="headerlink" title="Brace Expansion"></a>Brace Expansion</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ <span class="built_in">echo</span> &#123;Sunday,Month,Tuesday&#125;.lo</span><br><span class="line">Sunday.log Month.log Tuesday.log</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ <span class="built_in">echo</span> file&#123;a..c&#125;.txt</span><br><span class="line">filea.txt fileb.txt filec.txt</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ <span class="built_in">echo</span> file&#123;a,b&#125;&#123;1,2&#125;.txt</span><br><span class="line">filea1.txt filea2.txt fileb1.txt fileb2.txt</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ <span class="built_in">echo</span> file&#123;a&#123;1,2&#125;,b,c&#125;.txt</span><br><span class="line">filea1.txt filea2.txt fileb.txt filec.txt</span><br></pre></td></tr></table></figure><h4 id="Command-Substitution"><a href="#Command-Substitution" class="headerlink" title="Command Substitution"></a>Command Substitution</h4><ul><li><p>單引號中的變數 $ 無效</p></li><li><p>雙引號中的變數 $ 有效</p></li><li><p>較推薦加大括號確定變數名稱的方式，比較不容易跟單引號混淆</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ <span class="built_in">echo</span> Today is `date +%A`</span><br><span class="line">Today is Saturday</span><br><span class="line"></span><br><span class="line">[vagrant@server ~]$ <span class="built_in">echo</span> The time is $(date +%M) minutes past $(date +%l%p)</span><br><span class="line">The time is 33 minutes past 8AM</span><br></pre></td></tr></table></figure><h2 id="2-5-Lab-Managing-Files-with-Shell-Expansion"><a href="#2-5-Lab-Managing-Files-with-Shell-Expansion" class="headerlink" title="2.5 Lab: Managing Files with Shell Expansion"></a>2.5 Lab: Managing Files with Shell Expansion</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server lab]$ touch tv_season&#123;1,2&#125;_episode&#123;1..6&#125;.ogg</span><br><span class="line">[vagrant@server lab]$ ls</span><br><span class="line">tv_season1_episode1.ogg  tv_season1_episode3.ogg  tv_season1_episode5.ogg  tv_season2_episode1.ogg  tv_season2_episode3.ogg  tv_season2_episode5.ogg</span><br><span class="line">tv_season1_episode2.ogg  tv_season1_episode4.ogg  tv_season1_episode6.ogg  tv_season2_episode2.ogg  tv_season2_episode4.ogg  tv_season2_episode6.ogg</span><br><span class="line"></span><br><span class="line">[vagrant@server lab]$ touch mystery_chapter&#123;1..8&#125;.odf</span><br><span class="line">[vagrant@server lab]$ ls</span><br><span class="line">mystery_chapter1.odf  mystery_chapter4.odf  mystery_chapter7.odf     tv_season1_episode2.ogg  tv_season1_episode5.ogg  tv_season2_episode2.ogg  tv_season2_episode5.ogg</span><br><span class="line">mystery_chapter2.odf  mystery_chapter5.odf  mystery_chapter8.odf     tv_season1_episode3.ogg  tv_season1_episode6.ogg  tv_season2_episode3.ogg  tv_season2_episode6.ogg</span><br><span class="line">mystery_chapter3.odf  mystery_chapter6.odf  tv_season1_episode1.ogg  tv_season1_episode4.ogg  tv_season2_episode1.ogg  tv_season2_episode4.ogg</span><br><span class="line"></span><br><span class="line">[vagrant@server lab]$ mkdir -p Videos/season&#123;1,2&#125;</span><br><span class="line">[vagrant@server lab]$ ls Videos/</span><br><span class="line">season1  season2</span><br><span class="line"></span><br><span class="line">[vagrant@server lab]$ mv tv_season1* Videos/season1/</span><br><span class="line">[vagrant@server lab]$ mv tv_season2* Videos/season2/</span><br><span class="line">[vagrant@server lab]$ ls -R Videos/</span><br><span class="line">Videos/:</span><br><span class="line">season1  season2</span><br><span class="line"></span><br><span class="line">Videos/season1:</span><br><span class="line">tv_season1_episode1.ogg  tv_season1_episode2.ogg  tv_season1_episode3.ogg  tv_season1_episode4.ogg  tv_season1_episode5.ogg  tv_season1_episode6.ogg</span><br><span class="line"></span><br><span class="line">Videos/season2:</span><br><span class="line">tv_season2_episode1.ogg  tv_season2_episode2.ogg  tv_season2_episode3.ogg  tv_season2_episode4.ogg  tv_season2_episode5.ogg  tv_season2_episode6.ogg</span><br><span class="line"></span><br><span class="line">[vagrant@server lab]$ mkdir -p ./my_bestseller ./chapters</span><br><span class="line"></span><br><span class="line">[vagrant@server lab]$ mkdir ./my_bestseller/&#123;editor,plot_change,vacation&#125;</span><br><span class="line">[vagrant@server lab]$ ls ./my_bestseller/</span><br><span class="line">editor  plot_change  vacation</span><br><span class="line"></span><br><span class="line">[vagrant@server lab]$ <span class="built_in">cd</span> chapters/</span><br><span class="line">[vagrant@server chapters]$ mv ../*chapter*.odf ./</span><br><span class="line"></span><br><span class="line">[vagrant@server chapters]$ mv mystery_chapter&#123;1,2&#125;.odf ../my_bestseller/editor/</span><br><span class="line"></span><br><span class="line">[vagrant@server chapters]$ mv mystery_chapter&#123;7,8&#125;.odf ../my_bestseller/vacation/</span><br><span class="line"></span><br><span class="line">[vagrant@server chapters]$ <span class="built_in">cd</span> ../Videos/season2/</span><br><span class="line">[vagrant@server season2]$ cp tv_season2_episode1.ogg ../../my_bestseller/vacation/</span><br><span class="line"></span><br><span class="line">[vagrant@server season2]$ <span class="built_in">cd</span> /tmp/lab/my_bestseller/vacation/</span><br><span class="line">[vagrant@server vacation]$ ls</span><br><span class="line">mystery_chapter7.odf  mystery_chapter8.odf  tv_season2_episode1.ogg</span><br><span class="line">[vagrant@server vacation]$ <span class="built_in">cd</span> ~-</span><br><span class="line">[vagrant@server season2]$ cp tv_season2_episode2.ogg ~-</span><br><span class="line">[vagrant@server season2]$ <span class="built_in">cd</span> ~-</span><br><span class="line">[vagrant@server vacation]$ ls</span><br><span class="line">mystery_chapter7.odf  mystery_chapter8.odf  tv_season2_episode1.ogg  tv_season2_episode2.ogg</span><br><span class="line"></span><br><span class="line">[vagrant@server vacation]$ cp /tmp/lab/chapters/mystery_chapter5.odf /tmp/lab/my_bestseller/plot_change/mystery_chapter5_$(date +%F).odf</span><br><span class="line">[vagrant@server vacation]$ cp /tmp/lab/chapters/mystery_chapter5.odf /tmp/lab/my_bestseller/plot_change/mystery_chapter5_$(date +%s).odf</span><br><span class="line">[vagrant@server vacation]$ cp /tmp/lab/chapters/mystery_chapter5.odf /tmp/lab/my_bestseller/plot_change/mystery_chapter5_<span class="variable">$USER</span>.odf</span><br><span class="line">[vagrant@server vacation]$ ls /tmp/lab/my_bestseller/plot_change/</span><br><span class="line">mystery_chapter5_1455368579.odf  mystery_chapter5_2016-02-13.odf  mystery_chapter5_vagrant.odf</span><br></pre></td></tr></table></figure><hr><a name="ch3"/>Chapter 3. GETTING HELP IN RED HAT ENTERPRISE LINUX===================================================<h2 id="3-1-Reading-Documentation-Using-man-Command"><a href="#3-1-Reading-Documentation-Using-man-Command" class="headerlink" title="3.1 Reading Documentation Using man Command"></a>3.1 Reading Documentation Using man Command</h2><h3 id="3-1-1-Introducing-the-man-command"><a href="#3-1-1-Introducing-the-man-command" class="headerlink" title="3.1.1 Introducing the man command"></a>3.1.1 Introducing the man command</h3><p>Linux manual 包含多個 section：</p><table><thead><tr><th>Section</th><th>Content Type</th></tr></thead><tbody><tr><td><strong><font color='red'>1</font></strong></td><td>一般使用者命令(包含可執行程式 &amp; shell script)</td></tr><tr><td><code>2</code></td><td>System calls(kernal routines invoked from user space)</td></tr><tr><td><code>3</code></td><td>Library functions (程式函式庫提供)</td></tr><tr><td><code>4</code></td><td>特殊檔案(例如：設備檔 /dev 目錄中的檔案)</td></tr><tr><td><strong><font color='red'>5</font></strong></td><td><font color='blue'>檔案格式(設定檔 &amp; 內容結構說明)</font></td></tr><tr><td><code>6</code></td><td>Games</td></tr><tr><td><strong><font color='red'>7</font></strong></td><td>慣例、標準、其他…等等(協定、檔案系統)</td></tr><tr><td><strong><font color='red'>8</font></strong></td><td><font color='blue'>系統管理員以及特殊指令(用於維護工作)</font></td></tr><tr><td><code>9</code></td><td>Linux kernal API (internal kernel calls)</td></tr></tbody></table><p><code>man 1 passwd</code> or <code>man passwd</code>(未指定 section 則預設帶 1) 可以知道使用指令的方式 &amp; 相關參數</p><p><code>man 5 passwd</code> 則是說明 <strong>/etc/passwd</strong> 的檔案結構，組成內容….等資訊</p><blockquote><p>以上資訊要安裝 <strong><font color='red'>man-pages</font></strong> 套件才會有</p></blockquote><h3 id="3-1-3-Searching-for-man-pages-by-keywords"><a href="#3-1-3-Searching-for-man-pages-by-keywords" class="headerlink" title="3.1.3 Searching for man pages by keywords"></a>3.1.3 Searching for man pages by keywords</h3><p>小寫 k 僅針對 title &amp; description 搜尋關鍵字：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[vagrant@server ~]$ man -k passwd</span><br><span class="line">grub2-mkpasswd-pbkdf2 (1) - Generate a PBKDF2 password <span class="built_in">hash</span>.</span><br><span class="line">sslpasswd (1ssl)     - compute password hashes</span><br></pre></td></tr></table></figure><blockquote><p>大寫 K 會進行全文搜尋…..內容很多…</p></blockquote><p>透過 <code>mandb</code> 指令可以立即強制 man page 資料庫更新，但系統其實已經將更新資料庫的工作放在 <strong>/etc/cron.daily/man-db.cron</strong> 中。</p><h2 id="3-2-Reading-Documentation-Using-pinfo-Command"><a href="#3-2-Reading-Documentation-Using-pinfo-Command" class="headerlink" title="3.2 Reading Documentation Using pinfo Command"></a>3.2 Reading Documentation Using pinfo Command</h2><p>info 文件是以類似超連結網頁的方式進行編排，透過 pinfo 指令來啟動 <strong>lynx</strong> 文字網頁瀏覽器來瀏覽。</p><p><code>--</code> 在指令中代表 command option 的結束，表示後面接的是 command argument，例如：<code>touch -- -r</code> 會產生名稱為 <strong>-r</strong> 的檔案。</p>]]></content>
      
      
      <categories>
          
          <category> RHCE </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> RHCE </tag>
            
            <tag> RH124 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[VMware] 免費的 vSphere ESXi VM 備份方案 - XSIBACKUP</title>
      <link href="/blog/VMware/free-vsphere-esxi-vm-backup-solution-xsibackup/"/>
      <url>/blog/VMware/free-vsphere-esxi-vm-backup-solution-xsibackup/</url>
      
        <content type="html"><![CDATA[<p>最近公司在找給 VMware vSphere ESXi 用的 shared storage，想當然爾也會考慮到備份的問題</p><p>後來學長提供了 <a href="http://sourceforge.net/projects/xsibackup/">xsibackup</a> 這個 opensource 的免費軟體，雖然是免費，可是備份功能也不差呢。</p><h2 id="環境設定"><a href="#環境設定" class="headerlink" title="環境設定"></a>環境設定</h2><ul><li>vSphere ESXi <strong>5.5 Update 2</strong></li><li>esxibackup <strong>4.1.6</strong></li></ul><h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><p>1、 首先必須先開啟 ESXi Host 的 SSH servive，並透過 ssh client 登入到 esxi 中</p><p>2、 下載 xsibackup 程式並解壓縮，將 xsibackup 程式設定為可執行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要將檔案放到 ESXi 重開機後不會回復初始設定的路徑(可以是任何 DataStore 的目錄下，只要是 persistent folder 即可)</span></span><br><span class="line"><span class="comment"># 切換到 datastore1 folder，避免 ESXi 重開機之後將檔案刪除</span></span><br><span class="line">$ <span class="built_in">cd</span> /vmfs/volumes/datastore1</span><br><span class="line">$ wget http://sourceforge.net/projects/xsibackup/files/xsibackup_4.1.6.zip/download -O xsibackup.zip</span><br><span class="line">$ unzip xsibackup.zip</span><br><span class="line">$ chmod 0700 xsibackup*</span><br></pre></td></tr></table></figure><p>3、執行備份工作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 備份檔存放路徑：/vmfs/volumes/backup</span></span><br><span class="line"><span class="comment"># 備份類型：目前運行中的 VM (running)</span></span><br><span class="line"><span class="comment"># mail &amp; smpt 的相關設定都是與寄信相關</span></span><br><span class="line">$ ./xsibackup --backup-point=/vmfs/volumes/backup --backup-type=running </span><br><span class="line">--mail-from=email.sender@yourdomain.com --mail-to=email.recipient@anotherdomain.com </span><br><span class="line">--smtp-srv=smtp.yourdomain.com --smtp-port=25 --smtp-usr=username </span><br><span class="line">--smtp-pwd=password</span><br></pre></td></tr></table></figure><p><code>/vmfs/volumes/backup 目錄也可以是 remote host 所提供的 NFS share folder</code></p><p>其中 <code>--backup-type</code> 有以下三種：</p><ul><li><strong>all</strong> (所有 vm)</li><li><strong>running</strong> (執行中的 vm)</li><li><strong>custom</strong> (指定 vm，需搭配 –backup-vms 參數指令要備份的 vm，多個 vm 可用逗號隔開)</li></ul><p>custom 應用如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定備份 WINDOWSVM1 &amp; LINUXVM2 兩台 vm</span></span><br><span class="line">$ ./xsibackup --backup-point=/vmfs/volumes/backup --backup-type=custom --backup-vms=WINDOWSVM1,LINUXVM2</span><br></pre></td></tr></table></figure><h2 id="其他參數"><a href="#其他參數" class="headerlink" title="其他參數"></a>其他參數</h2><ul><li><code>--test-mode=true</code> (測試模式，不實際進行備份；但若有設定 EMail 相關參數則會發信)</li><li><code>--backup-how (hot | cold)</code> (hot 會在 vm 開機情況下備份，cold 則會將 vm 關機後再備份)`</li></ul><h2 id="寄送-Mail-的問題"><a href="#寄送-Mail-的問題" class="headerlink" title="寄送 Mail 的問題"></a>寄送 Mail 的問題</h2><p>設定了 EMail 發送相關參數後，實際執行會發現竟然不行，排除方法如下：</p><p>xsibackup 程式會在 <strong>/etc/vmware/firewall/service.xml</strong> 這個檔案補上這一段內容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">service</span> <span class="attr">id</span>=<span class="string">&#x27;9999&#x27;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>SMTPout<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">rule</span> <span class="attr">id</span>=<span class="string">&#x27;0000&#x27;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">direction</span>&gt;</span>outbound<span class="tag">&lt;/<span class="name">direction</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">protocol</span>&gt;</span>tcp<span class="tag">&lt;/<span class="name">protocol</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">porttype</span>&gt;</span>dst<span class="tag">&lt;/<span class="name">porttype</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">port</span>&gt;</span><span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">rule</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">enabled</span>&gt;</span>true<span class="tag">&lt;/<span class="name">enabled</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">required</span>&gt;</span>false<span class="tag">&lt;/<span class="name">required</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">service</span>&gt;</span></span><br></pre></td></tr></table></figure><p>但其實這是錯誤的，要把 <code>&lt;port&gt;&lt;/port&gt;</code>這個部分改成 <code>&lt;port&gt;25&lt;/port&gt;</code>，並執行以下指令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ esxcli network firewall refresh</span><br></pre></td></tr></table></figure><p>如此一來 EMail 的功能就會正常啟動了!</p><h2 id="排程備份"><a href="#排程備份" class="headerlink" title="排程備份"></a>排程備份</h2><p>xsibackup 也支援排程喔! 設定方式如下：</p><ol><li>在 ESXi 主機上執行 <code>xsibackup --install-cron</code> 指令，此時會在 <strong>/vmfs/volumes/datastore1</strong> 目錄中產生 <code>xsibackup-cron</code>這個檔案，可以直接進入編輯：(若是星期一、五晚上 20:00 要備份)</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加入 --time 參數，格式為 --time=&quot;Day HH:mm&quot;(注意這邊要用 UTC 時間)</span></span><br><span class="line"><span class="comment"># 星期一 20:00 備份</span></span><br><span class="line">/vmfs/volumes/datastore1/xsibackup --time=<span class="string">&quot;Mon 12:00&quot;</span> --backup-point=/vmfs/volumes/backup --backup-type=running --mail-from=email.sender@yourdomain.com --mail-to=email.recipient@anotherdomain.com --smtp-srv=smtp.yourdomain.com --smtp-port=25 --smtp-usr=username --smtp-pwd=password</span><br><span class="line"><span class="comment"># 星期五 20:00 備份</span></span><br><span class="line">/vmfs/volumes/datastore1/xsibackup --time=<span class="string">&quot;Fri 12:00&quot;</span> --backup-point=/vmfs/volumes/backup --backup-type=running --mail-from=email.sender@yourdomain.com --mail-to=email.recipient@anotherdomain.com --smtp-srv=smtp.yourdomain.com --smtp-port=25 --smtp-usr=username --smtp-pwd=password</span><br></pre></td></tr></table></figure><ol start="2"><li>重新啟動 ESXi Host 讓 cron 的功能啟用</li></ol><h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>因此總結一下，優缺點大致如下：</p><h3 id="優點"><a href="#優點" class="headerlink" title="優點"></a>優點</h3><ol><li><p>免費、開放</p></li><li><p>可進行完整備份，非特殊格式，不需要透過其他軟體還原</p></li><li><p>在單純的環境下使用簡單</p></li></ol><h3 id="缺點"><a href="#缺點" class="headerlink" title="缺點"></a>缺點</h3><ol><li><p>無法執行差異備份，自然也就沒有 dedupication 的功能。</p></li><li><p>目前沒有 exclude 的參數，若是有不想備份的 VM(例如：VDP)，只能透過 custom or running(搭配將 vm 關機)的方式來完成 (也可以透過改 source code 的方式來做….)</p></li><li><p>若是 vSphere 授權版本有 DRS(Dynamic Resource Scheduler) 的話，VM 可能會隨著資源耗損不同而跑來跑去，備份工作就很難透過 custom 方式來達成。</p></li></ol><h2 id="參考資料"><a href="#參考資料" class="headerlink" title="參考資料"></a>參考資料</h2><ul><li><p><a href="http://sourceforge.net/projects/xsibackup/">Free Backup Software for VMware ESXi VMs | SourceForge.net</a></p></li><li><p><a href="http://blog.depicus.com/add-outbound-port-25-for-smtp-in-vmware-esxi-v5/">Add outbound port 25 for SMTP in VMware ESXi v5 – depicus</a></p></li><li><p><a href="http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=2008226">VMware KB: Creating custom firewall rules in VMware ESXi 5.x</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> VMware </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VMware </tag>
            
            <tag> Backup </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
